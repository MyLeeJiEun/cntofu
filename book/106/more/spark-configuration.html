
<!DOCTYPE html>
<html lang="zh-CN">
<head>
<title>Spark配置-Spark 编程指南简体中文版</title>
<meta content='Spark配置,Spark 编程指南简体中文版' name='keywords'>
<meta content='Spark配置,Spark 编程指南简体中文版' name='description'>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta http-equiv="Content-Language" content="zh-CN" />
<meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1, maximum-scale=1, user-scalable=no"../../../>
<meta name="applicable-device" content="pc,mobile">
<link rel="shortcut icon" href="../../../favicon.ico" type="image/x-icon" />
<meta name="renderer" content="webkit">
<link rel="stylesheet" href="../../../static/components/uikit-2.27.5/css/uikit.custom.css">
<link rel="stylesheet" href="../../../static/components/social-share/social-share.min.css">
<link rel="stylesheet" href="../../../static/components/highlight/styles/custom.css">
<link rel="stylesheet" href="../../../static/components/css/base.css">
<link rel="stylesheet" href="../../../static/components/css/reader.css">
<link rel="stylesheet" href="../../../static/components/css/markdown.css">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5313208362165053" crossorigin="anonymous"></script>
</head>
<body>
<div class=" book-main-wrap uk-container uk-container-center uk-margin-top ">
<div class="uk-grid">
<div class="uk-width-1-1 reader-wrap ">
<div class=" bottom-nav uk-clearfix ">
<div class="uk-align-left ">
<a href="../../../book/106/deploying/running-spark-on-yarn.html">
<i class="nav-icon-left uk-icon-small  uk-icon-caret-left"></i>
<span class="">在yarn上运行Spa..</span>
</a>
</div>
<div class="uk-align-right ">
<a href="../../../book/106/programming-guide/rdds/rdd_persistence.html">
<span class="">RDD 持久化</span>
<i class="nav-icon-right uk-icon-small  uk-icon-caret-right"></i>
</a>
</div>
</div>
<div class="uk-text-center">
<h2 class="book-page-title uk-container-center">
<a href="../../../book/106/index.html">Spark 编程指南简体中文版</a>
<a target="_blank" rel="nofollow" href="https://github.com/aiyanbo/spark-programming-guide-zh-cn" class="uk-icon-button uk-icon-github" title="github项目地址"></a>
</h2>
</div>
<script type="text/javascript" src="../../../static/components/js/app_intro.js"></script>
<ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-5313208362165053" data-ad-slot="1328047120"></ins>
<script>(adsbygoogle =window.adsbygoogle ||[]).push({});</script>
<hr class="uk-article-divider">
<div class="book-content-section  md-content-section  uk-margin-bottom">
<h1 id="spark配置">Spark配置</h1>
<p>Spark提供三个位置用来配置系统：</p>
<ul>
<li>Spark properties控制大部分的应用程序参数，可以用SparkConf对象或者java系统属性设置</li>
<li>Environment variables可以通过每个节点的<code>conf/spark-env.sh</code>脚本设置每台机器的设置。例如IP地址</li>
<li>Logging可以通过log4j.properties配置</li>
</ul>
<h2 id="spark属性">Spark属性</h2>
<p>Spark属性控制大部分的应用程序设置，并且为每个应用程序分别配置它。这些属性可以直接在<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkConf">SparkConf</a>上配置，然后传递给<code>SparkContext</code>。<code>SparkConf</code> 允许你配置一些通用的属性（如master URL、应用程序明）以及通过<code>set()</code>方法设置的任意键值对。例如，我们可以用如下方式创建一个拥有两个线程的应用程序。注意，我们用<code>local[2]</code>运行，这意味着两个线程-表示最小的 并行度，它可以帮助我们检测当在分布式环境下运行的时才出现的错误。</p>
<pre><code class="language-scala">val conf = new SparkConf()
             .setMaster("local[2]")
             .setAppName("CountingSheep")
             .set("spark.executor.memory", "1g")
val sc = new SparkContext(conf)
</code></pre>
<p>注意，我们在本地模式中拥有超过1个线程。和Spark Streaming的情况一样，我们可能需要一个线程防止任何形式的饥饿问题。</p>
<h3 id="动态加载spark属性">动态加载Spark属性</h3>
<p>在一些情况下，你可能想在<code>SparkConf</code>中避免硬编码确定的配置。例如，你想用不同的master或者不同的内存数运行相同的应用程序。Spark允许你简单地创建一个空conf。</p>
<pre><code class="language-scala">val sc = new SparkContext(new SparkConf())
</code></pre>
<p>然后你在运行时提供值。</p>
<pre><code class="language-shell">./bin/spark-submit --name "My app" --master local[4] --conf spark.shuffle.spill=false
  --conf "spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps" myApp.jar
</code></pre>
<p>Spark shell和<code>spark-submit</code>工具支持两种方式动态加载配置。第一种方式是命令行选项，例如<code>--master</code>，如上面shell显示的那样。<code>spark-submit</code>可以接受任何Spark属性，用<code>--conf</code> 标记表示。但是那些参与Spark应用程序启动的属性要用特定的标记表示。运行<code>./bin/spark-submit --help</code>将会显示选项的整个列表。</p>
<p><code>bin/spark-submit</code>也会从<code>conf/spark-defaults.conf</code>中读取配置选项，这个配置文件中，每一行都包含一对以空格分开的键和值。例如：</p>
<pre><code>spark.master            spark://5.6.7.8:7077
spark.executor.memory   512m
spark.eventLog.enabled  true
spark.serializer        org.apache.spark.serializer.KryoSerializer
</code></pre>
<p>任何标签（flags）指定的值或者在配置文件中的值将会传递给应用程序，并且通过<code>SparkConf</code>合并这些值。在<code>SparkConf</code>上设置的属性具有最高的优先级，其次是传递给<code>spark-submit</code> 或者<code>spark-shell</code>的属性值，最后是<code>spark-defaults.conf</code>文件中的属性值。</p>
<h3 id="查看spark属性">查看Spark属性</h3>
<p>在<code>http://&lt;driver&gt;:4040</code>上的应用程序web UI在“Environment”标签中列出了所有的Spark属性。这对你确保设置的属性的正确性是很有用的。注意，只有通过spark-defaults.conf, SparkConf以及 命令行直接指定的值才会显示。对于其它的配置属性，你可以认为程序用到了默认的值。</p>
<h3 id="可用的属性">可用的属性</h3>
<p>控制内部设置的大部分属性都有合理的默认值，一些最通用的选项设置如下：</p>
<h4 id="应用程序属性">应用程序属性</h4>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Default</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>spark.app.name</td>
<td>(none)</td>
<td>你的应用程序的名字。这将在UI和日志数据中出现</td>
</tr>
<tr>
<td>spark.master</td>
<td>(none)</td>
<td>集群管理器连接的地方</td>
</tr>
<tr>
<td>spark.executor.memory</td>
<td>512m</td>
<td>每个executor进程使用的内存数。和JVM内存串拥有相同的格式（如512m,2g）</td>
</tr>
<tr>
<td>spark.driver.memory</td>
<td>512m</td>
<td>driver进程使用的内存数</td>
</tr>
<tr>
<td>spark.driver.maxResultSize</td>
<td>1g</td>
<td>每个Spark action(如collect)所有分区的序列化结果的总大小限制。设置的值应该不小于1m，0代表没有限制。如果总大小超过这个限制，工作将会终止。大的限制值可能导致driver出现内存溢出错误（依赖于spark.driver.memory和JVM中对象的内存消耗）。设置合理的限制，可以避免出现内存溢出错误。</td>
</tr>
<tr>
<td>spark.serializer</td>
<td>org.apache.spark.serializer.JavaSerializer</td>
<td>序列化对象使用的类。默认的java序列化类可以序列化任何可序列化的java对象但是它很慢。所有我们建议用<a href="http://spark.apache.org/docs/latest/tuning.html">org.apache.spark.serializer.KryoSerializer</a></td>
</tr>
<tr>
<td>spark.kryo.classesToRegister</td>
<td>(none)</td>
<td>如果你用Kryo序列化，给定的用逗号分隔的自定义类名列表表示要注册的类</td>
</tr>
<tr>
<td>spark.kryo.registrator</td>
<td>(none)</td>
<td>如果你用Kryo序列化，设置这个类去注册你的自定义类。如果你需要用自定义的方式注册你的类，那么这个属性是有用的。否则<code>spark.kryo.classesToRegister</code>会更简单。它应该设置一个继承自<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.serializer.KryoRegistrator">KryoRegistrator</a>的类</td>
</tr>
<tr>
<td>spark.local.dir</td>
<td>/tmp</td>
<td>Spark中暂存空间的使用目录。在Spark1.0以及更高的版本中，这个属性被SPARK_LOCAL_DIRS(Standalone, Mesos)和LOCAL_DIRS(YARN)环境变量覆盖。</td>
</tr>
<tr>
<td>spark.logConf</td>
<td>false</td>
<td>当SparkContext启动时，将有效的SparkConf记录为INFO。</td>
</tr>
</tbody>
</table>
<h4 id="运行环境">运行环境</h4>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Default</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>spark.executor.extraJavaOptions</td>
<td>(none)</td>
<td>传递给executors的JVM选项字符串。例如GC设置或者其它日志设置。注意，在这个选项中设置Spark属性或者堆大小是不合法的。Spark属性需要用SparkConf对象或者<code>spark-submit</code>脚本用到的<code>spark-defaults.conf</code>文件设置。堆内存可以通过<code>spark.executor.memory</code>设置</td>
</tr>
<tr>
<td>spark.executor.extraClassPath</td>
<td>(none)</td>
<td>附加到executors的classpath的额外的classpath实体。这个设置存在的主要目的是Spark与旧版本的向后兼容问题。用户一般不用设置这个选项</td>
</tr>
<tr>
<td>spark.executor.extraLibraryPath</td>
<td>(none)</td>
<td>指定启动executor的JVM时用到的库路径</td>
</tr>
<tr>
<td>spark.executor.logs.rolling.strategy</td>
<td>(none)</td>
<td>设置executor日志的滚动(rolling)策略。默认情况下没有开启。可以配置为<code>time</code>（基于时间的滚动）和<code>size</code>(基于大小的滚动)。对于<code>time</code>，用<code>spark.executor.logs.rolling.time.interval</code>设置滚动间隔；对于<code>size</code>，用<code>spark.executor.logs.rolling.size.maxBytes</code>设置最大的滚动大小</td>
</tr>
<tr>
<td>spark.executor.logs.rolling.time.interval</td>
<td>daily</td>
<td>executor日志滚动的时间间隔。默认情况下没有开启。合法的值是<code>daily</code>, <code>hourly</code>, <code>minutely</code>以及任意的秒。</td>
</tr>
<tr>
<td>spark.executor.logs.rolling.size.maxBytes</td>
<td>(none)</td>
<td>executor日志的最大滚动大小。默认情况下没有开启。值设置为字节</td>
</tr>
<tr>
<td>spark.executor.logs.rolling.maxRetainedFiles</td>
<td>(none)</td>
<td>设置被系统保留的最近滚动日志文件的数量。更老的日志文件将被删除。默认没有开启。</td>
</tr>
<tr>
<td>spark.files.userClassPathFirst</td>
<td>false</td>
<td>(实验性)当在Executors中加载类时，是否用户添加的jar比Spark自己的jar优先级高。这个属性可以降低Spark依赖和用户依赖的冲突。它现在还是一个实验性的特征。</td>
</tr>
<tr>
<td>spark.python.worker.memory</td>
<td>512m</td>
<td>在聚合期间，每个python worker进程使用的内存数。在聚合期间，如果内存超过了这个限制，它将会将数据塞进磁盘中</td>
</tr>
<tr>
<td>spark.python.profile</td>
<td>false</td>
<td>在Python worker中开启profiling。通过<code>sc.show_profiles()</code>展示分析结果。或者在driver退出前展示分析结果。可以通过<code>sc.dump_profiles(path)</code>将结果dump到磁盘中。如果一些分析结果已经手动展示，那么在driver退出前，它们再不会自动展示</td>
</tr>
<tr>
<td>spark.python.profile.dump</td>
<td>(none)</td>
<td>driver退出前保存分析结果的dump文件的目录。每个RDD都会分别dump一个文件。可以通过<code>ptats.Stats()</code>加载这些文件。如果指定了这个属性，分析结果不会自动展示</td>
</tr>
<tr>
<td>spark.python.worker.reuse</td>
<td>true</td>
<td>是否重用python worker。如果是，它将使用固定数量的Python workers，而不需要为每个任务fork()一个Python进程。如果有一个非常大的广播，这个设置将非常有用。因为，广播不需要为每个任务从JVM到Python worker传递一次</td>
</tr>
<tr>
<td>spark.executorEnv.[EnvironmentVariableName]</td>
<td>(none)</td>
<td>通过<code>EnvironmentVariableName</code>添加指定的环境变量到executor进程。用户可以指定多个<code>EnvironmentVariableName</code>，设置多个环境变量</td>
</tr>
<tr>
<td>spark.mesos.executor.home</td>
<td>driver side SPARK_HOME</td>
<td>设置安装在Mesos的executor上的Spark的目录。默认情况下，executors将使用driver的Spark本地（home）目录，这个目录对它们不可见。注意，如果没有通过<code>spark.executor.uri</code>指定Spark的二进制包，这个设置才起作用</td>
</tr>
<tr>
<td>spark.mesos.executor.memoryOverhead</td>
<td>executor memory * 0.07, 最小384m</td>
<td>这个值是<code>spark.executor.memory</code>的补充。它用来计算mesos任务的总内存。另外，有一个7%的硬编码设置。最后的值将选择<code>spark.mesos.executor.memoryOverhead</code>或者<code>spark.executor.memory</code>的7%二者之间的大者</td>
</tr>
</tbody>
</table>
<h4 id="shuffle行为behavior">Shuffle行为(Behavior)</h4>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Default</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>spark.shuffle.consolidateFiles</td>
<td>false</td>
<td>如果设置为"true"，在shuffle期间，合并的中间文件将会被创建。创建更少的文件可以提供文件系统的shuffle的效率。这些shuffle都伴随着大量递归任务。当用ext4和dfs文件系统时，推荐设置为"true"。在ext3中，因为文件系统的限制，这个选项可能机器（大于8核）降低效率</td>
</tr>
<tr>
<td>spark.shuffle.spill</td>
<td>true</td>
<td>如果设置为"true"，通过将多出的数据写入磁盘来限制内存数。通过<code>spark.shuffle.memoryFraction</code>来指定spilling的阈值</td>
</tr>
<tr>
<td>spark.shuffle.spill.compress</td>
<td>true</td>
<td>在shuffle时，是否将spilling的数据压缩。压缩算法通过<code>spark.io.compression.codec</code>指定。</td>
</tr>
<tr>
<td>spark.shuffle.memoryFraction</td>
<td>0.2</td>
<td>如果<code>spark.shuffle.spill</code>为“true”，shuffle中聚合和合并组操作使用的java堆内存占总内存的比重。在任何时候，shuffles使用的所有内存内maps的集合大小都受这个限制的约束。超过这个限制，spilling数据将会保存到磁盘上。如果spilling太过频繁，考虑增大这个值</td>
</tr>
<tr>
<td>spark.shuffle.compress</td>
<td>true</td>
<td>是否压缩map操作的输出文件。一般情况下，这是一个好的选择。</td>
</tr>
<tr>
<td>spark.shuffle.file.buffer.kb</td>
<td>32</td>
<td>每个shuffle文件输出流内存内缓存的大小，单位是kb。这个缓存减少了创建只中间shuffle文件中磁盘搜索和系统访问的数量</td>
</tr>
<tr>
<td>spark.reducer.maxMbInFlight</td>
<td>48</td>
<td>从递归任务中同时获取的map输出数据的最大大小（mb）。因为每一个输出都需要我们创建一个缓存用来接收，这个设置代表每个任务固定的内存上限，所以除非你有更大的内存，将其设置小一点</td>
</tr>
<tr>
<td>spark.shuffle.manager</td>
<td>sort</td>
<td>它的实现用于shuffle数据。有两种可用的实现：<code>sort</code>和<code>hash</code>。基于sort的shuffle有更高的内存使用率</td>
</tr>
<tr>
<td>spark.shuffle.sort.bypassMergeThreshold</td>
<td>200</td>
<td>(Advanced) In the sort-based shuffle manager, avoid merge-sorting data if there is no map-side aggregation and there are at most this many reduce partitions</td>
</tr>
<tr>
<td>spark.shuffle.blockTransferService</td>
<td>netty</td>
<td>实现用来在executor直接传递shuffle和缓存块。有两种可用的实现：<code>netty</code>和<code>nio</code>。基于netty的块传递在具有相同的效率情况下更简单</td>
</tr>
</tbody>
</table>
<h4 id="spark-ui">Spark UI</h4>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Default</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>spark.ui.port</td>
<td>4040</td>
<td>你的应用程序dashboard的端口。显示内存和工作量数据</td>
</tr>
<tr>
<td>spark.ui.retainedStages</td>
<td>1000</td>
<td>在垃圾回收之前，Spark UI和状态API记住的stage数</td>
</tr>
<tr>
<td>spark.ui.retainedJobs</td>
<td>1000</td>
<td>在垃圾回收之前，Spark UI和状态API记住的job数</td>
</tr>
<tr>
<td>spark.ui.killEnabled</td>
<td>true</td>
<td>运行在web UI中杀死stage和相应的job</td>
</tr>
<tr>
<td>spark.eventLog.enabled</td>
<td>false</td>
<td>是否记录Spark的事件日志。这在应用程序完成后，重新构造web UI是有用的</td>
</tr>
<tr>
<td>spark.eventLog.compress</td>
<td>false</td>
<td>是否压缩事件日志。需要<code>spark.eventLog.enabled</code>为true</td>
</tr>
<tr>
<td>spark.eventLog.dir</td>
<td>file:///tmp/spark-events</td>
<td>Spark事件日志记录的基本目录。在这个基本目录下，Spark为每个应用程序创建一个子目录。各个应用程序记录日志到直到的目录。用户可能想设置这为统一的地点，像HDFS一样，所以历史文件可以通过历史服务器读取</td>
</tr>
</tbody>
</table>
<h4 id="压缩和序列化">压缩和序列化</h4>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Default</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>spark.broadcast.compress</td>
<td>true</td>
<td>在发送广播变量之前是否压缩它</td>
</tr>
<tr>
<td>spark.rdd.compress</td>
<td>true</td>
<td>是否压缩序列化的RDD分区。在花费一些额外的CPU时间的同时节省大量的空间</td>
</tr>
<tr>
<td>spark.io.compression.codec</td>
<td>snappy</td>
<td>压缩诸如RDD分区、广播变量、shuffle输出等内部数据的编码解码器。默认情况下，Spark提供了三种选择：lz4, lzf和snappy。你也可以用完整的类名来制定。<code>org.apache.spark.io.LZ4CompressionCodec</code>，<code>org.apache.spark.io.LZFCompressionCodec</code>，<code>org.apache.spark.io.SnappyCompressionCodec</code></td>
</tr>
<tr>
<td>spark.io.compression.snappy.block.size</td>
<td>32768</td>
<td>Snappy压缩中用到的块大小。降低这个块的大小也会降低shuffle内存使用率</td>
</tr>
<tr>
<td>spark.io.compression.lz4.block.size</td>
<td>32768</td>
<td>LZ4压缩中用到的块大小。降低这个块的大小也会降低shuffle内存使用率</td>
</tr>
<tr>
<td>spark.closure.serializer</td>
<td>org.apache.spark.serializer.JavaSerializer</td>
<td>闭包用到的序列化类。目前只支持java序列化器</td>
</tr>
<tr>
<td>spark.serializer.objectStreamReset</td>
<td>100</td>
<td>当用<code>org.apache.spark.serializer.JavaSerializer</code>序列化时，序列化器通过缓存对象防止写多余的数据，然而这会造成这些对象的垃圾回收停止。通过请求'reset',你从序列化器中flush这些信息并允许收集老的数据。为了关闭这个周期性的reset，你可以将值设为-1。默认情况下，每一百个对象reset一次</td>
</tr>
<tr>
<td>spark.kryo.referenceTracking</td>
<td>true</td>
<td>当用Kryo序列化时，跟踪是否引用同一对象。如果你的对象图有环，这是必须的设置。如果他们包含相同对象的多个副本，这个设置对效率是有用的。如果你知道不在这两个场景，那么可以禁用它以提高效率</td>
</tr>
<tr>
<td>spark.kryo.registrationRequired</td>
<td>false</td>
<td>是否需要注册为Kyro可用。如果设置为true，然后如果一个没有注册的类序列化，Kyro会抛出异常。如果设置为false，Kryo将会同时写每个对象和其非注册类名。写类名可能造成显著地性能瓶颈。</td>
</tr>
<tr>
<td>spark.kryoserializer.buffer.mb</td>
<td>0.064</td>
<td>Kyro序列化缓存的大小。这样worker上的每个核都有一个缓存。如果有需要，缓存会涨到<code>spark.kryoserializer.buffer.max.mb</code>设置的值那么大。</td>
</tr>
<tr>
<td>spark.kryoserializer.buffer.max.mb</td>
<td>64</td>
<td>Kryo序列化缓存允许的最大值。这个值必须大于你尝试序列化的对象</td>
</tr>
</tbody>
</table>
<h4 id="networking">Networking</h4>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Default</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>spark.driver.host</td>
<td>(local hostname)</td>
<td>driver监听的主机名或者IP地址。这用于和executors以及独立的master通信</td>
</tr>
<tr>
<td>spark.driver.port</td>
<td>(random)</td>
<td>driver监听的接口。这用于和executors以及独立的master通信</td>
</tr>
<tr>
<td>spark.fileserver.port</td>
<td>(random)</td>
<td>driver的文件服务器监听的端口</td>
</tr>
<tr>
<td>spark.broadcast.port</td>
<td>(random)</td>
<td>driver的HTTP广播服务器监听的端口</td>
</tr>
<tr>
<td>spark.replClassServer.port</td>
<td>(random)</td>
<td>driver的HTTP类服务器监听的端口</td>
</tr>
<tr>
<td>spark.blockManager.port</td>
<td>(random)</td>
<td>块管理器监听的端口。这些同时存在于driver和executors</td>
</tr>
<tr>
<td>spark.executor.port</td>
<td>(random)</td>
<td>executor监听的端口。用于与driver通信</td>
</tr>
<tr>
<td>spark.port.maxRetries</td>
<td>16</td>
<td>当绑定到一个端口，在放弃前重试的最大次数</td>
</tr>
<tr>
<td>spark.akka.frameSize</td>
<td>10</td>
<td>在"control plane"通信中允许的最大消息大小。如果你的任务需要发送大的结果到driver中，调大这个值</td>
</tr>
<tr>
<td>spark.akka.threads</td>
<td>4</td>
<td>通信的actor线程数。当driver有很多CPU核时，调大它是有用的</td>
</tr>
<tr>
<td>spark.akka.timeout</td>
<td>100</td>
<td>Spark节点之间的通信超时。单位是s</td>
</tr>
<tr>
<td>spark.akka.heartbeat.pauses</td>
<td>6000</td>
<td>This is set to a larger value to disable failure detector that comes inbuilt akka. It can be enabled again, if you plan to use this feature (Not recommended). Acceptable heart beat pause in seconds for akka. This can be used to control sensitivity to gc pauses. Tune this in combination of <code>spark.akka.heartbeat.interval</code> and <code>spark.akka.failure-detector.threshold</code> if you need to.</td>
</tr>
<tr>
<td>spark.akka.failure-detector.threshold</td>
<td>300.0</td>
<td>This is set to a larger value to disable failure detector that comes inbuilt akka. It can be enabled again, if you plan to use this feature (Not recommended). This maps to akka's <code>akka.remote.transport-failure-detector.threshold</code>. Tune this in combination of <code>spark.akka.heartbeat.pauses</code> and <code>spark.akka.heartbeat.interval</code> if you need to.</td>
</tr>
<tr>
<td>spark.akka.heartbeat.interval</td>
<td>1000</td>
<td>This is set to a larger value to disable failure detector that comes inbuilt akka. It can be enabled again, if you plan to use this feature (Not recommended). A larger interval value in seconds reduces network overhead and a smaller value ( ~ 1 s) might be more informative for akka's failure detector. Tune this in combination of <code>spark.akka.heartbeat.pauses</code> and <code>spark.akka.failure-detector.threshold</code> if you need to. Only positive use case for using failure detector can be, a sensistive failure detector can help evict rogue executors really quick. However this is usually not the case as gc pauses and network lags are expected in a real Spark cluster. Apart from that enabling this leads to a lot of exchanges of heart beats between nodes leading to flooding the network with those.</td>
</tr>
</tbody>
</table>
<h4 id="security">Security</h4>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Default</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>spark.authenticate</td>
<td>false</td>
<td>是否Spark验证其内部连接。如果不是运行在YARN上，请看<code>spark.authenticate.secret</code></td>
</tr>
<tr>
<td>spark.authenticate.secret</td>
<td>None</td>
<td>设置Spark两个组件之间的密匙验证。如果不是运行在YARN上，但是需要验证，这个选项必须设置</td>
</tr>
<tr>
<td>spark.core.connection.auth.wait.timeout</td>
<td>30</td>
<td>连接时等待验证的实际。单位为秒</td>
</tr>
<tr>
<td>spark.core.connection.ack.wait.timeout</td>
<td>60</td>
<td>连接等待回答的时间。单位为秒。为了避免不希望的超时，你可以设置更大的值</td>
</tr>
<tr>
<td>spark.ui.filters</td>
<td>None</td>
<td>应用到Spark web UI的用于过滤类名的逗号分隔的列表。过滤器必须是标准的<a href="http://docs.oracle.com/javaee/6/api/javax/servlet/Filter.html">javax servlet Filter</a>。通过设置java系统属性也可以指定每个过滤器的参数。<code>spark.&lt;class name of filter&gt;.params='param1=value1,param2=value2'</code>。例如<code>-Dspark.ui.filters=com.test.filter1</code>、<code>-Dspark.com.test.filter1.params='param1=foo,param2=testing'</code></td>
</tr>
<tr>
<td>spark.acls.enable</td>
<td>false</td>
<td>是否开启Spark acls。如果开启了，它检查用户是否有权限去查看或修改job。 Note this requires the user to be known, so if the user comes across as null no checks are done。UI利用使用过滤器验证和设置用户</td>
</tr>
<tr>
<td>spark.ui.view.acls</td>
<td>empty</td>
<td>逗号分隔的用户列表，列表中的用户有查看(view)Spark web UI的权限。默认情况下，只有启动Spark job的用户有查看权限</td>
</tr>
<tr>
<td>spark.modify.acls</td>
<td>empty</td>
<td>逗号分隔的用户列表，列表中的用户有修改Spark job的权限。默认情况下，只有启动Spark job的用户有修改权限</td>
</tr>
<tr>
<td>spark.admin.acls</td>
<td>empty</td>
<td>逗号分隔的用户或者管理员列表，列表中的用户或管理员有查看和修改所有Spark job的权限。如果你运行在一个共享集群，有一组管理员或开发者帮助debug，这个选项有用</td>
</tr>
</tbody>
</table>
<h4 id="spark-streaming">Spark Streaming</h4>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Default</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>spark.streaming.blockInterval</td>
<td>200</td>
<td>在这个时间间隔（ms）内，通过Spark Streaming receivers接收的数据在保存到Spark之前，chunk为数据块。推荐的最小值为50ms</td>
</tr>
<tr>
<td>spark.streaming.receiver.maxRate</td>
<td>infinite</td>
<td>每秒钟每个receiver将接收的数据的最大记录数。有效的情况下，每个流将消耗至少这个数目的记录。设置这个配置为0或者-1将会不作限制</td>
</tr>
<tr>
<td>spark.streaming.receiver.writeAheadLogs.enable</td>
<td>false</td>
<td>Enable write ahead logs for receivers. All the input data received through receivers will be saved to write ahead logs that will allow it to be recovered after driver failures</td>
</tr>
<tr>
<td>spark.streaming.unpersist</td>
<td>true</td>
<td>强制通过Spark Streaming生成并持久化的RDD自动从Spark内存中非持久化。通过Spark Streaming接收的原始输入数据也将清除。设置这个属性为false允许流应用程序访问原始数据和持久化RDD，因为它们没有被自动清除。但是它会造成更高的内存花费</td>
</tr>
</tbody>
</table>
<h2 id="环境变量">环境变量</h2>
<p>通过环境变量配置确定的Spark设置。环境变量从Spark安装目录下的<code>conf/spark-env.sh</code>脚本读取（或者windows的<code>conf/spark-env.cmd</code>）。在独立的或者Mesos模式下，这个文件可以给机器 确定的信息，如主机名。当运行本地应用程序或者提交脚本时，它也起作用。</p>
<p>注意，当Spark安装时，<code>conf/spark-env.sh</code>默认是不存在的。你可以复制<code>conf/spark-env.sh.template</code>创建它。</p>
<p>可以在<code>spark-env.sh</code>中设置如下变量：</p>
<table>
<thead>
<tr>
<th>Environment Variable</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>JAVA_HOME</td>
<td>java安装的路径</td>
</tr>
<tr>
<td>PYSPARK_PYTHON</td>
<td>PySpark用到的Python二进制执行文件路径</td>
</tr>
<tr>
<td>SPARK_LOCAL_IP</td>
<td>机器绑定的IP地址</td>
</tr>
<tr>
<td>SPARK_PUBLIC_DNS</td>
<td>你Spark应用程序通知给其他机器的主机名</td>
</tr>
</tbody>
</table>
<p>除了以上这些，Spark <a href="http://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts">standalone cluster scripts</a>也可以设置一些选项。例如 每台机器使用的核数以及最大内存。</p>
<p>因为<code>spark-env.sh</code>是shell脚本，其中的一些可以以编程方式设置。例如，你可以通过特定的网络接口计算<code>SPARK_LOCAL_IP</code>。</p>
<h2 id="配置logging">配置Logging</h2>
<p>Spark用<a href="http://logging.apache.org/log4j/">log4j</a> logging。你可以通过在conf目录下添加<code>log4j.properties</code>文件来配置。一种方法是复制<code>log4j.properties.template</code>文件。</p>
</div>
<hr class="uk-article-divider">
<div class="uk-block uk-block-muted uk-padding-top-remove uk-padding-bottom-remove uk-margin-large-top  book-recommend-wrap">
<div class="uk-margin-top uk-margin-bottom uk-margin-left uk-margin-right">
<div class="uk-margin uk-text-muted "><i class="uk-icon-outdent uk-icon-justify uk-margin-small-right"></i>书籍推荐</div>
<div class="books">
<ul class="uk-book-list">
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/122/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/spark_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/122/index.html">Databricks Spark 知识库简体中文版</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/62.html">tzivanmoe</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="spark">spark</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">15页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年7月1日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 1个">1</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/120/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/spark_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/120/index.html">Openstack用户指南（简体中文版）</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/62.html">tzivanmoe</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="spark">spark</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">47页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年7月1日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 0个">0</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/156/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/156/index.html">pyspider中文文档</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/88.html">aaronhua123</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">18页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月12日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 1个">1</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/186/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/code_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/186/index.html">leetcode题解，记录自己的leetcode解题之路</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/105.html">azl397985856</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="code">code</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">92页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月26日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 8204个">8204</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/139/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/docker_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/139/index.html">Docker — 从入门到实践</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/48.html">yeasy</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="docker">docker</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">159页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年9月5日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 9408个">9408</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/131/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/visualstudio_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/131/index.html">Office 365 开发入门指南</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/70.html">chenxizhang</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="visualstudio">visualstudio</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">51页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年8月3日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 98个">98</span>
</div>
</div>
</div>
</li>
<hr>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
<nav class="tm-navbar uk-navbar uk-navbar-attached reader-nav">
<div class="uk-float-left uk-margin-small-top">
<a href="javascript:;" title="目录菜单" class="show-menu  uk-icon-hover  uk-icon-align-justify uk-margin-right"></a>
<div data-uk-dropdown="{mode:'click',pos:'bottom-left'}" class="font-setting-wrap">
<a class="uk-icon-hover uk-icon-font uk-margin-right" aria-label="字体设置" href="javascript:;"></a>
<div class="uk-dropdown dropdown-menu">
<div class="dropdown-caret"><span class="caret-outer"></span><span class="caret-inner"></span></div>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-reduce">小字</button>
<button class="uk-button-link button size-2 font-enlarge">大字</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-1 ">宋体</button>
<button class="uk-button-link button size-2 font-2 ">黑体</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-3 color-theme-sun "><i class="uk-icon-sun-o"></i>白天</button>
<button class="uk-button-link button size-3 color-theme-eye "><i class="uk-icon-eye"></i>护眼</button>
<button class="uk-button-link button size-3 color-theme-moon "><i class="uk-icon-moon-o"></i>夜晚</button></div>
</div>
</div>
<a class="logo uk-margin-right" href="../../../" title="返回首页"><img class="" src="../../../static/components/images/icon_32.png" /></a>
</div>
<div class="uk-navbar-flip  uk-hidden-small">
<div id="share-box"></div>
</div>
</nav>
<div id="menu-id" class="uk-offcanvas reader-offcanvas">
<div class="uk-offcanvas-bar">
<ul class="book-menu-bar uk-nav uk-nav-offcanvas" data-uk-nav>
<li>
<a href="../../../book/106/index.html" data-book-page-rel-url="index.html" data-book-page-id="0" title="封面">封面</a>
</li>
<li>
<a class="pjax" href="../../../book/106/readme.html" data-book-page-rel-url="readme.html" data-book-page-id="0" title="简介">简介</a>
</li>
<li>
<a class="pjax" href="../../../book/106/README.html" title="Introduction" data-book-page-rel-url="README.html" data-book-page-id="7668">Introduction</a>
</li>
<li>
<a class="pjax" href="../../../book/106/quick-start/README.html" title="快速上手" data-book-page-rel-url="quick-start/README.html" data-book-page-id="7669">快速上手</a>
<ul>
<li>
<a class="pjax" href="../../../book/106/quick-start/using-spark-shell.html" title="Spark Shell" data-book-page-rel-url="quick-start/using-spark-shell.html" data-book-page-id="7670">Spark Shell</a>
</li>
<li>
<a class="pjax" href="../../../book/106/quick-start/standalone-applications.html" title="独立应用程序" data-book-page-rel-url="quick-start/standalone-applications.html" data-book-page-id="7671">独立应用程序</a>
</li>
<li>
<a class="pjax" href="../../../book/106/quick-start/where-to-go-from-here.html" title="开始翻滚吧!" data-book-page-rel-url="quick-start/where-to-go-from-here.html" data-book-page-id="7672">开始翻滚吧!</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/106/programming-guide/README.html" title="编程指南" data-book-page-rel-url="programming-guide/README.html" data-book-page-id="7673">编程指南</a>
<ul>
<li>
<a class="pjax" href="../../../book/106/programming-guide/linking-with-spark.html" title="引入 Spark" data-book-page-rel-url="programming-guide/linking-with-spark.html" data-book-page-id="7674">引入 Spark</a>
</li>
<li>
<a class="pjax" href="../../../book/106/programming-guide/initializing-spark.html" title="初始化 Spark" data-book-page-rel-url="programming-guide/initializing-spark.html" data-book-page-id="7675">初始化 Spark</a>
</li>
<li>
<a class="pjax" href="../../../book/106/programming-guide/rdds/README.html" title="Spark RDDs" data-book-page-rel-url="programming-guide/rdds/README.html" data-book-page-id="7676">Spark RDDs</a>
<ul>
<li>
<a class="pjax" href="../../../book/106/programming-guide/rdds/parallelized-collections.html" title="并行集合" data-book-page-rel-url="programming-guide/rdds/parallelized-collections.html" data-book-page-id="7677">并行集合</a>
</li>
<li>
<a class="pjax" href="../../../book/106/programming-guide/rdds/external-datasets.html" title="外部数据集" data-book-page-rel-url="programming-guide/rdds/external-datasets.html" data-book-page-id="7678">外部数据集</a>
</li>
<li>
<a class="pjax" href="../../../book/106/programming-guide/rdds/rdd-operations.html" title="RDD 操作" data-book-page-rel-url="programming-guide/rdds/rdd-operations.html" data-book-page-id="7679">RDD 操作</a>
<ul>
<li>
<a class="pjax" href="../../../book/106/programming-guide/rdds/passing-functions-to-spark.html" title="传递函数到 Spark" data-book-page-rel-url="programming-guide/rdds/passing-functions-to-spark.html" data-book-page-id="7680">传递函数到 Spark</a>
</li>
<li>
<a class="pjax" href="../../../book/106/programming-guide/rdds/working-with-key-value-pairs.html" title="使用键值对" data-book-page-rel-url="programming-guide/rdds/working-with-key-value-pairs.html" data-book-page-id="7681">使用键值对</a>
</li>
<li>
<a class="pjax" href="../../../book/106/programming-guide/rdds/transformations.html" title="Transformations" data-book-page-rel-url="programming-guide/rdds/transformations.html" data-book-page-id="7682">Transformations</a>
</li>
<li>
<a class="pjax" href="../../../book/106/programming-guide/rdds/actions.html" title="Actions" data-book-page-rel-url="programming-guide/rdds/actions.html" data-book-page-id="7683">Actions</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/106/programming-guide/rdds/rdd-persistences.html" title="RDD持久化" data-book-page-rel-url="programming-guide/rdds/rdd-persistences.html" data-book-page-id="7684">RDD持久化</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/106/programming-guide/shared-variables.html" title="共享变量" data-book-page-rel-url="programming-guide/shared-variables.html" data-book-page-id="7685">共享变量</a>
</li>
<li>
<a class="pjax" href="../../../book/106/programming-guide/from-here.html" title="从这里开始" data-book-page-rel-url="programming-guide/from-here.html" data-book-page-id="7686">从这里开始</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/106/spark-streaming/README.html" title="Spark Streaming" data-book-page-rel-url="spark-streaming/README.html" data-book-page-id="7687">Spark Streaming</a>
<ul>
<li>
<a class="pjax" href="../../../book/106/spark-streaming/a-quick-example.html" title="一个快速的例子" data-book-page-rel-url="spark-streaming/a-quick-example.html" data-book-page-id="7688">一个快速的例子</a>
</li>
<li>
<a class="pjax" href="../../../book/106/spark-streaming/basic-concepts/README.html" title="基本概念" data-book-page-rel-url="spark-streaming/basic-concepts/README.html" data-book-page-id="7689">基本概念</a>
<ul>
<li>
<a class="pjax" href="../../../book/106/spark-streaming/basic-concepts/linking.html" title="关联" data-book-page-rel-url="spark-streaming/basic-concepts/linking.html" data-book-page-id="7690">关联</a>
</li>
<li>
<a class="pjax" href="../../../book/106/spark-streaming/basic-concepts/initializing-StreamingContext.html" title="初始化StreamingContext" data-book-page-rel-url="spark-streaming/basic-concepts/initializing-StreamingContext.html" data-book-page-id="7691">初始化StreamingContext</a>
</li>
<li>
<a class="pjax" href="../../../book/106/spark-streaming/basic-concepts/discretized-streams.html" title="离散流" data-book-page-rel-url="spark-streaming/basic-concepts/discretized-streams.html" data-book-page-id="7692">离散流</a>
</li>
<li>
<a class="pjax" href="../../../book/106/spark-streaming/basic-concepts/input-DStreams.html" title="输入DStreams" data-book-page-rel-url="spark-streaming/basic-concepts/input-DStreams.html" data-book-page-id="7693">输入DStreams</a>
</li>
<li>
<a class="pjax" href="../../../book/106/spark-streaming/basic-concepts/transformations-on-DStreams.html" title="DStream中的转换" data-book-page-rel-url="spark-streaming/basic-concepts/transformations-on-DStreams.html" data-book-page-id="7694">DStream中的转换</a>
</li>
<li>
<a class="pjax" href="../../../book/106/spark-streaming/basic-concepts/output-operations-on-DStreams.html" title="DStream的输出操作" data-book-page-rel-url="spark-streaming/basic-concepts/output-operations-on-DStreams.html" data-book-page-id="7695">DStream的输出操作</a>
</li>
<li>
<a class="pjax" href="../../../book/106/spark-streaming/basic-concepts/caching-persistence.html" title="缓存或持久化" data-book-page-rel-url="spark-streaming/basic-concepts/caching-persistence.html" data-book-page-id="7696">缓存或持久化</a>
</li>
<li>
<a class="pjax" href="../../../book/106/spark-streaming/basic-concepts/checkpointing.html" title="Checkpointing" data-book-page-rel-url="spark-streaming/basic-concepts/checkpointing.html" data-book-page-id="7697">Checkpointing</a>
</li>
<li>
<a class="pjax" href="../../../book/106/spark-streaming/basic-concepts/deploying-applications.html" title="部署应用程序" data-book-page-rel-url="spark-streaming/basic-concepts/deploying-applications.html" data-book-page-id="7698">部署应用程序</a>
</li>
<li>
<a class="pjax" href="../../../book/106/spark-streaming/basic-concepts/monitoring-applications.html" title="监控应用程序" data-book-page-rel-url="spark-streaming/basic-concepts/monitoring-applications.html" data-book-page-id="7699">监控应用程序</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/106/spark-streaming/performance-tuning/README.html" title="性能调优" data-book-page-rel-url="spark-streaming/performance-tuning/README.html" data-book-page-id="7700">性能调优</a>
<ul>
<li>
<a class="pjax" href="../../../book/106/spark-streaming/performance-tuning/reducing-processing-time.html" title="减少批数据的执行时间" data-book-page-rel-url="spark-streaming/performance-tuning/reducing-processing-time.html" data-book-page-id="7701">减少批数据的执行时间</a>
</li>
<li>
<a class="pjax" href="../../../book/106/spark-streaming/performance-tuning/setting-right-batch-size.html" title="设置正确的批容量" data-book-page-rel-url="spark-streaming/performance-tuning/setting-right-batch-size.html" data-book-page-id="7702">设置正确的批容量</a>
</li>
<li>
<a class="pjax" href="../../../book/106/spark-streaming/performance-tuning/memory-tuning.html" title="内存调优" data-book-page-rel-url="spark-streaming/performance-tuning/memory-tuning.html" data-book-page-id="7703">内存调优</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/106/spark-streaming/fault-tolerance-semantics/README.html" title="容错语义" data-book-page-rel-url="spark-streaming/fault-tolerance-semantics/README.html" data-book-page-id="7704">容错语义</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/106/spark-sql/README.html" title="Spark SQL" data-book-page-rel-url="spark-sql/README.html" data-book-page-id="7705">Spark SQL</a>
<ul>
<li>
<a class="pjax" href="../../../book/106/spark-sql/getting-started.html" title="开始" data-book-page-rel-url="spark-sql/getting-started.html" data-book-page-id="7706">开始</a>
</li>
<li>
<a class="pjax" href="../../../book/106/spark-sql/data-sources/README.html" title="数据源" data-book-page-rel-url="spark-sql/data-sources/README.html" data-book-page-id="7707">数据源</a>
<ul>
<li>
<a class="pjax" href="../../../book/106/spark-sql/data-sources/rdds.html" title="RDDs" data-book-page-rel-url="spark-sql/data-sources/rdds.html" data-book-page-id="7708">RDDs</a>
</li>
<li>
<a class="pjax" href="../../../book/106/spark-sql/data-sources/parquet-files.html" title="parquet文件" data-book-page-rel-url="spark-sql/data-sources/parquet-files.html" data-book-page-id="7709">parquet文件</a>
</li>
<li>
<a class="pjax" href="../../../book/106/spark-sql/data-sources/jSON-datasets.html" title="JSON数据集" data-book-page-rel-url="spark-sql/data-sources/jSON-datasets.html" data-book-page-id="7710">JSON数据集</a>
</li>
<li>
<a class="pjax" href="../../../book/106/spark-sql/data-sources/hive-tables.html" title="Hive表" data-book-page-rel-url="spark-sql/data-sources/hive-tables.html" data-book-page-id="7711">Hive表</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/106/spark-sql/performance-tuning.html" title="性能调优" data-book-page-rel-url="spark-sql/performance-tuning.html" data-book-page-id="7712">性能调优</a>
</li>
<li>
<a class="pjax" href="../../../book/106/spark-sql/other-sql-interfaces.html" title="其它SQL接口" data-book-page-rel-url="spark-sql/other-sql-interfaces.html" data-book-page-id="7713">其它SQL接口</a>
</li>
<li>
<a class="pjax" href="../../../book/106/spark-sql/writing-language-integrated-relational-queries.html" title="编写语言集成(Language-Integrated)的相关查询" data-book-page-rel-url="spark-sql/writing-language-integrated-relational-queries.html" data-book-page-id="7714">编写语言集成(Language-Integrated)的相关查询</a>
</li>
<li>
<a class="pjax" href="../../../book/106/spark-sql/spark-sql-dataType-reference.html" title="Spark SQL数据类型" data-book-page-rel-url="spark-sql/spark-sql-dataType-reference.html" data-book-page-id="7715">Spark SQL数据类型</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/106/graphx-programming-guide/README.html" title="GraphX编程指南" data-book-page-rel-url="graphx-programming-guide/README.html" data-book-page-id="7716">GraphX编程指南</a>
<ul>
<li>
<a class="pjax" href="../../../book/106/graphx-programming-guide/getting-started.html" title="开始" data-book-page-rel-url="graphx-programming-guide/getting-started.html" data-book-page-id="7717">开始</a>
</li>
<li>
<a class="pjax" href="../../../book/106/graphx-programming-guide/property-graph.html" title="属性图" data-book-page-rel-url="graphx-programming-guide/property-graph.html" data-book-page-id="7718">属性图</a>
</li>
<li>
<a class="pjax" href="../../../book/106/graphx-programming-guide/graph-operators.html" title="图操作符" data-book-page-rel-url="graphx-programming-guide/graph-operators.html" data-book-page-id="7719">图操作符</a>
</li>
<li>
<a class="pjax" href="../../../book/106/graphx-programming-guide/pregel-api.html" title="Pregel API" data-book-page-rel-url="graphx-programming-guide/pregel-api.html" data-book-page-id="7720">Pregel API</a>
</li>
<li>
<a class="pjax" href="../../../book/106/graphx-programming-guide/graph-builders.html" title="图构造者" data-book-page-rel-url="graphx-programming-guide/graph-builders.html" data-book-page-id="7721">图构造者</a>
</li>
<li>
<a class="pjax" href="../../../book/106/graphx-programming-guide/vertex-and-edge-rdds.html" title="顶点和边RDDs" data-book-page-rel-url="graphx-programming-guide/vertex-and-edge-rdds.html" data-book-page-id="7722">顶点和边RDDs</a>
</li>
<li>
<a class="pjax" href="../../../book/106/graphx-programming-guide/graph-algorithms.html" title="图算法" data-book-page-rel-url="graphx-programming-guide/graph-algorithms.html" data-book-page-id="7723">图算法</a>
</li>
<li>
<a class="pjax" href="../../../book/106/graphx-programming-guide/examples.html" title="例子" data-book-page-rel-url="graphx-programming-guide/examples.html" data-book-page-id="7724">例子</a>
</li>
<li>
<a class="pjax" href="../../../book/106/deploying/submitting-applications.html" title="提交应用程序" data-book-page-rel-url="deploying/submitting-applications.html" data-book-page-id="7725">提交应用程序</a>
</li>
<li>
<a class="pjax" href="../../../book/106/deploying/spark-standalone-mode.html" title="独立运行Spark" data-book-page-rel-url="deploying/spark-standalone-mode.html" data-book-page-id="7726">独立运行Spark</a>
</li>
<li>
<a class="pjax" href="../../../book/106/deploying/running-spark-on-yarn.html" title="在yarn上运行Spark" data-book-page-rel-url="deploying/running-spark-on-yarn.html" data-book-page-id="7727">在yarn上运行Spark</a>
</li>
<li>
<a class="pjax" href="../../../book/106/more/spark-configuration.html" title="Spark配置" data-book-page-rel-url="more/spark-configuration.html" data-book-page-id="7728">Spark配置</a>
<ul>
<li>
<a class="pjax" href="../../../book/106/programming-guide/rdds/rdd_persistence.html" title="RDD 持久化" data-book-page-rel-url="programming-guide/rdds/rdd_persistence.html" data-book-page-id="7729">RDD 持久化</a>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<script src="https://cdn.staticfile.net/jquery/1.12.4/jquery.min.js"></script>
<script type="text/javascript" src="../../../static/components/uikit-2.27.5/js/uikit.reader.js"></script>
<script type="text/javascript" src="../../../static/components/social-share/social-share.min.js"></script>
<script>(function(){var bp =document.createElement('script');var curProtocol =window.location.protocol.split(':')[0];if (curProtocol ==='https') {bp.src ='https://zz.bdstatic.com/linksubmit/push.js';}
else {bp.src ='http://push.zhanzhang.baidu.com/push.js';}
var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(bp,s);})();</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
<script src="https://cdn.staticfile.net/highlight.js/9.12.0/highlight.min.js"></script>
<script src="https://cdn.staticfile.net/jquery.pjax/2.0.1/jquery.pjax.min.js"></script>
<script src="https://cdn.staticfile.net/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="https://cdn.staticfile.net/uikit/2.27.5/js/components/lightbox.min.js"></script>
<link rel="dns-prefetch" href="../../..//cdn.mathjax.org" />
<script type="text/x-mathjax-config">
 function initMathJax() {
    var mathId = $("book-content-section")[0];
    MathJax.Hub.Config({
        tex2jax: {skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code','a']},
        showProcessingMessages: false,
        messageStyle: "none"
    });
    MathJax.Hub.Queue(["Typeset",MathJax.Hub,mathId]);
 };
initMathJax();
</script>
<script src='https://cdn.staticfile.net/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML' async></script>
<style>
	.MathJax_Display{display:inline!important;}
</style>
<script type="text/javascript" src="../../../static/components/js/reader.js"></script>
<script type="text/javascript">var bookId =106;var bookPageId =7728;var bookPageRelUrl ='more/spark-configuration.html';</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
</body>
</html>