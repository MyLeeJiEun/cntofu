
<!DOCTYPE html>
<html lang="zh-CN">
<head>
<title>特征工程-机器学习原理</title>
<meta content='特征工程,机器学习原理' name='keywords'>
<meta content='特征工程,机器学习原理' name='description'>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta http-equiv="Content-Language" content="zh-CN" />
<meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1, maximum-scale=1, user-scalable=no"../../../../>
<meta name="applicable-device" content="pc,mobile">
<link rel="shortcut icon" href="../../../../favicon.ico" type="image/x-icon" />
<meta name="renderer" content="webkit">
<link rel="stylesheet" href="../../../../static/components/uikit-2.27.5/css/uikit.custom.css">
<link rel="stylesheet" href="../../../../static/components/social-share/social-share.min.css">
<link rel="stylesheet" href="../../../../static/components/highlight/styles/custom.css">
<link rel="stylesheet" href="../../../../static/components/css/base.css">
<link rel="stylesheet" href="../../../../static/components/css/reader.css">
<link rel="stylesheet" href="../../../../static/components/css/markdown.css">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5313208362165053" crossorigin="anonymous"></script>
</head>
<body>
<div class=" book-main-wrap uk-container uk-container-center uk-margin-top ">
<div class="uk-grid">
<div class="uk-width-1-1 reader-wrap ">
<div class=" bottom-nav uk-clearfix ">
<div class="uk-align-left ">
<a href="../../../../book/85/ml/clean-feature/datapreprocess.html">
<i class="nav-icon-left uk-icon-small  uk-icon-caret-left"></i>
<span class="">数据预处理</span>
</a>
</div>
<div class="uk-align-right ">
<a href="../../../../book/85/ml/regression/regression.html">
<span class="">第七课： 回归</span>
<i class="nav-icon-right uk-icon-small  uk-icon-caret-right"></i>
</a>
</div>
</div>
<div class="uk-text-center">
<h2 class="book-page-title uk-container-center">
<a href="../../../../book/85/index.html">机器学习原理</a>
<a target="_blank" rel="nofollow" href="https://github.com/shunliz/Machine-Learning" class="uk-icon-button uk-icon-github" title="github项目地址"></a>
</h2>
</div>
<script type="text/javascript" src="../../../../static/components/js/app_intro.js"></script>
<ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-5313208362165053" data-ad-slot="1328047120"></ins>
<script>(adsbygoogle =window.adsbygoogle ||[]).push({});</script>
<hr class="uk-article-divider">
<div class="book-content-section  md-content-section  uk-margin-bottom">
<p>Feature engineering is an informal topic, but one that is absolutely known and agreed to be key to success in applied machine learning.</p>
<p>In creating this guide I went wide and deep and synthesized all of the material I could.</p>
<p>You will discover what feature engineering is, what problem it solves, why it matters, how to engineer features, who is doing it well and where you can go to learn more and get good at it.</p>
<p>If you read one article on feature engineering, I want it to be this one.</p>
<blockquote>
<p>feature engineering is another topic which doesn’t seem to merit any review papers or books, or even chapters in books, but it is absolutely vital to ML success. […] Much of the success of machine learning is actually success in engineering features that a learner can understand.</p>
</blockquote>
<p>— Scott Locklin, in “<a href="https://scottlocklin.wordpress.com/2014/07/22/neglected-machine-learning-ideas/">Neglected machine learning ideas</a>”</p>
<h2 id="problem-that-feature-engineering-solves">Problem that Feature Engineering Solves</h2>
<p><a href="https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2014/09/feature-engineering-is-hard.jpg"><a href="https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2014/09/feature-engineering-is-hard-300x225.jpg" data-uk-lightbox><img src="https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2014/09/feature-engineering-is-hard-300x225.jpg" alt="" title="Feature engineering is hard. Photo by Vik Nanda, some rights reserved"></a></a></p>
<p>Feature engineering is hard.<br> Photo by<a href="https://www.flickr.com/photos/viknanda/371160360">Vik Nanda</a>, some rights reserved</p>
<p>When your goal is to get the best possible results from a predictive model, you need to get the most from what you have.</p>
<p>This includes getting the best results from the algorithms you are using. It also involves getting the most out of the data for your algorithms to work with.</p>
<p><strong>How do you get the most out of your data for predictive modeling?</strong></p>
<p>This is the problem that the process and practice of feature engineering solves.</p>
<blockquote>
<p>Actually the success of all Machine Learning algorithms depends on how you present the data.</p>
</blockquote>
<p>— Mohammad Pezeshki, answer to “<a href="http://www.quora.com/What-are-some-general-tips-on-feature-selection-and-engineering-that-every-data-scientist-should-know">What are some general tips on feature selection and engineering that every data scientist should know?</a>”</p>
<h2 id="importance-of-feature-engineering">Importance of Feature Engineering</h2>
<p>The features in your data will directly influence the predictive models you use and the results you can achieve.</p>
<p>You can say that: the better the features that you prepare and choose, the better the results you will achieve. It is true, but it also misleading.</p>
<p>The results you achieve are a factor of the model you choose, the data you have available and the features you prepared. Even your framing of the problem and objective measures you’re using to estimate accuracy play a part. Your results are dependent on many inter-dependent properties.</p>
<p>You need great features that describe the structures inherent in your data.</p>
<p><strong>Better features means flexibility</strong>.</p>
<p>You can choose “the wrong models” (less than optimal) and still get good results. Most models can pick up on good structure in data. The flexibility of good features will allow you to use less complex models that are faster to run, easier to understand and easier to maintain. This is very desirable.</p>
<p><strong>Better features means simpler models</strong>.</p>
<p>With well engineered features, you can choose “the wrong parameters” (less than optimal) and still get good results, for much the same reasons. You do not need to work as hard to pick the right models and the most optimized parameters.</p>
<p>With good features, you are closer to the underlying problem and a representation of all the data you have available and could use to best characterize that underlying problem.</p>
<p><strong>Better features means better results</strong>.</p>
<blockquote>
<p>The algorithms we used are very standard for Kagglers. […] &nbsp;We spent most of our efforts in feature engineering.</p>
</blockquote>
<p>— Xavier Conort, on “<a href="http://blog.kaggle.com/2013/04/10/qa-with-xavier-conort/">Q&amp;A with Xavier Conort</a>” on winning the Flight Quest challenge on Kaggle</p>
<h2 id="what-is-feature-engineering">What is Feature Engineering?</h2>
<p>Here is how I define feature engineering:</p>
<blockquote>
<p>Feature engineering is the process of transforming raw data into features that better represent the&nbsp;underlying problem to the predictive models, resulting in&nbsp;improved&nbsp;model accuracy on unseen data.</p>
</blockquote>
<p>You can see the dependencies in this definition:</p>
<ul>
<li>The performance measures you’ve chosen (RMSE? AUC?)</li>
<li>The framing of the problem (classification? regression?)</li>
<li>The predictive models you’re using (SVM?)</li>
<li>The raw data you have selected and prepared (samples? formatting? cleaning?)</li>
</ul>
<blockquote>
<p>feature engineering is manually designing what the input x’s should be</p>
</blockquote>
<p>—&nbsp;Tomasz Malisiewicz, answer to “<a href="http://www.quora.com/What-is-feature-engineering">What is feature engineering?</a>”</p>
<h3 id="feature-engineering-is-a-representation-problem">Feature Engineering is a Representation Problem</h3>
<p>Machine learning algorithms learn a solution to a problem from sample data.</p>
<p>In this context, feature engineering asks: what is the best representation of the sample data to learn a solution to your problem?</p>
<p>It’s deep. Doing well in machine learning, even in artificial intelligence in general comes back to representation problems. It’s hard stuff, perhaps unknowable (or at best intractable) to know the best representation to use,<em>a priori</em>.</p>
<blockquote>
<p>you have to turn your inputs into things the algorithm can understand</p>
</blockquote>
<p>— Shayne Miel, answer to “<a href="http://www.quora.com/What-is-the-intuitive-explanation-of-feature-engineering-in-machine-learning">What is the intuitive explanation of feature engineering in machine learning?</a>”</p>
<h3 id="feature-engineering-is-an-art">Feature Engineering is an Art</h3>
<p>It is an art like engineering is an art, like programming is an art, like medicine is an art.</p>
<p>There are well defined procedures that are methodical, provable and understood.</p>
<p>The data is a variable and is different every time. You get good at deciding which procedures to use and when, by practice. By empirical apprenticeship. Like engineering, like programming, like medicine, like machine learning in general.</p>
<p>Mastery of feature engineering comes with hands on practice, and study of what others that are doing well are practicing.</p>
<blockquote>
<p>…some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used.</p>
</blockquote>
<p>— Pedro Domingos, in “<a href="http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf">A Few Useful Things to Know about Machine Learning</a>” (PDF)</p>
<h2 id="sub-problems-of-feature-engineering">Sub-Problems of Feature Engineering</h2>
<p>It is common to think of feature engineering as one thing.</p>
<p>For example, for a long time for me, feature engineering was feature construction.</p>
<p>I would think to myself “<em>I’m doing feature engineering now</em>” and I would pursue the question “<em>How can I decompose or aggregate raw data to better describe the underlying problem?</em>” The goal was right, but the approach was&nbsp;one of a many.</p>
<p>In this section we look at these many approaches and the specific sub-problems that they are intended to address. Each could be an in depth article of their own as they are large and important areas of practice and study.</p>
<h3 id="feature-an-attribute-useful-for-your-modeling-task">Feature: An attribute useful for your modeling task</h3>
<p>Let’s start with data and<a href="http://en.wikipedia.org/wiki/Feature_%28machine_learning%29">what is a feature</a>.</p>
<p>Tabular data is described in terms of observations or instances (rows) that are made up of variables or attributes (columns). An attribute could be a feature.</p>
<p>The idea of a feature, separate from an attribute, makes more sense in the context of a problem. A feature is an attribute that is useful or meaningful to your problem. It is an important part of an observation for learning about the structure of the problem that is being modeled.</p>
<p>I use “<em>meaningful</em>” to discriminate attributes from features. Some might not. I think there is no such thing as a non-meaningful feature. If a feature has no impact on the problem, it is not part of the problem.</p>
<p>In computer vision, an image is an observation, but a feature could be a line in the image. In natural language processing, a document or a tweet could be an observation, and a phrase or word count could be a feature. In speech recognition, an utterance could be an observation, but a feature might be a single word or phoneme.</p>
<h3 id="feature-importance-an-estimate-of-the-usefulness-of-a-feature">Feature Importance: An estimate of the usefulness of a feature</h3>
<p>You can objectively estimate the usefulness of features.</p>
<p>This can be helpful as a pre-cursor to selecting features. Features are allocated scores and can then be ranked by their scores. Those features with the highest scores can be selected for inclusion in the training dataset, whereas those remaining can be ignored.</p>
<p>Feature importance scores can also provide you with information that you can use to extract or construct new features, similar but different to those that have been estimated to be useful.</p>
<p>A feature may be important if it is highly correlated with the dependent variable (the thing being predicted). Correlation coefficients and other univariate (each attribute is considered independently) methods are common methods.</p>
<p>More complex predictive modeling algorithms perform feature importance and selection internally while constructing their model. Some examples include MARS,<a href="http://en.wikipedia.org/wiki/Random_forest#Variable_importance">Random Forest</a>and Gradient Boosted Machines. These models can also report on the variable importance determined during the model preparation&nbsp;process.</p>
<h3 id="feature-extraction-the-automatic-construction-of-new-features-from-raw-data">Feature Extraction: The automatic construction of new features from raw data</h3>
<p>Some observations are far too voluminous in their raw state to be modeled by predictive modeling algorithms directly.</p>
<p>Common examples include image, audio, and textual data, but could just as easily include tabular data with millions of attributes.</p>
<p><a href="http://en.wikipedia.org/wiki/Feature_extraction">Feature extraction</a>is a process of automatically reducing the dimensionality of these types of observations into a much smaller set that can be modelled.</p>
<p>For tabular data, this might include projection methods like Principal Component Analysis and unsupervised clustering methods. For image data, this might include line or edge detection. Depending on the domain, image, video and audio observations lend themselves to many of the same types of DSP methods.</p>
<p>Key to feature extraction is that the methods are automatic (although may need to be designed and constructed from simpler methods) and solve the problem of unmanageably high dimensional data, most typically used for analog observations stored in digital formats.</p>
<h3 id="feature-selection-from-many-features-to-a-few-that-are-useful">Feature Selection: From many features to a few that are useful</h3>
<p>Not all features are created equal.</p>
<p>Those attributes that are irrelevant to the problem need to be removed. There will be some features that will be more important than others to the model accuracy. There will also be features that will be redundant in the context of other features.</p>
<p><a href="http://en.wikipedia.org/wiki/Feature_selection">Feature selection</a>addresses these problems by automatically selecting a subset that are most useful to the problem.</p>
<p>Feature selection algorithms&nbsp;may use a scoring method to rank and choose features, such as correlation or other feature importance methods.</p>
<p>More advanced methods may search subsets of features by trial and error, creating and evaluating models automatically in pursuit of the objectively most predictive sub-group of features.</p>
<p>There are also methods that bake in feature selection or get it as a side effect of the model. Stepwise regression is an example of an algorithm that automatically performs feature selection as part of the model construction process.</p>
<p>Regularization methods like LASSO and ridge regression may also be considered algorithms with feature selection baked in, as they actively seek to remove or discount the contribution&nbsp;of features as part of the model building process.</p>
<p>Read more in the post:<a href="http://machinelearningmastery.com/an-introduction-to-feature-selection/">An Introduction to Feature Selection</a>.</p>
<h3 id="feature-construction-the-manual-construction-of-new-features-from-raw-data">Feature Construction: The manual construction of new features from raw data</h3>
<p>The best results come down to you, the practitioner, crafting the features.</p>
<p>Feature importance and selection can inform you about the objective utility of features, but those features have to come from somewhere.</p>
<p>You need to manually create them. This requires spending a lot of time with actual sample data (not aggregates) and thinking about the underlying form of the problem, structures in the data and how best to expose them to predictive modeling algorithms.</p>
<p>With tabular data, it often means a mixture of aggregating or combining features to create new features, and decomposing or splitting features to create new features.</p>
<p>With textual data, it often means devising document or context specific indicators relevant to the problem. With image data, it can often mean enormous amounts of time prescribing automatic filters to pick out relevant structures.</p>
<p>This is the part of feature engineering that is often talked the most about as an artform, the part that is attributed the importance and signalled as the differentiator in competitive machine learning.</p>
<p>It is manual, it is slow, it requires lots of human brain power, and it makes a big difference.</p>
<blockquote>
<p>Feature engineering and feature selection are not mutually exclusive. &nbsp;They are both useful. &nbsp;I’d say feature engineering is more important though, especially because you can’t really automate it.</p>
</blockquote>
<p>— Robert Neuhaus, answer to “<a href="http://www.quora.com/How-valuable-do-you-think-feature-selection-is-in-machine-learning-Which-do-you-think-improves-accuracy-more-feature-selection-or-feature-engineering">Which do you think improves accuracy more, feature selection or feature engineering?</a>”</p>
<h3 id="feature-learning-the-automatic-identification-and-use-of-features-in-raw-data">Feature Learning: The automatic identification and use of features in raw data</h3>
<p>Can we avoid the manual load of prescribing how to construct or extract features from raw data?</p>
<p>Representation learning or<a href="http://en.wikipedia.org/wiki/Feature_learning">feature learning</a>is an effort towards this goal.</p>
<p>Modern deep learning methods are achieving some success in this area, such as autoencoders and restricted Boltzmann machines. They have been shown to automatically and in a unsupervised or semi-supervised way, learn abstract representations of features (a compressed form), that in turn have supported state-of-the-art results in domains such as speech recognition, image classification, object recognition and other areas.</p>
<p>We do not have automatic feature extraction or construction, yet, and we will probably never have automatic feature engineering.</p>
<p>The abstract representations are prepared automatically, but you cannot understand and leverage what has been learned, other than in a black-box manner. They cannot (yet, or easily) inform you and the process on how to create more similar and different features like those that are doing well, on a given problem or on similar problems in the future. The acquired skill is trapped.</p>
<p>Nevertheless, it’s fascinating, exciting and an important and modern part of feature engineering.</p>
<h2 id="process-of-feature-engineering">Process of Feature Engineering</h2>
<p>Feature engineering is best understood in the broader process of applied machine learning.</p>
<p>You need this context.</p>
<h3 id="process-of-machine-learning">Process of Machine Learning</h3>
<p>The process of applied machine learning (for lack of a better name) that in a broad brush sense involves lots of activities. Up front is problem definition, next is &nbsp;data selection and preparation, in the middle is model preparation, evaluation and tuning and at the end is the presentation of results.</p>
<p>Process descriptions like<a href="http://machinelearningmastery.com/what-is-data-mining-and-kdd/">data mining and KDD</a>help to better understand the tasks and subtasks. You can pick and choose and phrase the process the way you like.<a href="http://machinelearningmastery.com/process-for-working-through-machine-learning-problems/">I’ve talked a lot about this before</a>.</p>
<p>A picture relevant to our discussion on feature engineering is the front-middle of this process. It might look something like the following:</p>
<ol>
<li>(tasks before here…)</li>
<li>
<dl>
<dt>
<strong>Select Data</strong>
</dt>
<dd>
Integrate data, de-normalize it into a dataset, collect it together.
</dd>
</dl> </li>
<li>
<dl>
<dt>
<strong>Preprocess Data</strong>
</dt>
<dd>
Format it, clean it, sample it so you can work with it.
</dd>
</dl> </li>
<li>
<dl>
<dt>
<strong>Transform Data</strong>
</dt>
<dd>
<em>Feature Engineer happens here</em> .
</dd>
</dl> </li>
<li>
<dl>
<dt>
<strong>Model Data</strong>
</dt>
<dd>
Create models, evaluate them and tune them.
</dd>
</dl> </li>
<li>(tasks after here…)</li>
</ol>
<p>The traditional idea of “<em>Transforming Data</em>” from a raw state to a state suitable for modeling is where feature engineering fits in. Transform data and feature engineering may in fact be synonyms.</p>
<p>This picture helps in a few ways.</p>
<p>You can see that before feature engineering, we are munging out data into a format we can even look at, and just before that we are collating and denormalizing data from databases into some kind of central picture.</p>
<p>We can, and should go back through these steps as we identify new perspectives on the data.</p>
<p>For example, we may have an attribute that is an aggregate field, like a sum. Rather than a single sum, we may decide to create features to describe the quantity by time interval, such as season. We need to step backward in the process through Preprocessing and even Selecting data to get access to the “real raw data” and create this feature.</p>
<p>We can see that feature engineering is followed by modeling.</p>
<p>It suggests a strong interaction with modeling, reminding us of the interplay of devising features and testing them against the coalface of our test harness and final performance measures.</p>
<p>This also suggests we may need to leave the data in a form suitable for the chosen modeling algorithm, such as normalize or standardize the features&nbsp;as a final step. This sounds like a preprocessing step, it probably is, but it helps us consider what types of finishing touches are needed to the data before effective modeling.</p>
<h3 id="iterative-process-of-feature-engineering">Iterative Process of Feature Engineering</h3>
<p>Knowing where feature engineering fits into the context of the process of applied machine learning highlights that it does not standalone.</p>
<p>It is an iterative process that interplays with data selection and model evaluation, again and again, until we run out of time on our problem.</p>
<p>The process might look as follows:</p>
<ol>
<li>
<dl>
<dt>
<strong>Brainstorm features</strong>
</dt>
<dd>
Really get into the problem, look at a lot of data, study feature engineering on other problems and see what you can steal.
</dd>
</dl> </li>
<li>
<dl>
<dt>
<strong>Devise features</strong>
</dt>
<dd>
Depends on your problem, but you may use automatic feature extraction, manual feature construction and mixtures of the two.
</dd>
</dl> </li>
<li>
<dl>
<dt>
<strong>Select features</strong>
</dt>
<dd>
Use different feature importance scorings and feature selection methods to prepare one or more “views” for your models to operate upon.
</dd>
</dl> </li>
<li>
<dl>
<dt>
<strong>Evaluate models</strong>
</dt>
<dd>
Estimate model accuracy on unseen data using the chosen features.
</dd>
</dl> </li>
</ol>
<p>You need a well defined problem so that you know when to stop this process and move on to trying other models, other model configurations, ensembles of models, and so on. There are gains to be had later in the pipeline once you plateau on ideas or the accuracy delta.</p>
<p>You need a well considered and designed test harness for objectively estimating model skill on unseen data. It will be the only measure you have of your feature engineering process, and you must trust it not to waste your time.</p>
<h2 id="general-examples-of-feature-engineering">General Examples of Feature Engineering</h2>
<p>Let’s make the concepts of feature engineering more concrete.</p>
<p>In this section we will consider tabular data like that you might have in an excel spreadsheet. We will look at some examples of manual feature construction that you might like to consider on your own problems.</p>
<p>When I hear “<em>feature engineering is critically important</em>”, this is the type of feature engineering I think of. It is the most common form that I am familiar with and practice.</p>
<p>Which of these is best? You cannot know before hand. You must try them and evaluate the results to achieve on your algorithm and performance measures.</p>
<h3 id="decompose-categorical-attributes">Decompose Categorical Attributes</h3>
<p>Imagine you have a categorical attribute, like “<em>Item_Color</em>” that can be_Red_,<em>Blue_or_Unknown</em>.</p>
<p>_Unknown_may be special, but to a model, it looks like just another colour choice. It might be beneficial to better expose this information.</p>
<p>You could create a new binary feature called “<em>Has_Color</em>” and assign it a value of “<em>1</em>” when an item has a color and “<em>0</em>” when the color is unknown.</p>
<p>Going a step further, you could create a binary feature for each value that_Item_Color_has. This would be three binary attributes:<em>Is_Red</em>,<em>Is_Blue_and_Is_Unknown</em>.</p>
<p>These additional features could be used instead of the_Item_Color_feature (if you wanted to try a simpler linear model) or in addition to it (if you wanted to get more out of something like a decision tree).</p>
<h3 id="decompose-a-date-time">Decompose a Date-Time</h3>
<p>A date-time contains a lot of information that can be difficult for a model to take advantage of in it’s native form, such as<a href="http://en.wikipedia.org/wiki/ISO_8601">ISO 8601</a>(i.e. 2014-09-20T20:45:40Z).</p>
<p>If you suspect there are relationships between times and other attributes, you can decompose a date-time into constituent parts that may allow models to discover and exploit these relationships.</p>
<p>For example, you may suspect that there is a relationship between the time of day and other attributes.</p>
<p>You could create a new numerical feature called_Hour_of_Day_for the hour that might help a regression model.</p>
<p>You could create a new ordinal feature called_Part_Of_Day_with 4 values_Morning_,<em>Midday</em>,<em>Afternoon</em>,_Night_with whatever hour boundaries you think are relevant. This might be useful for a decision tree.</p>
<p>You can use similar approaches to pick out time of week relationships, time of month relationships and various structures of seasonality across a year.</p>
<p>Date-times are rich in structure and if you suspect there is time dependence in your data, take your time and tease them out.</p>
<h3 id="reframe-numerical-quantities">Reframe Numerical Quantities</h3>
<p>Your data is very likely to contain quantities, which can be reframed to better expose relevant structures. This may be a transform into a new unit or the decomposition of a rate into time and amount components.</p>
<p>You may have a quantity like a weight, distance or timing. A linear transform may be useful to regression and other scale dependent methods.</p>
<p>For example, you may have_Item_Weight_in grams, with a value like 6289. You could create a new feature with this quantity in kilograms as 6.289 or rounded kilograms like 6. If the domain is shipping data, perhaps kilograms is sufficient or more useful (less noisy) a precision for_Item_Weight_.</p>
<p>The_Item_Weight_could be split into two features:<em>Item_Weight_Kilograms_and_Item_Weight_Remainder_Grams</em>, with example values of 6 and 289 respectively.</p>
<p>There may be domain knowledge that items with a weight above 4 incur a higher taxation rate. That magic domain number could be used to create a new binary feature_Item_Above_4kg_with a value of “<em>1</em>” for our example of 6289 grams.</p>
<p>You may also have a quantity stored as a rate or an aggregate quantity for an interval. For example,_Num_Customer_Purchases_aggregated over a year.</p>
<p>In this case you may want to go back to the data collection step and create new features in addition to this aggregate and try to expose more temporal structure in the purchases, like perhaps seasonality. For example, the following new binary features could be created:<em>Purchases_Summer</em>,<em>Purchases_Fall</em>,<em>Purchases_Winter_and_Purchases_Spring</em>.</p>
<h2 id="concrete-examples-of-feature-engineering">Concrete Examples of Feature Engineering</h2>
<p>A great place to study examples of feature engineering is in the results from competitive machine learning.</p>
<p>Competitions typically use data from a real-world problem domain. A write-up of methods and approach is required at the end of a competition. These write-ups give valuable insight into effective real-world machine learning processes and methods.</p>
<p>In this section we touch on a few examples of interesting and notable post-competition write-ups that focus on feature engineering.</p>
<h3 id="predicting-student-test-performance-in-kdd-cup-2010">Predicting Student Test Performance in KDD Cup 2010</h3>
<p>The<a href="http://www.sigkdd.org/kddcup/index.php">KDD Cup</a>is a machine learning competition held for attendees of the ACM Special Interest Group on Knowledge Discovery and Data Mining conferences, each year.</p>
<p>In 2010, the focus of the competition was the problem of modeling how students learn. A corpus of student results on algebraic problems was provided to be used to predict those students’ future performance.</p>
<p>The winner of the competition were a group of students and academics at the National Taiwan University. Their approach is described in the paper “<a href="http://pslcdatashop.org/KDDCup/workshop/papers/kdd2010ntu.pdf">Feature Engineering and Classifier Ensemble for KDD Cup 2010</a>”.</p>
<p>The paper credits feature engineering as a key method in winning. Feature engineering simplified the structure of the problem at the expense of creating millions of binary features. The simple structure allowed the team to use highly performant but very simple linear methods to achieve the winning predictive model.</p>
<p>The paper provides details of how specific temporal and other non-linearities in the problem structure were reduced to simple composite binary indicators.</p>
<p>This is an extreme and instructive example of what is possible with simple attribute decomposition.</p>
<h3 id="predictingpatient-admittance-in-the-heritage-health-prize">Predicting&nbsp;Patient Admittance in the Heritage Health Prize</h3>
<p>The<a href="https://www.heritagehealthprize.com/c/hhp">heritage health prize</a>was a 3 million dollar prize awarded to the team who could best predict which patients would be admitted to hospital within the next year.</p>
<p>The prize had milestone awards each year where the top teams would be awarded a prize and their processes and methods made public.</p>
<p>I remember reading the papers released at the first of the three milestones and being impressed with the amount of feature engineering involved.</p>
<p>Specifically, the paper “<a href="https://kaggle2.blob.core.windows.net/wiki-files/327/e4cd1d25-eca9-49ca-9593-b254a773fe03/Market Makers - Milestone 1 Description V2 1.pdf">Round 1 Milestone Prize: How We Did It – Team Market Makers</a>” by Phil Brierley, David Vogel and Randy Axelrod. Most competitions involve vast amounts of feature engineering, but it struck me how clearly this paper made the point.</p>
<p>The paper provides both tables of attributes and SQL required to construct the attributes.</p>
<p>The paper gives some great real-world examples of feature engineering by simple decomposition. There are a lot of counts, mins, maxes, lots of binary attributes, and discretized numerical attributes. Very simple methods used to great effect.</p>
<h2 id="more-resources-on-feature-engineering">More Resources on Feature Engineering</h2>
<p>We have covered a lot of ground in this article and I hope you have a much greater appreciation of what feature engineering is, where it fits in, and how to do it.</p>
<p>This is really the start of your journey. You need to practice feature engineering and you need to study great practitioners of feature engineering.</p>
<p>This section provides some resources that might help you on your journey.</p>
<h3 id="books">Books</h3>
<p>I cannot find any books or book chapters on the topic.</p>
<p>There are however some great books on feature extraction. If you are working with digital representations of analog observations like images, video, sound or text, you might like to dive deeper into some feature extraction literature.</p>
<ul>
<li><a href="http://www.amazon.com/dp/0792381963?tag=inspiredalgor-20">Feature Extraction, Construction and Selection: A Data Mining Perspective</a></li>
<li><a href="http://www.amazon.com/dp/3540354875?tag=inspiredalgor-20">Feature Extraction: Foundations and Applications</a> &nbsp;(I like this book)</li>
<li><a href="http://www.amazon.com/dp/0123965497?tag=inspiredalgor-20">Feature Extraction &amp; Image Processing for Computer Vision, Third Edition</a></li>
</ul>
<p>There are also lots of books on feature selection. If you are working to reduce your features by removing those that are redundant or irrelevant, dive deeper into feature selection.</p>
<ul>
<li><a href="http://www.amazon.com/dp/079238198X?tag=inspiredalgor-20">Feature Selection for Knowledge Discovery and Data Mining</a></li>
<li><a href="http://www.amazon.com/dp/1584888784?tag=inspiredalgor-20">Computational Methods of Feature Selection</a></li>
</ul>
<h3 id="papers-and-slides">Papers and Slides</h3>
<p>It is a hard topic to find papers on.</p>
<p>Again, there are plenty of papers of feature extraction and chapters in books of feature selection, but not much of feature engineering. Also feature engineering has a meaning in software engineering as well, one that is not relevant to our discussion.</p>
<p>Here are some generally relevant papers:</p>
<ul>
<li><a href="http://jmlr.org/papers/special/feature03.html">JMLR Special Issue on Variable and Feature Selection</a></li>
</ul>
<p>Here are some generally relevant and interesting slides:</p>
<ul>
<li><a href="http://kti.tugraz.at/staff/denis/courses/kddm1/featureengineering.pdf">Feature Engineering</a> (PDF), Knowledge Discover and Data Mining 1, by Roman Kern, <a href="http://kti.tugraz.at/staff/denis/courses/kddm1/">Knowledge Technologies Institute</a></li>
<li><a href="http://www.cs.berkeley.edu/~jordan/courses/294-fall09/lectures/feature/slides.pdf">Feature Engineering and Selection</a> (PDF), CS 294: <a href="http://www.cs.berkeley.edu/~jordan/courses/294-fall09/lectures/feature/">Practical Machine Learning</a> , Berkeley</li>
<li><a href="http://www.columbia.edu/~rsb2162/FES2013/materials.html">Feature Engineering Studio</a> , Course Lecture Slides and Materials, Columbia</li>
<li><a href="http://www.cs.princeton.edu/courses/archive/spring10/cos424/slides/18-feat.pdf">Feature Engineering</a> (PDF), Leon Bottou, Princeton</li>
</ul>
<h3 id="links">Links</h3>
<p>There blog posts here and there. The most useful links are tutorials that work through a problem and clearly articulate the intentional feature engineering.</p>
<p>Below are some generally interesting links:</p>
<ul>
<li><a href="http://trevorstephens.com/post/73461351896/titanic-getting-started-with-r-part-4-feature">Feature Engineering: How to perform feature engineering on the Titanic competition</a> (a getting started competition on Kaggle). There is more data munging than feature engineering, but it’s still instructive.</li>
<li><a href="http://nbviewer.ipython.org/url/trust.sce.ntu.edu.sg/~gguo1/blogs/Features.ipynb"><del>IPython Notebook</del></a> <del>by</del> <a href="http://trust.sce.ntu.edu.sg/~gguo1/"><del>Guibing Guo</del></a> <del>, dedicated to explaining feature engineering. A bit messy, but worth&nbsp;a skim</del> . (link appears broken, sorry.)</li>
</ul>
<h3 id="videos">Videos</h3>
<p>There are a few videos&nbsp;on the topic of feature engineering. The best by far is titled “<a href="https://www.youtube.com/watch?v=drUToKxEAUA">Feature Engineering</a>” by&nbsp;Ryan Baker. It’s short (9 minutes or so) and I recommend watching it for some good practical tips.</p>
</div>
<hr class="uk-article-divider">
<div class="uk-block uk-block-muted uk-padding-top-remove uk-padding-bottom-remove uk-margin-large-top  book-recommend-wrap">
<div class="uk-margin-top uk-margin-bottom uk-margin-left uk-margin-right">
<div class="uk-margin uk-text-muted "><i class="uk-icon-outdent uk-icon-justify uk-margin-small-right"></i>书籍推荐</div>
<div class="books">
<ul class="uk-book-list">
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/168/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/machine-learning_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/168/index.html">AiLearning: 机器学习</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/18.html">ApacheCN</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="machine-learning">machine-learning</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">20页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月26日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 14197个">14197</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/56/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/machine-learning_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/56/index.html">神经网络与深度学习</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/32.html">tigerneil</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="machine-learning">machine-learning</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">9页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月5日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 239个">239</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/94/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/machine-learning_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/94/index.html">机器学习实战</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/57.html">RedstoneWill</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="machine-learning">machine-learning</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">24页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月29日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 7个">7</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/125/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/html5_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/125/index.html">前端早读课</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/67.html">if2er</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="html5">html5</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">1页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年7月8日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 7个">7</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/49/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/cplusplus_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/49/index.html">如何设计计算机操作系统</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/26.html">SamyPesse</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="cplusplus">cplusplus</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">11页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月2日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 17175个">17175</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/52/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/logstash_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/52/index.html">Logstash最佳实践</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/29.html">chenryn</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="logstash">logstash</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">54页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月3日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 693个">693</span>
</div>
</div>
</div>
</li>
<hr>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
<nav class="tm-navbar uk-navbar uk-navbar-attached reader-nav">
<div class="uk-float-left uk-margin-small-top">
<a href="javascript:;" title="目录菜单" class="show-menu  uk-icon-hover  uk-icon-align-justify uk-margin-right"></a>
<div data-uk-dropdown="{mode:'click',pos:'bottom-left'}" class="font-setting-wrap">
<a class="uk-icon-hover uk-icon-font uk-margin-right" aria-label="字体设置" href="javascript:;"></a>
<div class="uk-dropdown dropdown-menu">
<div class="dropdown-caret"><span class="caret-outer"></span><span class="caret-inner"></span></div>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-reduce">小字</button>
<button class="uk-button-link button size-2 font-enlarge">大字</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-1 ">宋体</button>
<button class="uk-button-link button size-2 font-2 ">黑体</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-3 color-theme-sun "><i class="uk-icon-sun-o"></i>白天</button>
<button class="uk-button-link button size-3 color-theme-eye "><i class="uk-icon-eye"></i>护眼</button>
<button class="uk-button-link button size-3 color-theme-moon "><i class="uk-icon-moon-o"></i>夜晚</button></div>
</div>
</div>
<a class="logo uk-margin-right" href="../../../../" title="返回首页"><img class="" src="../../../../static/components/images/icon_32.png" /></a>
</div>
<div class="uk-navbar-flip  uk-hidden-small">
<div id="share-box"></div>
</div>
</nav>
<div id="menu-id" class="uk-offcanvas reader-offcanvas">
<div class="uk-offcanvas-bar">
<ul class="book-menu-bar uk-nav uk-nav-offcanvas" data-uk-nav>
<li>
<a href="../../../../book/85/index.html" data-book-page-rel-url="index.html" data-book-page-id="0" title="封面">封面</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/readme.html" data-book-page-rel-url="readme.html" data-book-page-id="0" title="简介">简介</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/README.html" title="前言" data-book-page-rel-url="README.html" data-book-page-id="6588">前言</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/math/math.html" title="第一部分 数学基础" data-book-page-rel-url="math/math.html" data-book-page-id="6589">第一部分 数学基础</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/math/analytic/introduction.html" title="第一章 数学分析" data-book-page-rel-url="math/analytic/introduction.html" data-book-page-id="6590">第一章 数学分析</a>
<ul>
<li>
<a class="pjax" href="../../../../book/85/math/analytic/gradient_descent.html" title="梯度下降" data-book-page-rel-url="math/analytic/gradient_descent.html" data-book-page-id="6591">梯度下降</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/math/analytic/shu-zhi-ji-suan.html" title="数值计算" data-book-page-rel-url="math/analytic/shu-zhi-ji-suan.html" data-book-page-id="6592">数值计算</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/math/analytic/overfitting.html" title="过拟合数学原理与解决方案" data-book-page-rel-url="math/analytic/overfitting.html" data-book-page-id="6593">过拟合数学原理与解决方案</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/math/analytic/cross-validation.html" title="交叉验证" data-book-page-rel-url="math/analytic/cross-validation.html" data-book-page-id="6594">交叉验证</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/math/analytic/least-square.html" title="最小二乘" data-book-page-rel-url="math/analytic/least-square.html" data-book-page-id="6595">最小二乘</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/math/analytic/lagelangri-kkt.html" title="拉格朗日乘子法（Lagrange Multiplier) 和KKT条件" data-book-page-rel-url="math/analytic/lagelangri-kkt.html" data-book-page-id="6596">拉格朗日乘子法（Lagrange Multiplier) 和KKT条件</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/math/analytic/niudun.html" title="牛顿法" data-book-page-rel-url="math/analytic/niudun.html" data-book-page-id="6597">牛顿法</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/math/analytic/tuyouhua.html" title="凸优化" data-book-page-rel-url="math/analytic/tuyouhua.html" data-book-page-id="6598">凸优化</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/math/analytic/common-function.html" title="常用函数" data-book-page-rel-url="math/analytic/common-function.html" data-book-page-id="6599">常用函数</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/85/math/probability.html" title="第二章 概率论" data-book-page-rel-url="math/probability.html" data-book-page-id="6600">第二章 概率论</a>
<ul>
<li>
<a class="pjax" href="../../../../book/85/math/probability/prob-methodology.html" title="统计学习方法概论" data-book-page-rel-url="math/probability/prob-methodology.html" data-book-page-id="6601">统计学习方法概论</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/math/probability/mle.html" title="最大似然估计" data-book-page-rel-url="math/probability/mle.html" data-book-page-id="6602">最大似然估计</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/math/probability/mcmc1.html" title="蒙特卡罗方法" data-book-page-rel-url="math/probability/mcmc1.html" data-book-page-id="6603">蒙特卡罗方法</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/math/probability/markov-chain.html" title="马尔科夫链" data-book-page-rel-url="math/probability/markov-chain.html" data-book-page-id="6604">马尔科夫链</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/math/probability/mcmc-mh.html" title="MCMC采样和M-H采样" data-book-page-rel-url="math/probability/mcmc-mh.html" data-book-page-id="6605">MCMC采样和M-H采样</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/math/probability/gibbs.html" title="Gibbs采样" data-book-page-rel-url="math/probability/gibbs.html" data-book-page-id="6606">Gibbs采样</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/85/math/linear-matrix/linear-matrix.html" title="第三章 矩阵和线性代数" data-book-page-rel-url="math/linear-matrix/linear-matrix.html" data-book-page-id="6607">第三章 矩阵和线性代数</a>
<ul>
<li>
<a class="pjax" href="../../../../book/85/math/linear-matrix/lapack.html" title="LAPACK" data-book-page-rel-url="math/linear-matrix/lapack.html" data-book-page-id="6608">LAPACK</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/math/linear-matrix/tezhengzhihetezhengxiangliang.html" title="特征值与特征向量" data-book-page-rel-url="math/linear-matrix/tezhengzhihetezhengxiangliang.html" data-book-page-id="6609">特征值与特征向量</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/ml.html" title="第二部分 机器学习" data-book-page-rel-url="ml/ml.html" data-book-page-id="6610">第二部分 机器学习</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/pythonml.html" title="第四章 机器学习基础" data-book-page-rel-url="ml/pythonml.html" data-book-page-id="6611">第四章 机器学习基础</a>
<ul>
<li>
<a class="pjax" href="../../../../book/85/ml/pythonml/pythonji-qi-shu-xue-ku.html" title="Python及其数学库" data-book-page-rel-url="ml/pythonml/pythonji-qi-shu-xue-ku.html" data-book-page-id="6612">Python及其数学库</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/pythonml/ji-qi-xue-xi-ku.html" title="机器学习库" data-book-page-rel-url="ml/pythonml/ji-qi-xue-xi-ku.html" data-book-page-id="6613">机器学习库</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/pythonml/ml-metrics.html" title="模型度量" data-book-page-rel-url="ml/pythonml/ml-metrics.html" data-book-page-id="6614">模型度量</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/pythonml/gen-descri.html" title="生成模型和判别模型" data-book-page-rel-url="ml/pythonml/gen-descri.html" data-book-page-id="6615">生成模型和判别模型</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/pythonml/distance.html" title="机器学习中的距离" data-book-page-rel-url="ml/pythonml/distance.html" data-book-page-id="6616">机器学习中的距离</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/clean-feature/cleanup-feature.html" title="第六课：数据清洗和特征选择" data-book-page-rel-url="ml/clean-feature/cleanup-feature.html" data-book-page-id="6617">第六课：数据清洗和特征选择</a>
<ul>
<li>
<a class="pjax" href="../../../../book/85/ml/clean-feature/pca.html" title="PCA" data-book-page-rel-url="ml/clean-feature/pca.html" data-book-page-id="6618">PCA</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/clean-feature/ica.html" title="ICA" data-book-page-rel-url="ml/clean-feature/ica.html" data-book-page-id="6619">ICA</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/clean-feature/one-hot.html" title="One-hot编码" data-book-page-rel-url="ml/clean-feature/one-hot.html" data-book-page-id="6620">One-hot编码</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/clean-feature/scikit-pca.html" title="scikit-learn PCA" data-book-page-rel-url="ml/clean-feature/scikit-pca.html" data-book-page-id="6621">scikit-learn PCA</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/clean-feature/xian-xing-pan-bie-fen-xi-lda.html" title="线性判别分析LDA" data-book-page-rel-url="ml/clean-feature/xian-xing-pan-bie-fen-xi-lda.html" data-book-page-id="6622">线性判别分析LDA</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/clean-feature/scikit-lda.html" title="用scikit-learn进行LDA降维" data-book-page-rel-url="ml/clean-feature/scikit-lda.html" data-book-page-id="6623">用scikit-learn进行LDA降维</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/clean-feature/svd.html" title="奇异值分解(SVD)原理与在降维中的应用" data-book-page-rel-url="ml/clean-feature/svd.html" data-book-page-id="6624">奇异值分解(SVD)原理与在降维中的应用</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/clean-feature/lle.html" title="局部线性嵌入(LLE)原理" data-book-page-rel-url="ml/clean-feature/lle.html" data-book-page-id="6625">局部线性嵌入(LLE)原理</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/clean-feature/scikit-lle.html" title="scikit-learn LLE" data-book-page-rel-url="ml/clean-feature/scikit-lle.html" data-book-page-id="6626">scikit-learn LLE</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/clean-feature/spark-fselect.html" title="spark特征选择" data-book-page-rel-url="ml/clean-feature/spark-fselect.html" data-book-page-id="6627">spark特征选择</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/clean-feature/spark-fextract.html" title="Spark特征提取" data-book-page-rel-url="ml/clean-feature/spark-fextract.html" data-book-page-id="6628">Spark特征提取</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/clean-feature/outlier-detect.html" title="异常数据监测" data-book-page-rel-url="ml/clean-feature/outlier-detect.html" data-book-page-id="6629">异常数据监测</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/clean-feature/datapreprocess.html" title="数据预处理" data-book-page-rel-url="ml/clean-feature/datapreprocess.html" data-book-page-id="6630">数据预处理</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/clean-feature/te-zheng-gong-cheng.html" title="特征工程" data-book-page-rel-url="ml/clean-feature/te-zheng-gong-cheng.html" data-book-page-id="6631">特征工程</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/regression/regression.html" title="第七课： 回归" data-book-page-rel-url="ml/regression/regression.html" data-book-page-id="6632">第七课： 回归</a>
<ul>
<li>
<a class="pjax" href="../../../../book/85/ml/regression/linear-regression.html" title="1.  线性回归" data-book-page-rel-url="ml/regression/linear-regression.html" data-book-page-id="6633">1. 线性回归</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/regression/max-entropy.html" title="10.最大熵模型" data-book-page-rel-url="ml/regression/max-entropy.html" data-book-page-id="6634">10.最大熵模型</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/regression/kl.html" title="11.K-L散度" data-book-page-rel-url="ml/regression/kl.html" data-book-page-id="6635">11.K-L散度</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/regression/cordinate-angle.html" title="坐标下降和最小角" data-book-page-rel-url="ml/regression/cordinate-angle.html" data-book-page-id="6636">坐标下降和最小角</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/regression/linear-regression-summary.html" title="线性回归小结" data-book-page-rel-url="ml/regression/linear-regression-summary.html" data-book-page-id="6637">线性回归小结</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/regression/logistic.html" title="Logistic回归" data-book-page-rel-url="ml/regression/logistic.html" data-book-page-id="6638">Logistic回归</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/regression/logistichui-gui-xiao-jie.html" title="Logistic回归小结" data-book-page-rel-url="ml/regression/logistichui-gui-xiao-jie.html" data-book-page-id="6639">Logistic回归小结</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/regression/softmax.html" title="SoftMax回归" data-book-page-rel-url="ml/regression/softmax.html" data-book-page-id="6640">SoftMax回归</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/decisiontree.html" title="第九课：决策树" data-book-page-rel-url="ml/decisiontree.html" data-book-page-id="6641">第九课：决策树</a>
<ul>
<li>
<a class="pjax" href="../../../../book/85/ml/decisiontree/id3.html" title="ID3" data-book-page-rel-url="ml/decisiontree/id3.html" data-book-page-id="6642">ID3</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/decisiontree/c45.html" title="C4.5" data-book-page-rel-url="ml/decisiontree/c45.html" data-book-page-id="6643">C4.5</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/decisiontree/cart.html" title="CART" data-book-page-rel-url="ml/decisiontree/cart.html" data-book-page-id="6644">CART</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/decisiontree/summary.html" title="总结" data-book-page-rel-url="ml/decisiontree/summary.html" data-book-page-id="6645">总结</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/decisiontree/code.html" title="实现代码" data-book-page-rel-url="ml/decisiontree/code.html" data-book-page-id="6646">实现代码</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/svm.html" title="第十三课：SVM" data-book-page-rel-url="ml/svm.html" data-book-page-id="6647">第十三课：SVM</a>
<ul>
<li>
<a class="pjax" href="../../../../book/85/ml/svm/gan-zhi-ji-mo-xing.html" title="感知机模型" data-book-page-rel-url="ml/svm/gan-zhi-ji-mo-xing.html" data-book-page-id="6648">感知机模型</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/svm/linear-svm.html" title="线性SVM" data-book-page-rel-url="ml/svm/linear-svm.html" data-book-page-id="6649">线性SVM</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/svm/soft-margin-max.html" title="软间隔最大化模型" data-book-page-rel-url="ml/svm/soft-margin-max.html" data-book-page-id="6650">软间隔最大化模型</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/svm/kernel-method.html" title="核函数" data-book-page-rel-url="ml/svm/kernel-method.html" data-book-page-id="6651">核函数</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/svm/smo.html" title="SMO算法原理" data-book-page-rel-url="ml/svm/smo.html" data-book-page-id="6652">SMO算法原理</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/svm/svm-regression.html" title="SVM回归" data-book-page-rel-url="ml/svm/svm-regression.html" data-book-page-id="6653">SVM回归</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/svm/scikit-learn-svm.html" title="scikit-learn SVM" data-book-page-rel-url="ml/svm/scikit-learn-svm.html" data-book-page-id="6654">scikit-learn SVM</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/svm/gaosi-kernel.html" title="支持向量机高斯核调参" data-book-page-rel-url="ml/svm/gaosi-kernel.html" data-book-page-id="6655">支持向量机高斯核调参</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/svm/svm-code.html" title="SVM代码实现" data-book-page-rel-url="ml/svm/svm-code.html" data-book-page-id="6656">SVM代码实现</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/integrate.html" title="集成学习" data-book-page-rel-url="ml/integrate.html" data-book-page-id="6657">集成学习</a>
<ul>
<li>
<a class="pjax" href="../../../../book/85/ml/integrate/adaboost.html" title="Adaboost原理" data-book-page-rel-url="ml/integrate/adaboost.html" data-book-page-id="6658">Adaboost原理</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/integrate/scikit-learn-adaboost.html" title="scikit-learn Adaboost" data-book-page-rel-url="ml/integrate/scikit-learn-adaboost.html" data-book-page-id="6659">scikit-learn Adaboost</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/integrate/gbdt.html" title="梯度提升树（GBDT）" data-book-page-rel-url="ml/integrate/gbdt.html" data-book-page-id="6660">梯度提升树（GBDT）</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/integrate/scikit-gbdt.html" title="scikit GBDT" data-book-page-rel-url="ml/integrate/scikit-gbdt.html" data-book-page-id="6661">scikit GBDT</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/integrate/random-forest.html" title="Bagging与随机森林" data-book-page-rel-url="ml/integrate/random-forest.html" data-book-page-id="6662">Bagging与随机森林</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/integrate/xgboost.html" title="XGBOOST" data-book-page-rel-url="ml/integrate/xgboost.html" data-book-page-id="6663">XGBOOST</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/integrate/scikit-learn-rf.html" title="scikit-learn 随机森林" data-book-page-rel-url="ml/integrate/scikit-learn-rf.html" data-book-page-id="6664">scikit-learn 随机森林</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/cluster.html" title="第十五课：聚类" data-book-page-rel-url="ml/cluster.html" data-book-page-id="6665">第十五课：聚类</a>
<ul>
<li>
<a class="pjax" href="../../../../book/85/ml/cluster/kmeans.html" title="K-Mean" data-book-page-rel-url="ml/cluster/kmeans.html" data-book-page-id="6666">K-Mean</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/cluster/KNN.html" title="KNN" data-book-page-rel-url="ml/cluster/KNN.html" data-book-page-id="6667">KNN</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/cluster/knnshi-jian.html" title="scikit-learn KNN" data-book-page-rel-url="ml/cluster/knnshi-jian.html" data-book-page-id="6668">scikit-learn KNN</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/cluster/knn-code.html" title="KNN 代码" data-book-page-rel-url="ml/cluster/knn-code.html" data-book-page-id="6669">KNN 代码</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/cluster/scikit-k-means.html" title="scikit-learn K-Means" data-book-page-rel-url="ml/cluster/scikit-k-means.html" data-book-page-id="6670">scikit-learn K-Means</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/cluster/birch.html" title="BIRCH聚类算法原理" data-book-page-rel-url="ml/cluster/birch.html" data-book-page-id="6671">BIRCH聚类算法原理</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/cluster/scikit-learn-birch.html" title="scikit-learn BIRCH" data-book-page-rel-url="ml/cluster/scikit-learn-birch.html" data-book-page-id="6672">scikit-learn BIRCH</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/cluster/dbscan.html" title="DBSCAN密度聚类算法" data-book-page-rel-url="ml/cluster/dbscan.html" data-book-page-id="6673">DBSCAN密度聚类算法</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/cluster/scikit-learn-dbscan.html" title="scikit-learn DBSCAN" data-book-page-rel-url="ml/cluster/scikit-learn-dbscan.html" data-book-page-id="6674">scikit-learn DBSCAN</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/cluster/spectral.html" title="谱聚类（spectral clustering）原理" data-book-page-rel-url="ml/cluster/spectral.html" data-book-page-id="6675">谱聚类（spectral clustering）原理</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/cluster/scikit-spectral.html" title="scikit-learn 谱聚类" data-book-page-rel-url="ml/cluster/scikit-spectral.html" data-book-page-id="6676">scikit-learn 谱聚类</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/cluster/ap.html" title="近邻传播算法" data-book-page-rel-url="ml/cluster/ap.html" data-book-page-id="6677">近邻传播算法</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/cluster/gmm.html" title="混合高斯模型" data-book-page-rel-url="ml/cluster/gmm.html" data-book-page-id="6678">混合高斯模型</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/associative/associative.html" title="关联分析" data-book-page-rel-url="ml/associative/associative.html" data-book-page-id="6679">关联分析</a>
<ul>
<li>
<a class="pjax" href="../../../../book/85/ml/associative/cca.html" title="典型关联分析(CCA)原理" data-book-page-rel-url="ml/associative/cca.html" data-book-page-id="6680">典型关联分析(CCA)原理</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/associative/apriori.html" title="Apriori算法原理" data-book-page-rel-url="ml/associative/apriori.html" data-book-page-id="6681">Apriori算法原理</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/associative/fptree.html" title="FP Tree算法原理" data-book-page-rel-url="ml/associative/fptree.html" data-book-page-id="6682">FP Tree算法原理</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/associative/prefixspan.html" title="PrefixSpan算法原理" data-book-page-rel-url="ml/associative/prefixspan.html" data-book-page-id="6683">PrefixSpan算法原理</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/associative/spark-fptree-prefixspan.html" title="Spark FP Tree算法和PrefixSpan算法" data-book-page-rel-url="ml/associative/spark-fptree-prefixspan.html" data-book-page-id="6684">Spark FP Tree算法和PrefixSpan算法</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/recommand/recommand.html" title="推荐算法" data-book-page-rel-url="ml/recommand/recommand.html" data-book-page-id="6685">推荐算法</a>
<ul>
<li>
<a class="pjax" href="../../../../book/85/ml/recommand/matrix-filter.html" title="矩阵分解协同过滤推荐算法" data-book-page-rel-url="ml/recommand/matrix-filter.html" data-book-page-id="6686">矩阵分解协同过滤推荐算法</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/recommand/simrank.html" title="SimRank协同过滤推荐算法" data-book-page-rel-url="ml/recommand/simrank.html" data-book-page-id="6687">SimRank协同过滤推荐算法</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/recomand/spark-factor.html" title="Spark矩阵分解推荐算法" data-book-page-rel-url="ml/recomand/spark-factor.html" data-book-page-id="6688">Spark矩阵分解推荐算法</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/recommand/fm.html" title="分解机(Factorization Machines)推荐算法原理" data-book-page-rel-url="ml/recommand/fm.html" data-book-page-id="6689">分解机(Factorization Machines)推荐算法原理</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/recommand/meituan.html" title="美团推荐算法" data-book-page-rel-url="ml/recommand/meituan.html" data-book-page-id="6690">美团推荐算法</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/recommand/mr-itemcf.html" title="MapReduce ItemCF" data-book-page-rel-url="ml/recommand/mr-itemcf.html" data-book-page-id="6691">MapReduce ItemCF</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/recommand/label-recommand.html" title="基于标签的用户推荐系统" data-book-page-rel-url="ml/recommand/label-recommand.html" data-book-page-id="6692">基于标签的用户推荐系统</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/em/em.html" title="第十七课：EM算法" data-book-page-rel-url="ml/em/em.html" data-book-page-id="6693">第十七课：EM算法</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/bayes.html" title="第十九课：贝叶斯网络" data-book-page-rel-url="ml/bayes.html" data-book-page-id="6694">第十九课：贝叶斯网络</a>
<ul>
<li>
<a class="pjax" href="../../../../book/85/ml/bayes/po-su-bei-xie-si.html" title="朴素贝叶斯" data-book-page-rel-url="ml/bayes/po-su-bei-xie-si.html" data-book-page-id="6695">朴素贝叶斯</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/bayes/scikit-simple-bayes.html" title="scikit-learn朴素贝叶斯" data-book-page-rel-url="ml/bayes/scikit-simple-bayes.html" data-book-page-id="6696">scikit-learn朴素贝叶斯</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/bayes/simple-bayes-real-use.html" title="朴素贝叶斯实际应用" data-book-page-rel-url="ml/bayes/simple-bayes-real-use.html" data-book-page-id="6697">朴素贝叶斯实际应用</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/bayes/simple-bayes-code.html" title="朴素贝叶斯代码" data-book-page-rel-url="ml/bayes/simple-bayes-code.html" data-book-page-id="6698">朴素贝叶斯代码</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/lda/lda.html" title="第二十一课：LDA主题模型" data-book-page-rel-url="ml/lda/lda.html" data-book-page-id="6699">第二十一课：LDA主题模型</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/hmm/hmm.html" title="第二十三课：隐马尔科夫模型HMM" data-book-page-rel-url="ml/hmm/hmm.html" data-book-page-id="6700">第二十三课：隐马尔科夫模型HMM</a>
<ul>
<li>
<a class="pjax" href="../../../../book/85/ml/hmm/hmm-forward-backward.html" title="HMM前向后向算法评估观察序列概率" data-book-page-rel-url="ml/hmm/hmm-forward-backward.html" data-book-page-id="6701">HMM前向后向算法评估观察序列概率</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/hmm/bmwl-hmm.html" title="鲍姆-韦尔奇算法求解HMM参数" data-book-page-rel-url="ml/hmm/bmwl-hmm.html" data-book-page-id="6702">鲍姆-韦尔奇算法求解HMM参数</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/hmm/viterb-hmm.html" title="维特比算法解码隐藏状态序列" data-book-page-rel-url="ml/hmm/viterb-hmm.html" data-book-page-id="6703">维特比算法解码隐藏状态序列</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/hmm/hmmlearn.html" title="用hmmlearn学习隐马尔科夫模型HMM" data-book-page-rel-url="ml/hmm/hmmlearn.html" data-book-page-id="6704">用hmmlearn学习隐马尔科夫模型HMM</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/hmm/markv-mengtekluo.html" title="马尔科夫蒙特卡洛" data-book-page-rel-url="ml/hmm/markv-mengtekluo.html" data-book-page-id="6705">马尔科夫蒙特卡洛</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/crf/crf.html" title="条件随机场CRF" data-book-page-rel-url="ml/crf/crf.html" data-book-page-id="6706">条件随机场CRF</a>
<ul>
<li>
<a class="pjax" href="../../../../book/85/ml/crf/linear-crf.html" title="从随机场到线性链条件随机场" data-book-page-rel-url="ml/crf/linear-crf.html" data-book-page-id="6707">从随机场到线性链条件随机场</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/crf/back-forth.html" title="前向后向算法评估标记序列概率" data-book-page-rel-url="ml/crf/back-forth.html" data-book-page-id="6708">前向后向算法评估标记序列概率</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/crf/crf-viterbi.html" title="维特比算法解码" data-book-page-rel-url="ml/crf/crf-viterbi.html" data-book-page-id="6709">维特比算法解码</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/dl.html" title="第三部分 深度学习" data-book-page-rel-url="dl/dl.html" data-book-page-id="6710">第三部分 深度学习</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/layers/layers.html" title="深度学习层" data-book-page-rel-url="dl/layers/layers.html" data-book-page-id="6711">深度学习层</a>
<ul>
<li>
<a class="pjax" href="../../../../book/85/dl/layers/core.html" title="核心层" data-book-page-rel-url="dl/layers/core.html" data-book-page-id="6712">核心层</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/layers/conv.html" title="卷积层" data-book-page-rel-url="dl/layers/conv.html" data-book-page-id="6713">卷积层</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/layers/pooling.html" title="池化层" data-book-page-rel-url="dl/layers/pooling.html" data-book-page-id="6714">池化层</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/layers/lcnn.html" title="局部连接层" data-book-page-rel-url="dl/layers/lcnn.html" data-book-page-id="6715">局部连接层</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/layers/rnn.html" title="循环层" data-book-page-rel-url="dl/layers/rnn.html" data-book-page-id="6716">循环层</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/layers/ebbedded.html" title="嵌入层" data-book-page-rel-url="dl/layers/ebbedded.html" data-book-page-id="6717">嵌入层</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/layers/merge.html" title="合并层" data-book-page-rel-url="dl/layers/merge.html" data-book-page-id="6718">合并层</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/layers/activation.html" title="高级激活层" data-book-page-rel-url="dl/layers/activation.html" data-book-page-id="6719">高级激活层</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/layers/regular.html" title="归一化层" data-book-page-rel-url="dl/layers/regular.html" data-book-page-id="6720">归一化层</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/layers/nosie.html" title="噪声层" data-book-page-rel-url="dl/layers/nosie.html" data-book-page-id="6721">噪声层</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/layers/wrapper.html" title="层包裹" data-book-page-rel-url="dl/layers/wrapper.html" data-book-page-id="6722">层包裹</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/layers/userdefine.html" title="自定义层" data-book-page-rel-url="dl/layers/userdefine.html" data-book-page-id="6723">自定义层</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/introduction/introduction.html" title="第二十五课：深度学习" data-book-page-rel-url="dl/introduction/introduction.html" data-book-page-id="6724">第二十五课：深度学习</a>
<ul>
<li>
<a class="pjax" href="../../../../book/85/dl/introduction/ji-ben-gai-nian.html" title="基本概念" data-book-page-rel-url="dl/introduction/ji-ben-gai-nian.html" data-book-page-id="6725">基本概念</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/introduction/dnn-fp.html" title="深度神经网络（DNN）模型与前向传播算法" data-book-page-rel-url="dl/introduction/dnn-fp.html" data-book-page-id="6726">深度神经网络（DNN）模型与前向传播算法</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/introduction/dnn-bp.html" title="深度神经网络（DNN）反向传播算法(BP)" data-book-page-rel-url="dl/introduction/dnn-bp.html" data-book-page-id="6727">深度神经网络（DNN）反向传播算法(BP)</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/introduction/back-propagation.html" title="反向传播" data-book-page-rel-url="dl/introduction/back-propagation.html" data-book-page-id="6728">反向传播</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/introduction/READ.html" title="反向传播2" data-book-page-rel-url="dl/introduction/READ.html" data-book-page-id="6729">反向传播2</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/introduction/dnn-loss.html" title="DNN损失函数和激活函数的选择" data-book-page-rel-url="dl/introduction/dnn-loss.html" data-book-page-id="6730">DNN损失函数和激活函数的选择</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/introduction/dnn-normal.html" title="深度神经网络（DNN）的正则化" data-book-page-rel-url="dl/introduction/dnn-normal.html" data-book-page-id="6731">深度神经网络（DNN）的正则化</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/reference.html" title="参考文献" data-book-page-rel-url="dl/reference.html" data-book-page-id="6732">参考文献</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/cnn/introduction.html" title="第二十六课 卷积神 经网络(Convolutional Neural Netowrk)" data-book-page-rel-url="dl/cnn/introduction.html" data-book-page-id="6733">第二十六课 卷积神 经网络(Convolutional Neural Netowrk)</a>
<ul>
<li>
<a class="pjax" href="../../../../book/85/dl/cnn/cnn-arch.html" title="卷积神经网络(CNN)模型结构" data-book-page-rel-url="dl/cnn/cnn-arch.html" data-book-page-id="6734">卷积神经网络(CNN)模型结构</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/cnn/cnn-fp.html" title="卷积神经网络(CNN)前向传播算法" data-book-page-rel-url="dl/cnn/cnn-fp.html" data-book-page-id="6735">卷积神经网络(CNN)前向传播算法</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/cnn/cnn-bp.html" title="卷积神经网络(CNN)反向传播算法" data-book-page-rel-url="dl/cnn/cnn-bp.html" data-book-page-id="6736">卷积神经网络(CNN)反向传播算法</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/gan/gan.html" title="对抗生成网络(Generative Adversarial Networks)" data-book-page-rel-url="dl/gan/gan.html" data-book-page-id="6737">对抗生成网络(Generative Adversarial Networks)</a>
<ul>
<li>
<a class="pjax" href="../../../../book/85/ml/gan/gan-principle.html" title="GAN原理" data-book-page-rel-url="ml/gan/gan-principle.html" data-book-page-id="6738">GAN原理</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/gan/infogan.html" title="InfoGAN" data-book-page-rel-url="dl/gan/infogan.html" data-book-page-id="6739">InfoGAN</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/gan/dcgan.html" title="DCGAN" data-book-page-rel-url="dl/gan/dcgan.html" data-book-page-id="6740">DCGAN</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/gan/vae.html" title="VAE" data-book-page-rel-url="dl/gan/vae.html" data-book-page-id="6741">VAE</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/rbm/rbm.html" title="受限波尔兹曼机" data-book-page-rel-url="dl/rbm/rbm.html" data-book-page-id="6742">受限波尔兹曼机</a>
<ul>
<li>
<a class="pjax" href="../../../../book/85/dl/rbm/rbm-code.html" title="RBM code" data-book-page-rel-url="dl/rbm/rbm-code.html" data-book-page-id="6743">RBM code</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/rbm/dbn.html" title="DBN" data-book-page-rel-url="dl/rbm/dbn.html" data-book-page-id="6744">DBN</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/rbm/rbm-yuanli.html" title="RBM原理" data-book-page-rel-url="dl/rbm/rbm-yuanli.html" data-book-page-id="6745">RBM原理</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/rnn/rnn.html" title="RNN" data-book-page-rel-url="dl/rnn/rnn.html" data-book-page-id="6746">RNN</a>
<ul>
<li>
<a class="pjax" href="../../../../book/85/dl/rnn/bidirectional-rnns.html" title="Bidirectional RNNs" data-book-page-rel-url="dl/rnn/bidirectional-rnns.html" data-book-page-id="6747">Bidirectional RNNs</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/rnn/deep-bidirectional-rnns.html" title="Deep (Bidirectional) RNNs" data-book-page-rel-url="dl/rnn/deep-bidirectional-rnns.html" data-book-page-id="6748">Deep (Bidirectional) RNNs</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/rnn/lstm.html" title="LSTM模型与前向反向传播算法" data-book-page-rel-url="dl/rnn/lstm.html" data-book-page-id="6749">LSTM模型与前向反向传播算法</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/rnn/bptt.html" title="随时间反向传播（BPTT）算法" data-book-page-rel-url="dl/rnn/bptt.html" data-book-page-id="6750">随时间反向传播（BPTT）算法</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/rnn/rnn-bptt.html" title="循环神经网络(RNN)模型与前向反向传播算法" data-book-page-rel-url="dl/rnn/rnn-bptt.html" data-book-page-id="6751">循环神经网络(RNN)模型与前向反向传播算法</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/encoder/encoder.html" title="自动编码器" data-book-page-rel-url="dl/encoder/encoder.html" data-book-page-id="6752">自动编码器</a>
<ul>
<li>
<a class="pjax" href="../../../../book/85/dl/encoder/stack-denoise-encoder.html" title="堆叠降噪自动编码器" data-book-page-rel-url="dl/encoder/stack-denoise-encoder.html" data-book-page-id="6753">堆叠降噪自动编码器</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/encoder/denoise-encoder.html" title="降噪自动编码器" data-book-page-rel-url="dl/encoder/denoise-encoder.html" data-book-page-id="6754">降噪自动编码器</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/encoder/sparse-autoencoder.html" title="sparse自动编码器" data-book-page-rel-url="dl/encoder/sparse-autoencoder.html" data-book-page-id="6755">sparse自动编码器</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/encoder/keras-autoencoder.html" title="Keras自动编码器" data-book-page-rel-url="dl/encoder/keras-autoencoder.html" data-book-page-id="6756">Keras自动编码器</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/word2vec/word2vec.html" title="word2vec" data-book-page-rel-url="dl/word2vec/word2vec.html" data-book-page-id="6757">word2vec</a>
<ul>
<li>
<a class="pjax" href="../../../../book/85/dl/word2vec/cbow-skip-n.html" title="CBOW与Skip-Gram模型基础" data-book-page-rel-url="dl/word2vec/cbow-skip-n.html" data-book-page-id="6758">CBOW与Skip-Gram模型基础</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/word2vec/hierarc-softmax.html" title="基于Hierarchical Softmax的模型" data-book-page-rel-url="dl/word2vec/hierarc-softmax.html" data-book-page-id="6759">基于Hierarchical Softmax的模型</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/word2vec/negative-sampling.html" title="基于Negative Sampling的模型" data-book-page-rel-url="dl/word2vec/negative-sampling.html" data-book-page-id="6760">基于Negative Sampling的模型</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/reinforcement/reinforcement.html" title="增强学习" data-book-page-rel-url="dl/reinforcement/reinforcement.html" data-book-page-id="6761">增强学习</a>
<ul>
<li>
<a class="pjax" href="../../../../book/85/dl/reinforcement/q-learning.html" title="Q-Learning" data-book-page-rel-url="dl/reinforcement/q-learning.html" data-book-page-id="6762">Q-Learning</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/reinforcement/policy-network.html" title="策略网络" data-book-page-rel-url="dl/reinforcement/policy-network.html" data-book-page-id="6763">策略网络</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/reinforcement/banditsuan-fa.html" title="bandit算法" data-book-page-rel-url="dl/reinforcement/banditsuan-fa.html" data-book-page-id="6764">bandit算法</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/reinforcement/meng-te-qia-luo-shu-sou-suo.html" title="蒙特卡洛树搜索" data-book-page-rel-url="dl/reinforcement/meng-te-qia-luo-shu-sou-suo.html" data-book-page-id="6765">蒙特卡洛树搜索</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/reinforcement/multi-bandit.html" title="多臂赌博机(Multi-arm Bandits)" data-book-page-rel-url="dl/reinforcement/multi-bandit.html" data-book-page-id="6766">多臂赌博机(Multi-arm Bandits)</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/reinforcement/mdp.html" title="马尔可夫决策过程MDP" data-book-page-rel-url="dl/reinforcement/mdp.html" data-book-page-id="6767">马尔可夫决策过程MDP</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/reinforcement/dynamic-programming.html" title="动态编程" data-book-page-rel-url="dl/reinforcement/dynamic-programming.html" data-book-page-id="6768">动态编程</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/reinforcement/monte-carlo.html" title="蒙特卡洛方法" data-book-page-rel-url="dl/reinforcement/monte-carlo.html" data-book-page-id="6769">蒙特卡洛方法</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/reinforcement/shi-xu-cha-fen-xue-xi.html" title="时序差分学习" data-book-page-rel-url="dl/reinforcement/shi-xu-cha-fen-xue-xi.html" data-book-page-id="6770">时序差分学习</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/reinforcement/a3csuan-fa.html" title="A3C算法" data-book-page-rel-url="dl/reinforcement/a3csuan-fa.html" data-book-page-id="6771">A3C算法</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/reinforcement/multi-steps-bootstraping.html" title="Multi-steps bootstraping" data-book-page-rel-url="dl/reinforcement/multi-steps-bootstraping.html" data-book-page-id="6772">Multi-steps bootstraping</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/reinforcement/tabular-method.html" title="Planning and Learning with Tabular Methods" data-book-page-rel-url="dl/reinforcement/tabular-method.html" data-book-page-id="6773">Planning and Learning with Tabular Methods</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/reinforcement/dqn.html" title="DQN" data-book-page-rel-url="dl/reinforcement/dqn.html" data-book-page-id="6774">DQN</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/reinforcement/policy-gridient.html" title="Policy Gridient" data-book-page-rel-url="dl/reinforcement/policy-gridient.html" data-book-page-id="6775">Policy Gridient</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/reinforcement/actor-critic.html" title="Actor Critic" data-book-page-rel-url="dl/reinforcement/actor-critic.html" data-book-page-id="6776">Actor Critic</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/reinforcement/ddpg.html" title="DDPG (Deep Deterministic Policy Gradient)" data-book-page-rel-url="dl/reinforcement/ddpg.html" data-book-page-id="6777">DDPG (Deep Deterministic Policy Gradient)</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/reinforcement/ppo.html" title="PPO(Proximal Policy Optimization )" data-book-page-rel-url="dl/reinforcement/ppo.html" data-book-page-id="6778">PPO(Proximal Policy Optimization )</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/reinforcement/alpha-beta.html" title="Alpha-Beta剪枝算法详解" data-book-page-rel-url="dl/reinforcement/alpha-beta.html" data-book-page-id="6779">Alpha-Beta剪枝算法详解</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/evolution/evolution.html" title="进化算法" data-book-page-rel-url="ml/evolution/evolution.html" data-book-page-id="6780">进化算法</a>
<ul>
<li>
<a class="pjax" href="../../../../book/85/ml/evolution/yichuansuanfa.html" title="遗传算法" data-book-page-rel-url="ml/evolution/yichuansuanfa.html" data-book-page-id="6781">遗传算法</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/ml/evolution/evolution-strategy.html" title="进化策略" data-book-page-rel-url="ml/evolution/evolution-strategy.html" data-book-page-id="6782">进化策略</a>
</li>
<li>
<a class="pjax" href="javascript:;" class="uk-link-muted uk-text-muted" title="NEAT" disabled data-book-page-rel-url="ml/evolution/neat.html" data-book-page-id="6783">NEAT</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/85/nlp/nlp.html" title="自然语言处理" data-book-page-rel-url="nlp/nlp.html" data-book-page-id="6784">自然语言处理</a>
<ul>
<li>
<a class="pjax" href="../../../../book/85/nlp/text-mine.html" title="文本挖掘的分词原理" data-book-page-rel-url="nlp/text-mine.html" data-book-page-id="6785">文本挖掘的分词原理</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/nlp/hashtrick.html" title="HashTrick" data-book-page-rel-url="nlp/hashtrick.html" data-book-page-id="6786">HashTrick</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/nlp/tf-idf.html" title="TF-IDF" data-book-page-rel-url="nlp/tf-idf.html" data-book-page-id="6787">TF-IDF</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/nlp/preprocessing.html" title="中文文本挖掘预处理" data-book-page-rel-url="nlp/preprocessing.html" data-book-page-id="6788">中文文本挖掘预处理</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/nlp/english-preprocess.html" title="英文文本挖掘预处理" data-book-page-rel-url="nlp/english-preprocess.html" data-book-page-id="6789">英文文本挖掘预处理</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/nlp/lda/lsi.html" title="潜在语义索引(LSI)" data-book-page-rel-url="nlp/lda/lsi.html" data-book-page-id="6790">潜在语义索引(LSI)</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/nlp/lda/nmf.html" title="非负矩阵分解(NMF)" data-book-page-rel-url="nlp/lda/nmf.html" data-book-page-id="6791">非负矩阵分解(NMF)</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/nlp/lda/lda.html" title="LDA基础" data-book-page-rel-url="nlp/lda/lda.html" data-book-page-id="6792">LDA基础</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/nlp/lda/lda-gibbs.html" title="LDA求解之Gibbs采样算法" data-book-page-rel-url="nlp/lda/lda-gibbs.html" data-book-page-id="6793">LDA求解之Gibbs采样算法</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/nlp/lda/vi-em.html" title="LDA求解之变分推断EM算法" data-book-page-rel-url="nlp/lda/vi-em.html" data-book-page-id="6794">LDA求解之变分推断EM算法</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/nlp/lda/scikit-learn-lda.html" title="scikit-learn LDA主题模型" data-book-page-rel-url="nlp/lda/scikit-learn-lda.html" data-book-page-id="6795">scikit-learn LDA主题模型</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/85/voice/introduction.html" title="语音识别" data-book-page-rel-url="voice/introduction.html" data-book-page-id="6796">语音识别</a>
<ul>
<li>
<a class="pjax" href="../../../../book/85/voice/gmm-hmm.html" title="GMM-HMM" data-book-page-rel-url="voice/gmm-hmm.html" data-book-page-id="6797">GMM-HMM</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/voice/voicemulumd.html" title="目录" data-book-page-rel-url="voice/voicemulumd.html" data-book-page-id="6798">目录</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/85/resources/introduction.html" title="第四部分 学习资源" data-book-page-rel-url="resources/introduction.html" data-book-page-id="6799">第四部分 学习资源</a>
<ul>
<li>
<a class="pjax" href="../../../../book/85/resources/ml/list.html" title="机器学习" data-book-page-rel-url="resources/ml/list.html" data-book-page-id="6800">机器学习</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/resources/rl/list.html" title="强化学习" data-book-page-rel-url="resources/rl/list.html" data-book-page-id="6801">强化学习</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/resources/nlp/list.html" title="自然语言处理" data-book-page-rel-url="resources/nlp/list.html" data-book-page-id="6802">自然语言处理</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/resources/dl/list.html" title="深度学习" data-book-page-rel-url="resources/dl/list.html" data-book-page-id="6803">深度学习</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="javascript:;" class="uk-link-muted uk-text-muted" title="流行网络结构" disabled data-book-page-rel-url="" data-book-page-id="6804">流行网络结构</a>
<ul>
<li>
<a class="pjax" href="../../../../book/85/dl/popnet/mobilenet.html" title="mobilenet" data-book-page-rel-url="dl/popnet/mobilenet.html" data-book-page-id="6805">mobilenet</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/popnet/resnet.html" title="ResNet" data-book-page-rel-url="dl/popnet/resnet.html" data-book-page-id="6806">ResNet</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="javascript:;" class="uk-link-muted uk-text-muted" title="并行学习" disabled data-book-page-rel-url="" data-book-page-id="6807">并行学习</a>
<ul>
<li>
<a class="pjax" href="../../../../book/85/dl/paralleldl/mei-tuan-bing-xing-xue-xi-shi-jian.html" title="美团并行学习实践" data-book-page-rel-url="dl/paralleldl/mei-tuan-bing-xing-xue-xi-shi-jian.html" data-book-page-id="6808">美团并行学习实践</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="javascript:;" class="uk-link-muted uk-text-muted" title="AI应用" disabled data-book-page-rel-url="" data-book-page-id="6809">AI应用</a>
<ul>
<li>
<a class="pjax" href="../../../../book/85/dl/dlapp/mei-tuan-wai-mai-ai-ji-zhu.html" title="美团外卖AI技术" data-book-page-rel-url="dl/dlapp/mei-tuan-wai-mai-ai-ji-zhu.html" data-book-page-id="6810">美团外卖AI技术</a>
</li>
<li>
<a class="pjax" href="../../../../book/85/dl/dlapp/mei-tuan-tui-jian-pai-xu.html" title="美团推荐排序" data-book-page-rel-url="dl/dlapp/mei-tuan-tui-jian-pai-xu.html" data-book-page-id="6811">美团推荐排序</a>
</li>
</ul>
</li>
</ul>
</div>
</div>
<script src="https://cdn.staticfile.net/jquery/1.12.4/jquery.min.js"></script>
<script type="text/javascript" src="../../../../static/components/uikit-2.27.5/js/uikit.reader.js"></script>
<script type="text/javascript" src="../../../../static/components/social-share/social-share.min.js"></script>
<script>(function(){var bp =document.createElement('script');var curProtocol =window.location.protocol.split(':')[0];if (curProtocol ==='https') {bp.src ='https://zz.bdstatic.com/linksubmit/push.js';}
else {bp.src ='http://push.zhanzhang.baidu.com/push.js';}
var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(bp,s);})();</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
<script src="https://cdn.staticfile.net/highlight.js/9.12.0/highlight.min.js"></script>
<script src="https://cdn.staticfile.net/jquery.pjax/2.0.1/jquery.pjax.min.js"></script>
<script src="https://cdn.staticfile.net/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="https://cdn.staticfile.net/uikit/2.27.5/js/components/lightbox.min.js"></script>
<link rel="dns-prefetch" href="../../../..//cdn.mathjax.org" />
<script type="text/x-mathjax-config">
 function initMathJax() {
    var mathId = $("book-content-section")[0];
    MathJax.Hub.Config({
        tex2jax: {skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code','a']},
        showProcessingMessages: false,
        messageStyle: "none"
    });
    MathJax.Hub.Queue(["Typeset",MathJax.Hub,mathId]);
 };
initMathJax();
</script>
<script src='https://cdn.staticfile.net/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML' async></script>
<style>
	.MathJax_Display{display:inline!important;}
</style>
<script type="text/javascript" src="../../../../static/components/js/reader.js"></script>
<script type="text/javascript">var bookId =85;var bookPageId =6631;var bookPageRelUrl ='ml/clean-feature/te-zheng-gong-cheng.html';</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
</body>
</html>