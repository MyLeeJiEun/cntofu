
<!DOCTYPE html>
<html lang="zh-CN">
<head>
<title>聊天机器人教程-PyTorch 1.0 中文文档 & 教程</title>
<meta content='聊天机器人教程,PyTorch 1.0 中文文档 & 教程' name='keywords'>
<meta content='聊天机器人教程,PyTorch 1.0 中文文档 & 教程' name='description'>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta http-equiv="Content-Language" content="zh-CN" />
<meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1, maximum-scale=1, user-scalable=no"../../../../>
<meta name="applicable-device" content="pc,mobile">
<link rel="shortcut icon" href="../../../../favicon.ico" type="image/x-icon" />
<meta name="renderer" content="webkit">
<link rel="stylesheet" href="../../../../static/components/uikit-2.27.5/css/uikit.custom.css">
<link rel="stylesheet" href="../../../../static/components/social-share/social-share.min.css">
<link rel="stylesheet" href="../../../../static/components/highlight/styles/custom.css">
<link rel="stylesheet" href="../../../../static/components/css/base.css">
<link rel="stylesheet" href="../../../../static/components/css/reader.css">
<link rel="stylesheet" href="../../../../static/components/css/markdown.css">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5313208362165053" crossorigin="anonymous"></script>
</head>
<body>
<div class=" book-main-wrap uk-container uk-container-center uk-margin-top ">
<div class="uk-grid">
<div class="uk-width-1-1 reader-wrap ">
<div class=" bottom-nav uk-clearfix ">
<div class="uk-align-left ">
<a href="../../../../book/169/docs/1.0/tut_text.html">
<i class="nav-icon-left uk-icon-small  uk-icon-caret-left"></i>
<span class="">文本</span>
</a>
</div>
<div class="uk-align-right ">
<a href="../../../../book/169/docs/1.0/char_rnn_generation_tutorial.html">
<span class="">使用字符级别特征的 R..</span>
<i class="nav-icon-right uk-icon-small  uk-icon-caret-right"></i>
</a>
</div>
</div>
<div class="uk-text-center">
<h2 class="book-page-title uk-container-center">
<a href="../../../../book/169/index.html">PyTorch 1.0 中文文档 & 教程</a>
<a target="_blank" rel="nofollow" href="https://github.com/apachecn/pytorch-doc-zh" class="uk-icon-button uk-icon-github" title="github项目地址"></a>
</h2>
</div>
<script type="text/javascript" src="../../../../static/components/js/app_intro.js"></script>
<ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-5313208362165053" data-ad-slot="1328047120"></ins>
<script>(adsbygoogle =window.adsbygoogle ||[]).push({});</script>
<hr class="uk-article-divider">
<div class="book-content-section  md-content-section  uk-margin-bottom">
<h1 id="聊天机器人教程">聊天机器人教程</h1>
<blockquote>
<p>译者：<a href="https://github.com/a625687551">毛毛虫</a></p>
</blockquote>
<p><strong>作者</strong>: <a href="https://github.com/MatthewInkawhich">Matthew Inkawhich</a></p>
<p>在本教程中，我们探索了一个好玩和有趣的循环序列到序列的模型用例。我们将用 <a href="https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html">Cornell Movie-Dialogs Corpus</a>处的电影剧本来训练一个简单的聊天机器人。</p>
<p>在人工智能研究领域中对话模型模型是一个非常热门的话题。聊天机器人可以在各种设置中找到，包括客户服务应用和在线帮助。这些机器人通常由基于检索的模型提供支持，这些输出是某些形式问题预先定义的响应。在像公司IT服务台这样高度受限制的领域中，这些模型可能足够了，但是，对于更一般的用例它们不够健壮。教一台机器与多领域的人进行有意义的对话是一个远未解决的研究问题。最近，深度学习热潮已经允许强大的生成模型，如谷歌的神经对话模型 <a href="https://arxiv.org/abs/1506.05869">Neural Conversational Model</a>，这标志着向多领域生成对话模型迈出了一大步。 在本教程中，我们将在PyTorch中实现这种模型。</p>
<p><a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/0b5c97b14a430a501451aadc5b8fcfad.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/0b5c97b14a430a501451aadc5b8fcfad.jpg" alt="bot"></a></p>
<pre><code class="language-py">&gt; hello?
Bot: hello .
&gt; where am I?
Bot: you re in a hospital .
&gt; who are you?
Bot: i m a lawyer .
&gt; how are you doing?
Bot: i m fine .
&gt; are you my friend?
Bot: no .
&gt; you're under arrest
Bot: i m trying to help you !
&gt; i'm just kidding
Bot: i m sorry .
&gt; where are you from?
Bot: san francisco .
&gt; it's time for me to leave
Bot: i know .
&gt; goodbye
Bot: goodbye .

</code></pre>
<p><strong>教程要点</strong></p>
<ul>
<li>对<a href="https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html">Cornell Movie-Dialogs Corpus</a> 数据集的加载和预处理</li>
<li>用 <a href="https://arxiv.org/abs/1508.04025">Luong attention mechanism(s)</a>实现一个sequence-to-sequence模型</li>
<li>使用小批量数据联合训练解码器和编码器模型</li>
<li>实现贪婪搜索解码模块</li>
<li>与训练好的聊天机器人互动</li>
</ul>
<p><strong>Acknowledgements</strong></p>
<p><strong>鸣谢</strong></p>
<p>本教程借用以下来源的代码：</p>
<ol>
<li>Yuan-Kuei Wu’s pytorch-chatbot implementation: <a href="https://github.com/ywk991112/pytorch-chatbot">https://github.com/ywk991112/pytorch-chatbot</a></li>
<li>Sean Robertson’s practical-pytorch seq2seq-translation example: <a href="https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation">https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation</a></li>
<li>FloydHub’s Cornell Movie Corpus preprocessing code: <a href="https://github.com/floydhub/textutil-preprocess-cornell-movie-corpus">https://github.com/floydhub/textutil-preprocess-cornell-movie-corpus</a></li>
</ol>
<h2 id="准备工作">准备工作</h2>
<p>首先，下载数据文件 <a href="https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html">here</a> 并将其放入当前目录下的<code>data/</code>文件夹下</p>
<p>之后，让我们引入一些必须的包。</p>
<pre><code class="language-py">from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from __future__ import unicode_literals

import torch
from torch.jit import script, trace
import torch.nn as nn
from torch import optim
import torch.nn.functional as F
import csv
import random
import re
import os
import unicodedata
import codecs
from io import open
import itertools
import math

USE_CUDA = torch.cuda.is_available()
device = torch.device("cuda" if USE_CUDA else "cpu")

</code></pre>
<h2 id="加载和预处理数据">加载和预处理数据</h2>
<p>下一步就是格式化处理我们的数据文件并加载到我们可以使用的结构中</p>
<p><a href="https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html">Cornell Movie-Dialogs Corpus</a> 是一个丰富的电影角色对话数据集：</p>
<ul>
<li>10,292 对电影角色的220,579 次对话</li>
<li>617部电影中的9,035电影角色</li>
<li>总共304,713中语调</li>
</ul>
<p>这个数据集庞大而多样，在语言形式、时间段、情感上等都有很大的变化。我们希望这种多样性使我们的模型能够适应多种形式的输入和查询。</p>
<p>首先，我们通过数据文件的某些行来查看原始数据的格式</p>
<pre><code class="language-py">corpus_name = "cornell movie-dialogs corpus"
corpus = os.path.join("data", corpus_name)

def printLines(file, n=10):
    with open(file, 'rb') as datafile:
        lines = datafile.readlines()
    for line in lines[:n]:
        print(line)

printLines(os.path.join(corpus, "movie_lines.txt"))

</code></pre>
<p>输出:</p>
<pre><code class="language-py">b'L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n'
b'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\n'
b'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\n'
b'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\n'
b"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\n"
b'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow\n'
b"L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\n"
b'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No\n'
b'L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\'m kidding.  You know how sometimes you just become this "persona"?  And you don\'t know how to quit?\n'
b'L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?\n'

</code></pre>
<h3 id="创建格式化数据文件">创建格式化数据文件</h3>
<p>为了方便起见，我们将创建一个格式良好的数据文件，其中每一行包含一个由<code>tab</code>制表符分隔的查询语句和响应语句对。</p>
<p>以下函数用于解析原始数据文件raw <em>movie_lines.txt</em></p>
<ul>
<li><code>loadLines</code> 将文件的每一行拆分为字段(lineID, characterID, movieID, character, text)组合的字典</li>
<li><code>loadConversations</code> 根据movie_conversations.txt将<code>loadLines</code> 中的每一行数据进行归类</li>
<li><code>extractSentencePairs</code> 从对话中提取一对句子</li>
</ul>
<pre><code class="language-py"># Splits each line of the file into a dictionary of fields
def loadLines(fileName, fields):
    lines = {}
    with open(fileName, 'r', encoding='iso-8859-1') as f:
        for line in f:
            values = line.split(" +++$+++ ")
            # Extract fields
            lineObj = {}
            for i, field in enumerate(fields):
                lineObj[field] = values[i]
            lines[lineObj['lineID']] = lineObj
    return lines

# Groups fields of lines from `loadLines` into conversations based on *movie_conversations.txt*
def loadConversations(fileName, lines, fields):
    conversations = []
    with open(fileName, 'r', encoding='iso-8859-1') as f:
        for line in f:
            values = line.split(" +++$+++ ")
            # Extract fields
            convObj = {}
            for i, field in enumerate(fields):
                convObj[field] = values[i]
            # Convert string to list (convObj["utteranceIDs"] == "['L598485', 'L598486', ...]")
            lineIds = eval(convObj["utteranceIDs"])
            # Reassemble lines
            convObj["lines"] = []
            for lineId in lineIds:
                convObj["lines"].append(lines[lineId])
            conversations.append(convObj)
    return conversations

# Extracts pairs of sentences from conversations
def extractSentencePairs(conversations):
    qa_pairs = []
    for conversation in conversations:
        # Iterate over all the lines of the conversation
        for i in range(len(conversation["lines"]) - 1):  # We ignore the last line (no answer for it)
            inputLine = conversation["lines"][i]["text"].strip()
            targetLine = conversation["lines"][i+1]["text"].strip()
            # Filter wrong samples (if one of the lists is empty)
            if inputLine and targetLine:
                qa_pairs.append([inputLine, targetLine])
    return qa_pairs

</code></pre>
<p>现在我们将调用这些函数来创建文件，我们命名为 <em>formatted_movie_lines.txt</em>.</p>
<pre><code class="language-py"># Define path to new file
datafile = os.path.join(corpus, "formatted_movie_lines.txt")

delimiter = '\t'
# Unescape the delimiter
delimiter = str(codecs.decode(delimiter, "unicode_escape"))

# Initialize lines dict, conversations list, and field ids
lines = {}
conversations = []
MOVIE_LINES_FIELDS = ["lineID", "characterID", "movieID", "character", "text"]
MOVIE_CONVERSATIONS_FIELDS = ["character1ID", "character2ID", "movieID", "utteranceIDs"]

# Load lines and process conversations
print("\nProcessing corpus...")
lines = loadLines(os.path.join(corpus, "movie_lines.txt"), MOVIE_LINES_FIELDS)
print("\nLoading conversations...")
conversations = loadConversations(os.path.join(corpus, "movie_conversations.txt"),
                                  lines, MOVIE_CONVERSATIONS_FIELDS)

# Write new csv file
print("\nWriting newly formatted file...")
with open(datafile, 'w', encoding='utf-8') as outputfile:
    writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\n')
    for pair in extractSentencePairs(conversations):
        writer.writerow(pair)

# Print a sample of lines
print("\nSample lines from file:")
printLines(datafile)

</code></pre>
<p>输出:</p>
<pre><code class="language-py">Processing corpus...

Loading conversations...

Writing newly formatted file...

Sample lines from file:
b"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\tWell, I thought we'd start with pronunciation, if that's okay with you.\n"
b"Well, I thought we'd start with pronunciation, if that's okay with you.\tNot the hacking and gagging and spitting part.  Please.\n"
b"Not the hacking and gagging and spitting part.  Please.\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n"
b"You're asking me out.  That's so cute. What's your name again?\tForget it.\n"
b"No, no, it's my fault -- we didn't have a proper introduction ---\tCameron.\n"
b"Cameron.\tThe thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\n"
b"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\tSeems like she could get a date easy enough...\n"
b'Why?\tUnsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\n'
b"Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\tThat's a shame.\n"
b'Gosh, if only we could find Kat a boyfriend...\tLet me see what I can do.\n'

</code></pre>
<h3 id="加载和清洗数据">加载和清洗数据</h3>
<p>我们下一个任务是创建词汇表并将查询/响应句子对（对话）加载到内存。</p>
<p>注意我们正在处理<strong>词序</strong>，这些词序没有映射到离散数值空间。因此，我们必须通过数据集中的单词来创建一个索引。</p>
<p>为此我们创建了一个<code>Voc</code>类,它会存储从单词到索引的映射、索引到单词的反向映射、每个单词的计数和总单词量。这个类提供向词汇表中添加单词的方法(<code>addWord</code>)、添加所有单词到句子中的方法 (<code>addSentence</code>) 和清洗不常见的单词方法(<code>trim</code>)。更多的数据清洗在后面进行。</p>
<pre><code class="language-py"># Default word tokens
PAD_token = 0  # Used for padding short sentences
SOS_token = 1  # Start-of-sentence token
EOS_token = 2  # End-of-sentence token

class Voc:
    def __init__(self, name):
        self.name = name
        self.trimmed = False
        self.word2index = {}
        self.word2count = {}
        self.index2word = {PAD_token: "PAD", SOS_token: "SOS", EOS_token: "EOS"}
        self.num_words = 3  # Count SOS, EOS, PAD

    def addSentence(self, sentence):
        for word in sentence.split(' '):
            self.addWord(word)

    def addWord(self, word):
        if word not in self.word2index:
            self.word2index[word] = self.num_words
            self.word2count[word] = 1
            self.index2word[self.num_words] = word
            self.num_words += 1
        else:
            self.word2count[word] += 1

    # Remove words below a certain count threshold
    def trim(self, min_count):
        if self.trimmed:
            return
        self.trimmed = True

        keep_words = []

        for k, v in self.word2count.items():
            if v &gt;= min_count:
                keep_words.append(k)

        print('keep_words {} / {} = {:.4f}'.format(
            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)
        ))

        # Reinitialize dictionaries
        self.word2index = {}
        self.word2count = {}
        self.index2word = {PAD_token: "PAD", SOS_token: "SOS", EOS_token: "EOS"}
        self.num_words = 3 # Count default tokens

        for word in keep_words:
            self.addWord(word)

</code></pre>
<p>现在我们可以组装词汇表和查询/响应语句对。在使用数据之前，我们必须做一些预处理。</p>
<p>首先，我们必须使用<code>unicodeToAscii</code>将unicode字符串转换为ASCII。然后，我们应该将所有字母转换为小写字母并清洗掉除基本标点之外的所有非字母字符 (<code>normalizeString</code>)。最后，为了帮助训练收敛，我们将过滤掉长度大于<code>MAX_LENGTH</code> 的句子 (<code>filterPairs</code>)。</p>
<pre><code class="language-py">MAX_LENGTH = 10  # Maximum sentence length to consider

# Turn a Unicode string to plain ASCII, thanks to
# https://stackoverflow.com/a/518232/2809427
def unicodeToAscii(s):
    return ''.join(
        c for c in unicodedata.normalize('NFD', s)
        if unicodedata.category(c) != 'Mn'
    )

# Lowercase, trim, and remove non-letter characters
def normalizeString(s):
    s = unicodeToAscii(s.lower().strip())
    s = re.sub(r"([.!?])", r" \1", s)
    s = re.sub(r"[^a-zA-Z.!?]+", r" ", s)
    s = re.sub(r"\s+", r" ", s).strip()
    return s

# Read query/response pairs and return a voc object
def readVocs(datafile, corpus_name):
    print("Reading lines...")
    # Read the file and split into lines
    lines = open(datafile, encoding='utf-8').\
        read().strip().split('\n')
    # Split every line into pairs and normalize
    pairs = [[normalizeString(s) for s in l.split('\t')] for l in lines]
    voc = Voc(corpus_name)
    return voc, pairs

# Returns True iff both sentences in a pair 'p' are under the MAX_LENGTH threshold
def filterPair(p):
    # Input sequences need to preserve the last word for EOS token
    return len(p[0].split(' ')) &lt; MAX_LENGTH and len(p[1].split(' ')) &lt; MAX_LENGTH

# Filter pairs using filterPair condition
def filterPairs(pairs):
    return [pair for pair in pairs if filterPair(pair)]

# Using the functions defined above, return a populated voc object and pairs list
def loadPrepareData(corpus, corpus_name, datafile, save_dir):
    print("Start preparing training data ...")
    voc, pairs = readVocs(datafile, corpus_name)
    print("Read {!s} sentence pairs".format(len(pairs)))
    pairs = filterPairs(pairs)
    print("Trimmed to {!s} sentence pairs".format(len(pairs)))
    print("Counting words...")
    for pair in pairs:
        voc.addSentence(pair[0])
        voc.addSentence(pair[1])
    print("Counted words:", voc.num_words)
    return voc, pairs

# Load/Assemble voc and pairs
save_dir = os.path.join("data", "save")
voc, pairs = loadPrepareData(corpus, corpus_name, datafile, save_dir)
# Print some pairs to validate
print("\npairs:")
for pair in pairs[:10]:
    print(pair)

</code></pre>
<p>输出：</p>
<pre><code class="language-py">Start preparing training data ...
Reading lines...
Read 221282 sentence pairs
Trimmed to 64271 sentence pairs
Counting words...
Counted words: 18008

pairs:
['there .', 'where ?']
['you have my word . as a gentleman', 'you re sweet .']
['hi .', 'looks like things worked out tonight huh ?']
['you know chastity ?', 'i believe we share an art instructor']
['have fun tonight ?', 'tons']
['well no . . .', 'then that s all you had to say .']
['then that s all you had to say .', 'but']
['but', 'you always been this selfish ?']
['do you listen to this crap ?', 'what crap ?']
['what good stuff ?', 'the real you .']

</code></pre>
<p>另一种有利于让训练更快收敛的策略是去除词汇表中很少使用的单词。减少特征空间也会降低模型学习目标函数的难度。我们通过以下两个步骤完成这个操作:</p>
<ol>
<li>使用<code>voc.trim</code> 函数去除 <code>MIN_COUNT</code>阈值以下单词 。</li>
<li>如果句子中包含词频过小的单词，那么整个句子也被过滤掉。</li>
</ol>
<pre><code class="language-py">MIN_COUNT = 3    # Minimum word count threshold for trimming

def trimRareWords(voc, pairs, MIN_COUNT):
    # Trim words used under the MIN_COUNT from the voc
    voc.trim(MIN_COUNT)
    # Filter out pairs with trimmed words
    keep_pairs = []
    for pair in pairs:
        input_sentence = pair[0]
        output_sentence = pair[1]
        keep_input = True
        keep_output = True
        # Check input sentence
        for word in input_sentence.split(' '):
            if word not in voc.word2index:
                keep_input = False
                break
        # Check output sentence
        for word in output_sentence.split(' '):
            if word not in voc.word2index:
                keep_output = False
                break

        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence
        if keep_input and keep_output:
            keep_pairs.append(pair)

    print("Trimmed from {} pairs to {}, {:.4f} of total".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))
    return keep_pairs

# Trim voc and pairs
pairs = trimRareWords(voc, pairs, MIN_COUNT)

</code></pre>
<p>输出:</p>
<pre><code class="language-py">keep_words 7823 / 18005 = 0.4345
Trimmed from 64271 pairs to 53165, 0.8272 of total

</code></pre>
<h2 id="为模型准备数据">为模型准备数据</h2>
<p>尽管我们已经投入了大量精力来准备和清洗我们的数据变成一个很好的词汇对象和一系列的句子对，但我们的模型最终希望以numerical torch 张量作为输入。 可以在<a href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html">seq2seq translation tutorial</a>中找到为模型准备处理数据的一种方法。 在该教程中，我们使用batch size 大小为1，这意味着我们所要做的就是将句子对中的单词转换为词汇表中的相应索引，并将其提供给模型。</p>
<p>但是，如果你想要加速训练或者想要利用GPU并行计算能力，则需要使用小批量（mini-batches）来训练。</p>
<p>使用小批量（mini-batches）也意味着我们必须注意批量处理中句子长度的变化。 为了容纳同一批次中不同大小的句子，我们将使我们的批量输入张量大小（<em>max_length，batch_size</em>），其中短于<em>max_length</em>的句子在<em>EOS_token</em>之后进行零填充（zero padded）。</p>
<p>如果我们简单地通过将单词转换为索引（<code>indicesFromSentence</code>）和零填充（ zero-pad）将我们的英文句子转换为张量，我们的张量将具有大小（<em>batch_size，max_length</em>），并且索引第一维将在所有时间步骤中返回完整序列。 但是，我们需要沿着时间对我们批量数据进行索引并且包括批量数据中所有序列。 因此，我们将输入批处理大小转换为（<em>max_length，batch_size</em>），以便跨第一维的索引返回批处理中所有句子的时间步长。 我们在<code>zeroPadding</code>函数中隐式处理这个转置。</p>
<p><a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/b2f1969c698070d055c23fc81ab07b1b.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/b2f1969c698070d055c23fc81ab07b1b.jpg" alt="batches"></a></p>
<p><code>inputvar</code>函数处理将句子转换为张量的过程，最终创建正确大小的零填充张量。它还返回批处理中每个序列的长度张量（ tensor of <code>lengths</code>)，长度张量稍后将传递给我们的解码器。</p>
<p><code>outputvar</code>函数执行与<code>inputvar</code>类似的函数，但他不返回长度张量，而是返回二进制mask tensor和最大目标句子长度。二进制mask tensor的大小与输出目标张量的大小相同，但作为<em>PAD_token</em>的每个元素都是0而其他元素都是1。</p>
<p><code>batch2traindata</code>只需要取一批句子对，并使用上述函数返回输入张量和目标张量。</p>
<pre><code class="language-py">def indexesFromSentence(voc, sentence):
    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]

def zeroPadding(l, fillvalue=PAD_token):
    return list(itertools.zip_longest(*l, fillvalue=fillvalue))

def binaryMatrix(l, value=PAD_token):
    m = []
    for i, seq in enumerate(l):
        m.append([])
        for token in seq:
            if token == PAD_token:
                m[i].append(0)
            else:
                m[i].append(1)
    return m

# Returns padded input sequence tensor and lengths
def inputVar(l, voc):
    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]
    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])
    padList = zeroPadding(indexes_batch)
    padVar = torch.LongTensor(padList)
    return padVar, lengths

# Returns padded target sequence tensor, padding mask, and max target length
def outputVar(l, voc):
    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]
    max_target_len = max([len(indexes) for indexes in indexes_batch])
    padList = zeroPadding(indexes_batch)
    mask = binaryMatrix(padList)
    mask = torch.ByteTensor(mask)
    padVar = torch.LongTensor(padList)
    return padVar, mask, max_target_len

# Returns all items for a given batch of pairs
def batch2TrainData(voc, pair_batch):
    pair_batch.sort(key=lambda x: len(x[0].split(" ")), reverse=True)
    input_batch, output_batch = [], []
    for pair in pair_batch:
        input_batch.append(pair[0])
        output_batch.append(pair[1])
    inp, lengths = inputVar(input_batch, voc)
    output, mask, max_target_len = outputVar(output_batch, voc)
    return inp, lengths, output, mask, max_target_len

# Example for validation
small_batch_size = 5
batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])
input_variable, lengths, target_variable, mask, max_target_len = batches

print("input_variable:", input_variable)
print("lengths:", lengths)
print("target_variable:", target_variable)
print("mask:", mask)
print("max_target_len:", max_target_len)

</code></pre>
<p>输出：</p>
<pre><code class="language-py">input_variable: tensor([[ 614,  281,   77,  387,  965],
        [  83,   25,   53,   25, 6430],
        [  11,  697, 5046,  920,    4],
        [1054,   50,   14,  174,    2],
        [  11,    7,    7,    6,    0],
        [   7, 1825,    6,    2,    0],
        [  14,  234,    2,    0,    0],
        [5401,   36,    0,    0,    0],
        [   4,    4,    0,    0,    0],
        [   2,    2,    0,    0,    0]])
lengths: tensor([10, 10,  7,  6,  4])
target_variable: tensor([[  25,    7,    7,  601,   45],
        [ 356,  697,   53,    4,  410],
        [   7, 2182, 1231,    2,  218],
        [   4,    4, 5240,    0,  492],
        [   2,    2,    6,    0,  227],
        [   0,    0,    2,    0,    4],
        [   0,    0,    0,    0,    2]])
mask: tensor([[1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1],
        [1, 1, 1, 0, 1],
        [1, 1, 1, 0, 1],
        [0, 0, 1, 0, 1],
        [0, 0, 0, 0, 1]], dtype=torch.uint8)
max_target_len: 7

</code></pre>
<h2 id="定义模型">定义模型</h2>
<h3 id="seq2seq模型">Seq2Seq模型</h3>
<p>我们聊天机器人的大脑是序列到序列（seq2seq）模型。 seq2seq模型的目标是将可变长度序列作为输入，并使用固定大小的模型将可变长度序列作为输出返回。</p>
<p><a href="https://arxiv.org/abs/1409.3215">Sutskever et al.</a> 发现通过一起使用两个独立的RNN，我们可以完成这项任务。 第一个RNN充当<strong>编码器</strong>，其将可变长度输入序列编码为固定长度上下文向量。 理论上，该上下文向量（RNN的最终隐藏层）将包含关于输入到机器人的查询语句的语义信息。 第二个RNN是一个<strong>解码器</strong>，它接收输入文字和上下文矢量，并返回序列中下一句文字的概率和在下一次迭代中使用的隐藏状态。</p>
<p><a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/32a87cf8d0353ceb0037776f833b92a7.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/32a87cf8d0353ceb0037776f833b92a7.jpg" alt="model"></a></p>
<p>图片来源: <a href="https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_intro/">https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_intro/</a></p>
<h3 id="编码器">编码器</h3>
<p>编码器RNN每次迭代中输入一个语句输出一个token（例如，一个单词），同时在这时间内输出“输出”向量和“隐藏状态”向量。 然后将隐藏状态向量传递到下一步，并记录输出向量。 编码器将其在序列中的每一点处看到的上下文转换为高维空间中的一系列点，解码器将使用这些点为给定任务生成有意义的输出。</p>
<p>我们的编码器的核心是由 <a href="https://arxiv.org/pdf/1406.1078v3.pdf">Cho et al.</a> 等人发明的多层门循环单元。 在2014年，我们将使用GRU的双向变体，这意味着基本上有两个独立的RNN：一个以正常的顺序输入输入序列，另一个以相反的顺序输入输入序列。 每个网络的输出在每个时间步骤求和。 使用双向GRU将为我们提供编码过去和未来上下文的优势。</p>
<p>双向RNN：</p>
<p><a href="https://pytorch.org/tutorials/_images/RNN-bidirectional.png"><a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/c653271eb5fb762482bceb5e2464e680.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/c653271eb5fb762482bceb5e2464e680.jpg" alt="rnn_bidir"></a></a></p>
<p>图片来源: <a href="https://colah.github.io/posts/2015-09-NN-Types-FP/">https://colah.github.io/posts/2015-09-NN-Types-FP/</a></p>
<p>注意:<code>embedding</code>层用于在任意大小的特征空间中对我们的单词索引进行编码。 对于我们的模型，此图层会将每个单词映射到大小为<em>hidden_size</em>的特征空间。 训练后，这些值会被编码成和他们相似的有意义词语。</p>
<p>最后，如果将填充的一批序列传递给RNN模块，我们必须分别使用<code>torch.nn.utils.rnn.pack_padded_sequence</code>和<code>torch.nn.utils.rnn.pad_packed_sequence</code>在RNN传递时分别进行填充和反填充。</p>
<p><strong>计算图:</strong></p>
<blockquote>
<ol>
<li>将单词索引转换为词嵌入 embeddings。</li>
<li>为RNN模块打包填充批次序列。</li>
<li>通过GRU进行前向传播。</li>
<li>反填充。</li>
<li>对双向GRU输出求和。</li>
<li>返回输出和最终隐藏状态。</li>
</ol>
</blockquote>
<p><strong>输入:</strong></p>
<ul>
<li><code>input_seq</code>：一批输入句子; shape =（<em>max_length，batch_size</em>）</li>
<li><code>input_lengths</code>：一批次中每个句子对应的句子长度列表;shape=(<em>batch_size</em>)</li>
<li><code>hidden</code>:隐藏状态; shape =(<em>n_layers x num_directions，batch_size，hidden_size</em>)</li>
</ul>
<p><strong>输出:</strong></p>
<ul>
<li><code>outputs</code>：GRU最后一个隐藏层的输出特征（双向输出之和）; shape =（<em>max_length，batch_size，hidden_size</em>）</li>
<li><code>hidden</code>：从GRU更新隐藏状态; shape =（<em>n_layers x num_directions，batch_size，hidden_size</em>）</li>
</ul>
<pre><code class="language-py">class EncoderRNN(nn.Module):
    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):
        super(EncoderRNN, self).__init__()
        self.n_layers = n_layers
        self.hidden_size = hidden_size
        self.embedding = embedding

        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'
        #   because our input size is a word embedding with number of features == hidden_size
        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,
                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)

    def forward(self, input_seq, input_lengths, hidden=None):
        # Convert word indexes to embeddings
        embedded = self.embedding(input_seq)
        # Pack padded batch of sequences for RNN module
        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)
        # Forward pass through GRU
        outputs, hidden = self.gru(packed, hidden)
        # Unpack padding
        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)
        # Sum bidirectional GRU outputs
        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]
        # Return output and final hidden state
        return outputs, hidden

</code></pre>
<h3 id="解码器">解码器</h3>
<p>解码器RNN以token-by-token的方式生成响应语句。 它使用编码器的上下文向量和内部隐藏状态来生成序列中的下一个单词。 它持续生成单词，直到输出是<em>EOS_token</em>，这个表示句子的结尾。 一个vanilla seq2seq解码器的常见问题是，如果我们只依赖于上下文向量来编码整个输入序列的含义，那么我们很可能会丢失信息。尤其是在处理长输入序列时，这极大地限制了我们的解码器的能力。</p>
<p>为了解决这个问题，,<a href="https://arxiv.org/abs/1409.0473">Bahdanau et al.</a> 等人创建了一种“attention mechanism”，允许解码器关注输入序列的某些部分，而不是在每一步都使用完全固定的上下文。</p>
<p>在一个高的层级中，用解码器的当前隐藏状态和编码器输出来计算注意力。 输出注意力的权重与输入序列具有相同的大小，允许我们将它们乘以编码器输出，给出一个加权和，表示要注意的编码器输出部分。 <a href="https://github.com/spro">Sean Robertson</a> 的图片很好地描述了这一点：</p>
<p><a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/603ac943f18d1acfa71487283e63f35f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/603ac943f18d1acfa71487283e63f35f.jpg" alt="attn2"></a></p>
<p><a href="https://arxiv.org/abs/1508.04025">Luong et al.</a> 通过创造“Global attention”，改善了<a href="https://arxiv.org/abs/1409.0473">Bahdanau et al.</a> 的基础工作。 关键的区别在于，对于“Global attention”，我们考虑所有编码器的隐藏状态，而不是Bahdanau等人的“Local attention”，它只考虑当前步中编码器的隐藏状态。 另一个区别在于，通过“Global attention”，我们仅使用当前步的解码器的隐藏状态来计算注意力权重（或者能量）。 Bahdanau等人的注意力计算需要知道前一步中解码器的状态。 此外，Luong等人提供各种方法来计算编码器输出和解码器输出之间的注意权重（能量），称之为“score functions”：</p>
<p><a href="https://pytorch.org/tutorials/_images/scores.png"><a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/7818f6b40cbd799eddec20743b45fde5.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/7818f6b40cbd799eddec20743b45fde5.jpg" alt="scores"></a></a></p>
<p>其中$h_t $=当前目标解码器状态和$\bar {h} _s $=所有编码器状态。</p>
<p>总体而言，Global attention机制可以通过下图进行总结。 请注意，我们将“Attention Layer”用一个名为<code>Attn</code>的<code>nn.Module</code>来单独实现。 该模块的输出是经过softmax标准化后权重张量的大小（<em>batch_size，1，max_length</em>）。</p>
<p><a href="https://pytorch.org/tutorials/_images/global_attn.png"><a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/c8c749463168f40707b8cd12477a4e3e.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/c8c749463168f40707b8cd12477a4e3e.jpg" alt="global_attn"></a></a></p>
<pre><code class="language-py"># Luong attention layer
class Attn(torch.nn.Module):
    def __init__(self, method, hidden_size):
        super(Attn, self).__init__()
        self.method = method
        if self.method not in ['dot', 'general', 'concat']:
            raise ValueError(self.method, "is not an appropriate attention method.")
        self.hidden_size = hidden_size
        if self.method == 'general':
            self.attn = torch.nn.Linear(self.hidden_size, hidden_size)
        elif self.method == 'concat':
            self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size)
            self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))

    def dot_score(self, hidden, encoder_output):
        return torch.sum(hidden * encoder_output, dim=2)

    def general_score(self, hidden, encoder_output):
        energy = self.attn(encoder_output)
        return torch.sum(hidden * energy, dim=2)

    def concat_score(self, hidden, encoder_output):
        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()
        return torch.sum(self.v * energy, dim=2)

    def forward(self, hidden, encoder_outputs):
        # Calculate the attention weights (energies) based on the given method
        if self.method == 'general':
            attn_energies = self.general_score(hidden, encoder_outputs)
        elif self.method == 'concat':
            attn_energies = self.concat_score(hidden, encoder_outputs)
        elif self.method == 'dot':
            attn_energies = self.dot_score(hidden, encoder_outputs)

        # Transpose max_length and batch_size dimensions
        attn_energies = attn_energies.t()

        # Return the softmax normalized probability scores (with added dimension)
        return F.softmax(attn_energies, dim=1).unsqueeze(1)

</code></pre>
<p>现在我们已经定义了注意力子模块，我们可以实现真实的解码器模型。 对于解码器，我们将每次手动进行一批次的输入。 这意味着我们的词嵌入张量和GRU输出都将具有相同大小（<em>1，batch_size，hidden_size</em>）。</p>
<p><strong>计算图:</strong></p>
<blockquote>
</blockquote>
<p>1.获取当前输入的词嵌入。 2.通过单向GRU进行前向传播。 3.通过2输出的当前GRU计算注意力权重。 4.将注意力权重乘以编码器输出以获得新的“weighted sum”上下文向量。 5.使用Luong eq. 5连接加权上下文向量和GRU输出 6.使用Luong eq.6预测下一个单词（没有softmax）。 7.返回输出和最终隐藏状态。</p>
<p><strong>输入:</strong></p>
<ul>
<li><code>input_step</code>：每一步输入序列批次（一个单词）; shape =（<em>1，batch_size</em>）</li>
<li><code>last_hidden</code>：GRU的最终隐藏层; shape =（<em>n_layers x num_directions，batch_size，hidden_size</em>）</li>
<li><code>encoder_outputs</code>：编码器模型的输出; shape =（<em>max_length，batch_size，hidden_size</em>）</li>
</ul>
<p><strong>输出:</strong></p>
<ul>
<li><code>output</code>: 一个softmax标准化后的张量， 代表了每个单词在解码序列中是下一个输出单词的概率; shape =（<em>batch_size，voc.num_words</em>）</li>
<li><code>hidden</code>: GRU的最终隐藏状态; shape =（<em>n_layers x num_directions，batch_size，hidden_size</em>）</li>
</ul>
<pre><code class="language-py">class LuongAttnDecoderRNN(nn.Module):
    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):
        super(LuongAttnDecoderRNN, self).__init__()

        # Keep for reference
        self.attn_model = attn_model
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.n_layers = n_layers
        self.dropout = dropout

        # Define layers
        self.embedding = embedding
        self.embedding_dropout = nn.Dropout(dropout)
        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))
        self.concat = nn.Linear(hidden_size * 2, hidden_size)
        self.out = nn.Linear(hidden_size, output_size)

        self.attn = Attn(attn_model, hidden_size)

    def forward(self, input_step, last_hidden, encoder_outputs):
        # Note: we run this one step (word) at a time
        # Get embedding of current input word
        embedded = self.embedding(input_step)
        embedded = self.embedding_dropout(embedded)
        # Forward through unidirectional GRU
        rnn_output, hidden = self.gru(embedded, last_hidden)
        # Calculate attention weights from the current GRU output
        attn_weights = self.attn(rnn_output, encoder_outputs)
        # Multiply attention weights to encoder outputs to get new "weighted sum" context vector
        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))
        # Concatenate weighted context vector and GRU output using Luong eq. 5
        rnn_output = rnn_output.squeeze(0)
        context = context.squeeze(1)
        concat_input = torch.cat((rnn_output, context), 1)
        concat_output = torch.tanh(self.concat(concat_input))
        # Predict next word using Luong eq. 6
        output = self.out(concat_output)
        output = F.softmax(output, dim=1)
        # Return output and final hidden state
        return output, hidden

</code></pre>
<h2 id="定义训练步骤">定义训练步骤</h2>
<h3 id="masked-损失">Masked 损失</h3>
<p>由于我们处理的是批量填充序列，因此在计算损失时我们不能简单地考虑张量的所有元素。 我们定义<code>maskNLLLoss</code>可以根据解码器的输出张量、描述目标张量填充的binary mask张量来计算损失。 该损失函数计算与mask tensor中的1对应的元素的平均负对数似然。</p>
<pre><code class="language-py">def maskNLLLoss(inp, target, mask):
    nTotal = mask.sum()
    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))
    loss = crossEntropy.masked_select(mask).mean()
    loss = loss.to(device)
    return loss, nTotal.item()

</code></pre>
<h3 id="单次训练迭代">单次训练迭代</h3>
<p><code>train</code> 函数包含单次训练迭代的算法（单批输入）。</p>
<p>我们将使用一些巧妙的技巧来帮助融合：</p>
<ul>
<li>第一个技巧是使用 <strong>teacher forcing</strong>。 这意味着在一些概率是由<code>teacher_forcing_ratio</code>设置，我们使用当前目标单词作为解码器的下一个输入，而不是使用解码器的当前推测。 该技巧充当解码器的 training wheels，有助于更有效的训练。 然而， teacher forcing可能导致推导中的模型不稳定，因为解码器可能没有足够的机会在训练期间真正地制作自己的输出序列。 因此，我们必须注意我们如何设置<code>teacher_forcing_ratio</code>，同时不要被快速的收敛所迷惑。</li>
<li>我们实现的第二个技巧是<strong>梯度裁剪(gradient clipping)</strong>。 这是一种用于对抗“爆炸梯度（exploding gradient）”问题的常用技术。 本质上，通过将梯度剪切或阈值化到最大值，我们可以防止在损失函数中梯度以指数方式增长并发生溢出（NaN）或者越过梯度陡峭的悬崖。</li>
</ul>
<p><a href="https://pytorch.org/tutorials/_images/grad_clip.png"><a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/35f76328fb2b83228804b30cf4978e40.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/35f76328fb2b83228804b30cf4978e40.jpg" alt="grad_clip"></a></a></p>
<p>图片来源: Goodfellow et al. <em>Deep Learning</em>. 2016. <a href="https://www.deeplearningbook.org/">https://www.deeplearningbook.org/</a></p>
<p><strong>Sequence of Operations:</strong></p>
<p><strong>操作顺序:</strong></p>
<blockquote>
<ol>
<li>通过编码器前向计算整个批次输入。</li>
<li>将解码器输入初始化为SOS_token，将隐藏状态初始化为编码器的最终隐藏状态。</li>
<li>通过解码器一次一步地前向计算输入一批序列。</li>
<li>如果teacher forcing算法：将下一个解码器输入设置为当前目标; 否则：将下一个解码器输入设置为当前解码器输出。</li>
<li>计算并累积损失。</li>
<li>执行反向传播。</li>
<li>裁剪梯度。</li>
<li>更新编码器和解码器模型参数。</li>
</ol>
</blockquote>
<p>注意:</p>
<p>PyTorch的RNN模块（<code>RNN</code>，<code>LSTM</code>，<code>GRU</code>）可以像任何其他非重复层一样使用，只需将整个输入序列（或一批序列）传递给它们。 我们在<code>编码器</code>中使用<code>GRU</code>层就是这样的。 实际情况是，在计算中有一个迭代过程循环计算隐藏状态的每一步。 或者，你每次只运行一个模块。 在这种情况下，我们在训练过程中手动循环遍历序列就像我们必须为<code>解码器</code>模型做的那样。 只要你正确的维护这些模型的模块，就可以非常简单的实现顺序模型。</p>
<pre><code class="language-py">def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,
          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):

    # Zero gradients
    encoder_optimizer.zero_grad()
    decoder_optimizer.zero_grad()

    # Set device options
    input_variable = input_variable.to(device)
    lengths = lengths.to(device)
    target_variable = target_variable.to(device)
    mask = mask.to(device)

    # Initialize variables
    loss = 0
    print_losses = []
    n_totals = 0

    # Forward pass through encoder
    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)

    # Create initial decoder input (start with SOS tokens for each sentence)
    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])
    decoder_input = decoder_input.to(device)

    # Set initial decoder hidden state to the encoder's final hidden state
    decoder_hidden = encoder_hidden[:decoder.n_layers]

    # Determine if we are using teacher forcing this iteration
    use_teacher_forcing = True if random.random() &lt; teacher_forcing_ratio else False

    # Forward batch of sequences through decoder one time step at a time
    if use_teacher_forcing:
        for t in range(max_target_len):
            decoder_output, decoder_hidden = decoder(
                decoder_input, decoder_hidden, encoder_outputs
            )
            # Teacher forcing: next input is current target
            decoder_input = target_variable[t].view(1, -1)
            # Calculate and accumulate loss
            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])
            loss += mask_loss
            print_losses.append(mask_loss.item() * nTotal)
            n_totals += nTotal
    else:
        for t in range(max_target_len):
            decoder_output, decoder_hidden = decoder(
                decoder_input, decoder_hidden, encoder_outputs
            )
            # No teacher forcing: next input is decoder's own current output
            _, topi = decoder_output.topk(1)
            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])
            decoder_input = decoder_input.to(device)
            # Calculate and accumulate loss
            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])
            loss += mask_loss
            print_losses.append(mask_loss.item() * nTotal)
            n_totals += nTotal

    # Perform backpropatation
    loss.backward()

    # Clip gradients: gradients are modified in place
    _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)
    _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)

    # Adjust model weights
    encoder_optimizer.step()
    decoder_optimizer.step()

    return sum(print_losses) / n_totals

</code></pre>
<h3 id="训练迭代">训练迭代</h3>
<p>现在终于将完整的训练步骤与数据结合在一起了。 给定传递的模型，优化器，数据等，<code>trainIters</code>函数负责运行<code>n_iterations</code>的训练。这个功能不言自明，因为我们通过<code>train</code>函数的完成了繁重工作。</p>
<p>需要注意的一点是，当我们保存模型时，我们会保存一个包含编码器和解码器<code>state_dicts</code>（参数）、优化器的state_dicts、损失、迭代等的压缩包。以这种方式保存模型将为我们checkpoint,提供最大的灵活性。 加载checkpoint后，我们将能够使用模型参数进行推理，或者我们可以在我们中断的地方继续训练。</p>
<pre><code class="language-py">def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):

    # Load batches for each iteration
    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])
                      for _ in range(n_iteration)]

    # Initializations
    print('Initializing ...')
    start_iteration = 1
    print_loss = 0
    if loadFilename:
        start_iteration = checkpoint['iteration'] + 1

    # Training loop
    print("Training...")
    for iteration in range(start_iteration, n_iteration + 1):
        training_batch = training_batches[iteration - 1]
        # Extract fields from batch
        input_variable, lengths, target_variable, mask, max_target_len = training_batch

        # Run a training iteration with batch
        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,
                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)
        print_loss += loss

        # Print progress
        if iteration % print_every == 0:
            print_loss_avg = print_loss / print_every
            print("Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}".format(iteration, iteration / n_iteration * 100, print_loss_avg))
            print_loss = 0

        # Save checkpoint
        if (iteration % save_every == 0):
            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))
            if not os.path.exists(directory):
                os.makedirs(directory)
            torch.save({
                'iteration': iteration,
                'en': encoder.state_dict(),
                'de': decoder.state_dict(),
                'en_opt': encoder_optimizer.state_dict(),
                'de_opt': decoder_optimizer.state_dict(),
                'loss': loss,
                'voc_dict': voc.__dict__,
                'embedding': embedding.state_dict()
            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))

</code></pre>
<h2 id="评估定义">评估定义</h2>
<p>在训练模型后，我们希望能够自己与机器人交谈。 首先，我们必须定义我们希望模型如何解码编码输入。</p>
<h3 id="贪婪解码">贪婪解码</h3>
<p>贪婪解码是我们在不使用 teacher forcing时在训练期间使用的解码方法。 换句话说，对于每一步，我们只需从具有最高softmax值的<code>decoder_output</code>中选择单词。 该解码方法在单步长级别上是最佳的。</p>
<p>为了便于贪婪解码操作，我们定义了一个<code>GreedySearchDecoder</code>类。 当运行时，类的实例化对象输入序列（<code>input_seq</code>）的大小是（<em>input_seq length，1</em>），标量输入（<code>input_length</code>）长度的张量和<code>max_length</code>来约束响应句子长度。 使用以下计算图来评估输入句子：</p>
<p><strong>计算图:</strong></p>
<blockquote>
<ol>
<li>通过编码器模型前向计算。</li>
<li>准备编码器的最终隐藏层，作为解码器的第一个隐藏输入。</li>
<li>将解码器的第一个输入初始化为SOS_token。</li>
<li>将初始化张量追加到解码后的单词中。</li>
<li>一次迭代解码一个单词token：
<ol>
<li>通过解码器进行前向计算。</li>
<li>获得最可能的单词token及其softmax分数。</li>
<li>记录token和分数。</li>
<li>准备当前token作为下一个解码器的输入。</li>
</ol> </li>
<li>返回收集到的单词 tokens 和 分数。</li>
</ol>
</blockquote>
<pre><code class="language-py">class GreedySearchDecoder(nn.Module):
    def __init__(self, encoder, decoder):
        super(GreedySearchDecoder, self).__init__()
        self.encoder = encoder
        self.decoder = decoder

    def forward(self, input_seq, input_length, max_length):
        # Forward input through encoder model
        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)
        # Prepare encoder's final hidden layer to be first hidden input to the decoder
        decoder_hidden = encoder_hidden[:decoder.n_layers]
        # Initialize decoder input with SOS_token
        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token
        # Initialize tensors to append decoded words to
        all_tokens = torch.zeros([0], device=device, dtype=torch.long)
        all_scores = torch.zeros([0], device=device)
        # Iteratively decode one word token at a time
        for _ in range(max_length):
            # Forward pass through decoder
            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)
            # Obtain most likely word token and its softmax score
            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)
            # Record token and score
            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)
            all_scores = torch.cat((all_scores, decoder_scores), dim=0)
            # Prepare current token to be next decoder input (add a dimension)
            decoder_input = torch.unsqueeze(decoder_input, 0)
        # Return collections of word tokens and scores
        return all_tokens, all_scores

</code></pre>
<h3 id="评估我们的文本">评估我们的文本</h3>
<p>现在我们已经定义了解码方法，我们可以编写用于评估字符串输入句子的函数。 <code>evaluate</code>函数管理输入句子的低层级处理过程。我们首先使用batch_size == 1将句子格式化为输入批量的单词索引。我们通过将句子的单词转换为相应的索引，并通过转换维度来为我们的模型准备张量。我们还创建了一个 <code>lengths</code> 张量，其中包含输入句子的长度。在这种情况下，<code>lengths</code> 是标量因为我们一次只评估一个句子（batch_size == 1）。接下来，我们使用我们的<code>GreedySearchDecoder</code>实例化后的对象（<code>searcher</code>）获得解码响应句子的张量。最后，我们将响应的索引转换为单词并返回已解码单词的列表。</p>
<p><code>evaluateInput</code>充当聊天机器人的用户接口。调用时，将生成一个输入文本字段，我们可以在其中输入查询语句。在输入我们的输入句子并按Enter后，我们的文本以与训练数据相同的方式标准化，并最终被输入到评估函数以获得解码的输出句子。我们循环这个过程，这样我们可以继续与我们的机器人聊天直到我们输入“q”或“quit”。</p>
<p>最后，如果输入的句子包含一个不在词汇表中的单词，我们会通过打印错误消息并提示用户输入另一个句子来优雅地处理。</p>
<pre><code class="language-py">def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):
    ### Format input sentence as a batch
    # words -&gt; indexes
    indexes_batch = [indexesFromSentence(voc, sentence)]
    # Create lengths tensor
    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])
    # Transpose dimensions of batch to match models' expectations
    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)
    # Use appropriate device
    input_batch = input_batch.to(device)
    lengths = lengths.to(device)
    # Decode sentence with searcher
    tokens, scores = searcher(input_batch, lengths, max_length)
    # indexes -&gt; words
    decoded_words = [voc.index2word[token.item()] for token in tokens]
    return decoded_words

def evaluateInput(encoder, decoder, searcher, voc):
    input_sentence = ''
    while(1):
        try:
            # Get input sentence
            input_sentence = input('&gt; ')
            # Check if it is quit case
            if input_sentence == 'q' or input_sentence == 'quit': break
            # Normalize sentence
            input_sentence = normalizeString(input_sentence)
            # Evaluate sentence
            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)
            # Format and print response sentence
            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]
            print('Bot:', ' '.join(output_words))

        except KeyError:
            print("Error: Encountered unknown word.")

</code></pre>
<h2 id="运行模型">运行模型</h2>
<p>最后，是时候运行我们的模型了！</p>
<p>无论我们是否想要训练或测试聊天机器人模型，我们都必须初始化各个编码器和解码器模型。 在接下来的部分中，我们设置所需要的配置，选择从头开始或设置检查点以从中加载，并构建和初始化模型。 您可以随意使用不同的配置来优化性能。</p>
<pre><code class="language-py"># Configure models
model_name = 'cb_model'
attn_model = 'dot'
#attn_model = 'general'
#attn_model = 'concat'
hidden_size = 500
encoder_n_layers = 2
decoder_n_layers = 2
dropout = 0.1
batch_size = 64

# Set checkpoint to load from; set to None if starting from scratch
loadFilename = None
checkpoint_iter = 4000
#loadFilename = os.path.join(save_dir, model_name, corpus_name,
#                            '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),
#                            '{}_checkpoint.tar'.format(checkpoint_iter))

# Load model if a loadFilename is provided
if loadFilename:
    # If loading on same machine the model was trained on
    checkpoint = torch.load(loadFilename)
    # If loading a model trained on GPU to CPU
    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))
    encoder_sd = checkpoint['en']
    decoder_sd = checkpoint['de']
    encoder_optimizer_sd = checkpoint['en_opt']
    decoder_optimizer_sd = checkpoint['de_opt']
    embedding_sd = checkpoint['embedding']
    voc.__dict__ = checkpoint['voc_dict']

print('Building encoder and decoder ...')
# Initialize word embeddings
embedding = nn.Embedding(voc.num_words, hidden_size)
if loadFilename:
    embedding.load_state_dict(embedding_sd)
# Initialize encoder &amp; decoder models
encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)
decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)
if loadFilename:
    encoder.load_state_dict(encoder_sd)
    decoder.load_state_dict(decoder_sd)
# Use appropriate device
encoder = encoder.to(device)
decoder = decoder.to(device)
print('Models built and ready to go!')

</code></pre>
<p>输出:</p>
<pre><code class="language-py">Building encoder and decoder ...
Models built and ready to go!

</code></pre>
<h3 id="执行训练">执行训练</h3>
<p>如果要训练模型，请运行以下部分。</p>
<p>首先我们设置训练参数，然后初始化我们的优化器，最后我们调用<code>trainIters</code>函数来运行我们的训练迭代。</p>
<pre><code class="language-py"># Configure training/optimization
clip = 50.0
teacher_forcing_ratio = 1.0
learning_rate = 0.0001
decoder_learning_ratio = 5.0
n_iteration = 4000
print_every = 1
save_every = 500

# Ensure dropout layers are in train mode
encoder.train()
decoder.train()

# Initialize optimizers
print('Building optimizers ...')
encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)
decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)
if loadFilename:
    encoder_optimizer.load_state_dict(encoder_optimizer_sd)
    decoder_optimizer.load_state_dict(decoder_optimizer_sd)

# Run training iterations
print("Starting Training!")
trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,
           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,
           print_every, save_every, clip, corpus_name, loadFilename)

</code></pre>
<p>输出:</p>
<pre><code class="language-py">Building optimizers ...
Starting Training!
Initializing ...
Training...
Iteration: 1; Percent complete: 0.0%; Average loss: 8.9717
Iteration: 2; Percent complete: 0.1%; Average loss: 8.8521
Iteration: 3; Percent complete: 0.1%; Average loss: 8.6360
Iteration: 4; Percent complete: 0.1%; Average loss: 8.4234
Iteration: 5; Percent complete: 0.1%; Average loss: 7.9403
Iteration: 6; Percent complete: 0.1%; Average loss: 7.3892
Iteration: 7; Percent complete: 0.2%; Average loss: 7.0589
Iteration: 8; Percent complete: 0.2%; Average loss: 7.0130
Iteration: 9; Percent complete: 0.2%; Average loss: 6.7383
Iteration: 10; Percent complete: 0.2%; Average loss: 6.5343
Iteration: 11; Percent complete: 0.3%; Average loss: 6.2447
Iteration: 12; Percent complete: 0.3%; Average loss: 5.6430
Iteration: 13; Percent complete: 0.3%; Average loss: 5.6374
Iteration: 14; Percent complete: 0.4%; Average loss: 5.4907
Iteration: 15; Percent complete: 0.4%; Average loss: 5.6124
Iteration: 16; Percent complete: 0.4%; Average loss: 5.2904
Iteration: 17; Percent complete: 0.4%; Average loss: 5.2002
Iteration: 18; Percent complete: 0.4%; Average loss: 5.0836
Iteration: 19; Percent complete: 0.5%; Average loss: 4.9366
Iteration: 20; Percent complete: 0.5%; Average loss: 4.8628
Iteration: 21; Percent complete: 0.5%; Average loss: 4.9279
Iteration: 22; Percent complete: 0.5%; Average loss: 5.0988
Iteration: 23; Percent complete: 0.6%; Average loss: 5.1028
Iteration: 24; Percent complete: 0.6%; Average loss: 4.8794
Iteration: 25; Percent complete: 0.6%; Average loss: 5.0407
Iteration: 26; Percent complete: 0.7%; Average loss: 5.0295
Iteration: 27; Percent complete: 0.7%; Average loss: 5.0517
Iteration: 28; Percent complete: 0.7%; Average loss: 5.0007
Iteration: 29; Percent complete: 0.7%; Average loss: 4.9401
Iteration: 30; Percent complete: 0.8%; Average loss: 4.8149
Iteration: 31; Percent complete: 0.8%; Average loss: 4.8139
Iteration: 32; Percent complete: 0.8%; Average loss: 4.6654
Iteration: 33; Percent complete: 0.8%; Average loss: 4.6645
Iteration: 34; Percent complete: 0.9%; Average loss: 4.8487
Iteration: 35; Percent complete: 0.9%; Average loss: 4.6754
Iteration: 36; Percent complete: 0.9%; Average loss: 4.6852
Iteration: 37; Percent complete: 0.9%; Average loss: 4.5751
Iteration: 38; Percent complete: 0.9%; Average loss: 4.5829
Iteration: 39; Percent complete: 1.0%; Average loss: 4.9451
Iteration: 40; Percent complete: 1.0%; Average loss: 4.8213
Iteration: 41; Percent complete: 1.0%; Average loss: 4.7620
Iteration: 42; Percent complete: 1.1%; Average loss: 4.7541
Iteration: 43; Percent complete: 1.1%; Average loss: 4.9032
Iteration: 44; Percent complete: 1.1%; Average loss: 4.7077
Iteration: 45; Percent complete: 1.1%; Average loss: 4.6614
Iteration: 46; Percent complete: 1.1%; Average loss: 4.7222
Iteration: 47; Percent complete: 1.2%; Average loss: 4.5944
Iteration: 48; Percent complete: 1.2%; Average loss: 4.7945
Iteration: 49; Percent complete: 1.2%; Average loss: 4.7767
Iteration: 50; Percent complete: 1.2%; Average loss: 4.4527
Iteration: 51; Percent complete: 1.3%; Average loss: 4.5769
Iteration: 52; Percent complete: 1.3%; Average loss: 4.7469
Iteration: 53; Percent complete: 1.3%; Average loss: 4.6774
Iteration: 54; Percent complete: 1.4%; Average loss: 4.7518
Iteration: 55; Percent complete: 1.4%; Average loss: 4.6182
Iteration: 56; Percent complete: 1.4%; Average loss: 4.7044
Iteration: 57; Percent complete: 1.4%; Average loss: 4.6130
Iteration: 58; Percent complete: 1.5%; Average loss: 4.5904
Iteration: 59; Percent complete: 1.5%; Average loss: 4.6147
Iteration: 60; Percent complete: 1.5%; Average loss: 4.6120
Iteration: 61; Percent complete: 1.5%; Average loss: 4.5591
Iteration: 62; Percent complete: 1.6%; Average loss: 4.5794
Iteration: 63; Percent complete: 1.6%; Average loss: 4.6481
Iteration: 64; Percent complete: 1.6%; Average loss: 4.5223
Iteration: 65; Percent complete: 1.6%; Average loss: 4.4783
Iteration: 66; Percent complete: 1.7%; Average loss: 4.6142
Iteration: 67; Percent complete: 1.7%; Average loss: 4.4324
Iteration: 68; Percent complete: 1.7%; Average loss: 4.4219
Iteration: 69; Percent complete: 1.7%; Average loss: 4.3429
Iteration: 70; Percent complete: 1.8%; Average loss: 4.4445
Iteration: 71; Percent complete: 1.8%; Average loss: 4.4065
Iteration: 72; Percent complete: 1.8%; Average loss: 4.5943
Iteration: 73; Percent complete: 1.8%; Average loss: 4.4968
Iteration: 74; Percent complete: 1.8%; Average loss: 4.4825
Iteration: 75; Percent complete: 1.9%; Average loss: 4.7467
Iteration: 76; Percent complete: 1.9%; Average loss: 4.5209
Iteration: 77; Percent complete: 1.9%; Average loss: 4.5702
Iteration: 78; Percent complete: 1.9%; Average loss: 4.5504
Iteration: 79; Percent complete: 2.0%; Average loss: 4.5743
Iteration: 80; Percent complete: 2.0%; Average loss: 4.4362
Iteration: 81; Percent complete: 2.0%; Average loss: 4.5783
Iteration: 82; Percent complete: 2.1%; Average loss: 4.4552
Iteration: 83; Percent complete: 2.1%; Average loss: 4.5058
Iteration: 84; Percent complete: 2.1%; Average loss: 4.5080
Iteration: 85; Percent complete: 2.1%; Average loss: 4.4323
Iteration: 86; Percent complete: 2.1%; Average loss: 4.4641
Iteration: 87; Percent complete: 2.2%; Average loss: 4.6508
Iteration: 88; Percent complete: 2.2%; Average loss: 4.6689
Iteration: 89; Percent complete: 2.2%; Average loss: 4.2447
Iteration: 90; Percent complete: 2.2%; Average loss: 4.2338
Iteration: 91; Percent complete: 2.3%; Average loss: 4.2885
Iteration: 92; Percent complete: 2.3%; Average loss: 4.6373
Iteration: 93; Percent complete: 2.3%; Average loss: 4.7384
Iteration: 94; Percent complete: 2.4%; Average loss: 4.5983
Iteration: 95; Percent complete: 2.4%; Average loss: 4.6401
Iteration: 96; Percent complete: 2.4%; Average loss: 4.5892
Iteration: 97; Percent complete: 2.4%; Average loss: 4.1389
Iteration: 98; Percent complete: 2.5%; Average loss: 4.4241
Iteration: 99; Percent complete: 2.5%; Average loss: 4.4749
Iteration: 100; Percent complete: 2.5%; Average loss: 4.2317
Iteration: 101; Percent complete: 2.5%; Average loss: 4.4089
Iteration: 102; Percent complete: 2.5%; Average loss: 4.6892
Iteration: 103; Percent complete: 2.6%; Average loss: 4.2630
Iteration: 104; Percent complete: 2.6%; Average loss: 4.2508
Iteration: 105; Percent complete: 2.6%; Average loss: 4.5219
Iteration: 106; Percent complete: 2.6%; Average loss: 4.5305
Iteration: 107; Percent complete: 2.7%; Average loss: 4.3298
Iteration: 108; Percent complete: 2.7%; Average loss: 4.7073
Iteration: 109; Percent complete: 2.7%; Average loss: 4.3004
Iteration: 110; Percent complete: 2.8%; Average loss: 4.3952
Iteration: 111; Percent complete: 2.8%; Average loss: 4.3096
Iteration: 112; Percent complete: 2.8%; Average loss: 4.7122
Iteration: 113; Percent complete: 2.8%; Average loss: 4.4780
Iteration: 114; Percent complete: 2.9%; Average loss: 4.5441
Iteration: 115; Percent complete: 2.9%; Average loss: 4.2960
Iteration: 116; Percent complete: 2.9%; Average loss: 4.5099
Iteration: 117; Percent complete: 2.9%; Average loss: 4.3581
Iteration: 118; Percent complete: 2.9%; Average loss: 4.4584
Iteration: 119; Percent complete: 3.0%; Average loss: 4.2360
Iteration: 120; Percent complete: 3.0%; Average loss: 4.3155
Iteration: 121; Percent complete: 3.0%; Average loss: 4.4440
Iteration: 122; Percent complete: 3.0%; Average loss: 4.5894
Iteration: 123; Percent complete: 3.1%; Average loss: 4.3351
Iteration: 124; Percent complete: 3.1%; Average loss: 4.3746
Iteration: 125; Percent complete: 3.1%; Average loss: 4.3414
Iteration: 126; Percent complete: 3.1%; Average loss: 4.2252
Iteration: 127; Percent complete: 3.2%; Average loss: 4.3677
Iteration: 128; Percent complete: 3.2%; Average loss: 4.3547
Iteration: 129; Percent complete: 3.2%; Average loss: 4.3559
Iteration: 130; Percent complete: 3.2%; Average loss: 4.6573
Iteration: 131; Percent complete: 3.3%; Average loss: 4.1838
Iteration: 132; Percent complete: 3.3%; Average loss: 4.1756
Iteration: 133; Percent complete: 3.3%; Average loss: 4.3297
Iteration: 134; Percent complete: 3.4%; Average loss: 4.0736
Iteration: 135; Percent complete: 3.4%; Average loss: 4.6701
Iteration: 136; Percent complete: 3.4%; Average loss: 4.1686
Iteration: 137; Percent complete: 3.4%; Average loss: 4.2030
Iteration: 138; Percent complete: 3.5%; Average loss: 4.4072
Iteration: 139; Percent complete: 3.5%; Average loss: 4.5601
Iteration: 140; Percent complete: 3.5%; Average loss: 4.2193
Iteration: 141; Percent complete: 3.5%; Average loss: 4.3285
Iteration: 142; Percent complete: 3.5%; Average loss: 4.4945
Iteration: 143; Percent complete: 3.6%; Average loss: 4.1252
Iteration: 144; Percent complete: 3.6%; Average loss: 4.2928
Iteration: 145; Percent complete: 3.6%; Average loss: 4.1889
Iteration: 146; Percent complete: 3.6%; Average loss: 4.3024
Iteration: 147; Percent complete: 3.7%; Average loss: 4.1944
Iteration: 148; Percent complete: 3.7%; Average loss: 4.3375
Iteration: 149; Percent complete: 3.7%; Average loss: 4.4679
Iteration: 150; Percent complete: 3.8%; Average loss: 4.3313
Iteration: 151; Percent complete: 3.8%; Average loss: 3.9579
Iteration: 152; Percent complete: 3.8%; Average loss: 3.9818
Iteration: 153; Percent complete: 3.8%; Average loss: 4.1957
Iteration: 154; Percent complete: 3.9%; Average loss: 4.2789
Iteration: 155; Percent complete: 3.9%; Average loss: 4.5184
Iteration: 156; Percent complete: 3.9%; Average loss: 4.2531
Iteration: 157; Percent complete: 3.9%; Average loss: 4.3260
Iteration: 158; Percent complete: 4.0%; Average loss: 4.1684
Iteration: 159; Percent complete: 4.0%; Average loss: 4.0900
Iteration: 160; Percent complete: 4.0%; Average loss: 4.1219
Iteration: 161; Percent complete: 4.0%; Average loss: 4.1080
Iteration: 162; Percent complete: 4.0%; Average loss: 4.1377
Iteration: 163; Percent complete: 4.1%; Average loss: 4.2920
Iteration: 164; Percent complete: 4.1%; Average loss: 4.1214
Iteration: 165; Percent complete: 4.1%; Average loss: 4.1482
Iteration: 166; Percent complete: 4.2%; Average loss: 4.4033
Iteration: 167; Percent complete: 4.2%; Average loss: 4.0143
Iteration: 168; Percent complete: 4.2%; Average loss: 4.4109
Iteration: 169; Percent complete: 4.2%; Average loss: 4.3594
Iteration: 170; Percent complete: 4.2%; Average loss: 4.4708
Iteration: 171; Percent complete: 4.3%; Average loss: 4.1432
Iteration: 172; Percent complete: 4.3%; Average loss: 4.4049
Iteration: 173; Percent complete: 4.3%; Average loss: 4.2759
Iteration: 174; Percent complete: 4.3%; Average loss: 4.1589
Iteration: 175; Percent complete: 4.4%; Average loss: 4.3259
Iteration: 176; Percent complete: 4.4%; Average loss: 4.2007
Iteration: 177; Percent complete: 4.4%; Average loss: 4.1972
Iteration: 178; Percent complete: 4.5%; Average loss: 3.9117
Iteration: 179; Percent complete: 4.5%; Average loss: 4.3908
Iteration: 180; Percent complete: 4.5%; Average loss: 4.0536
Iteration: 181; Percent complete: 4.5%; Average loss: 4.0614
Iteration: 182; Percent complete: 4.5%; Average loss: 4.2998
Iteration: 183; Percent complete: 4.6%; Average loss: 4.2539
Iteration: 184; Percent complete: 4.6%; Average loss: 4.0711
Iteration: 185; Percent complete: 4.6%; Average loss: 4.1461
Iteration: 186; Percent complete: 4.7%; Average loss: 3.8714
Iteration: 187; Percent complete: 4.7%; Average loss: 3.9925
Iteration: 188; Percent complete: 4.7%; Average loss: 4.2529
Iteration: 189; Percent complete: 4.7%; Average loss: 4.1469
Iteration: 190; Percent complete: 4.8%; Average loss: 4.2735
Iteration: 191; Percent complete: 4.8%; Average loss: 4.1593
Iteration: 192; Percent complete: 4.8%; Average loss: 4.3866
Iteration: 193; Percent complete: 4.8%; Average loss: 4.2076
Iteration: 194; Percent complete: 4.9%; Average loss: 4.4958
Iteration: 195; Percent complete: 4.9%; Average loss: 4.1040
Iteration: 196; Percent complete: 4.9%; Average loss: 4.0899
Iteration: 197; Percent complete: 4.9%; Average loss: 4.0125
Iteration: 198; Percent complete: 5.0%; Average loss: 4.0401
Iteration: 199; Percent complete: 5.0%; Average loss: 4.1501
Iteration: 200; Percent complete: 5.0%; Average loss: 4.2680
Iteration: 201; Percent complete: 5.0%; Average loss: 4.0493
Iteration: 202; Percent complete: 5.1%; Average loss: 4.0258
Iteration: 203; Percent complete: 5.1%; Average loss: 4.3527
Iteration: 204; Percent complete: 5.1%; Average loss: 4.3189
Iteration: 205; Percent complete: 5.1%; Average loss: 4.1491
Iteration: 206; Percent complete: 5.1%; Average loss: 3.9139
Iteration: 207; Percent complete: 5.2%; Average loss: 4.0318
Iteration: 208; Percent complete: 5.2%; Average loss: 4.1470
Iteration: 209; Percent complete: 5.2%; Average loss: 3.9458
Iteration: 210; Percent complete: 5.2%; Average loss: 4.1037
Iteration: 211; Percent complete: 5.3%; Average loss: 3.9207
Iteration: 212; Percent complete: 5.3%; Average loss: 4.1549
Iteration: 213; Percent complete: 5.3%; Average loss: 3.9287
Iteration: 214; Percent complete: 5.3%; Average loss: 4.1011
Iteration: 215; Percent complete: 5.4%; Average loss: 3.8291
Iteration: 216; Percent complete: 5.4%; Average loss: 3.9876
Iteration: 217; Percent complete: 5.4%; Average loss: 4.1672
Iteration: 218; Percent complete: 5.5%; Average loss: 4.1727
Iteration: 219; Percent complete: 5.5%; Average loss: 4.3491
Iteration: 220; Percent complete: 5.5%; Average loss: 4.0052
Iteration: 221; Percent complete: 5.5%; Average loss: 4.0791
Iteration: 222; Percent complete: 5.5%; Average loss: 4.1255
Iteration: 223; Percent complete: 5.6%; Average loss: 4.0544
Iteration: 224; Percent complete: 5.6%; Average loss: 3.8379
Iteration: 225; Percent complete: 5.6%; Average loss: 4.1956
Iteration: 226; Percent complete: 5.7%; Average loss: 4.0552
Iteration: 227; Percent complete: 5.7%; Average loss: 4.3452
Iteration: 228; Percent complete: 5.7%; Average loss: 3.9688
Iteration: 229; Percent complete: 5.7%; Average loss: 3.8466
Iteration: 230; Percent complete: 5.8%; Average loss: 3.8589
Iteration: 231; Percent complete: 5.8%; Average loss: 3.9890
Iteration: 232; Percent complete: 5.8%; Average loss: 3.9500
Iteration: 233; Percent complete: 5.8%; Average loss: 4.1904
Iteration: 234; Percent complete: 5.9%; Average loss: 3.9440
Iteration: 235; Percent complete: 5.9%; Average loss: 4.1642
Iteration: 236; Percent complete: 5.9%; Average loss: 4.0305
Iteration: 237; Percent complete: 5.9%; Average loss: 4.0435
Iteration: 238; Percent complete: 5.9%; Average loss: 3.8137
Iteration: 239; Percent complete: 6.0%; Average loss: 4.0108
Iteration: 240; Percent complete: 6.0%; Average loss: 3.9870
Iteration: 241; Percent complete: 6.0%; Average loss: 4.1772
Iteration: 242; Percent complete: 6.0%; Average loss: 4.1099
Iteration: 243; Percent complete: 6.1%; Average loss: 4.3331
Iteration: 244; Percent complete: 6.1%; Average loss: 4.1136
Iteration: 245; Percent complete: 6.1%; Average loss: 3.9791
Iteration: 246; Percent complete: 6.2%; Average loss: 3.8711
Iteration: 247; Percent complete: 6.2%; Average loss: 4.0778
Iteration: 248; Percent complete: 6.2%; Average loss: 4.3057
Iteration: 249; Percent complete: 6.2%; Average loss: 4.1551
Iteration: 250; Percent complete: 6.2%; Average loss: 4.0561
Iteration: 251; Percent complete: 6.3%; Average loss: 4.0730
Iteration: 252; Percent complete: 6.3%; Average loss: 4.1348
Iteration: 253; Percent complete: 6.3%; Average loss: 4.2022
Iteration: 254; Percent complete: 6.3%; Average loss: 3.9785
Iteration: 255; Percent complete: 6.4%; Average loss: 3.9322
Iteration: 256; Percent complete: 6.4%; Average loss: 4.2772
Iteration: 257; Percent complete: 6.4%; Average loss: 4.1284
Iteration: 258; Percent complete: 6.5%; Average loss: 4.0533
Iteration: 259; Percent complete: 6.5%; Average loss: 3.9609
Iteration: 260; Percent complete: 6.5%; Average loss: 3.9970
Iteration: 261; Percent complete: 6.5%; Average loss: 4.2053
Iteration: 262; Percent complete: 6.6%; Average loss: 3.7393
Iteration: 263; Percent complete: 6.6%; Average loss: 3.9946
Iteration: 264; Percent complete: 6.6%; Average loss: 4.0573
Iteration: 265; Percent complete: 6.6%; Average loss: 4.1438
Iteration: 266; Percent complete: 6.7%; Average loss: 3.9137
Iteration: 267; Percent complete: 6.7%; Average loss: 3.9766
Iteration: 268; Percent complete: 6.7%; Average loss: 3.9470
Iteration: 269; Percent complete: 6.7%; Average loss: 3.9636
Iteration: 270; Percent complete: 6.8%; Average loss: 3.9435
Iteration: 271; Percent complete: 6.8%; Average loss: 4.0961
Iteration: 272; Percent complete: 6.8%; Average loss: 3.9841
Iteration: 273; Percent complete: 6.8%; Average loss: 4.1268
Iteration: 274; Percent complete: 6.9%; Average loss: 3.9016
Iteration: 275; Percent complete: 6.9%; Average loss: 3.9104
Iteration: 276; Percent complete: 6.9%; Average loss: 4.2420
Iteration: 277; Percent complete: 6.9%; Average loss: 3.5340
Iteration: 278; Percent complete: 7.0%; Average loss: 3.8910
Iteration: 279; Percent complete: 7.0%; Average loss: 3.8792
Iteration: 280; Percent complete: 7.0%; Average loss: 4.1170
Iteration: 281; Percent complete: 7.0%; Average loss: 3.7653
Iteration: 282; Percent complete: 7.0%; Average loss: 4.1480
Iteration: 283; Percent complete: 7.1%; Average loss: 3.9134
Iteration: 284; Percent complete: 7.1%; Average loss: 3.4908
Iteration: 285; Percent complete: 7.1%; Average loss: 3.8491
Iteration: 286; Percent complete: 7.1%; Average loss: 4.0041
Iteration: 287; Percent complete: 7.2%; Average loss: 3.6933
Iteration: 288; Percent complete: 7.2%; Average loss: 3.8829
Iteration: 289; Percent complete: 7.2%; Average loss: 4.1592
Iteration: 290; Percent complete: 7.2%; Average loss: 4.0245
Iteration: 291; Percent complete: 7.3%; Average loss: 3.7307
Iteration: 292; Percent complete: 7.3%; Average loss: 3.9195
Iteration: 293; Percent complete: 7.3%; Average loss: 4.1200
Iteration: 294; Percent complete: 7.3%; Average loss: 3.9338
Iteration: 295; Percent complete: 7.4%; Average loss: 3.9722
Iteration: 296; Percent complete: 7.4%; Average loss: 4.0899
Iteration: 297; Percent complete: 7.4%; Average loss: 3.7855
Iteration: 298; Percent complete: 7.4%; Average loss: 4.2284
Iteration: 299; Percent complete: 7.5%; Average loss: 4.0504
Iteration: 300; Percent complete: 7.5%; Average loss: 4.0613
Iteration: 301; Percent complete: 7.5%; Average loss: 3.8873
Iteration: 302; Percent complete: 7.5%; Average loss: 3.8841
Iteration: 303; Percent complete: 7.6%; Average loss: 3.8058
Iteration: 304; Percent complete: 7.6%; Average loss: 4.1760
Iteration: 305; Percent complete: 7.6%; Average loss: 3.9467
Iteration: 306; Percent complete: 7.6%; Average loss: 4.0063
Iteration: 307; Percent complete: 7.7%; Average loss: 3.9969
Iteration: 308; Percent complete: 7.7%; Average loss: 4.1442
Iteration: 309; Percent complete: 7.7%; Average loss: 3.9107
Iteration: 310; Percent complete: 7.8%; Average loss: 3.7964
Iteration: 311; Percent complete: 7.8%; Average loss: 3.9390
Iteration: 312; Percent complete: 7.8%; Average loss: 3.8308
Iteration: 313; Percent complete: 7.8%; Average loss: 3.7199
Iteration: 314; Percent complete: 7.8%; Average loss: 3.9681
Iteration: 315; Percent complete: 7.9%; Average loss: 3.8578
Iteration: 316; Percent complete: 7.9%; Average loss: 3.9486
Iteration: 317; Percent complete: 7.9%; Average loss: 3.7967
Iteration: 318; Percent complete: 8.0%; Average loss: 3.8965
Iteration: 319; Percent complete: 8.0%; Average loss: 3.7745
Iteration: 320; Percent complete: 8.0%; Average loss: 3.7473
Iteration: 321; Percent complete: 8.0%; Average loss: 3.8783
Iteration: 322; Percent complete: 8.1%; Average loss: 4.0011
Iteration: 323; Percent complete: 8.1%; Average loss: 4.1263
Iteration: 324; Percent complete: 8.1%; Average loss: 3.5326
Iteration: 325; Percent complete: 8.1%; Average loss: 3.9985
Iteration: 326; Percent complete: 8.2%; Average loss: 3.8760
Iteration: 327; Percent complete: 8.2%; Average loss: 3.9855
Iteration: 328; Percent complete: 8.2%; Average loss: 3.8733
Iteration: 329; Percent complete: 8.2%; Average loss: 4.0473
Iteration: 330; Percent complete: 8.2%; Average loss: 3.9721
Iteration: 331; Percent complete: 8.3%; Average loss: 3.9639
Iteration: 332; Percent complete: 8.3%; Average loss: 3.8026
Iteration: 333; Percent complete: 8.3%; Average loss: 3.8714
Iteration: 334; Percent complete: 8.3%; Average loss: 4.0038
Iteration: 335; Percent complete: 8.4%; Average loss: 3.6649
Iteration: 336; Percent complete: 8.4%; Average loss: 3.7037
Iteration: 337; Percent complete: 8.4%; Average loss: 4.1008
Iteration: 338; Percent complete: 8.5%; Average loss: 3.8629
Iteration: 339; Percent complete: 8.5%; Average loss: 3.9220
Iteration: 340; Percent complete: 8.5%; Average loss: 3.5953
Iteration: 341; Percent complete: 8.5%; Average loss: 3.8714
Iteration: 342; Percent complete: 8.6%; Average loss: 3.8149
Iteration: 343; Percent complete: 8.6%; Average loss: 3.9895
Iteration: 344; Percent complete: 8.6%; Average loss: 4.0144
Iteration: 345; Percent complete: 8.6%; Average loss: 4.3132
Iteration: 346; Percent complete: 8.6%; Average loss: 3.7997
Iteration: 347; Percent complete: 8.7%; Average loss: 4.0285
Iteration: 348; Percent complete: 8.7%; Average loss: 3.9660
Iteration: 349; Percent complete: 8.7%; Average loss: 4.1557
Iteration: 350; Percent complete: 8.8%; Average loss: 4.0216
Iteration: 351; Percent complete: 8.8%; Average loss: 4.0790
Iteration: 352; Percent complete: 8.8%; Average loss: 3.8840
Iteration: 353; Percent complete: 8.8%; Average loss: 3.9844
Iteration: 354; Percent complete: 8.8%; Average loss: 3.5503
Iteration: 355; Percent complete: 8.9%; Average loss: 3.8337
Iteration: 356; Percent complete: 8.9%; Average loss: 3.8704
Iteration: 357; Percent complete: 8.9%; Average loss: 4.0207
Iteration: 358; Percent complete: 8.9%; Average loss: 3.9529
Iteration: 359; Percent complete: 9.0%; Average loss: 3.9415
Iteration: 360; Percent complete: 9.0%; Average loss: 4.2388
Iteration: 361; Percent complete: 9.0%; Average loss: 3.8879
Iteration: 362; Percent complete: 9.0%; Average loss: 4.0102
Iteration: 363; Percent complete: 9.1%; Average loss: 3.7179
Iteration: 364; Percent complete: 9.1%; Average loss: 4.0275
Iteration: 365; Percent complete: 9.1%; Average loss: 3.9630
Iteration: 366; Percent complete: 9.2%; Average loss: 3.8017
Iteration: 367; Percent complete: 9.2%; Average loss: 4.0135
Iteration: 368; Percent complete: 9.2%; Average loss: 3.8387
Iteration: 369; Percent complete: 9.2%; Average loss: 3.7709
Iteration: 370; Percent complete: 9.2%; Average loss: 3.9343
Iteration: 371; Percent complete: 9.3%; Average loss: 3.5596
Iteration: 372; Percent complete: 9.3%; Average loss: 3.5784
Iteration: 373; Percent complete: 9.3%; Average loss: 3.9005
Iteration: 374; Percent complete: 9.3%; Average loss: 3.9209
Iteration: 375; Percent complete: 9.4%; Average loss: 4.1042
Iteration: 376; Percent complete: 9.4%; Average loss: 3.9750
Iteration: 377; Percent complete: 9.4%; Average loss: 3.8513
Iteration: 378; Percent complete: 9.4%; Average loss: 3.7611
Iteration: 379; Percent complete: 9.5%; Average loss: 3.9822
Iteration: 380; Percent complete: 9.5%; Average loss: 3.9575
Iteration: 381; Percent complete: 9.5%; Average loss: 4.0639
Iteration: 382; Percent complete: 9.6%; Average loss: 3.6902
Iteration: 383; Percent complete: 9.6%; Average loss: 4.1257
Iteration: 384; Percent complete: 9.6%; Average loss: 3.6915
Iteration: 385; Percent complete: 9.6%; Average loss: 3.4405
Iteration: 386; Percent complete: 9.7%; Average loss: 3.8807
Iteration: 387; Percent complete: 9.7%; Average loss: 3.7904
Iteration: 388; Percent complete: 9.7%; Average loss: 3.6995
Iteration: 389; Percent complete: 9.7%; Average loss: 3.8066
Iteration: 390; Percent complete: 9.8%; Average loss: 3.9115
Iteration: 391; Percent complete: 9.8%; Average loss: 3.8799
Iteration: 392; Percent complete: 9.8%; Average loss: 3.6337
Iteration: 393; Percent complete: 9.8%; Average loss: 3.6836
Iteration: 394; Percent complete: 9.8%; Average loss: 3.7636
Iteration: 395; Percent complete: 9.9%; Average loss: 3.6423
Iteration: 396; Percent complete: 9.9%; Average loss: 3.5154
Iteration: 397; Percent complete: 9.9%; Average loss: 3.7928
Iteration: 398; Percent complete: 10.0%; Average loss: 3.9949
Iteration: 399; Percent complete: 10.0%; Average loss: 3.5852
Iteration: 400; Percent complete: 10.0%; Average loss: 3.9150
Iteration: 401; Percent complete: 10.0%; Average loss: 3.9314
Iteration: 402; Percent complete: 10.1%; Average loss: 3.7731
Iteration: 403; Percent complete: 10.1%; Average loss: 3.4910
Iteration: 404; Percent complete: 10.1%; Average loss: 3.8729
Iteration: 405; Percent complete: 10.1%; Average loss: 3.9997
Iteration: 406; Percent complete: 10.2%; Average loss: 3.7262
Iteration: 407; Percent complete: 10.2%; Average loss: 3.9816
Iteration: 408; Percent complete: 10.2%; Average loss: 3.8404
Iteration: 409; Percent complete: 10.2%; Average loss: 4.1739
Iteration: 410; Percent complete: 10.2%; Average loss: 3.8125
Iteration: 411; Percent complete: 10.3%; Average loss: 3.9859
Iteration: 412; Percent complete: 10.3%; Average loss: 3.9420
Iteration: 413; Percent complete: 10.3%; Average loss: 3.6940
Iteration: 414; Percent complete: 10.3%; Average loss: 3.8119
Iteration: 415; Percent complete: 10.4%; Average loss: 3.8037
Iteration: 416; Percent complete: 10.4%; Average loss: 3.6291
Iteration: 417; Percent complete: 10.4%; Average loss: 3.8572
Iteration: 418; Percent complete: 10.4%; Average loss: 3.7687
Iteration: 419; Percent complete: 10.5%; Average loss: 3.8557
Iteration: 420; Percent complete: 10.5%; Average loss: 3.7862
Iteration: 421; Percent complete: 10.5%; Average loss: 3.8662
Iteration: 422; Percent complete: 10.5%; Average loss: 3.9068
Iteration: 423; Percent complete: 10.6%; Average loss: 4.0992
Iteration: 424; Percent complete: 10.6%; Average loss: 3.9112
Iteration: 425; Percent complete: 10.6%; Average loss: 3.7536
Iteration: 426; Percent complete: 10.7%; Average loss: 3.7157
Iteration: 427; Percent complete: 10.7%; Average loss: 3.9345
Iteration: 428; Percent complete: 10.7%; Average loss: 3.8467
Iteration: 429; Percent complete: 10.7%; Average loss: 3.6694
Iteration: 430; Percent complete: 10.8%; Average loss: 3.9218
Iteration: 431; Percent complete: 10.8%; Average loss: 3.9447
Iteration: 432; Percent complete: 10.8%; Average loss: 3.8068
Iteration: 433; Percent complete: 10.8%; Average loss: 3.8930
Iteration: 434; Percent complete: 10.8%; Average loss: 3.5453
Iteration: 435; Percent complete: 10.9%; Average loss: 3.5276
Iteration: 436; Percent complete: 10.9%; Average loss: 3.6158
Iteration: 437; Percent complete: 10.9%; Average loss: 3.9393
Iteration: 438; Percent complete: 10.9%; Average loss: 3.9685
Iteration: 439; Percent complete: 11.0%; Average loss: 4.0207
Iteration: 440; Percent complete: 11.0%; Average loss: 3.8926
Iteration: 441; Percent complete: 11.0%; Average loss: 3.7586
Iteration: 442; Percent complete: 11.1%; Average loss: 3.6520
Iteration: 443; Percent complete: 11.1%; Average loss: 3.5789
Iteration: 444; Percent complete: 11.1%; Average loss: 3.6692
Iteration: 445; Percent complete: 11.1%; Average loss: 3.6447
Iteration: 446; Percent complete: 11.2%; Average loss: 3.8651
Iteration: 447; Percent complete: 11.2%; Average loss: 3.5822
Iteration: 448; Percent complete: 11.2%; Average loss: 3.8130
Iteration: 449; Percent complete: 11.2%; Average loss: 3.8297
Iteration: 450; Percent complete: 11.2%; Average loss: 3.8652
Iteration: 451; Percent complete: 11.3%; Average loss: 3.7777
Iteration: 452; Percent complete: 11.3%; Average loss: 3.5719
Iteration: 453; Percent complete: 11.3%; Average loss: 3.6373
Iteration: 454; Percent complete: 11.3%; Average loss: 3.6458
Iteration: 455; Percent complete: 11.4%; Average loss: 3.5769
Iteration: 456; Percent complete: 11.4%; Average loss: 3.8366
Iteration: 457; Percent complete: 11.4%; Average loss: 3.7184
Iteration: 458; Percent complete: 11.5%; Average loss: 3.7776
Iteration: 459; Percent complete: 11.5%; Average loss: 4.0468
Iteration: 460; Percent complete: 11.5%; Average loss: 3.6377
Iteration: 461; Percent complete: 11.5%; Average loss: 3.7295
Iteration: 462; Percent complete: 11.6%; Average loss: 3.6649
Iteration: 463; Percent complete: 11.6%; Average loss: 3.8361
Iteration: 464; Percent complete: 11.6%; Average loss: 3.9704
Iteration: 465; Percent complete: 11.6%; Average loss: 4.1443
Iteration: 466; Percent complete: 11.7%; Average loss: 4.1027
Iteration: 467; Percent complete: 11.7%; Average loss: 3.9759
Iteration: 468; Percent complete: 11.7%; Average loss: 3.7751
Iteration: 469; Percent complete: 11.7%; Average loss: 3.7219
Iteration: 470; Percent complete: 11.8%; Average loss: 3.6162
Iteration: 471; Percent complete: 11.8%; Average loss: 3.6991
Iteration: 472; Percent complete: 11.8%; Average loss: 3.7891
Iteration: 473; Percent complete: 11.8%; Average loss: 3.8827
Iteration: 474; Percent complete: 11.8%; Average loss: 3.7488
Iteration: 475; Percent complete: 11.9%; Average loss: 3.7549
Iteration: 476; Percent complete: 11.9%; Average loss: 3.6530
Iteration: 477; Percent complete: 11.9%; Average loss: 3.7783
Iteration: 478; Percent complete: 11.9%; Average loss: 3.9091
Iteration: 479; Percent complete: 12.0%; Average loss: 3.7925
Iteration: 480; Percent complete: 12.0%; Average loss: 3.6856
Iteration: 481; Percent complete: 12.0%; Average loss: 3.7511
Iteration: 482; Percent complete: 12.0%; Average loss: 3.8710
Iteration: 483; Percent complete: 12.1%; Average loss: 3.6753
Iteration: 484; Percent complete: 12.1%; Average loss: 3.6560
Iteration: 485; Percent complete: 12.1%; Average loss: 3.8460
Iteration: 486; Percent complete: 12.2%; Average loss: 4.0778
Iteration: 487; Percent complete: 12.2%; Average loss: 3.5532
Iteration: 488; Percent complete: 12.2%; Average loss: 3.7296
Iteration: 489; Percent complete: 12.2%; Average loss: 3.8907
Iteration: 490; Percent complete: 12.2%; Average loss: 3.8339
Iteration: 491; Percent complete: 12.3%; Average loss: 3.8718
Iteration: 492; Percent complete: 12.3%; Average loss: 3.8679
Iteration: 493; Percent complete: 12.3%; Average loss: 3.7897
Iteration: 494; Percent complete: 12.3%; Average loss: 3.6723
Iteration: 495; Percent complete: 12.4%; Average loss: 3.7722
Iteration: 496; Percent complete: 12.4%; Average loss: 3.7809
Iteration: 497; Percent complete: 12.4%; Average loss: 3.5971
Iteration: 498; Percent complete: 12.4%; Average loss: 3.7290
Iteration: 499; Percent complete: 12.5%; Average loss: 3.9975
Iteration: 500; Percent complete: 12.5%; Average loss: 3.8210
Iteration: 501; Percent complete: 12.5%; Average loss: 3.8155
Iteration: 502; Percent complete: 12.6%; Average loss: 3.4825
Iteration: 503; Percent complete: 12.6%; Average loss: 3.6385
Iteration: 504; Percent complete: 12.6%; Average loss: 3.7699
Iteration: 505; Percent complete: 12.6%; Average loss: 3.5448
Iteration: 506; Percent complete: 12.7%; Average loss: 3.7796
Iteration: 507; Percent complete: 12.7%; Average loss: 3.8116
Iteration: 508; Percent complete: 12.7%; Average loss: 3.7971
Iteration: 509; Percent complete: 12.7%; Average loss: 3.7767
Iteration: 510; Percent complete: 12.8%; Average loss: 3.7537
Iteration: 511; Percent complete: 12.8%; Average loss: 3.6228
Iteration: 512; Percent complete: 12.8%; Average loss: 3.9341
Iteration: 513; Percent complete: 12.8%; Average loss: 3.8303
Iteration: 514; Percent complete: 12.8%; Average loss: 3.9985
Iteration: 515; Percent complete: 12.9%; Average loss: 3.6326
Iteration: 516; Percent complete: 12.9%; Average loss: 3.9068
Iteration: 517; Percent complete: 12.9%; Average loss: 3.7940
Iteration: 518; Percent complete: 13.0%; Average loss: 3.7433
Iteration: 519; Percent complete: 13.0%; Average loss: 3.9661
Iteration: 520; Percent complete: 13.0%; Average loss: 3.6564
Iteration: 521; Percent complete: 13.0%; Average loss: 3.8278
Iteration: 522; Percent complete: 13.1%; Average loss: 3.8361
Iteration: 523; Percent complete: 13.1%; Average loss: 3.5481
Iteration: 524; Percent complete: 13.1%; Average loss: 3.6035
Iteration: 525; Percent complete: 13.1%; Average loss: 3.8090
Iteration: 526; Percent complete: 13.2%; Average loss: 3.6949
Iteration: 527; Percent complete: 13.2%; Average loss: 3.7893
Iteration: 528; Percent complete: 13.2%; Average loss: 3.5766
Iteration: 529; Percent complete: 13.2%; Average loss: 3.5775
Iteration: 530; Percent complete: 13.2%; Average loss: 3.9601
Iteration: 531; Percent complete: 13.3%; Average loss: 3.6468
Iteration: 532; Percent complete: 13.3%; Average loss: 3.7376
Iteration: 533; Percent complete: 13.3%; Average loss: 3.5185
Iteration: 534; Percent complete: 13.4%; Average loss: 3.5733
Iteration: 535; Percent complete: 13.4%; Average loss: 3.5991
Iteration: 536; Percent complete: 13.4%; Average loss: 3.8449
Iteration: 537; Percent complete: 13.4%; Average loss: 3.7074
Iteration: 538; Percent complete: 13.5%; Average loss: 3.8023
Iteration: 539; Percent complete: 13.5%; Average loss: 3.8198
Iteration: 540; Percent complete: 13.5%; Average loss: 3.5990
Iteration: 541; Percent complete: 13.5%; Average loss: 3.7893
Iteration: 542; Percent complete: 13.6%; Average loss: 3.7010
Iteration: 543; Percent complete: 13.6%; Average loss: 4.0502
Iteration: 544; Percent complete: 13.6%; Average loss: 3.6867
Iteration: 545; Percent complete: 13.6%; Average loss: 3.3169
Iteration: 546; Percent complete: 13.7%; Average loss: 3.6549
Iteration: 547; Percent complete: 13.7%; Average loss: 3.6206
Iteration: 548; Percent complete: 13.7%; Average loss: 3.8818
Iteration: 549; Percent complete: 13.7%; Average loss: 3.7617
Iteration: 550; Percent complete: 13.8%; Average loss: 3.6921
Iteration: 551; Percent complete: 13.8%; Average loss: 3.8400
Iteration: 552; Percent complete: 13.8%; Average loss: 3.6079
Iteration: 553; Percent complete: 13.8%; Average loss: 3.8986
Iteration: 554; Percent complete: 13.9%; Average loss: 3.8411
Iteration: 555; Percent complete: 13.9%; Average loss: 3.6539
Iteration: 556; Percent complete: 13.9%; Average loss: 3.5988
Iteration: 557; Percent complete: 13.9%; Average loss: 3.8247
Iteration: 558; Percent complete: 14.0%; Average loss: 3.7369
Iteration: 559; Percent complete: 14.0%; Average loss: 3.8180
Iteration: 560; Percent complete: 14.0%; Average loss: 3.7970
Iteration: 561; Percent complete: 14.0%; Average loss: 3.7486
Iteration: 562; Percent complete: 14.1%; Average loss: 3.8791
Iteration: 563; Percent complete: 14.1%; Average loss: 3.4945
Iteration: 564; Percent complete: 14.1%; Average loss: 3.7619
Iteration: 565; Percent complete: 14.1%; Average loss: 3.7745
Iteration: 566; Percent complete: 14.1%; Average loss: 3.5081
Iteration: 567; Percent complete: 14.2%; Average loss: 3.5076
Iteration: 568; Percent complete: 14.2%; Average loss: 3.7228
Iteration: 569; Percent complete: 14.2%; Average loss: 3.6408
Iteration: 570; Percent complete: 14.2%; Average loss: 3.3930
Iteration: 571; Percent complete: 14.3%; Average loss: 3.7081
Iteration: 572; Percent complete: 14.3%; Average loss: 3.7083
Iteration: 573; Percent complete: 14.3%; Average loss: 3.7165
Iteration: 574; Percent complete: 14.3%; Average loss: 3.6226
Iteration: 575; Percent complete: 14.4%; Average loss: 3.8727
Iteration: 576; Percent complete: 14.4%; Average loss: 3.8501
Iteration: 577; Percent complete: 14.4%; Average loss: 3.7943
Iteration: 578; Percent complete: 14.4%; Average loss: 3.7059
Iteration: 579; Percent complete: 14.5%; Average loss: 3.4931
Iteration: 580; Percent complete: 14.5%; Average loss: 3.3854
Iteration: 581; Percent complete: 14.5%; Average loss: 3.9041
Iteration: 582; Percent complete: 14.5%; Average loss: 3.6568
Iteration: 583; Percent complete: 14.6%; Average loss: 3.6157
Iteration: 584; Percent complete: 14.6%; Average loss: 3.7490
Iteration: 585; Percent complete: 14.6%; Average loss: 3.7024
Iteration: 586; Percent complete: 14.6%; Average loss: 3.5887
Iteration: 587; Percent complete: 14.7%; Average loss: 3.6320
Iteration: 588; Percent complete: 14.7%; Average loss: 3.7465
Iteration: 589; Percent complete: 14.7%; Average loss: 3.7720
Iteration: 590; Percent complete: 14.8%; Average loss: 3.7187
Iteration: 591; Percent complete: 14.8%; Average loss: 3.1790
Iteration: 592; Percent complete: 14.8%; Average loss: 3.7377
Iteration: 593; Percent complete: 14.8%; Average loss: 3.7778
Iteration: 594; Percent complete: 14.8%; Average loss: 3.6722
Iteration: 595; Percent complete: 14.9%; Average loss: 3.8500
Iteration: 596; Percent complete: 14.9%; Average loss: 3.6174
Iteration: 597; Percent complete: 14.9%; Average loss: 3.7075
Iteration: 598; Percent complete: 14.9%; Average loss: 3.5355
Iteration: 599; Percent complete: 15.0%; Average loss: 3.6380
Iteration: 600; Percent complete: 15.0%; Average loss: 3.7724
Iteration: 601; Percent complete: 15.0%; Average loss: 3.4950
Iteration: 602; Percent complete: 15.0%; Average loss: 3.6070
Iteration: 603; Percent complete: 15.1%; Average loss: 3.7670
Iteration: 604; Percent complete: 15.1%; Average loss: 3.7851
Iteration: 605; Percent complete: 15.1%; Average loss: 3.4809
Iteration: 606; Percent complete: 15.2%; Average loss: 3.5961
Iteration: 607; Percent complete: 15.2%; Average loss: 3.8061
Iteration: 608; Percent complete: 15.2%; Average loss: 3.8477
Iteration: 609; Percent complete: 15.2%; Average loss: 3.5346
Iteration: 610; Percent complete: 15.2%; Average loss: 3.6985
Iteration: 611; Percent complete: 15.3%; Average loss: 3.5933
Iteration: 612; Percent complete: 15.3%; Average loss: 3.6340
Iteration: 613; Percent complete: 15.3%; Average loss: 3.4956
Iteration: 614; Percent complete: 15.3%; Average loss: 3.9414
Iteration: 615; Percent complete: 15.4%; Average loss: 3.8771
Iteration: 616; Percent complete: 15.4%; Average loss: 3.6987
Iteration: 617; Percent complete: 15.4%; Average loss: 3.7928
Iteration: 618; Percent complete: 15.4%; Average loss: 3.7381
Iteration: 619; Percent complete: 15.5%; Average loss: 3.4584
Iteration: 620; Percent complete: 15.5%; Average loss: 3.5053
Iteration: 621; Percent complete: 15.5%; Average loss: 3.7473
Iteration: 622; Percent complete: 15.6%; Average loss: 3.6375
Iteration: 623; Percent complete: 15.6%; Average loss: 3.8684
Iteration: 624; Percent complete: 15.6%; Average loss: 3.8001
Iteration: 625; Percent complete: 15.6%; Average loss: 3.3583
Iteration: 626; Percent complete: 15.7%; Average loss: 3.5143
Iteration: 627; Percent complete: 15.7%; Average loss: 3.5727
Iteration: 628; Percent complete: 15.7%; Average loss: 3.4051
Iteration: 629; Percent complete: 15.7%; Average loss: 3.5350
Iteration: 630; Percent complete: 15.8%; Average loss: 3.6365
Iteration: 631; Percent complete: 15.8%; Average loss: 3.6950
Iteration: 632; Percent complete: 15.8%; Average loss: 3.7169
Iteration: 633; Percent complete: 15.8%; Average loss: 3.6180
Iteration: 634; Percent complete: 15.8%; Average loss: 3.7794
Iteration: 635; Percent complete: 15.9%; Average loss: 3.8577
Iteration: 636; Percent complete: 15.9%; Average loss: 3.9380
Iteration: 637; Percent complete: 15.9%; Average loss: 3.6676
Iteration: 638; Percent complete: 16.0%; Average loss: 3.6855
Iteration: 639; Percent complete: 16.0%; Average loss: 3.6588
Iteration: 640; Percent complete: 16.0%; Average loss: 3.7216
Iteration: 641; Percent complete: 16.0%; Average loss: 3.5801
Iteration: 642; Percent complete: 16.1%; Average loss: 3.8569
Iteration: 643; Percent complete: 16.1%; Average loss: 3.6985
Iteration: 644; Percent complete: 16.1%; Average loss: 3.6474
Iteration: 645; Percent complete: 16.1%; Average loss: 3.9993
Iteration: 646; Percent complete: 16.2%; Average loss: 3.4071
Iteration: 647; Percent complete: 16.2%; Average loss: 3.5342
Iteration: 648; Percent complete: 16.2%; Average loss: 3.7269
Iteration: 649; Percent complete: 16.2%; Average loss: 3.6078
Iteration: 650; Percent complete: 16.2%; Average loss: 3.6668
Iteration: 651; Percent complete: 16.3%; Average loss: 3.6261
Iteration: 652; Percent complete: 16.3%; Average loss: 3.3969
Iteration: 653; Percent complete: 16.3%; Average loss: 3.6889
Iteration: 654; Percent complete: 16.4%; Average loss: 3.7844
Iteration: 655; Percent complete: 16.4%; Average loss: 3.5658
Iteration: 656; Percent complete: 16.4%; Average loss: 3.4151
Iteration: 657; Percent complete: 16.4%; Average loss: 3.8214
Iteration: 658; Percent complete: 16.4%; Average loss: 3.7584
Iteration: 659; Percent complete: 16.5%; Average loss: 3.5224
Iteration: 660; Percent complete: 16.5%; Average loss: 3.8510
Iteration: 661; Percent complete: 16.5%; Average loss: 3.6627
Iteration: 662; Percent complete: 16.6%; Average loss: 3.5533
Iteration: 663; Percent complete: 16.6%; Average loss: 3.6711
Iteration: 664; Percent complete: 16.6%; Average loss: 3.6634
Iteration: 665; Percent complete: 16.6%; Average loss: 3.6791
Iteration: 666; Percent complete: 16.7%; Average loss: 3.7154
Iteration: 667; Percent complete: 16.7%; Average loss: 3.7941
Iteration: 668; Percent complete: 16.7%; Average loss: 3.5511
Iteration: 669; Percent complete: 16.7%; Average loss: 3.6178
Iteration: 670; Percent complete: 16.8%; Average loss: 3.7721
Iteration: 671; Percent complete: 16.8%; Average loss: 3.3845
Iteration: 672; Percent complete: 16.8%; Average loss: 3.4929
Iteration: 673; Percent complete: 16.8%; Average loss: 3.6514
Iteration: 674; Percent complete: 16.9%; Average loss: 3.7860
Iteration: 675; Percent complete: 16.9%; Average loss: 3.8767
Iteration: 676; Percent complete: 16.9%; Average loss: 3.6906
Iteration: 677; Percent complete: 16.9%; Average loss: 3.5207
Iteration: 678; Percent complete: 17.0%; Average loss: 3.5382
Iteration: 679; Percent complete: 17.0%; Average loss: 3.5114
Iteration: 680; Percent complete: 17.0%; Average loss: 3.4834
Iteration: 681; Percent complete: 17.0%; Average loss: 3.3778
Iteration: 682; Percent complete: 17.1%; Average loss: 3.9384
Iteration: 683; Percent complete: 17.1%; Average loss: 3.8102
Iteration: 684; Percent complete: 17.1%; Average loss: 3.6572
Iteration: 685; Percent complete: 17.1%; Average loss: 3.4872
Iteration: 686; Percent complete: 17.2%; Average loss: 3.8682
Iteration: 687; Percent complete: 17.2%; Average loss: 3.5840
Iteration: 688; Percent complete: 17.2%; Average loss: 3.5547
Iteration: 689; Percent complete: 17.2%; Average loss: 3.6038
Iteration: 690; Percent complete: 17.2%; Average loss: 3.7427
Iteration: 691; Percent complete: 17.3%; Average loss: 3.7686
Iteration: 692; Percent complete: 17.3%; Average loss: 3.6312
Iteration: 693; Percent complete: 17.3%; Average loss: 3.5735
Iteration: 694; Percent complete: 17.3%; Average loss: 3.6773
Iteration: 695; Percent complete: 17.4%; Average loss: 3.6137
Iteration: 696; Percent complete: 17.4%; Average loss: 3.6667
Iteration: 697; Percent complete: 17.4%; Average loss: 3.4786
Iteration: 698; Percent complete: 17.4%; Average loss: 3.6050
Iteration: 699; Percent complete: 17.5%; Average loss: 3.5791
Iteration: 700; Percent complete: 17.5%; Average loss: 3.6784
Iteration: 701; Percent complete: 17.5%; Average loss: 3.8551
Iteration: 702; Percent complete: 17.5%; Average loss: 3.7108
Iteration: 703; Percent complete: 17.6%; Average loss: 3.3580
Iteration: 704; Percent complete: 17.6%; Average loss: 3.6564
Iteration: 705; Percent complete: 17.6%; Average loss: 3.7138
Iteration: 706; Percent complete: 17.6%; Average loss: 3.3509
Iteration: 707; Percent complete: 17.7%; Average loss: 3.7810
Iteration: 708; Percent complete: 17.7%; Average loss: 3.5728
Iteration: 709; Percent complete: 17.7%; Average loss: 3.3741
Iteration: 710; Percent complete: 17.8%; Average loss: 3.4819
Iteration: 711; Percent complete: 17.8%; Average loss: 3.7630
Iteration: 712; Percent complete: 17.8%; Average loss: 3.5554
Iteration: 713; Percent complete: 17.8%; Average loss: 3.8994
Iteration: 714; Percent complete: 17.8%; Average loss: 3.7005
Iteration: 715; Percent complete: 17.9%; Average loss: 3.6032
Iteration: 716; Percent complete: 17.9%; Average loss: 3.6464
Iteration: 717; Percent complete: 17.9%; Average loss: 3.7606
Iteration: 718; Percent complete: 17.9%; Average loss: 3.5255
Iteration: 719; Percent complete: 18.0%; Average loss: 3.4778
Iteration: 720; Percent complete: 18.0%; Average loss: 3.3623
Iteration: 721; Percent complete: 18.0%; Average loss: 3.4830
Iteration: 722; Percent complete: 18.1%; Average loss: 3.4859
Iteration: 723; Percent complete: 18.1%; Average loss: 3.5891
Iteration: 724; Percent complete: 18.1%; Average loss: 3.5373
Iteration: 725; Percent complete: 18.1%; Average loss: 3.6263
Iteration: 726; Percent complete: 18.1%; Average loss: 3.7788
Iteration: 727; Percent complete: 18.2%; Average loss: 3.7124
Iteration: 728; Percent complete: 18.2%; Average loss: 3.3709
Iteration: 729; Percent complete: 18.2%; Average loss: 3.5668
Iteration: 730; Percent complete: 18.2%; Average loss: 3.5469
Iteration: 731; Percent complete: 18.3%; Average loss: 3.4835
Iteration: 732; Percent complete: 18.3%; Average loss: 3.6393
Iteration: 733; Percent complete: 18.3%; Average loss: 3.7937
Iteration: 734; Percent complete: 18.4%; Average loss: 3.5766
Iteration: 735; Percent complete: 18.4%; Average loss: 3.5250
Iteration: 736; Percent complete: 18.4%; Average loss: 3.7008
Iteration: 737; Percent complete: 18.4%; Average loss: 3.9265
Iteration: 738; Percent complete: 18.4%; Average loss: 3.8525
Iteration: 739; Percent complete: 18.5%; Average loss: 3.5290
Iteration: 740; Percent complete: 18.5%; Average loss: 3.5411
Iteration: 741; Percent complete: 18.5%; Average loss: 3.2921
Iteration: 742; Percent complete: 18.6%; Average loss: 3.6313
Iteration: 743; Percent complete: 18.6%; Average loss: 3.5117
Iteration: 744; Percent complete: 18.6%; Average loss: 3.3694
Iteration: 745; Percent complete: 18.6%; Average loss: 3.4915
Iteration: 746; Percent complete: 18.6%; Average loss: 3.3940
Iteration: 747; Percent complete: 18.7%; Average loss: 3.7672
Iteration: 748; Percent complete: 18.7%; Average loss: 3.5573
Iteration: 749; Percent complete: 18.7%; Average loss: 3.4664
Iteration: 750; Percent complete: 18.8%; Average loss: 3.6706
Iteration: 751; Percent complete: 18.8%; Average loss: 3.5979
Iteration: 752; Percent complete: 18.8%; Average loss: 3.3263
Iteration: 753; Percent complete: 18.8%; Average loss: 3.5501
Iteration: 754; Percent complete: 18.9%; Average loss: 3.8513
Iteration: 755; Percent complete: 18.9%; Average loss: 3.4378
Iteration: 756; Percent complete: 18.9%; Average loss: 3.8051
Iteration: 757; Percent complete: 18.9%; Average loss: 3.6093
Iteration: 758; Percent complete: 18.9%; Average loss: 3.6190
Iteration: 759; Percent complete: 19.0%; Average loss: 4.0447
Iteration: 760; Percent complete: 19.0%; Average loss: 3.5628
Iteration: 761; Percent complete: 19.0%; Average loss: 3.6589
Iteration: 762; Percent complete: 19.1%; Average loss: 3.6831
Iteration: 763; Percent complete: 19.1%; Average loss: 3.4636
Iteration: 764; Percent complete: 19.1%; Average loss: 3.7514
Iteration: 765; Percent complete: 19.1%; Average loss: 3.5236
Iteration: 766; Percent complete: 19.1%; Average loss: 3.3289
Iteration: 767; Percent complete: 19.2%; Average loss: 3.3317
Iteration: 768; Percent complete: 19.2%; Average loss: 3.6668
Iteration: 769; Percent complete: 19.2%; Average loss: 3.8804
Iteration: 770; Percent complete: 19.2%; Average loss: 3.8430
Iteration: 771; Percent complete: 19.3%; Average loss: 3.7315
Iteration: 772; Percent complete: 19.3%; Average loss: 3.6038
Iteration: 773; Percent complete: 19.3%; Average loss: 3.5041
Iteration: 774; Percent complete: 19.4%; Average loss: 3.2938
Iteration: 775; Percent complete: 19.4%; Average loss: 3.6466
Iteration: 776; Percent complete: 19.4%; Average loss: 3.8297
Iteration: 777; Percent complete: 19.4%; Average loss: 3.4287
Iteration: 778; Percent complete: 19.4%; Average loss: 3.7216
Iteration: 779; Percent complete: 19.5%; Average loss: 3.2919
Iteration: 780; Percent complete: 19.5%; Average loss: 3.5961
Iteration: 781; Percent complete: 19.5%; Average loss: 3.3670
Iteration: 782; Percent complete: 19.6%; Average loss: 3.6892
Iteration: 783; Percent complete: 19.6%; Average loss: 3.4795
Iteration: 784; Percent complete: 19.6%; Average loss: 3.2663
Iteration: 785; Percent complete: 19.6%; Average loss: 3.6213
Iteration: 786; Percent complete: 19.7%; Average loss: 3.6283
Iteration: 787; Percent complete: 19.7%; Average loss: 3.8044
Iteration: 788; Percent complete: 19.7%; Average loss: 3.5477
Iteration: 789; Percent complete: 19.7%; Average loss: 3.7215
Iteration: 790; Percent complete: 19.8%; Average loss: 3.6374
Iteration: 791; Percent complete: 19.8%; Average loss: 3.5820
Iteration: 792; Percent complete: 19.8%; Average loss: 3.5166
Iteration: 793; Percent complete: 19.8%; Average loss: 3.4899
Iteration: 794; Percent complete: 19.9%; Average loss: 3.6333
Iteration: 795; Percent complete: 19.9%; Average loss: 3.6678
Iteration: 796; Percent complete: 19.9%; Average loss: 3.8028
Iteration: 797; Percent complete: 19.9%; Average loss: 3.5520
Iteration: 798; Percent complete: 20.0%; Average loss: 3.6987
Iteration: 799; Percent complete: 20.0%; Average loss: 3.7315
Iteration: 800; Percent complete: 20.0%; Average loss: 3.5392
Iteration: 801; Percent complete: 20.0%; Average loss: 3.6433
Iteration: 802; Percent complete: 20.1%; Average loss: 3.3263
Iteration: 803; Percent complete: 20.1%; Average loss: 3.6362
Iteration: 804; Percent complete: 20.1%; Average loss: 3.3664
Iteration: 805; Percent complete: 20.1%; Average loss: 3.7407
Iteration: 806; Percent complete: 20.2%; Average loss: 3.7484
Iteration: 807; Percent complete: 20.2%; Average loss: 3.7347
Iteration: 808; Percent complete: 20.2%; Average loss: 3.7799
Iteration: 809; Percent complete: 20.2%; Average loss: 3.6903
Iteration: 810; Percent complete: 20.2%; Average loss: 3.3630
Iteration: 811; Percent complete: 20.3%; Average loss: 3.3783
Iteration: 812; Percent complete: 20.3%; Average loss: 3.6083
Iteration: 813; Percent complete: 20.3%; Average loss: 3.4677
Iteration: 814; Percent complete: 20.3%; Average loss: 3.5065
Iteration: 815; Percent complete: 20.4%; Average loss: 3.4976
Iteration: 816; Percent complete: 20.4%; Average loss: 3.5071
Iteration: 817; Percent complete: 20.4%; Average loss: 3.6650
Iteration: 818; Percent complete: 20.4%; Average loss: 3.3375
Iteration: 819; Percent complete: 20.5%; Average loss: 3.3970
Iteration: 820; Percent complete: 20.5%; Average loss: 3.7062
Iteration: 821; Percent complete: 20.5%; Average loss: 3.2955
Iteration: 822; Percent complete: 20.5%; Average loss: 3.5801
Iteration: 823; Percent complete: 20.6%; Average loss: 3.6023
Iteration: 824; Percent complete: 20.6%; Average loss: 3.4727
Iteration: 825; Percent complete: 20.6%; Average loss: 3.4694
Iteration: 826; Percent complete: 20.6%; Average loss: 3.3680
Iteration: 827; Percent complete: 20.7%; Average loss: 3.6212
Iteration: 828; Percent complete: 20.7%; Average loss: 3.1719
Iteration: 829; Percent complete: 20.7%; Average loss: 3.4341
Iteration: 830; Percent complete: 20.8%; Average loss: 3.2643
Iteration: 831; Percent complete: 20.8%; Average loss: 3.5202
Iteration: 832; Percent complete: 20.8%; Average loss: 3.3058
Iteration: 833; Percent complete: 20.8%; Average loss: 3.6784
Iteration: 834; Percent complete: 20.8%; Average loss: 3.3424
Iteration: 835; Percent complete: 20.9%; Average loss: 3.5271
Iteration: 836; Percent complete: 20.9%; Average loss: 3.4466
Iteration: 837; Percent complete: 20.9%; Average loss: 3.5893
Iteration: 838; Percent complete: 20.9%; Average loss: 3.6049
Iteration: 839; Percent complete: 21.0%; Average loss: 3.5635
Iteration: 840; Percent complete: 21.0%; Average loss: 3.5580
Iteration: 841; Percent complete: 21.0%; Average loss: 3.4376
Iteration: 842; Percent complete: 21.1%; Average loss: 3.6559
Iteration: 843; Percent complete: 21.1%; Average loss: 3.4265
Iteration: 844; Percent complete: 21.1%; Average loss: 3.7832
Iteration: 845; Percent complete: 21.1%; Average loss: 3.2643
Iteration: 846; Percent complete: 21.1%; Average loss: 3.6200
Iteration: 847; Percent complete: 21.2%; Average loss: 3.7572
Iteration: 848; Percent complete: 21.2%; Average loss: 3.5456
Iteration: 849; Percent complete: 21.2%; Average loss: 3.4865
Iteration: 850; Percent complete: 21.2%; Average loss: 3.6434
Iteration: 851; Percent complete: 21.3%; Average loss: 3.5088
Iteration: 852; Percent complete: 21.3%; Average loss: 3.8174
Iteration: 853; Percent complete: 21.3%; Average loss: 3.3734
Iteration: 854; Percent complete: 21.3%; Average loss: 3.7169
Iteration: 855; Percent complete: 21.4%; Average loss: 3.6630
Iteration: 856; Percent complete: 21.4%; Average loss: 3.4326
Iteration: 857; Percent complete: 21.4%; Average loss: 3.4992
Iteration: 858; Percent complete: 21.4%; Average loss: 3.5260
Iteration: 859; Percent complete: 21.5%; Average loss: 3.3860
Iteration: 860; Percent complete: 21.5%; Average loss: 3.4184
Iteration: 861; Percent complete: 21.5%; Average loss: 3.3057
Iteration: 862; Percent complete: 21.6%; Average loss: 3.3436
Iteration: 863; Percent complete: 21.6%; Average loss: 3.5127
Iteration: 864; Percent complete: 21.6%; Average loss: 3.5594
Iteration: 865; Percent complete: 21.6%; Average loss: 3.5942
Iteration: 866; Percent complete: 21.6%; Average loss: 3.7544
Iteration: 867; Percent complete: 21.7%; Average loss: 3.2667
Iteration: 868; Percent complete: 21.7%; Average loss: 3.8602
Iteration: 869; Percent complete: 21.7%; Average loss: 3.3873
Iteration: 870; Percent complete: 21.8%; Average loss: 3.2928
Iteration: 871; Percent complete: 21.8%; Average loss: 3.3166
Iteration: 872; Percent complete: 21.8%; Average loss: 3.2659
Iteration: 873; Percent complete: 21.8%; Average loss: 3.3056
Iteration: 874; Percent complete: 21.9%; Average loss: 3.5915
Iteration: 875; Percent complete: 21.9%; Average loss: 3.7223
Iteration: 876; Percent complete: 21.9%; Average loss: 3.5076
Iteration: 877; Percent complete: 21.9%; Average loss: 3.4717
Iteration: 878; Percent complete: 21.9%; Average loss: 3.4899
Iteration: 879; Percent complete: 22.0%; Average loss: 3.8972
Iteration: 880; Percent complete: 22.0%; Average loss: 3.4413
Iteration: 881; Percent complete: 22.0%; Average loss: 3.6941
Iteration: 882; Percent complete: 22.1%; Average loss: 3.7830
Iteration: 883; Percent complete: 22.1%; Average loss: 3.3801
Iteration: 884; Percent complete: 22.1%; Average loss: 3.7830
Iteration: 885; Percent complete: 22.1%; Average loss: 3.3651
Iteration: 886; Percent complete: 22.1%; Average loss: 3.6668
Iteration: 887; Percent complete: 22.2%; Average loss: 3.3449
Iteration: 888; Percent complete: 22.2%; Average loss: 3.6855
Iteration: 889; Percent complete: 22.2%; Average loss: 3.7665
Iteration: 890; Percent complete: 22.2%; Average loss: 3.6161
Iteration: 891; Percent complete: 22.3%; Average loss: 3.6999
Iteration: 892; Percent complete: 22.3%; Average loss: 3.4895
Iteration: 893; Percent complete: 22.3%; Average loss: 3.7532
Iteration: 894; Percent complete: 22.4%; Average loss: 3.1438
Iteration: 895; Percent complete: 22.4%; Average loss: 3.2681
Iteration: 896; Percent complete: 22.4%; Average loss: 3.5675
Iteration: 897; Percent complete: 22.4%; Average loss: 3.4439
Iteration: 898; Percent complete: 22.4%; Average loss: 3.6110
Iteration: 899; Percent complete: 22.5%; Average loss: 3.3462
Iteration: 900; Percent complete: 22.5%; Average loss: 3.7786
Iteration: 901; Percent complete: 22.5%; Average loss: 3.3670
Iteration: 902; Percent complete: 22.6%; Average loss: 3.6647
Iteration: 903; Percent complete: 22.6%; Average loss: 3.6157
Iteration: 904; Percent complete: 22.6%; Average loss: 3.6095
Iteration: 905; Percent complete: 22.6%; Average loss: 3.5313
Iteration: 906; Percent complete: 22.7%; Average loss: 3.3572
Iteration: 907; Percent complete: 22.7%; Average loss: 3.5281
Iteration: 908; Percent complete: 22.7%; Average loss: 3.9167
Iteration: 909; Percent complete: 22.7%; Average loss: 3.3926
Iteration: 910; Percent complete: 22.8%; Average loss: 3.3687
Iteration: 911; Percent complete: 22.8%; Average loss: 3.5040
Iteration: 912; Percent complete: 22.8%; Average loss: 3.5179
Iteration: 913; Percent complete: 22.8%; Average loss: 3.5977
Iteration: 914; Percent complete: 22.9%; Average loss: 3.5263
Iteration: 915; Percent complete: 22.9%; Average loss: 3.6819
Iteration: 916; Percent complete: 22.9%; Average loss: 3.6763
Iteration: 917; Percent complete: 22.9%; Average loss: 3.6939
Iteration: 918; Percent complete: 22.9%; Average loss: 3.5333
Iteration: 919; Percent complete: 23.0%; Average loss: 3.2761
Iteration: 920; Percent complete: 23.0%; Average loss: 3.5738
Iteration: 921; Percent complete: 23.0%; Average loss: 3.3807
Iteration: 922; Percent complete: 23.1%; Average loss: 3.3986
Iteration: 923; Percent complete: 23.1%; Average loss: 3.6455
Iteration: 924; Percent complete: 23.1%; Average loss: 3.7322
Iteration: 925; Percent complete: 23.1%; Average loss: 3.5420
Iteration: 926; Percent complete: 23.2%; Average loss: 3.4794
Iteration: 927; Percent complete: 23.2%; Average loss: 3.8940
Iteration: 928; Percent complete: 23.2%; Average loss: 3.6800
Iteration: 929; Percent complete: 23.2%; Average loss: 3.4595
Iteration: 930; Percent complete: 23.2%; Average loss: 3.5588
Iteration: 931; Percent complete: 23.3%; Average loss: 3.3522
Iteration: 932; Percent complete: 23.3%; Average loss: 3.6909
Iteration: 933; Percent complete: 23.3%; Average loss: 3.5827
Iteration: 934; Percent complete: 23.4%; Average loss: 3.6138
Iteration: 935; Percent complete: 23.4%; Average loss: 3.4866
Iteration: 936; Percent complete: 23.4%; Average loss: 3.6234
Iteration: 937; Percent complete: 23.4%; Average loss: 3.3080
Iteration: 938; Percent complete: 23.4%; Average loss: 3.5412
Iteration: 939; Percent complete: 23.5%; Average loss: 3.4447
Iteration: 940; Percent complete: 23.5%; Average loss: 3.4463
Iteration: 941; Percent complete: 23.5%; Average loss: 3.5354
Iteration: 942; Percent complete: 23.5%; Average loss: 3.3701
Iteration: 943; Percent complete: 23.6%; Average loss: 3.5361
Iteration: 944; Percent complete: 23.6%; Average loss: 3.3955
Iteration: 945; Percent complete: 23.6%; Average loss: 3.3821
Iteration: 946; Percent complete: 23.6%; Average loss: 3.5770
Iteration: 947; Percent complete: 23.7%; Average loss: 3.5352
Iteration: 948; Percent complete: 23.7%; Average loss: 3.3229
Iteration: 949; Percent complete: 23.7%; Average loss: 3.3868
Iteration: 950; Percent complete: 23.8%; Average loss: 3.0652
Iteration: 951; Percent complete: 23.8%; Average loss: 3.4448
Iteration: 952; Percent complete: 23.8%; Average loss: 3.4699
Iteration: 953; Percent complete: 23.8%; Average loss: 3.1696
Iteration: 954; Percent complete: 23.8%; Average loss: 3.6839
Iteration: 955; Percent complete: 23.9%; Average loss: 3.6220
Iteration: 956; Percent complete: 23.9%; Average loss: 3.5972
Iteration: 957; Percent complete: 23.9%; Average loss: 3.3914
Iteration: 958; Percent complete: 23.9%; Average loss: 3.4779
Iteration: 959; Percent complete: 24.0%; Average loss: 3.5170
Iteration: 960; Percent complete: 24.0%; Average loss: 3.4790
Iteration: 961; Percent complete: 24.0%; Average loss: 3.3865
Iteration: 962; Percent complete: 24.1%; Average loss: 3.2074
Iteration: 963; Percent complete: 24.1%; Average loss: 3.4672
Iteration: 964; Percent complete: 24.1%; Average loss: 3.5134
Iteration: 965; Percent complete: 24.1%; Average loss: 3.4514
Iteration: 966; Percent complete: 24.1%; Average loss: 3.4251
Iteration: 967; Percent complete: 24.2%; Average loss: 3.5863
Iteration: 968; Percent complete: 24.2%; Average loss: 3.5353
Iteration: 969; Percent complete: 24.2%; Average loss: 3.6847
Iteration: 970; Percent complete: 24.2%; Average loss: 3.4021
Iteration: 971; Percent complete: 24.3%; Average loss: 3.4445
Iteration: 972; Percent complete: 24.3%; Average loss: 3.6236
Iteration: 973; Percent complete: 24.3%; Average loss: 3.3442
Iteration: 974; Percent complete: 24.3%; Average loss: 3.2902
Iteration: 975; Percent complete: 24.4%; Average loss: 3.3664
Iteration: 976; Percent complete: 24.4%; Average loss: 3.0515
Iteration: 977; Percent complete: 24.4%; Average loss: 3.4845
Iteration: 978; Percent complete: 24.4%; Average loss: 3.3494
Iteration: 979; Percent complete: 24.5%; Average loss: 3.4150
Iteration: 980; Percent complete: 24.5%; Average loss: 3.4482
Iteration: 981; Percent complete: 24.5%; Average loss: 3.8267
Iteration: 982; Percent complete: 24.6%; Average loss: 3.5635
Iteration: 983; Percent complete: 24.6%; Average loss: 3.8119
Iteration: 984; Percent complete: 24.6%; Average loss: 3.6585
Iteration: 985; Percent complete: 24.6%; Average loss: 3.6026
Iteration: 986; Percent complete: 24.6%; Average loss: 3.4747
Iteration: 987; Percent complete: 24.7%; Average loss: 3.4534
Iteration: 988; Percent complete: 24.7%; Average loss: 3.4495
Iteration: 989; Percent complete: 24.7%; Average loss: 3.7138
Iteration: 990; Percent complete: 24.8%; Average loss: 3.3339
Iteration: 991; Percent complete: 24.8%; Average loss: 3.4596
Iteration: 992; Percent complete: 24.8%; Average loss: 3.6984
Iteration: 993; Percent complete: 24.8%; Average loss: 3.5571
Iteration: 994; Percent complete: 24.9%; Average loss: 3.4528
Iteration: 995; Percent complete: 24.9%; Average loss: 3.5408
Iteration: 996; Percent complete: 24.9%; Average loss: 3.8421
Iteration: 997; Percent complete: 24.9%; Average loss: 3.3465
Iteration: 998; Percent complete: 24.9%; Average loss: 3.8325
Iteration: 999; Percent complete: 25.0%; Average loss: 3.5467
Iteration: 1000; Percent complete: 25.0%; Average loss: 3.6337
Iteration: 1001; Percent complete: 25.0%; Average loss: 3.6576
Iteration: 1002; Percent complete: 25.1%; Average loss: 3.4400
Iteration: 1003; Percent complete: 25.1%; Average loss: 3.4604
Iteration: 1004; Percent complete: 25.1%; Average loss: 3.6977
Iteration: 1005; Percent complete: 25.1%; Average loss: 3.1991
Iteration: 1006; Percent complete: 25.1%; Average loss: 3.6351
Iteration: 1007; Percent complete: 25.2%; Average loss: 3.5158
Iteration: 1008; Percent complete: 25.2%; Average loss: 3.5982
Iteration: 1009; Percent complete: 25.2%; Average loss: 3.7045
Iteration: 1010; Percent complete: 25.2%; Average loss: 3.5205
Iteration: 1011; Percent complete: 25.3%; Average loss: 3.5985
Iteration: 1012; Percent complete: 25.3%; Average loss: 3.4862
Iteration: 1013; Percent complete: 25.3%; Average loss: 3.5099
Iteration: 1014; Percent complete: 25.4%; Average loss: 3.6044
Iteration: 1015; Percent complete: 25.4%; Average loss: 3.8710
Iteration: 1016; Percent complete: 25.4%; Average loss: 3.3688
Iteration: 1017; Percent complete: 25.4%; Average loss: 3.5713
Iteration: 1018; Percent complete: 25.4%; Average loss: 3.6101
Iteration: 1019; Percent complete: 25.5%; Average loss: 3.5934
Iteration: 1020; Percent complete: 25.5%; Average loss: 3.5548
Iteration: 1021; Percent complete: 25.5%; Average loss: 3.6125
Iteration: 1022; Percent complete: 25.6%; Average loss: 3.4670
Iteration: 1023; Percent complete: 25.6%; Average loss: 3.5429
Iteration: 1024; Percent complete: 25.6%; Average loss: 3.5147
Iteration: 1025; Percent complete: 25.6%; Average loss: 3.5912
Iteration: 1026; Percent complete: 25.7%; Average loss: 3.6994
Iteration: 1027; Percent complete: 25.7%; Average loss: 3.5276
Iteration: 1028; Percent complete: 25.7%; Average loss: 3.6008
Iteration: 1029; Percent complete: 25.7%; Average loss: 3.3630
Iteration: 1030; Percent complete: 25.8%; Average loss: 3.5895
Iteration: 1031; Percent complete: 25.8%; Average loss: 3.9452
Iteration: 1032; Percent complete: 25.8%; Average loss: 3.3449
Iteration: 1033; Percent complete: 25.8%; Average loss: 3.4959
Iteration: 1034; Percent complete: 25.9%; Average loss: 3.4540
Iteration: 1035; Percent complete: 25.9%; Average loss: 3.7477
Iteration: 1036; Percent complete: 25.9%; Average loss: 3.4098
Iteration: 1037; Percent complete: 25.9%; Average loss: 3.5049
Iteration: 1038; Percent complete: 25.9%; Average loss: 3.5907
Iteration: 1039; Percent complete: 26.0%; Average loss: 3.5395
Iteration: 1040; Percent complete: 26.0%; Average loss: 3.3829
Iteration: 1041; Percent complete: 26.0%; Average loss: 3.4368
Iteration: 1042; Percent complete: 26.1%; Average loss: 3.4037
Iteration: 1043; Percent complete: 26.1%; Average loss: 3.5377
Iteration: 1044; Percent complete: 26.1%; Average loss: 3.3827
Iteration: 1045; Percent complete: 26.1%; Average loss: 3.3389
Iteration: 1046; Percent complete: 26.2%; Average loss: 3.7578
Iteration: 1047; Percent complete: 26.2%; Average loss: 3.3409
Iteration: 1048; Percent complete: 26.2%; Average loss: 3.5233
Iteration: 1049; Percent complete: 26.2%; Average loss: 3.5048
Iteration: 1050; Percent complete: 26.2%; Average loss: 3.3014
Iteration: 1051; Percent complete: 26.3%; Average loss: 3.4232
Iteration: 1052; Percent complete: 26.3%; Average loss: 3.7719
Iteration: 1053; Percent complete: 26.3%; Average loss: 3.2702
Iteration: 1054; Percent complete: 26.4%; Average loss: 3.3185
Iteration: 1055; Percent complete: 26.4%; Average loss: 3.4090
Iteration: 1056; Percent complete: 26.4%; Average loss: 3.5883
Iteration: 1057; Percent complete: 26.4%; Average loss: 3.6685
Iteration: 1058; Percent complete: 26.5%; Average loss: 3.2193
Iteration: 1059; Percent complete: 26.5%; Average loss: 3.5284
Iteration: 1060; Percent complete: 26.5%; Average loss: 3.3012
Iteration: 1061; Percent complete: 26.5%; Average loss: 3.4425
Iteration: 1062; Percent complete: 26.6%; Average loss: 3.4912
Iteration: 1063; Percent complete: 26.6%; Average loss: 3.6459
Iteration: 1064; Percent complete: 26.6%; Average loss: 3.5543
Iteration: 1065; Percent complete: 26.6%; Average loss: 3.4119
Iteration: 1066; Percent complete: 26.7%; Average loss: 3.2644
Iteration: 1067; Percent complete: 26.7%; Average loss: 3.3014
Iteration: 1068; Percent complete: 26.7%; Average loss: 3.1957
Iteration: 1069; Percent complete: 26.7%; Average loss: 3.5218
Iteration: 1070; Percent complete: 26.8%; Average loss: 3.3099
Iteration: 1071; Percent complete: 26.8%; Average loss: 3.7786
Iteration: 1072; Percent complete: 26.8%; Average loss: 3.5192
Iteration: 1073; Percent complete: 26.8%; Average loss: 3.5627
Iteration: 1074; Percent complete: 26.9%; Average loss: 3.3568
Iteration: 1075; Percent complete: 26.9%; Average loss: 3.3409
Iteration: 1076; Percent complete: 26.9%; Average loss: 3.3272
Iteration: 1077; Percent complete: 26.9%; Average loss: 3.4789
Iteration: 1078; Percent complete: 27.0%; Average loss: 3.4947
Iteration: 1079; Percent complete: 27.0%; Average loss: 3.3548
Iteration: 1080; Percent complete: 27.0%; Average loss: 3.3422
Iteration: 1081; Percent complete: 27.0%; Average loss: 3.6754
Iteration: 1082; Percent complete: 27.1%; Average loss: 3.6859
Iteration: 1083; Percent complete: 27.1%; Average loss: 3.2652
Iteration: 1084; Percent complete: 27.1%; Average loss: 3.3051
Iteration: 1085; Percent complete: 27.1%; Average loss: 3.1456
Iteration: 1086; Percent complete: 27.2%; Average loss: 3.3160
Iteration: 1087; Percent complete: 27.2%; Average loss: 3.2383
Iteration: 1088; Percent complete: 27.2%; Average loss: 3.6991
Iteration: 1089; Percent complete: 27.2%; Average loss: 3.3675
Iteration: 1090; Percent complete: 27.3%; Average loss: 3.1971
Iteration: 1091; Percent complete: 27.3%; Average loss: 3.4751
Iteration: 1092; Percent complete: 27.3%; Average loss: 3.3651
Iteration: 1093; Percent complete: 27.3%; Average loss: 3.3085
Iteration: 1094; Percent complete: 27.4%; Average loss: 3.5053
Iteration: 1095; Percent complete: 27.4%; Average loss: 3.4228
Iteration: 1096; Percent complete: 27.4%; Average loss: 3.4013
Iteration: 1097; Percent complete: 27.4%; Average loss: 3.1676
Iteration: 1098; Percent complete: 27.5%; Average loss: 3.5056
Iteration: 1099; Percent complete: 27.5%; Average loss: 3.3766
Iteration: 1100; Percent complete: 27.5%; Average loss: 3.6332
Iteration: 1101; Percent complete: 27.5%; Average loss: 3.2239
Iteration: 1102; Percent complete: 27.6%; Average loss: 3.0318
Iteration: 1103; Percent complete: 27.6%; Average loss: 3.1951
Iteration: 1104; Percent complete: 27.6%; Average loss: 3.5514
Iteration: 1105; Percent complete: 27.6%; Average loss: 3.4499
Iteration: 1106; Percent complete: 27.7%; Average loss: 3.5584
Iteration: 1107; Percent complete: 27.7%; Average loss: 3.5201
Iteration: 1108; Percent complete: 27.7%; Average loss: 3.5834
Iteration: 1109; Percent complete: 27.7%; Average loss: 3.4120
Iteration: 1110; Percent complete: 27.8%; Average loss: 3.2852
Iteration: 1111; Percent complete: 27.8%; Average loss: 3.4640
Iteration: 1112; Percent complete: 27.8%; Average loss: 3.2124
Iteration: 1113; Percent complete: 27.8%; Average loss: 3.4265
Iteration: 1114; Percent complete: 27.9%; Average loss: 3.2247
Iteration: 1115; Percent complete: 27.9%; Average loss: 3.4662
Iteration: 1116; Percent complete: 27.9%; Average loss: 3.4341
Iteration: 1117; Percent complete: 27.9%; Average loss: 3.9109
Iteration: 1118; Percent complete: 28.0%; Average loss: 3.1430
Iteration: 1119; Percent complete: 28.0%; Average loss: 3.2884
Iteration: 1120; Percent complete: 28.0%; Average loss: 3.4809
Iteration: 1121; Percent complete: 28.0%; Average loss: 3.1756
Iteration: 1122; Percent complete: 28.1%; Average loss: 3.3253
Iteration: 1123; Percent complete: 28.1%; Average loss: 3.3120
Iteration: 1124; Percent complete: 28.1%; Average loss: 3.5287
Iteration: 1125; Percent complete: 28.1%; Average loss: 3.5084
Iteration: 1126; Percent complete: 28.1%; Average loss: 3.3006
Iteration: 1127; Percent complete: 28.2%; Average loss: 3.4035
Iteration: 1128; Percent complete: 28.2%; Average loss: 3.2090
Iteration: 1129; Percent complete: 28.2%; Average loss: 3.5402
Iteration: 1130; Percent complete: 28.2%; Average loss: 3.5414
Iteration: 1131; Percent complete: 28.3%; Average loss: 3.0871
Iteration: 1132; Percent complete: 28.3%; Average loss: 3.5253
Iteration: 1133; Percent complete: 28.3%; Average loss: 3.5404
Iteration: 1134; Percent complete: 28.3%; Average loss: 3.2518
Iteration: 1135; Percent complete: 28.4%; Average loss: 3.1747
Iteration: 1136; Percent complete: 28.4%; Average loss: 3.3929
Iteration: 1137; Percent complete: 28.4%; Average loss: 3.6998
Iteration: 1138; Percent complete: 28.4%; Average loss: 3.3781
Iteration: 1139; Percent complete: 28.5%; Average loss: 3.5646
Iteration: 1140; Percent complete: 28.5%; Average loss: 3.4501
Iteration: 1141; Percent complete: 28.5%; Average loss: 3.6031
Iteration: 1142; Percent complete: 28.5%; Average loss: 3.2544
Iteration: 1143; Percent complete: 28.6%; Average loss: 3.2347
Iteration: 1144; Percent complete: 28.6%; Average loss: 3.5455
Iteration: 1145; Percent complete: 28.6%; Average loss: 3.6934
Iteration: 1146; Percent complete: 28.6%; Average loss: 3.3970
Iteration: 1147; Percent complete: 28.7%; Average loss: 3.3732
Iteration: 1148; Percent complete: 28.7%; Average loss: 3.4170
Iteration: 1149; Percent complete: 28.7%; Average loss: 3.5575
Iteration: 1150; Percent complete: 28.7%; Average loss: 3.6792
Iteration: 1151; Percent complete: 28.8%; Average loss: 3.3033
Iteration: 1152; Percent complete: 28.8%; Average loss: 3.2564
Iteration: 1153; Percent complete: 28.8%; Average loss: 3.4307
Iteration: 1154; Percent complete: 28.8%; Average loss: 3.5470
Iteration: 1155; Percent complete: 28.9%; Average loss: 3.2345
Iteration: 1156; Percent complete: 28.9%; Average loss: 3.3622
Iteration: 1157; Percent complete: 28.9%; Average loss: 3.1594
Iteration: 1158; Percent complete: 28.9%; Average loss: 3.4212
Iteration: 1159; Percent complete: 29.0%; Average loss: 3.3899
Iteration: 1160; Percent complete: 29.0%; Average loss: 3.2646
Iteration: 1161; Percent complete: 29.0%; Average loss: 3.2558
Iteration: 1162; Percent complete: 29.0%; Average loss: 3.6901
Iteration: 1163; Percent complete: 29.1%; Average loss: 3.5664
Iteration: 1164; Percent complete: 29.1%; Average loss: 3.5953
Iteration: 1165; Percent complete: 29.1%; Average loss: 3.4374
Iteration: 1166; Percent complete: 29.1%; Average loss: 3.6218
Iteration: 1167; Percent complete: 29.2%; Average loss: 3.2094
Iteration: 1168; Percent complete: 29.2%; Average loss: 3.3094
Iteration: 1169; Percent complete: 29.2%; Average loss: 3.4649
Iteration: 1170; Percent complete: 29.2%; Average loss: 3.3952
Iteration: 1171; Percent complete: 29.3%; Average loss: 3.4410
Iteration: 1172; Percent complete: 29.3%; Average loss: 3.1296
Iteration: 1173; Percent complete: 29.3%; Average loss: 3.4415
Iteration: 1174; Percent complete: 29.3%; Average loss: 3.4011
Iteration: 1175; Percent complete: 29.4%; Average loss: 3.2577
Iteration: 1176; Percent complete: 29.4%; Average loss: 3.5207
Iteration: 1177; Percent complete: 29.4%; Average loss: 3.2558
Iteration: 1178; Percent complete: 29.4%; Average loss: 3.5886
Iteration: 1179; Percent complete: 29.5%; Average loss: 3.3415
Iteration: 1180; Percent complete: 29.5%; Average loss: 3.9094
Iteration: 1181; Percent complete: 29.5%; Average loss: 3.4758
Iteration: 1182; Percent complete: 29.5%; Average loss: 3.7543
Iteration: 1183; Percent complete: 29.6%; Average loss: 3.3038
Iteration: 1184; Percent complete: 29.6%; Average loss: 3.3530
Iteration: 1185; Percent complete: 29.6%; Average loss: 3.1440
Iteration: 1186; Percent complete: 29.6%; Average loss: 3.3339
Iteration: 1187; Percent complete: 29.7%; Average loss: 3.3935
Iteration: 1188; Percent complete: 29.7%; Average loss: 3.4210
Iteration: 1189; Percent complete: 29.7%; Average loss: 3.5882
Iteration: 1190; Percent complete: 29.8%; Average loss: 3.3458
Iteration: 1191; Percent complete: 29.8%; Average loss: 3.6337
Iteration: 1192; Percent complete: 29.8%; Average loss: 3.3997
Iteration: 1193; Percent complete: 29.8%; Average loss: 3.4744
Iteration: 1194; Percent complete: 29.8%; Average loss: 3.1326
Iteration: 1195; Percent complete: 29.9%; Average loss: 3.4964
Iteration: 1196; Percent complete: 29.9%; Average loss: 3.4769
Iteration: 1197; Percent complete: 29.9%; Average loss: 3.2520
Iteration: 1198; Percent complete: 29.9%; Average loss: 3.1631
Iteration: 1199; Percent complete: 30.0%; Average loss: 3.2645
Iteration: 1200; Percent complete: 30.0%; Average loss: 3.3770
Iteration: 1201; Percent complete: 30.0%; Average loss: 3.2579
Iteration: 1202; Percent complete: 30.0%; Average loss: 3.6342
Iteration: 1203; Percent complete: 30.1%; Average loss: 3.5715
Iteration: 1204; Percent complete: 30.1%; Average loss: 3.5472
Iteration: 1205; Percent complete: 30.1%; Average loss: 3.5031
Iteration: 1206; Percent complete: 30.1%; Average loss: 3.5368
Iteration: 1207; Percent complete: 30.2%; Average loss: 3.4204
Iteration: 1208; Percent complete: 30.2%; Average loss: 3.6173
Iteration: 1209; Percent complete: 30.2%; Average loss: 3.4859
Iteration: 1210; Percent complete: 30.2%; Average loss: 3.2003
Iteration: 1211; Percent complete: 30.3%; Average loss: 3.2830
Iteration: 1212; Percent complete: 30.3%; Average loss: 3.4200
Iteration: 1213; Percent complete: 30.3%; Average loss: 3.4545
Iteration: 1214; Percent complete: 30.3%; Average loss: 3.5588
Iteration: 1215; Percent complete: 30.4%; Average loss: 3.5704
Iteration: 1216; Percent complete: 30.4%; Average loss: 3.3979
Iteration: 1217; Percent complete: 30.4%; Average loss: 3.3916
Iteration: 1218; Percent complete: 30.4%; Average loss: 3.0994
Iteration: 1219; Percent complete: 30.5%; Average loss: 3.3514
Iteration: 1220; Percent complete: 30.5%; Average loss: 3.3608
Iteration: 1221; Percent complete: 30.5%; Average loss: 3.5811
Iteration: 1222; Percent complete: 30.6%; Average loss: 3.1688
Iteration: 1223; Percent complete: 30.6%; Average loss: 3.2820
Iteration: 1224; Percent complete: 30.6%; Average loss: 3.4487
Iteration: 1225; Percent complete: 30.6%; Average loss: 3.2149
Iteration: 1226; Percent complete: 30.6%; Average loss: 3.6662
Iteration: 1227; Percent complete: 30.7%; Average loss: 3.3791
Iteration: 1228; Percent complete: 30.7%; Average loss: 3.3493
Iteration: 1229; Percent complete: 30.7%; Average loss: 3.2153
Iteration: 1230; Percent complete: 30.8%; Average loss: 3.5384
Iteration: 1231; Percent complete: 30.8%; Average loss: 3.3849
Iteration: 1232; Percent complete: 30.8%; Average loss: 3.5693
Iteration: 1233; Percent complete: 30.8%; Average loss: 3.6652
Iteration: 1234; Percent complete: 30.9%; Average loss: 3.5322
Iteration: 1235; Percent complete: 30.9%; Average loss: 3.2745
Iteration: 1236; Percent complete: 30.9%; Average loss: 3.4447
Iteration: 1237; Percent complete: 30.9%; Average loss: 3.3700
Iteration: 1238; Percent complete: 30.9%; Average loss: 3.1717
Iteration: 1239; Percent complete: 31.0%; Average loss: 3.7434
Iteration: 1240; Percent complete: 31.0%; Average loss: 3.2401
Iteration: 1241; Percent complete: 31.0%; Average loss: 3.5069
Iteration: 1242; Percent complete: 31.1%; Average loss: 3.5274
Iteration: 1243; Percent complete: 31.1%; Average loss: 3.3543
Iteration: 1244; Percent complete: 31.1%; Average loss: 3.3266
Iteration: 1245; Percent complete: 31.1%; Average loss: 3.3219
Iteration: 1246; Percent complete: 31.1%; Average loss: 3.4467
Iteration: 1247; Percent complete: 31.2%; Average loss: 3.2503
Iteration: 1248; Percent complete: 31.2%; Average loss: 3.3283
Iteration: 1249; Percent complete: 31.2%; Average loss: 3.4780
Iteration: 1250; Percent complete: 31.2%; Average loss: 3.4037
Iteration: 1251; Percent complete: 31.3%; Average loss: 3.4734
Iteration: 1252; Percent complete: 31.3%; Average loss: 3.5020
Iteration: 1253; Percent complete: 31.3%; Average loss: 3.2649
Iteration: 1254; Percent complete: 31.4%; Average loss: 3.4377
Iteration: 1255; Percent complete: 31.4%; Average loss: 3.5198
Iteration: 1256; Percent complete: 31.4%; Average loss: 3.5730
Iteration: 1257; Percent complete: 31.4%; Average loss: 2.9440
Iteration: 1258; Percent complete: 31.4%; Average loss: 3.5160
Iteration: 1259; Percent complete: 31.5%; Average loss: 3.6354
Iteration: 1260; Percent complete: 31.5%; Average loss: 3.1201
Iteration: 1261; Percent complete: 31.5%; Average loss: 3.5460
Iteration: 1262; Percent complete: 31.6%; Average loss: 3.3252
Iteration: 1263; Percent complete: 31.6%; Average loss: 3.4768
Iteration: 1264; Percent complete: 31.6%; Average loss: 3.4633
Iteration: 1265; Percent complete: 31.6%; Average loss: 3.3188
Iteration: 1266; Percent complete: 31.6%; Average loss: 3.3458
Iteration: 1267; Percent complete: 31.7%; Average loss: 3.4967
Iteration: 1268; Percent complete: 31.7%; Average loss: 3.4557
Iteration: 1269; Percent complete: 31.7%; Average loss: 3.2548
Iteration: 1270; Percent complete: 31.8%; Average loss: 3.5186
Iteration: 1271; Percent complete: 31.8%; Average loss: 3.3131
Iteration: 1272; Percent complete: 31.8%; Average loss: 3.5059
Iteration: 1273; Percent complete: 31.8%; Average loss: 3.3996
Iteration: 1274; Percent complete: 31.9%; Average loss: 3.5387
Iteration: 1275; Percent complete: 31.9%; Average loss: 3.2684
Iteration: 1276; Percent complete: 31.9%; Average loss: 3.1083
Iteration: 1277; Percent complete: 31.9%; Average loss: 3.2813
Iteration: 1278; Percent complete: 31.9%; Average loss: 3.4270
Iteration: 1279; Percent complete: 32.0%; Average loss: 3.2442
Iteration: 1280; Percent complete: 32.0%; Average loss: 3.5004
Iteration: 1281; Percent complete: 32.0%; Average loss: 3.7703
Iteration: 1282; Percent complete: 32.0%; Average loss: 3.3647
Iteration: 1283; Percent complete: 32.1%; Average loss: 3.5828
Iteration: 1284; Percent complete: 32.1%; Average loss: 3.2657
Iteration: 1285; Percent complete: 32.1%; Average loss: 3.5538
Iteration: 1286; Percent complete: 32.1%; Average loss: 3.4294
Iteration: 1287; Percent complete: 32.2%; Average loss: 3.2758
Iteration: 1288; Percent complete: 32.2%; Average loss: 3.3442
Iteration: 1289; Percent complete: 32.2%; Average loss: 3.4655
Iteration: 1290; Percent complete: 32.2%; Average loss: 3.1601
Iteration: 1291; Percent complete: 32.3%; Average loss: 3.3369
Iteration: 1292; Percent complete: 32.3%; Average loss: 3.3291
Iteration: 1293; Percent complete: 32.3%; Average loss: 3.1926
Iteration: 1294; Percent complete: 32.4%; Average loss: 3.3180
Iteration: 1295; Percent complete: 32.4%; Average loss: 3.2994
Iteration: 1296; Percent complete: 32.4%; Average loss: 3.4385
Iteration: 1297; Percent complete: 32.4%; Average loss: 3.3457
Iteration: 1298; Percent complete: 32.5%; Average loss: 3.1356
Iteration: 1299; Percent complete: 32.5%; Average loss: 3.4860
Iteration: 1300; Percent complete: 32.5%; Average loss: 3.3414
Iteration: 1301; Percent complete: 32.5%; Average loss: 3.1535
Iteration: 1302; Percent complete: 32.6%; Average loss: 3.2875
Iteration: 1303; Percent complete: 32.6%; Average loss: 3.4219
Iteration: 1304; Percent complete: 32.6%; Average loss: 3.4106
Iteration: 1305; Percent complete: 32.6%; Average loss: 3.5208
Iteration: 1306; Percent complete: 32.6%; Average loss: 3.2243
Iteration: 1307; Percent complete: 32.7%; Average loss: 3.4571
Iteration: 1308; Percent complete: 32.7%; Average loss: 3.6242
Iteration: 1309; Percent complete: 32.7%; Average loss: 3.3851
Iteration: 1310; Percent complete: 32.8%; Average loss: 3.3686
Iteration: 1311; Percent complete: 32.8%; Average loss: 3.2049
Iteration: 1312; Percent complete: 32.8%; Average loss: 3.5098
Iteration: 1313; Percent complete: 32.8%; Average loss: 3.5942
Iteration: 1314; Percent complete: 32.9%; Average loss: 3.5196
Iteration: 1315; Percent complete: 32.9%; Average loss: 3.4953
Iteration: 1316; Percent complete: 32.9%; Average loss: 3.6309
Iteration: 1317; Percent complete: 32.9%; Average loss: 3.4848
Iteration: 1318; Percent complete: 33.0%; Average loss: 3.2788
Iteration: 1319; Percent complete: 33.0%; Average loss: 3.2690
Iteration: 1320; Percent complete: 33.0%; Average loss: 3.4459
Iteration: 1321; Percent complete: 33.0%; Average loss: 3.2304
Iteration: 1322; Percent complete: 33.1%; Average loss: 3.3154
Iteration: 1323; Percent complete: 33.1%; Average loss: 3.4971
Iteration: 1324; Percent complete: 33.1%; Average loss: 3.4169
Iteration: 1325; Percent complete: 33.1%; Average loss: 3.2081
Iteration: 1326; Percent complete: 33.1%; Average loss: 3.3209
Iteration: 1327; Percent complete: 33.2%; Average loss: 3.3936
Iteration: 1328; Percent complete: 33.2%; Average loss: 3.6791
Iteration: 1329; Percent complete: 33.2%; Average loss: 3.0768
Iteration: 1330; Percent complete: 33.2%; Average loss: 3.2382
Iteration: 1331; Percent complete: 33.3%; Average loss: 3.1356
Iteration: 1332; Percent complete: 33.3%; Average loss: 3.4362
Iteration: 1333; Percent complete: 33.3%; Average loss: 3.3664
Iteration: 1334; Percent complete: 33.4%; Average loss: 3.4706
Iteration: 1335; Percent complete: 33.4%; Average loss: 3.4224
Iteration: 1336; Percent complete: 33.4%; Average loss: 3.4528
Iteration: 1337; Percent complete: 33.4%; Average loss: 3.5693
Iteration: 1338; Percent complete: 33.5%; Average loss: 3.3248
Iteration: 1339; Percent complete: 33.5%; Average loss: 3.5140
Iteration: 1340; Percent complete: 33.5%; Average loss: 3.4347
Iteration: 1341; Percent complete: 33.5%; Average loss: 3.2553
Iteration: 1342; Percent complete: 33.6%; Average loss: 3.2469
Iteration: 1343; Percent complete: 33.6%; Average loss: 3.3042
Iteration: 1344; Percent complete: 33.6%; Average loss: 3.2831
Iteration: 1345; Percent complete: 33.6%; Average loss: 3.2968
Iteration: 1346; Percent complete: 33.7%; Average loss: 3.1196
Iteration: 1347; Percent complete: 33.7%; Average loss: 3.4326
Iteration: 1348; Percent complete: 33.7%; Average loss: 3.5663
Iteration: 1349; Percent complete: 33.7%; Average loss: 3.0985
Iteration: 1350; Percent complete: 33.8%; Average loss: 3.1694
Iteration: 1351; Percent complete: 33.8%; Average loss: 3.5839
Iteration: 1352; Percent complete: 33.8%; Average loss: 3.5751
Iteration: 1353; Percent complete: 33.8%; Average loss: 3.1925
Iteration: 1354; Percent complete: 33.9%; Average loss: 3.0739
Iteration: 1355; Percent complete: 33.9%; Average loss: 3.4630
Iteration: 1356; Percent complete: 33.9%; Average loss: 3.4070
Iteration: 1357; Percent complete: 33.9%; Average loss: 3.4054
Iteration: 1358; Percent complete: 34.0%; Average loss: 3.8000
Iteration: 1359; Percent complete: 34.0%; Average loss: 3.2166
Iteration: 1360; Percent complete: 34.0%; Average loss: 3.5341
Iteration: 1361; Percent complete: 34.0%; Average loss: 3.1658
Iteration: 1362; Percent complete: 34.1%; Average loss: 3.5160
Iteration: 1363; Percent complete: 34.1%; Average loss: 3.1900
Iteration: 1364; Percent complete: 34.1%; Average loss: 3.3847
Iteration: 1365; Percent complete: 34.1%; Average loss: 3.3239
Iteration: 1366; Percent complete: 34.2%; Average loss: 3.2847
Iteration: 1367; Percent complete: 34.2%; Average loss: 3.5521
Iteration: 1368; Percent complete: 34.2%; Average loss: 3.4128
Iteration: 1369; Percent complete: 34.2%; Average loss: 3.4265
Iteration: 1370; Percent complete: 34.2%; Average loss: 3.3772
Iteration: 1371; Percent complete: 34.3%; Average loss: 3.3410
Iteration: 1372; Percent complete: 34.3%; Average loss: 3.3797
Iteration: 1373; Percent complete: 34.3%; Average loss: 3.4849
Iteration: 1374; Percent complete: 34.4%; Average loss: 3.3541
Iteration: 1375; Percent complete: 34.4%; Average loss: 3.0653
Iteration: 1376; Percent complete: 34.4%; Average loss: 3.3230
Iteration: 1377; Percent complete: 34.4%; Average loss: 3.2020
Iteration: 1378; Percent complete: 34.4%; Average loss: 3.2310
Iteration: 1379; Percent complete: 34.5%; Average loss: 3.4130
Iteration: 1380; Percent complete: 34.5%; Average loss: 2.9774
Iteration: 1381; Percent complete: 34.5%; Average loss: 3.4025
Iteration: 1382; Percent complete: 34.5%; Average loss: 3.5000
Iteration: 1383; Percent complete: 34.6%; Average loss: 3.2790
Iteration: 1384; Percent complete: 34.6%; Average loss: 3.6877
Iteration: 1385; Percent complete: 34.6%; Average loss: 3.6276
Iteration: 1386; Percent complete: 34.6%; Average loss: 3.3119
Iteration: 1387; Percent complete: 34.7%; Average loss: 3.3071
Iteration: 1388; Percent complete: 34.7%; Average loss: 3.1553
Iteration: 1389; Percent complete: 34.7%; Average loss: 3.4375
Iteration: 1390; Percent complete: 34.8%; Average loss: 3.1378
Iteration: 1391; Percent complete: 34.8%; Average loss: 3.1351
Iteration: 1392; Percent complete: 34.8%; Average loss: 3.3266
Iteration: 1393; Percent complete: 34.8%; Average loss: 3.1936
Iteration: 1394; Percent complete: 34.8%; Average loss: 3.0613
Iteration: 1395; Percent complete: 34.9%; Average loss: 3.4180
Iteration: 1396; Percent complete: 34.9%; Average loss: 3.3024
Iteration: 1397; Percent complete: 34.9%; Average loss: 3.3265
Iteration: 1398; Percent complete: 34.9%; Average loss: 3.3254
Iteration: 1399; Percent complete: 35.0%; Average loss: 3.6290
Iteration: 1400; Percent complete: 35.0%; Average loss: 3.1794
Iteration: 1401; Percent complete: 35.0%; Average loss: 3.6308
Iteration: 1402; Percent complete: 35.0%; Average loss: 3.1938
Iteration: 1403; Percent complete: 35.1%; Average loss: 3.6219
Iteration: 1404; Percent complete: 35.1%; Average loss: 3.4049
Iteration: 1405; Percent complete: 35.1%; Average loss: 3.2912
Iteration: 1406; Percent complete: 35.1%; Average loss: 3.1452
Iteration: 1407; Percent complete: 35.2%; Average loss: 3.3914
Iteration: 1408; Percent complete: 35.2%; Average loss: 3.4154
Iteration: 1409; Percent complete: 35.2%; Average loss: 3.4537
Iteration: 1410; Percent complete: 35.2%; Average loss: 3.5983
Iteration: 1411; Percent complete: 35.3%; Average loss: 3.1899
Iteration: 1412; Percent complete: 35.3%; Average loss: 3.2384
Iteration: 1413; Percent complete: 35.3%; Average loss: 3.3096
Iteration: 1414; Percent complete: 35.4%; Average loss: 3.6383
Iteration: 1415; Percent complete: 35.4%; Average loss: 3.4834
Iteration: 1416; Percent complete: 35.4%; Average loss: 3.5056
Iteration: 1417; Percent complete: 35.4%; Average loss: 3.4362
Iteration: 1418; Percent complete: 35.4%; Average loss: 3.2877
Iteration: 1419; Percent complete: 35.5%; Average loss: 3.4066
Iteration: 1420; Percent complete: 35.5%; Average loss: 3.1265
Iteration: 1421; Percent complete: 35.5%; Average loss: 3.2515
Iteration: 1422; Percent complete: 35.5%; Average loss: 3.3655
Iteration: 1423; Percent complete: 35.6%; Average loss: 3.3727
Iteration: 1424; Percent complete: 35.6%; Average loss: 3.3856
Iteration: 1425; Percent complete: 35.6%; Average loss: 3.3604
Iteration: 1426; Percent complete: 35.6%; Average loss: 3.3639
Iteration: 1427; Percent complete: 35.7%; Average loss: 3.2383
Iteration: 1428; Percent complete: 35.7%; Average loss: 3.3694
Iteration: 1429; Percent complete: 35.7%; Average loss: 3.3980
Iteration: 1430; Percent complete: 35.8%; Average loss: 3.3576
Iteration: 1431; Percent complete: 35.8%; Average loss: 3.4987
Iteration: 1432; Percent complete: 35.8%; Average loss: 3.4660
Iteration: 1433; Percent complete: 35.8%; Average loss: 3.3788
Iteration: 1434; Percent complete: 35.9%; Average loss: 3.3156
Iteration: 1435; Percent complete: 35.9%; Average loss: 3.5199
Iteration: 1436; Percent complete: 35.9%; Average loss: 3.2436
Iteration: 1437; Percent complete: 35.9%; Average loss: 3.1873
Iteration: 1438; Percent complete: 35.9%; Average loss: 3.5283
Iteration: 1439; Percent complete: 36.0%; Average loss: 3.2382
Iteration: 1440; Percent complete: 36.0%; Average loss: 3.2461
Iteration: 1441; Percent complete: 36.0%; Average loss: 3.3011
Iteration: 1442; Percent complete: 36.0%; Average loss: 3.6347
Iteration: 1443; Percent complete: 36.1%; Average loss: 3.2159
Iteration: 1444; Percent complete: 36.1%; Average loss: 3.1145
Iteration: 1445; Percent complete: 36.1%; Average loss: 3.1570
Iteration: 1446; Percent complete: 36.1%; Average loss: 3.2607
Iteration: 1447; Percent complete: 36.2%; Average loss: 3.5929
Iteration: 1448; Percent complete: 36.2%; Average loss: 3.4605
Iteration: 1449; Percent complete: 36.2%; Average loss: 2.9370
Iteration: 1450; Percent complete: 36.2%; Average loss: 3.2582
Iteration: 1451; Percent complete: 36.3%; Average loss: 3.2760
Iteration: 1452; Percent complete: 36.3%; Average loss: 3.5603
Iteration: 1453; Percent complete: 36.3%; Average loss: 3.4607
Iteration: 1454; Percent complete: 36.4%; Average loss: 3.4162
Iteration: 1455; Percent complete: 36.4%; Average loss: 3.3987
Iteration: 1456; Percent complete: 36.4%; Average loss: 3.4900
Iteration: 1457; Percent complete: 36.4%; Average loss: 3.2782
Iteration: 1458; Percent complete: 36.4%; Average loss: 3.3432
Iteration: 1459; Percent complete: 36.5%; Average loss: 3.5472
Iteration: 1460; Percent complete: 36.5%; Average loss: 3.3672
Iteration: 1461; Percent complete: 36.5%; Average loss: 3.3841
Iteration: 1462; Percent complete: 36.5%; Average loss: 3.3776
Iteration: 1463; Percent complete: 36.6%; Average loss: 3.5594
Iteration: 1464; Percent complete: 36.6%; Average loss: 3.2554
Iteration: 1465; Percent complete: 36.6%; Average loss: 3.4349
Iteration: 1466; Percent complete: 36.6%; Average loss: 3.3799
Iteration: 1467; Percent complete: 36.7%; Average loss: 3.2799
Iteration: 1468; Percent complete: 36.7%; Average loss: 3.4332
Iteration: 1469; Percent complete: 36.7%; Average loss: 3.2609
Iteration: 1470; Percent complete: 36.8%; Average loss: 3.3631
Iteration: 1471; Percent complete: 36.8%; Average loss: 3.1532
Iteration: 1472; Percent complete: 36.8%; Average loss: 3.1810
Iteration: 1473; Percent complete: 36.8%; Average loss: 3.4099
Iteration: 1474; Percent complete: 36.9%; Average loss: 3.5099
Iteration: 1475; Percent complete: 36.9%; Average loss: 3.5979
Iteration: 1476; Percent complete: 36.9%; Average loss: 3.3958
Iteration: 1477; Percent complete: 36.9%; Average loss: 3.3398
Iteration: 1478; Percent complete: 37.0%; Average loss: 3.3685
Iteration: 1479; Percent complete: 37.0%; Average loss: 3.3285
Iteration: 1480; Percent complete: 37.0%; Average loss: 3.2710
Iteration: 1481; Percent complete: 37.0%; Average loss: 3.5556
Iteration: 1482; Percent complete: 37.0%; Average loss: 2.9342
Iteration: 1483; Percent complete: 37.1%; Average loss: 3.5365
Iteration: 1484; Percent complete: 37.1%; Average loss: 3.0743
Iteration: 1485; Percent complete: 37.1%; Average loss: 3.3556
Iteration: 1486; Percent complete: 37.1%; Average loss: 3.6665
Iteration: 1487; Percent complete: 37.2%; Average loss: 3.2355
Iteration: 1488; Percent complete: 37.2%; Average loss: 3.7349
Iteration: 1489; Percent complete: 37.2%; Average loss: 3.4234
Iteration: 1490; Percent complete: 37.2%; Average loss: 3.3258
Iteration: 1491; Percent complete: 37.3%; Average loss: 3.4261
Iteration: 1492; Percent complete: 37.3%; Average loss: 3.3261
Iteration: 1493; Percent complete: 37.3%; Average loss: 3.4540
Iteration: 1494; Percent complete: 37.4%; Average loss: 3.4718
Iteration: 1495; Percent complete: 37.4%; Average loss: 3.3283
Iteration: 1496; Percent complete: 37.4%; Average loss: 3.2661
Iteration: 1497; Percent complete: 37.4%; Average loss: 3.3927
Iteration: 1498; Percent complete: 37.5%; Average loss: 3.2335
Iteration: 1499; Percent complete: 37.5%; Average loss: 3.3454
Iteration: 1500; Percent complete: 37.5%; Average loss: 3.2342
Iteration: 1501; Percent complete: 37.5%; Average loss: 3.3583
Iteration: 1502; Percent complete: 37.5%; Average loss: 3.5507
Iteration: 1503; Percent complete: 37.6%; Average loss: 3.3369
Iteration: 1504; Percent complete: 37.6%; Average loss: 3.3146
Iteration: 1505; Percent complete: 37.6%; Average loss: 3.2465
Iteration: 1506; Percent complete: 37.6%; Average loss: 3.0565
Iteration: 1507; Percent complete: 37.7%; Average loss: 3.3981
Iteration: 1508; Percent complete: 37.7%; Average loss: 3.4152
Iteration: 1509; Percent complete: 37.7%; Average loss: 3.3520
Iteration: 1510; Percent complete: 37.8%; Average loss: 3.3927
Iteration: 1511; Percent complete: 37.8%; Average loss: 3.0636
Iteration: 1512; Percent complete: 37.8%; Average loss: 3.0871
Iteration: 1513; Percent complete: 37.8%; Average loss: 3.5533
Iteration: 1514; Percent complete: 37.9%; Average loss: 3.3411
Iteration: 1515; Percent complete: 37.9%; Average loss: 3.3356
Iteration: 1516; Percent complete: 37.9%; Average loss: 3.0382
Iteration: 1517; Percent complete: 37.9%; Average loss: 3.0542
Iteration: 1518; Percent complete: 38.0%; Average loss: 3.3697
Iteration: 1519; Percent complete: 38.0%; Average loss: 3.2872
Iteration: 1520; Percent complete: 38.0%; Average loss: 3.0307
Iteration: 1521; Percent complete: 38.0%; Average loss: 3.3232
Iteration: 1522; Percent complete: 38.0%; Average loss: 3.1095
Iteration: 1523; Percent complete: 38.1%; Average loss: 3.4481
Iteration: 1524; Percent complete: 38.1%; Average loss: 3.1305
Iteration: 1525; Percent complete: 38.1%; Average loss: 3.1554
Iteration: 1526; Percent complete: 38.1%; Average loss: 3.4321
Iteration: 1527; Percent complete: 38.2%; Average loss: 3.2202
Iteration: 1528; Percent complete: 38.2%; Average loss: 3.1776
Iteration: 1529; Percent complete: 38.2%; Average loss: 3.1612
Iteration: 1530; Percent complete: 38.2%; Average loss: 3.2894
Iteration: 1531; Percent complete: 38.3%; Average loss: 3.3191
Iteration: 1532; Percent complete: 38.3%; Average loss: 3.2976
Iteration: 1533; Percent complete: 38.3%; Average loss: 3.4458
Iteration: 1534; Percent complete: 38.4%; Average loss: 3.2094
Iteration: 1535; Percent complete: 38.4%; Average loss: 3.3420
Iteration: 1536; Percent complete: 38.4%; Average loss: 3.4083
Iteration: 1537; Percent complete: 38.4%; Average loss: 3.4862
Iteration: 1538; Percent complete: 38.5%; Average loss: 3.4402
Iteration: 1539; Percent complete: 38.5%; Average loss: 3.2271
Iteration: 1540; Percent complete: 38.5%; Average loss: 3.3137
Iteration: 1541; Percent complete: 38.5%; Average loss: 3.5192
Iteration: 1542; Percent complete: 38.6%; Average loss: 3.3531
Iteration: 1543; Percent complete: 38.6%; Average loss: 3.2088
Iteration: 1544; Percent complete: 38.6%; Average loss: 3.2809
Iteration: 1545; Percent complete: 38.6%; Average loss: 3.2150
Iteration: 1546; Percent complete: 38.6%; Average loss: 3.3342
Iteration: 1547; Percent complete: 38.7%; Average loss: 3.2690
Iteration: 1548; Percent complete: 38.7%; Average loss: 3.5981
Iteration: 1549; Percent complete: 38.7%; Average loss: 3.0459
Iteration: 1550; Percent complete: 38.8%; Average loss: 3.1966
Iteration: 1551; Percent complete: 38.8%; Average loss: 3.2858
Iteration: 1552; Percent complete: 38.8%; Average loss: 3.4281
Iteration: 1553; Percent complete: 38.8%; Average loss: 3.4889
Iteration: 1554; Percent complete: 38.9%; Average loss: 3.1481
Iteration: 1555; Percent complete: 38.9%; Average loss: 3.2595
Iteration: 1556; Percent complete: 38.9%; Average loss: 2.9694
Iteration: 1557; Percent complete: 38.9%; Average loss: 3.2403
Iteration: 1558; Percent complete: 39.0%; Average loss: 3.4217
Iteration: 1559; Percent complete: 39.0%; Average loss: 3.3204
Iteration: 1560; Percent complete: 39.0%; Average loss: 3.2960
Iteration: 1561; Percent complete: 39.0%; Average loss: 3.1510
Iteration: 1562; Percent complete: 39.1%; Average loss: 3.4429
Iteration: 1563; Percent complete: 39.1%; Average loss: 3.3423
Iteration: 1564; Percent complete: 39.1%; Average loss: 3.3377
Iteration: 1565; Percent complete: 39.1%; Average loss: 3.0656
Iteration: 1566; Percent complete: 39.1%; Average loss: 3.3956
Iteration: 1567; Percent complete: 39.2%; Average loss: 3.4167
Iteration: 1568; Percent complete: 39.2%; Average loss: 3.4176
Iteration: 1569; Percent complete: 39.2%; Average loss: 3.3589
Iteration: 1570; Percent complete: 39.2%; Average loss: 3.1635
Iteration: 1571; Percent complete: 39.3%; Average loss: 3.3148
Iteration: 1572; Percent complete: 39.3%; Average loss: 3.1834
Iteration: 1573; Percent complete: 39.3%; Average loss: 3.1290
Iteration: 1574; Percent complete: 39.4%; Average loss: 3.0825
Iteration: 1575; Percent complete: 39.4%; Average loss: 3.3485
Iteration: 1576; Percent complete: 39.4%; Average loss: 3.2812
Iteration: 1577; Percent complete: 39.4%; Average loss: 3.3240
Iteration: 1578; Percent complete: 39.5%; Average loss: 3.3646
Iteration: 1579; Percent complete: 39.5%; Average loss: 3.1661
Iteration: 1580; Percent complete: 39.5%; Average loss: 3.4525
Iteration: 1581; Percent complete: 39.5%; Average loss: 3.4362
Iteration: 1582; Percent complete: 39.6%; Average loss: 3.3491
Iteration: 1583; Percent complete: 39.6%; Average loss: 3.4084
Iteration: 1584; Percent complete: 39.6%; Average loss: 3.4499
Iteration: 1585; Percent complete: 39.6%; Average loss: 3.1861
Iteration: 1586; Percent complete: 39.6%; Average loss: 3.1970
Iteration: 1587; Percent complete: 39.7%; Average loss: 3.2369
Iteration: 1588; Percent complete: 39.7%; Average loss: 3.2890
Iteration: 1589; Percent complete: 39.7%; Average loss: 3.2916
Iteration: 1590; Percent complete: 39.8%; Average loss: 3.3476
Iteration: 1591; Percent complete: 39.8%; Average loss: 2.9243
Iteration: 1592; Percent complete: 39.8%; Average loss: 3.2848
Iteration: 1593; Percent complete: 39.8%; Average loss: 3.4647
Iteration: 1594; Percent complete: 39.9%; Average loss: 3.5938
Iteration: 1595; Percent complete: 39.9%; Average loss: 3.3073
Iteration: 1596; Percent complete: 39.9%; Average loss: 3.4477
Iteration: 1597; Percent complete: 39.9%; Average loss: 3.4798
Iteration: 1598; Percent complete: 40.0%; Average loss: 3.1965
Iteration: 1599; Percent complete: 40.0%; Average loss: 3.2303
Iteration: 1600; Percent complete: 40.0%; Average loss: 3.3082
Iteration: 1601; Percent complete: 40.0%; Average loss: 3.3265
Iteration: 1602; Percent complete: 40.1%; Average loss: 3.4936
Iteration: 1603; Percent complete: 40.1%; Average loss: 3.0333
Iteration: 1604; Percent complete: 40.1%; Average loss: 3.4063
Iteration: 1605; Percent complete: 40.1%; Average loss: 3.5029
Iteration: 1606; Percent complete: 40.2%; Average loss: 3.4718
Iteration: 1607; Percent complete: 40.2%; Average loss: 3.0415
Iteration: 1608; Percent complete: 40.2%; Average loss: 3.2149
Iteration: 1609; Percent complete: 40.2%; Average loss: 3.2805
Iteration: 1610; Percent complete: 40.2%; Average loss: 3.3799
Iteration: 1611; Percent complete: 40.3%; Average loss: 3.3280
Iteration: 1612; Percent complete: 40.3%; Average loss: 3.1124
Iteration: 1613; Percent complete: 40.3%; Average loss: 3.4279
Iteration: 1614; Percent complete: 40.4%; Average loss: 3.2256
Iteration: 1615; Percent complete: 40.4%; Average loss: 3.1454
Iteration: 1616; Percent complete: 40.4%; Average loss: 3.2338
Iteration: 1617; Percent complete: 40.4%; Average loss: 3.1854
Iteration: 1618; Percent complete: 40.5%; Average loss: 3.2117
Iteration: 1619; Percent complete: 40.5%; Average loss: 3.5425
Iteration: 1620; Percent complete: 40.5%; Average loss: 3.2872
Iteration: 1621; Percent complete: 40.5%; Average loss: 3.2356
Iteration: 1622; Percent complete: 40.6%; Average loss: 3.2958
Iteration: 1623; Percent complete: 40.6%; Average loss: 3.3555
Iteration: 1624; Percent complete: 40.6%; Average loss: 3.5200
Iteration: 1625; Percent complete: 40.6%; Average loss: 3.0883
Iteration: 1626; Percent complete: 40.6%; Average loss: 3.4337
Iteration: 1627; Percent complete: 40.7%; Average loss: 3.5496
Iteration: 1628; Percent complete: 40.7%; Average loss: 3.2069
Iteration: 1629; Percent complete: 40.7%; Average loss: 3.4837
Iteration: 1630; Percent complete: 40.8%; Average loss: 3.3554
Iteration: 1631; Percent complete: 40.8%; Average loss: 3.4038
Iteration: 1632; Percent complete: 40.8%; Average loss: 3.3578
Iteration: 1633; Percent complete: 40.8%; Average loss: 3.4242
Iteration: 1634; Percent complete: 40.8%; Average loss: 3.0809
Iteration: 1635; Percent complete: 40.9%; Average loss: 3.3361
Iteration: 1636; Percent complete: 40.9%; Average loss: 3.2553
Iteration: 1637; Percent complete: 40.9%; Average loss: 3.0617
Iteration: 1638; Percent complete: 40.9%; Average loss: 3.0215
Iteration: 1639; Percent complete: 41.0%; Average loss: 3.2551
Iteration: 1640; Percent complete: 41.0%; Average loss: 3.2298
Iteration: 1641; Percent complete: 41.0%; Average loss: 3.4044
Iteration: 1642; Percent complete: 41.0%; Average loss: 3.2034
Iteration: 1643; Percent complete: 41.1%; Average loss: 3.2290
Iteration: 1644; Percent complete: 41.1%; Average loss: 3.2053
Iteration: 1645; Percent complete: 41.1%; Average loss: 3.1165
Iteration: 1646; Percent complete: 41.1%; Average loss: 3.4056
Iteration: 1647; Percent complete: 41.2%; Average loss: 3.4593
Iteration: 1648; Percent complete: 41.2%; Average loss: 3.2681
Iteration: 1649; Percent complete: 41.2%; Average loss: 3.1222
Iteration: 1650; Percent complete: 41.2%; Average loss: 3.4429
Iteration: 1651; Percent complete: 41.3%; Average loss: 3.3108
Iteration: 1652; Percent complete: 41.3%; Average loss: 3.0939
Iteration: 1653; Percent complete: 41.3%; Average loss: 2.9088
Iteration: 1654; Percent complete: 41.3%; Average loss: 3.3454
Iteration: 1655; Percent complete: 41.4%; Average loss: 3.1446
Iteration: 1656; Percent complete: 41.4%; Average loss: 3.0494
Iteration: 1657; Percent complete: 41.4%; Average loss: 3.2163
Iteration: 1658; Percent complete: 41.4%; Average loss: 3.3137
Iteration: 1659; Percent complete: 41.5%; Average loss: 3.0870
Iteration: 1660; Percent complete: 41.5%; Average loss: 3.2541
Iteration: 1661; Percent complete: 41.5%; Average loss: 3.4036
Iteration: 1662; Percent complete: 41.5%; Average loss: 3.2368
Iteration: 1663; Percent complete: 41.6%; Average loss: 3.1352
Iteration: 1664; Percent complete: 41.6%; Average loss: 3.2808
Iteration: 1665; Percent complete: 41.6%; Average loss: 3.0967
Iteration: 1666; Percent complete: 41.6%; Average loss: 3.3245
Iteration: 1667; Percent complete: 41.7%; Average loss: 3.4882
Iteration: 1668; Percent complete: 41.7%; Average loss: 3.2723
Iteration: 1669; Percent complete: 41.7%; Average loss: 3.4296
Iteration: 1670; Percent complete: 41.8%; Average loss: 3.1698
Iteration: 1671; Percent complete: 41.8%; Average loss: 3.2829
Iteration: 1672; Percent complete: 41.8%; Average loss: 3.0076
Iteration: 1673; Percent complete: 41.8%; Average loss: 3.2739
Iteration: 1674; Percent complete: 41.9%; Average loss: 3.4148
Iteration: 1675; Percent complete: 41.9%; Average loss: 3.3145
Iteration: 1676; Percent complete: 41.9%; Average loss: 3.1481
Iteration: 1677; Percent complete: 41.9%; Average loss: 2.9845
Iteration: 1678; Percent complete: 41.9%; Average loss: 3.1237
Iteration: 1679; Percent complete: 42.0%; Average loss: 3.0163
Iteration: 1680; Percent complete: 42.0%; Average loss: 3.0681
Iteration: 1681; Percent complete: 42.0%; Average loss: 3.1463
Iteration: 1682; Percent complete: 42.0%; Average loss: 3.1257
Iteration: 1683; Percent complete: 42.1%; Average loss: 3.3441
Iteration: 1684; Percent complete: 42.1%; Average loss: 3.3192
Iteration: 1685; Percent complete: 42.1%; Average loss: 3.1999
Iteration: 1686; Percent complete: 42.1%; Average loss: 3.1435
Iteration: 1687; Percent complete: 42.2%; Average loss: 3.1975
Iteration: 1688; Percent complete: 42.2%; Average loss: 3.0775
Iteration: 1689; Percent complete: 42.2%; Average loss: 3.4429
Iteration: 1690; Percent complete: 42.2%; Average loss: 3.3501
Iteration: 1691; Percent complete: 42.3%; Average loss: 3.2284
Iteration: 1692; Percent complete: 42.3%; Average loss: 3.2926
Iteration: 1693; Percent complete: 42.3%; Average loss: 3.0568
Iteration: 1694; Percent complete: 42.4%; Average loss: 3.2421
Iteration: 1695; Percent complete: 42.4%; Average loss: 3.1694
Iteration: 1696; Percent complete: 42.4%; Average loss: 3.2055
Iteration: 1697; Percent complete: 42.4%; Average loss: 3.1804
Iteration: 1698; Percent complete: 42.4%; Average loss: 3.0111
Iteration: 1699; Percent complete: 42.5%; Average loss: 3.2921
Iteration: 1700; Percent complete: 42.5%; Average loss: 3.3794
Iteration: 1701; Percent complete: 42.5%; Average loss: 3.2864
Iteration: 1702; Percent complete: 42.5%; Average loss: 3.3279
Iteration: 1703; Percent complete: 42.6%; Average loss: 3.2183
Iteration: 1704; Percent complete: 42.6%; Average loss: 3.2022
Iteration: 1705; Percent complete: 42.6%; Average loss: 3.2489
Iteration: 1706; Percent complete: 42.6%; Average loss: 3.1092
Iteration: 1707; Percent complete: 42.7%; Average loss: 3.2465
Iteration: 1708; Percent complete: 42.7%; Average loss: 3.4439
Iteration: 1709; Percent complete: 42.7%; Average loss: 3.2147
Iteration: 1710; Percent complete: 42.8%; Average loss: 3.2246
Iteration: 1711; Percent complete: 42.8%; Average loss: 3.4309
Iteration: 1712; Percent complete: 42.8%; Average loss: 3.1978
Iteration: 1713; Percent complete: 42.8%; Average loss: 3.1832
Iteration: 1714; Percent complete: 42.9%; Average loss: 2.9348
Iteration: 1715; Percent complete: 42.9%; Average loss: 3.1165
Iteration: 1716; Percent complete: 42.9%; Average loss: 3.2754
Iteration: 1717; Percent complete: 42.9%; Average loss: 3.2970
Iteration: 1718; Percent complete: 43.0%; Average loss: 3.2105
Iteration: 1719; Percent complete: 43.0%; Average loss: 3.1408
Iteration: 1720; Percent complete: 43.0%; Average loss: 3.3326
Iteration: 1721; Percent complete: 43.0%; Average loss: 3.2737
Iteration: 1722; Percent complete: 43.0%; Average loss: 3.2798
Iteration: 1723; Percent complete: 43.1%; Average loss: 3.3567
Iteration: 1724; Percent complete: 43.1%; Average loss: 3.5117
Iteration: 1725; Percent complete: 43.1%; Average loss: 3.4254
Iteration: 1726; Percent complete: 43.1%; Average loss: 3.3096
Iteration: 1727; Percent complete: 43.2%; Average loss: 2.9994
Iteration: 1728; Percent complete: 43.2%; Average loss: 3.3530
Iteration: 1729; Percent complete: 43.2%; Average loss: 3.1274
Iteration: 1730; Percent complete: 43.2%; Average loss: 3.4691
Iteration: 1731; Percent complete: 43.3%; Average loss: 3.2008
Iteration: 1732; Percent complete: 43.3%; Average loss: 3.4627
Iteration: 1733; Percent complete: 43.3%; Average loss: 3.0892
Iteration: 1734; Percent complete: 43.4%; Average loss: 3.3325
Iteration: 1735; Percent complete: 43.4%; Average loss: 3.2009
Iteration: 1736; Percent complete: 43.4%; Average loss: 3.3588
Iteration: 1737; Percent complete: 43.4%; Average loss: 3.4514
Iteration: 1738; Percent complete: 43.5%; Average loss: 3.0880
Iteration: 1739; Percent complete: 43.5%; Average loss: 3.6023
Iteration: 1740; Percent complete: 43.5%; Average loss: 3.2950
Iteration: 1741; Percent complete: 43.5%; Average loss: 3.1759
Iteration: 1742; Percent complete: 43.5%; Average loss: 3.1124
Iteration: 1743; Percent complete: 43.6%; Average loss: 3.1621
Iteration: 1744; Percent complete: 43.6%; Average loss: 3.2698
Iteration: 1745; Percent complete: 43.6%; Average loss: 3.4637
Iteration: 1746; Percent complete: 43.6%; Average loss: 3.1413
Iteration: 1747; Percent complete: 43.7%; Average loss: 3.3707
Iteration: 1748; Percent complete: 43.7%; Average loss: 3.4810
Iteration: 1749; Percent complete: 43.7%; Average loss: 3.1620
Iteration: 1750; Percent complete: 43.8%; Average loss: 3.3594
Iteration: 1751; Percent complete: 43.8%; Average loss: 3.4423
Iteration: 1752; Percent complete: 43.8%; Average loss: 3.3311
Iteration: 1753; Percent complete: 43.8%; Average loss: 3.3244
Iteration: 1754; Percent complete: 43.9%; Average loss: 3.3408
Iteration: 1755; Percent complete: 43.9%; Average loss: 3.3609
Iteration: 1756; Percent complete: 43.9%; Average loss: 3.3307
Iteration: 1757; Percent complete: 43.9%; Average loss: 3.6105
Iteration: 1758; Percent complete: 44.0%; Average loss: 3.1019
Iteration: 1759; Percent complete: 44.0%; Average loss: 3.3361
Iteration: 1760; Percent complete: 44.0%; Average loss: 3.1821
Iteration: 1761; Percent complete: 44.0%; Average loss: 3.0467
Iteration: 1762; Percent complete: 44.0%; Average loss: 3.0959
Iteration: 1763; Percent complete: 44.1%; Average loss: 3.2248
Iteration: 1764; Percent complete: 44.1%; Average loss: 3.4957
Iteration: 1765; Percent complete: 44.1%; Average loss: 3.3497
Iteration: 1766; Percent complete: 44.1%; Average loss: 3.4560
Iteration: 1767; Percent complete: 44.2%; Average loss: 3.4778
Iteration: 1768; Percent complete: 44.2%; Average loss: 3.1772
Iteration: 1769; Percent complete: 44.2%; Average loss: 3.1882
Iteration: 1770; Percent complete: 44.2%; Average loss: 3.3254
Iteration: 1771; Percent complete: 44.3%; Average loss: 3.2395
Iteration: 1772; Percent complete: 44.3%; Average loss: 3.1329
Iteration: 1773; Percent complete: 44.3%; Average loss: 3.0258
Iteration: 1774; Percent complete: 44.4%; Average loss: 3.1821
Iteration: 1775; Percent complete: 44.4%; Average loss: 3.1454
Iteration: 1776; Percent complete: 44.4%; Average loss: 3.1472
Iteration: 1777; Percent complete: 44.4%; Average loss: 3.1871
Iteration: 1778; Percent complete: 44.5%; Average loss: 3.0499
Iteration: 1779; Percent complete: 44.5%; Average loss: 3.2367
Iteration: 1780; Percent complete: 44.5%; Average loss: 3.2122
Iteration: 1781; Percent complete: 44.5%; Average loss: 3.2287
Iteration: 1782; Percent complete: 44.5%; Average loss: 3.1984
Iteration: 1783; Percent complete: 44.6%; Average loss: 3.2691
Iteration: 1784; Percent complete: 44.6%; Average loss: 3.0927
Iteration: 1785; Percent complete: 44.6%; Average loss: 3.1588
Iteration: 1786; Percent complete: 44.6%; Average loss: 3.2080
Iteration: 1787; Percent complete: 44.7%; Average loss: 3.3672
Iteration: 1788; Percent complete: 44.7%; Average loss: 3.3192
Iteration: 1789; Percent complete: 44.7%; Average loss: 3.3378
Iteration: 1790; Percent complete: 44.8%; Average loss: 3.0083
Iteration: 1791; Percent complete: 44.8%; Average loss: 3.2315
Iteration: 1792; Percent complete: 44.8%; Average loss: 3.2118
Iteration: 1793; Percent complete: 44.8%; Average loss: 3.1078
Iteration: 1794; Percent complete: 44.9%; Average loss: 3.1438
Iteration: 1795; Percent complete: 44.9%; Average loss: 3.3424
Iteration: 1796; Percent complete: 44.9%; Average loss: 3.0927
Iteration: 1797; Percent complete: 44.9%; Average loss: 3.3067
Iteration: 1798; Percent complete: 45.0%; Average loss: 3.1832
Iteration: 1799; Percent complete: 45.0%; Average loss: 3.1486
Iteration: 1800; Percent complete: 45.0%; Average loss: 3.1468
Iteration: 1801; Percent complete: 45.0%; Average loss: 3.3576
Iteration: 1802; Percent complete: 45.1%; Average loss: 3.3435
Iteration: 1803; Percent complete: 45.1%; Average loss: 3.2118
Iteration: 1804; Percent complete: 45.1%; Average loss: 3.1194
Iteration: 1805; Percent complete: 45.1%; Average loss: 3.1760
Iteration: 1806; Percent complete: 45.1%; Average loss: 3.0876
Iteration: 1807; Percent complete: 45.2%; Average loss: 3.0001
Iteration: 1808; Percent complete: 45.2%; Average loss: 3.3212
Iteration: 1809; Percent complete: 45.2%; Average loss: 3.0747
Iteration: 1810; Percent complete: 45.2%; Average loss: 3.1378
Iteration: 1811; Percent complete: 45.3%; Average loss: 3.2143
Iteration: 1812; Percent complete: 45.3%; Average loss: 3.2845
Iteration: 1813; Percent complete: 45.3%; Average loss: 3.2456
Iteration: 1814; Percent complete: 45.4%; Average loss: 3.4705
Iteration: 1815; Percent complete: 45.4%; Average loss: 3.4603
Iteration: 1816; Percent complete: 45.4%; Average loss: 3.1711
Iteration: 1817; Percent complete: 45.4%; Average loss: 3.3940
Iteration: 1818; Percent complete: 45.5%; Average loss: 3.2837
Iteration: 1819; Percent complete: 45.5%; Average loss: 3.1576
Iteration: 1820; Percent complete: 45.5%; Average loss: 3.1928
Iteration: 1821; Percent complete: 45.5%; Average loss: 3.2113
Iteration: 1822; Percent complete: 45.6%; Average loss: 3.2331
Iteration: 1823; Percent complete: 45.6%; Average loss: 2.8564
Iteration: 1824; Percent complete: 45.6%; Average loss: 3.2547
Iteration: 1825; Percent complete: 45.6%; Average loss: 3.1266
Iteration: 1826; Percent complete: 45.6%; Average loss: 3.2477
Iteration: 1827; Percent complete: 45.7%; Average loss: 3.2799
Iteration: 1828; Percent complete: 45.7%; Average loss: 3.2873
Iteration: 1829; Percent complete: 45.7%; Average loss: 3.0923
Iteration: 1830; Percent complete: 45.8%; Average loss: 3.6774
Iteration: 1831; Percent complete: 45.8%; Average loss: 3.3871
Iteration: 1832; Percent complete: 45.8%; Average loss: 3.3018
Iteration: 1833; Percent complete: 45.8%; Average loss: 3.4188
Iteration: 1834; Percent complete: 45.9%; Average loss: 3.0830
Iteration: 1835; Percent complete: 45.9%; Average loss: 3.3541
Iteration: 1836; Percent complete: 45.9%; Average loss: 3.2012
Iteration: 1837; Percent complete: 45.9%; Average loss: 3.0433
Iteration: 1838; Percent complete: 46.0%; Average loss: 2.9448
Iteration: 1839; Percent complete: 46.0%; Average loss: 3.1363
Iteration: 1840; Percent complete: 46.0%; Average loss: 3.0838
Iteration: 1841; Percent complete: 46.0%; Average loss: 3.1054
Iteration: 1842; Percent complete: 46.1%; Average loss: 2.9844
Iteration: 1843; Percent complete: 46.1%; Average loss: 3.3757
Iteration: 1844; Percent complete: 46.1%; Average loss: 3.4922
Iteration: 1845; Percent complete: 46.1%; Average loss: 3.3765
Iteration: 1846; Percent complete: 46.2%; Average loss: 3.2166
Iteration: 1847; Percent complete: 46.2%; Average loss: 3.4877
Iteration: 1848; Percent complete: 46.2%; Average loss: 3.2102
Iteration: 1849; Percent complete: 46.2%; Average loss: 3.1843
Iteration: 1850; Percent complete: 46.2%; Average loss: 3.1585
Iteration: 1851; Percent complete: 46.3%; Average loss: 3.0850
Iteration: 1852; Percent complete: 46.3%; Average loss: 3.0443
Iteration: 1853; Percent complete: 46.3%; Average loss: 3.0474
Iteration: 1854; Percent complete: 46.4%; Average loss: 3.0667
Iteration: 1855; Percent complete: 46.4%; Average loss: 3.1823
Iteration: 1856; Percent complete: 46.4%; Average loss: 3.1642
Iteration: 1857; Percent complete: 46.4%; Average loss: 2.9919
Iteration: 1858; Percent complete: 46.5%; Average loss: 3.0739
Iteration: 1859; Percent complete: 46.5%; Average loss: 3.2227
Iteration: 1860; Percent complete: 46.5%; Average loss: 3.4637
Iteration: 1861; Percent complete: 46.5%; Average loss: 3.3900
Iteration: 1862; Percent complete: 46.6%; Average loss: 3.3671
Iteration: 1863; Percent complete: 46.6%; Average loss: 3.2458
Iteration: 1864; Percent complete: 46.6%; Average loss: 3.0434
Iteration: 1865; Percent complete: 46.6%; Average loss: 3.1118
Iteration: 1866; Percent complete: 46.7%; Average loss: 3.1986
Iteration: 1867; Percent complete: 46.7%; Average loss: 3.2798
Iteration: 1868; Percent complete: 46.7%; Average loss: 3.0425
Iteration: 1869; Percent complete: 46.7%; Average loss: 3.0024
Iteration: 1870; Percent complete: 46.8%; Average loss: 3.4101
Iteration: 1871; Percent complete: 46.8%; Average loss: 3.0298
Iteration: 1872; Percent complete: 46.8%; Average loss: 3.0817
Iteration: 1873; Percent complete: 46.8%; Average loss: 3.3114
Iteration: 1874; Percent complete: 46.9%; Average loss: 3.1915
Iteration: 1875; Percent complete: 46.9%; Average loss: 2.9455
Iteration: 1876; Percent complete: 46.9%; Average loss: 3.0969
Iteration: 1877; Percent complete: 46.9%; Average loss: 3.2609
Iteration: 1878; Percent complete: 46.9%; Average loss: 3.1630
Iteration: 1879; Percent complete: 47.0%; Average loss: 3.1499
Iteration: 1880; Percent complete: 47.0%; Average loss: 3.2294
Iteration: 1881; Percent complete: 47.0%; Average loss: 3.1489
Iteration: 1882; Percent complete: 47.0%; Average loss: 3.1579
Iteration: 1883; Percent complete: 47.1%; Average loss: 3.1377
Iteration: 1884; Percent complete: 47.1%; Average loss: 3.1614
Iteration: 1885; Percent complete: 47.1%; Average loss: 3.2382
Iteration: 1886; Percent complete: 47.1%; Average loss: 2.8228
Iteration: 1887; Percent complete: 47.2%; Average loss: 3.0212
Iteration: 1888; Percent complete: 47.2%; Average loss: 3.2332
Iteration: 1889; Percent complete: 47.2%; Average loss: 3.0741
Iteration: 1890; Percent complete: 47.2%; Average loss: 3.3743
Iteration: 1891; Percent complete: 47.3%; Average loss: 3.0351
Iteration: 1892; Percent complete: 47.3%; Average loss: 3.2789
Iteration: 1893; Percent complete: 47.3%; Average loss: 3.0805
Iteration: 1894; Percent complete: 47.3%; Average loss: 3.2444
Iteration: 1895; Percent complete: 47.4%; Average loss: 3.1813
Iteration: 1896; Percent complete: 47.4%; Average loss: 2.9114
Iteration: 1897; Percent complete: 47.4%; Average loss: 3.2814
Iteration: 1898; Percent complete: 47.4%; Average loss: 3.2162
Iteration: 1899; Percent complete: 47.5%; Average loss: 3.2158
Iteration: 1900; Percent complete: 47.5%; Average loss: 3.2281
Iteration: 1901; Percent complete: 47.5%; Average loss: 3.2652
Iteration: 1902; Percent complete: 47.5%; Average loss: 3.0684
Iteration: 1903; Percent complete: 47.6%; Average loss: 3.1480
Iteration: 1904; Percent complete: 47.6%; Average loss: 3.0012
Iteration: 1905; Percent complete: 47.6%; Average loss: 3.4863
Iteration: 1906; Percent complete: 47.6%; Average loss: 3.2037
Iteration: 1907; Percent complete: 47.7%; Average loss: 3.3240
Iteration: 1908; Percent complete: 47.7%; Average loss: 3.3889
Iteration: 1909; Percent complete: 47.7%; Average loss: 3.2657
Iteration: 1910; Percent complete: 47.8%; Average loss: 3.0695
Iteration: 1911; Percent complete: 47.8%; Average loss: 3.0461
Iteration: 1912; Percent complete: 47.8%; Average loss: 3.0855
Iteration: 1913; Percent complete: 47.8%; Average loss: 3.3415
Iteration: 1914; Percent complete: 47.9%; Average loss: 3.2660
Iteration: 1915; Percent complete: 47.9%; Average loss: 3.3069
Iteration: 1916; Percent complete: 47.9%; Average loss: 3.1336
Iteration: 1917; Percent complete: 47.9%; Average loss: 3.2413
Iteration: 1918; Percent complete: 47.9%; Average loss: 3.1759
Iteration: 1919; Percent complete: 48.0%; Average loss: 3.1083
Iteration: 1920; Percent complete: 48.0%; Average loss: 3.0906
Iteration: 1921; Percent complete: 48.0%; Average loss: 3.2096
Iteration: 1922; Percent complete: 48.0%; Average loss: 3.3555
Iteration: 1923; Percent complete: 48.1%; Average loss: 2.7038
Iteration: 1924; Percent complete: 48.1%; Average loss: 3.0970
Iteration: 1925; Percent complete: 48.1%; Average loss: 3.2579
Iteration: 1926; Percent complete: 48.1%; Average loss: 3.2623
Iteration: 1927; Percent complete: 48.2%; Average loss: 3.1448
Iteration: 1928; Percent complete: 48.2%; Average loss: 3.0334
Iteration: 1929; Percent complete: 48.2%; Average loss: 3.3225
Iteration: 1930; Percent complete: 48.2%; Average loss: 3.4636
Iteration: 1931; Percent complete: 48.3%; Average loss: 3.0732
Iteration: 1932; Percent complete: 48.3%; Average loss: 2.9041
Iteration: 1933; Percent complete: 48.3%; Average loss: 3.0589
Iteration: 1934; Percent complete: 48.4%; Average loss: 3.1853
Iteration: 1935; Percent complete: 48.4%; Average loss: 3.2347
Iteration: 1936; Percent complete: 48.4%; Average loss: 3.1921
Iteration: 1937; Percent complete: 48.4%; Average loss: 3.3058
Iteration: 1938; Percent complete: 48.4%; Average loss: 3.1646
Iteration: 1939; Percent complete: 48.5%; Average loss: 3.1610
Iteration: 1940; Percent complete: 48.5%; Average loss: 3.0315
Iteration: 1941; Percent complete: 48.5%; Average loss: 3.1535
Iteration: 1942; Percent complete: 48.5%; Average loss: 3.2861
Iteration: 1943; Percent complete: 48.6%; Average loss: 3.1328
Iteration: 1944; Percent complete: 48.6%; Average loss: 3.2201
Iteration: 1945; Percent complete: 48.6%; Average loss: 3.5356
Iteration: 1946; Percent complete: 48.6%; Average loss: 3.1633
Iteration: 1947; Percent complete: 48.7%; Average loss: 3.1976
Iteration: 1948; Percent complete: 48.7%; Average loss: 3.0311
Iteration: 1949; Percent complete: 48.7%; Average loss: 3.4672
Iteration: 1950; Percent complete: 48.8%; Average loss: 2.9158
Iteration: 1951; Percent complete: 48.8%; Average loss: 3.1156
Iteration: 1952; Percent complete: 48.8%; Average loss: 2.9029
Iteration: 1953; Percent complete: 48.8%; Average loss: 3.2531
Iteration: 1954; Percent complete: 48.9%; Average loss: 3.0609
Iteration: 1955; Percent complete: 48.9%; Average loss: 3.2488
Iteration: 1956; Percent complete: 48.9%; Average loss: 3.2038
Iteration: 1957; Percent complete: 48.9%; Average loss: 3.1243
Iteration: 1958; Percent complete: 48.9%; Average loss: 3.2534
Iteration: 1959; Percent complete: 49.0%; Average loss: 3.1924
Iteration: 1960; Percent complete: 49.0%; Average loss: 3.1940
Iteration: 1961; Percent complete: 49.0%; Average loss: 3.3061
Iteration: 1962; Percent complete: 49.0%; Average loss: 3.1187
Iteration: 1963; Percent complete: 49.1%; Average loss: 3.1034
Iteration: 1964; Percent complete: 49.1%; Average loss: 2.9845
Iteration: 1965; Percent complete: 49.1%; Average loss: 3.1251
Iteration: 1966; Percent complete: 49.1%; Average loss: 3.3741
Iteration: 1967; Percent complete: 49.2%; Average loss: 2.9719
Iteration: 1968; Percent complete: 49.2%; Average loss: 3.3161
Iteration: 1969; Percent complete: 49.2%; Average loss: 2.8727
Iteration: 1970; Percent complete: 49.2%; Average loss: 3.3538
Iteration: 1971; Percent complete: 49.3%; Average loss: 3.0560
Iteration: 1972; Percent complete: 49.3%; Average loss: 3.0091
Iteration: 1973; Percent complete: 49.3%; Average loss: 3.1630
Iteration: 1974; Percent complete: 49.4%; Average loss: 3.1195
Iteration: 1975; Percent complete: 49.4%; Average loss: 3.5873
Iteration: 1976; Percent complete: 49.4%; Average loss: 3.4223
Iteration: 1977; Percent complete: 49.4%; Average loss: 3.4113
Iteration: 1978; Percent complete: 49.5%; Average loss: 3.1963
Iteration: 1979; Percent complete: 49.5%; Average loss: 3.2918
Iteration: 1980; Percent complete: 49.5%; Average loss: 3.1942
Iteration: 1981; Percent complete: 49.5%; Average loss: 3.2366
Iteration: 1982; Percent complete: 49.5%; Average loss: 3.0893
Iteration: 1983; Percent complete: 49.6%; Average loss: 3.0944
Iteration: 1984; Percent complete: 49.6%; Average loss: 2.9211
Iteration: 1985; Percent complete: 49.6%; Average loss: 3.0974
Iteration: 1986; Percent complete: 49.6%; Average loss: 3.2170
Iteration: 1987; Percent complete: 49.7%; Average loss: 3.1702
Iteration: 1988; Percent complete: 49.7%; Average loss: 3.1153
Iteration: 1989; Percent complete: 49.7%; Average loss: 3.0498
Iteration: 1990; Percent complete: 49.8%; Average loss: 3.0951
Iteration: 1991; Percent complete: 49.8%; Average loss: 3.3470
Iteration: 1992; Percent complete: 49.8%; Average loss: 3.3595
Iteration: 1993; Percent complete: 49.8%; Average loss: 3.0150
Iteration: 1994; Percent complete: 49.9%; Average loss: 3.0618
Iteration: 1995; Percent complete: 49.9%; Average loss: 2.9816
Iteration: 1996; Percent complete: 49.9%; Average loss: 2.9674
Iteration: 1997; Percent complete: 49.9%; Average loss: 3.5206
Iteration: 1998; Percent complete: 50.0%; Average loss: 3.4279
Iteration: 1999; Percent complete: 50.0%; Average loss: 3.2094
Iteration: 2000; Percent complete: 50.0%; Average loss: 2.9671
Iteration: 2001; Percent complete: 50.0%; Average loss: 3.2539
Iteration: 2002; Percent complete: 50.0%; Average loss: 3.0392
Iteration: 2003; Percent complete: 50.1%; Average loss: 3.0933
Iteration: 2004; Percent complete: 50.1%; Average loss: 3.4691
Iteration: 2005; Percent complete: 50.1%; Average loss: 3.2291
Iteration: 2006; Percent complete: 50.1%; Average loss: 3.2952
Iteration: 2007; Percent complete: 50.2%; Average loss: 3.0817
Iteration: 2008; Percent complete: 50.2%; Average loss: 3.0328
Iteration: 2009; Percent complete: 50.2%; Average loss: 3.0837
Iteration: 2010; Percent complete: 50.2%; Average loss: 3.3648
Iteration: 2011; Percent complete: 50.3%; Average loss: 3.3357
Iteration: 2012; Percent complete: 50.3%; Average loss: 3.2764
Iteration: 2013; Percent complete: 50.3%; Average loss: 3.2454
Iteration: 2014; Percent complete: 50.3%; Average loss: 2.9803
Iteration: 2015; Percent complete: 50.4%; Average loss: 3.2560
Iteration: 2016; Percent complete: 50.4%; Average loss: 3.2916
Iteration: 2017; Percent complete: 50.4%; Average loss: 2.9885
Iteration: 2018; Percent complete: 50.4%; Average loss: 3.1135
Iteration: 2019; Percent complete: 50.5%; Average loss: 3.3137
Iteration: 2020; Percent complete: 50.5%; Average loss: 3.2685
Iteration: 2021; Percent complete: 50.5%; Average loss: 3.2411
Iteration: 2022; Percent complete: 50.5%; Average loss: 3.0517
Iteration: 2023; Percent complete: 50.6%; Average loss: 3.1009
Iteration: 2024; Percent complete: 50.6%; Average loss: 2.9962
Iteration: 2025; Percent complete: 50.6%; Average loss: 2.8266
Iteration: 2026; Percent complete: 50.6%; Average loss: 3.3397
Iteration: 2027; Percent complete: 50.7%; Average loss: 2.9954
Iteration: 2028; Percent complete: 50.7%; Average loss: 3.3947
Iteration: 2029; Percent complete: 50.7%; Average loss: 2.9282
Iteration: 2030; Percent complete: 50.7%; Average loss: 2.8987
Iteration: 2031; Percent complete: 50.8%; Average loss: 3.2468
Iteration: 2032; Percent complete: 50.8%; Average loss: 3.2660
Iteration: 2033; Percent complete: 50.8%; Average loss: 3.2576
Iteration: 2034; Percent complete: 50.8%; Average loss: 3.1521
Iteration: 2035; Percent complete: 50.9%; Average loss: 3.1029
Iteration: 2036; Percent complete: 50.9%; Average loss: 3.1468
Iteration: 2037; Percent complete: 50.9%; Average loss: 2.7858
Iteration: 2038; Percent complete: 50.9%; Average loss: 3.0302
Iteration: 2039; Percent complete: 51.0%; Average loss: 2.9199
Iteration: 2040; Percent complete: 51.0%; Average loss: 3.2064
Iteration: 2041; Percent complete: 51.0%; Average loss: 2.9712
Iteration: 2042; Percent complete: 51.0%; Average loss: 2.9949
Iteration: 2043; Percent complete: 51.1%; Average loss: 3.1905
Iteration: 2044; Percent complete: 51.1%; Average loss: 3.2360
Iteration: 2045; Percent complete: 51.1%; Average loss: 3.0355
Iteration: 2046; Percent complete: 51.1%; Average loss: 3.2660
Iteration: 2047; Percent complete: 51.2%; Average loss: 3.2619
Iteration: 2048; Percent complete: 51.2%; Average loss: 3.3057
Iteration: 2049; Percent complete: 51.2%; Average loss: 3.0181
Iteration: 2050; Percent complete: 51.2%; Average loss: 3.2143
Iteration: 2051; Percent complete: 51.3%; Average loss: 3.0008
Iteration: 2052; Percent complete: 51.3%; Average loss: 3.1396
Iteration: 2053; Percent complete: 51.3%; Average loss: 3.0965
Iteration: 2054; Percent complete: 51.3%; Average loss: 3.0353
Iteration: 2055; Percent complete: 51.4%; Average loss: 3.3068
Iteration: 2056; Percent complete: 51.4%; Average loss: 3.2964
Iteration: 2057; Percent complete: 51.4%; Average loss: 3.1602
Iteration: 2058; Percent complete: 51.4%; Average loss: 3.1470
Iteration: 2059; Percent complete: 51.5%; Average loss: 3.1613
Iteration: 2060; Percent complete: 51.5%; Average loss: 3.2148
Iteration: 2061; Percent complete: 51.5%; Average loss: 3.1266
Iteration: 2062; Percent complete: 51.5%; Average loss: 3.2188
Iteration: 2063; Percent complete: 51.6%; Average loss: 3.1813
Iteration: 2064; Percent complete: 51.6%; Average loss: 3.2504
Iteration: 2065; Percent complete: 51.6%; Average loss: 3.1229
Iteration: 2066; Percent complete: 51.6%; Average loss: 2.9781
Iteration: 2067; Percent complete: 51.7%; Average loss: 3.3055
Iteration: 2068; Percent complete: 51.7%; Average loss: 3.2609
Iteration: 2069; Percent complete: 51.7%; Average loss: 3.2495
Iteration: 2070; Percent complete: 51.7%; Average loss: 3.0653
Iteration: 2071; Percent complete: 51.8%; Average loss: 3.1937
Iteration: 2072; Percent complete: 51.8%; Average loss: 3.0234
Iteration: 2073; Percent complete: 51.8%; Average loss: 3.3199
Iteration: 2074; Percent complete: 51.8%; Average loss: 3.0536
Iteration: 2075; Percent complete: 51.9%; Average loss: 3.2229
Iteration: 2076; Percent complete: 51.9%; Average loss: 3.2557
Iteration: 2077; Percent complete: 51.9%; Average loss: 3.3590
Iteration: 2078; Percent complete: 51.9%; Average loss: 3.2364
Iteration: 2079; Percent complete: 52.0%; Average loss: 3.0631
Iteration: 2080; Percent complete: 52.0%; Average loss: 3.2804
Iteration: 2081; Percent complete: 52.0%; Average loss: 3.0834
Iteration: 2082; Percent complete: 52.0%; Average loss: 3.1850
Iteration: 2083; Percent complete: 52.1%; Average loss: 3.0540
Iteration: 2084; Percent complete: 52.1%; Average loss: 3.2882
Iteration: 2085; Percent complete: 52.1%; Average loss: 3.2305
Iteration: 2086; Percent complete: 52.1%; Average loss: 3.3442
Iteration: 2087; Percent complete: 52.2%; Average loss: 3.0450
Iteration: 2088; Percent complete: 52.2%; Average loss: 3.1225
Iteration: 2089; Percent complete: 52.2%; Average loss: 3.2991
Iteration: 2090; Percent complete: 52.2%; Average loss: 3.1543
Iteration: 2091; Percent complete: 52.3%; Average loss: 2.9181
Iteration: 2092; Percent complete: 52.3%; Average loss: 3.1340
Iteration: 2093; Percent complete: 52.3%; Average loss: 2.9708
Iteration: 2094; Percent complete: 52.3%; Average loss: 3.3283
Iteration: 2095; Percent complete: 52.4%; Average loss: 3.1485
Iteration: 2096; Percent complete: 52.4%; Average loss: 3.2235
Iteration: 2097; Percent complete: 52.4%; Average loss: 3.0464
Iteration: 2098; Percent complete: 52.4%; Average loss: 3.0377
Iteration: 2099; Percent complete: 52.5%; Average loss: 3.0099
Iteration: 2100; Percent complete: 52.5%; Average loss: 3.2566
Iteration: 2101; Percent complete: 52.5%; Average loss: 3.2969
Iteration: 2102; Percent complete: 52.5%; Average loss: 2.9802
Iteration: 2103; Percent complete: 52.6%; Average loss: 3.1553
Iteration: 2104; Percent complete: 52.6%; Average loss: 3.5613
Iteration: 2105; Percent complete: 52.6%; Average loss: 3.0491
Iteration: 2106; Percent complete: 52.6%; Average loss: 3.4243
Iteration: 2107; Percent complete: 52.7%; Average loss: 2.7678
Iteration: 2108; Percent complete: 52.7%; Average loss: 3.1217
Iteration: 2109; Percent complete: 52.7%; Average loss: 3.1990
Iteration: 2110; Percent complete: 52.8%; Average loss: 3.3289
Iteration: 2111; Percent complete: 52.8%; Average loss: 3.2559
Iteration: 2112; Percent complete: 52.8%; Average loss: 2.9934
Iteration: 2113; Percent complete: 52.8%; Average loss: 2.9431
Iteration: 2114; Percent complete: 52.8%; Average loss: 2.9755
Iteration: 2115; Percent complete: 52.9%; Average loss: 3.1184
Iteration: 2116; Percent complete: 52.9%; Average loss: 3.2406
Iteration: 2117; Percent complete: 52.9%; Average loss: 3.2291
Iteration: 2118; Percent complete: 52.9%; Average loss: 3.3070
Iteration: 2119; Percent complete: 53.0%; Average loss: 3.2874
Iteration: 2120; Percent complete: 53.0%; Average loss: 3.3621
Iteration: 2121; Percent complete: 53.0%; Average loss: 3.1642
Iteration: 2122; Percent complete: 53.0%; Average loss: 3.2592
Iteration: 2123; Percent complete: 53.1%; Average loss: 3.2576
Iteration: 2124; Percent complete: 53.1%; Average loss: 3.0687
Iteration: 2125; Percent complete: 53.1%; Average loss: 3.1832
Iteration: 2126; Percent complete: 53.1%; Average loss: 3.0575
Iteration: 2127; Percent complete: 53.2%; Average loss: 3.3430
Iteration: 2128; Percent complete: 53.2%; Average loss: 3.2379
Iteration: 2129; Percent complete: 53.2%; Average loss: 3.0160
Iteration: 2130; Percent complete: 53.2%; Average loss: 3.3869
Iteration: 2131; Percent complete: 53.3%; Average loss: 3.0446
Iteration: 2132; Percent complete: 53.3%; Average loss: 3.2948
Iteration: 2133; Percent complete: 53.3%; Average loss: 3.1648
Iteration: 2134; Percent complete: 53.3%; Average loss: 3.0070
Iteration: 2135; Percent complete: 53.4%; Average loss: 3.4653
Iteration: 2136; Percent complete: 53.4%; Average loss: 2.9302
Iteration: 2137; Percent complete: 53.4%; Average loss: 3.1119
Iteration: 2138; Percent complete: 53.4%; Average loss: 3.3310
Iteration: 2139; Percent complete: 53.5%; Average loss: 3.4272
Iteration: 2140; Percent complete: 53.5%; Average loss: 3.1991
Iteration: 2141; Percent complete: 53.5%; Average loss: 2.9839
Iteration: 2142; Percent complete: 53.5%; Average loss: 2.9447
Iteration: 2143; Percent complete: 53.6%; Average loss: 3.2466
Iteration: 2144; Percent complete: 53.6%; Average loss: 3.1524
Iteration: 2145; Percent complete: 53.6%; Average loss: 2.9659
Iteration: 2146; Percent complete: 53.6%; Average loss: 3.0275
Iteration: 2147; Percent complete: 53.7%; Average loss: 3.1468
Iteration: 2148; Percent complete: 53.7%; Average loss: 3.2277
Iteration: 2149; Percent complete: 53.7%; Average loss: 3.1719
Iteration: 2150; Percent complete: 53.8%; Average loss: 3.0523
Iteration: 2151; Percent complete: 53.8%; Average loss: 3.3761
Iteration: 2152; Percent complete: 53.8%; Average loss: 2.9462
Iteration: 2153; Percent complete: 53.8%; Average loss: 3.3455
Iteration: 2154; Percent complete: 53.8%; Average loss: 3.3633
Iteration: 2155; Percent complete: 53.9%; Average loss: 2.8838
Iteration: 2156; Percent complete: 53.9%; Average loss: 3.0512
Iteration: 2157; Percent complete: 53.9%; Average loss: 2.9611
Iteration: 2158; Percent complete: 53.9%; Average loss: 3.3054
Iteration: 2159; Percent complete: 54.0%; Average loss: 2.9643
Iteration: 2160; Percent complete: 54.0%; Average loss: 3.0352
Iteration: 2161; Percent complete: 54.0%; Average loss: 3.0225
Iteration: 2162; Percent complete: 54.0%; Average loss: 3.0978
Iteration: 2163; Percent complete: 54.1%; Average loss: 3.1262
Iteration: 2164; Percent complete: 54.1%; Average loss: 3.2430
Iteration: 2165; Percent complete: 54.1%; Average loss: 3.1551
Iteration: 2166; Percent complete: 54.1%; Average loss: 3.2139
Iteration: 2167; Percent complete: 54.2%; Average loss: 2.9654
Iteration: 2168; Percent complete: 54.2%; Average loss: 3.1093
Iteration: 2169; Percent complete: 54.2%; Average loss: 3.2525
Iteration: 2170; Percent complete: 54.2%; Average loss: 3.2309
Iteration: 2171; Percent complete: 54.3%; Average loss: 3.2442
Iteration: 2172; Percent complete: 54.3%; Average loss: 3.3889
Iteration: 2173; Percent complete: 54.3%; Average loss: 3.1537
Iteration: 2174; Percent complete: 54.4%; Average loss: 3.0329
Iteration: 2175; Percent complete: 54.4%; Average loss: 2.8991
Iteration: 2176; Percent complete: 54.4%; Average loss: 3.1286
Iteration: 2177; Percent complete: 54.4%; Average loss: 3.0220
Iteration: 2178; Percent complete: 54.4%; Average loss: 3.2324
Iteration: 2179; Percent complete: 54.5%; Average loss: 3.2228
Iteration: 2180; Percent complete: 54.5%; Average loss: 2.8501
Iteration: 2181; Percent complete: 54.5%; Average loss: 3.1101
Iteration: 2182; Percent complete: 54.5%; Average loss: 3.0302
Iteration: 2183; Percent complete: 54.6%; Average loss: 2.8602
Iteration: 2184; Percent complete: 54.6%; Average loss: 2.9914
Iteration: 2185; Percent complete: 54.6%; Average loss: 3.1989
Iteration: 2186; Percent complete: 54.6%; Average loss: 3.3166
Iteration: 2187; Percent complete: 54.7%; Average loss: 3.1083
Iteration: 2188; Percent complete: 54.7%; Average loss: 2.9821
Iteration: 2189; Percent complete: 54.7%; Average loss: 2.9258
Iteration: 2190; Percent complete: 54.8%; Average loss: 3.1255
Iteration: 2191; Percent complete: 54.8%; Average loss: 3.1517
Iteration: 2192; Percent complete: 54.8%; Average loss: 2.9195
Iteration: 2193; Percent complete: 54.8%; Average loss: 3.0157
Iteration: 2194; Percent complete: 54.9%; Average loss: 2.9829
Iteration: 2195; Percent complete: 54.9%; Average loss: 3.1035
Iteration: 2196; Percent complete: 54.9%; Average loss: 3.1365
Iteration: 2197; Percent complete: 54.9%; Average loss: 3.1128
Iteration: 2198; Percent complete: 54.9%; Average loss: 2.9151
Iteration: 2199; Percent complete: 55.0%; Average loss: 3.1031
Iteration: 2200; Percent complete: 55.0%; Average loss: 3.1123
Iteration: 2201; Percent complete: 55.0%; Average loss: 3.2231
Iteration: 2202; Percent complete: 55.0%; Average loss: 3.4465
Iteration: 2203; Percent complete: 55.1%; Average loss: 3.0164
Iteration: 2204; Percent complete: 55.1%; Average loss: 3.1677
Iteration: 2205; Percent complete: 55.1%; Average loss: 2.9102
Iteration: 2206; Percent complete: 55.1%; Average loss: 3.2593
Iteration: 2207; Percent complete: 55.2%; Average loss: 2.7826
Iteration: 2208; Percent complete: 55.2%; Average loss: 3.1096
Iteration: 2209; Percent complete: 55.2%; Average loss: 2.9453
Iteration: 2210; Percent complete: 55.2%; Average loss: 3.0966
Iteration: 2211; Percent complete: 55.3%; Average loss: 3.0756
Iteration: 2212; Percent complete: 55.3%; Average loss: 3.1052
Iteration: 2213; Percent complete: 55.3%; Average loss: 2.9739
Iteration: 2214; Percent complete: 55.4%; Average loss: 3.1554
Iteration: 2215; Percent complete: 55.4%; Average loss: 2.9836
Iteration: 2216; Percent complete: 55.4%; Average loss: 3.2177
Iteration: 2217; Percent complete: 55.4%; Average loss: 3.3641
Iteration: 2218; Percent complete: 55.5%; Average loss: 3.0482
Iteration: 2219; Percent complete: 55.5%; Average loss: 3.2014
Iteration: 2220; Percent complete: 55.5%; Average loss: 2.8063
Iteration: 2221; Percent complete: 55.5%; Average loss: 3.0839
Iteration: 2222; Percent complete: 55.5%; Average loss: 3.2264
Iteration: 2223; Percent complete: 55.6%; Average loss: 3.3470
Iteration: 2224; Percent complete: 55.6%; Average loss: 3.1147
Iteration: 2225; Percent complete: 55.6%; Average loss: 3.1416
Iteration: 2226; Percent complete: 55.6%; Average loss: 3.0157
Iteration: 2227; Percent complete: 55.7%; Average loss: 3.1322
Iteration: 2228; Percent complete: 55.7%; Average loss: 2.8983
Iteration: 2229; Percent complete: 55.7%; Average loss: 3.0841
Iteration: 2230; Percent complete: 55.8%; Average loss: 3.1986
Iteration: 2231; Percent complete: 55.8%; Average loss: 3.2412
Iteration: 2232; Percent complete: 55.8%; Average loss: 3.1737
Iteration: 2233; Percent complete: 55.8%; Average loss: 2.7695
Iteration: 2234; Percent complete: 55.9%; Average loss: 3.3308
Iteration: 2235; Percent complete: 55.9%; Average loss: 3.1687
Iteration: 2236; Percent complete: 55.9%; Average loss: 3.0750
Iteration: 2237; Percent complete: 55.9%; Average loss: 3.0318
Iteration: 2238; Percent complete: 56.0%; Average loss: 3.0161
Iteration: 2239; Percent complete: 56.0%; Average loss: 3.2499
Iteration: 2240; Percent complete: 56.0%; Average loss: 2.9001
Iteration: 2241; Percent complete: 56.0%; Average loss: 3.2914
Iteration: 2242; Percent complete: 56.0%; Average loss: 2.9222
Iteration: 2243; Percent complete: 56.1%; Average loss: 3.1749
Iteration: 2244; Percent complete: 56.1%; Average loss: 2.8781
Iteration: 2245; Percent complete: 56.1%; Average loss: 3.1543
Iteration: 2246; Percent complete: 56.1%; Average loss: 3.3200
Iteration: 2247; Percent complete: 56.2%; Average loss: 3.0140
Iteration: 2248; Percent complete: 56.2%; Average loss: 2.8564
Iteration: 2249; Percent complete: 56.2%; Average loss: 3.2251
Iteration: 2250; Percent complete: 56.2%; Average loss: 3.0345
Iteration: 2251; Percent complete: 56.3%; Average loss: 3.2783
Iteration: 2252; Percent complete: 56.3%; Average loss: 3.0233
Iteration: 2253; Percent complete: 56.3%; Average loss: 3.0598
Iteration: 2254; Percent complete: 56.4%; Average loss: 3.1172
Iteration: 2255; Percent complete: 56.4%; Average loss: 3.2475
Iteration: 2256; Percent complete: 56.4%; Average loss: 2.9622
Iteration: 2257; Percent complete: 56.4%; Average loss: 3.0526
Iteration: 2258; Percent complete: 56.5%; Average loss: 2.7701
Iteration: 2259; Percent complete: 56.5%; Average loss: 3.1865
Iteration: 2260; Percent complete: 56.5%; Average loss: 3.0243
Iteration: 2261; Percent complete: 56.5%; Average loss: 3.2526
Iteration: 2262; Percent complete: 56.5%; Average loss: 3.0812
Iteration: 2263; Percent complete: 56.6%; Average loss: 3.1681
Iteration: 2264; Percent complete: 56.6%; Average loss: 3.1205
Iteration: 2265; Percent complete: 56.6%; Average loss: 2.9167
Iteration: 2266; Percent complete: 56.6%; Average loss: 3.2245
Iteration: 2267; Percent complete: 56.7%; Average loss: 3.0760
Iteration: 2268; Percent complete: 56.7%; Average loss: 2.9054
Iteration: 2269; Percent complete: 56.7%; Average loss: 2.9942
Iteration: 2270; Percent complete: 56.8%; Average loss: 3.2724
Iteration: 2271; Percent complete: 56.8%; Average loss: 2.8420
Iteration: 2272; Percent complete: 56.8%; Average loss: 3.0801
Iteration: 2273; Percent complete: 56.8%; Average loss: 3.2386
Iteration: 2274; Percent complete: 56.9%; Average loss: 3.2016
Iteration: 2275; Percent complete: 56.9%; Average loss: 3.0877
Iteration: 2276; Percent complete: 56.9%; Average loss: 3.1792
Iteration: 2277; Percent complete: 56.9%; Average loss: 3.1773
Iteration: 2278; Percent complete: 57.0%; Average loss: 3.1433
Iteration: 2279; Percent complete: 57.0%; Average loss: 3.2356
Iteration: 2280; Percent complete: 57.0%; Average loss: 3.0561
Iteration: 2281; Percent complete: 57.0%; Average loss: 2.7662
Iteration: 2282; Percent complete: 57.0%; Average loss: 3.1379
Iteration: 2283; Percent complete: 57.1%; Average loss: 3.1384
Iteration: 2284; Percent complete: 57.1%; Average loss: 3.1243
Iteration: 2285; Percent complete: 57.1%; Average loss: 3.2174
Iteration: 2286; Percent complete: 57.1%; Average loss: 2.9120
Iteration: 2287; Percent complete: 57.2%; Average loss: 3.2033
Iteration: 2288; Percent complete: 57.2%; Average loss: 3.0877
Iteration: 2289; Percent complete: 57.2%; Average loss: 3.0647
Iteration: 2290; Percent complete: 57.2%; Average loss: 3.1005
Iteration: 2291; Percent complete: 57.3%; Average loss: 3.0996
Iteration: 2292; Percent complete: 57.3%; Average loss: 3.1786
Iteration: 2293; Percent complete: 57.3%; Average loss: 3.0114
Iteration: 2294; Percent complete: 57.4%; Average loss: 3.1725
Iteration: 2295; Percent complete: 57.4%; Average loss: 3.1952
Iteration: 2296; Percent complete: 57.4%; Average loss: 3.0107
Iteration: 2297; Percent complete: 57.4%; Average loss: 3.1696
Iteration: 2298; Percent complete: 57.5%; Average loss: 3.0284
Iteration: 2299; Percent complete: 57.5%; Average loss: 3.1846
Iteration: 2300; Percent complete: 57.5%; Average loss: 2.9550
Iteration: 2301; Percent complete: 57.5%; Average loss: 2.9933
Iteration: 2302; Percent complete: 57.6%; Average loss: 2.9989
Iteration: 2303; Percent complete: 57.6%; Average loss: 3.0508
Iteration: 2304; Percent complete: 57.6%; Average loss: 2.8507
Iteration: 2305; Percent complete: 57.6%; Average loss: 3.2871
Iteration: 2306; Percent complete: 57.6%; Average loss: 3.0525
Iteration: 2307; Percent complete: 57.7%; Average loss: 3.1172
Iteration: 2308; Percent complete: 57.7%; Average loss: 2.9677
Iteration: 2309; Percent complete: 57.7%; Average loss: 3.0189
Iteration: 2310; Percent complete: 57.8%; Average loss: 3.1507
Iteration: 2311; Percent complete: 57.8%; Average loss: 3.1924
Iteration: 2312; Percent complete: 57.8%; Average loss: 3.1760
Iteration: 2313; Percent complete: 57.8%; Average loss: 3.0167
Iteration: 2314; Percent complete: 57.9%; Average loss: 2.9345
Iteration: 2315; Percent complete: 57.9%; Average loss: 3.0555
Iteration: 2316; Percent complete: 57.9%; Average loss: 3.1473
Iteration: 2317; Percent complete: 57.9%; Average loss: 2.8469
Iteration: 2318; Percent complete: 58.0%; Average loss: 3.0928
Iteration: 2319; Percent complete: 58.0%; Average loss: 3.0846
Iteration: 2320; Percent complete: 58.0%; Average loss: 2.9577
Iteration: 2321; Percent complete: 58.0%; Average loss: 3.2400
Iteration: 2322; Percent complete: 58.1%; Average loss: 2.8740
Iteration: 2323; Percent complete: 58.1%; Average loss: 2.9336
Iteration: 2324; Percent complete: 58.1%; Average loss: 3.0583
Iteration: 2325; Percent complete: 58.1%; Average loss: 3.1645
Iteration: 2326; Percent complete: 58.1%; Average loss: 3.1975
Iteration: 2327; Percent complete: 58.2%; Average loss: 3.1283
Iteration: 2328; Percent complete: 58.2%; Average loss: 3.0268
Iteration: 2329; Percent complete: 58.2%; Average loss: 3.1275
Iteration: 2330; Percent complete: 58.2%; Average loss: 3.0674
Iteration: 2331; Percent complete: 58.3%; Average loss: 2.8535
Iteration: 2332; Percent complete: 58.3%; Average loss: 2.9368
Iteration: 2333; Percent complete: 58.3%; Average loss: 3.1889
Iteration: 2334; Percent complete: 58.4%; Average loss: 3.0470
Iteration: 2335; Percent complete: 58.4%; Average loss: 3.1223
Iteration: 2336; Percent complete: 58.4%; Average loss: 2.9404
Iteration: 2337; Percent complete: 58.4%; Average loss: 3.3852
Iteration: 2338; Percent complete: 58.5%; Average loss: 3.4619
Iteration: 2339; Percent complete: 58.5%; Average loss: 2.9081
Iteration: 2340; Percent complete: 58.5%; Average loss: 3.1749
Iteration: 2341; Percent complete: 58.5%; Average loss: 2.9684
Iteration: 2342; Percent complete: 58.6%; Average loss: 2.7791
Iteration: 2343; Percent complete: 58.6%; Average loss: 2.7938
Iteration: 2344; Percent complete: 58.6%; Average loss: 3.1929
Iteration: 2345; Percent complete: 58.6%; Average loss: 3.2244
Iteration: 2346; Percent complete: 58.7%; Average loss: 3.0696
Iteration: 2347; Percent complete: 58.7%; Average loss: 2.9240
Iteration: 2348; Percent complete: 58.7%; Average loss: 2.9415
Iteration: 2349; Percent complete: 58.7%; Average loss: 3.3589
Iteration: 2350; Percent complete: 58.8%; Average loss: 2.9255
Iteration: 2351; Percent complete: 58.8%; Average loss: 3.0369
Iteration: 2352; Percent complete: 58.8%; Average loss: 3.2037
Iteration: 2353; Percent complete: 58.8%; Average loss: 3.2648
Iteration: 2354; Percent complete: 58.9%; Average loss: 3.1076
Iteration: 2355; Percent complete: 58.9%; Average loss: 2.9616
Iteration: 2356; Percent complete: 58.9%; Average loss: 3.0291
Iteration: 2357; Percent complete: 58.9%; Average loss: 3.0558
Iteration: 2358; Percent complete: 59.0%; Average loss: 3.0439
Iteration: 2359; Percent complete: 59.0%; Average loss: 2.9123
Iteration: 2360; Percent complete: 59.0%; Average loss: 2.7836
Iteration: 2361; Percent complete: 59.0%; Average loss: 3.0663
Iteration: 2362; Percent complete: 59.1%; Average loss: 2.9804
Iteration: 2363; Percent complete: 59.1%; Average loss: 2.8873
Iteration: 2364; Percent complete: 59.1%; Average loss: 3.1563
Iteration: 2365; Percent complete: 59.1%; Average loss: 3.1381
Iteration: 2366; Percent complete: 59.2%; Average loss: 3.0762
Iteration: 2367; Percent complete: 59.2%; Average loss: 3.1886
Iteration: 2368; Percent complete: 59.2%; Average loss: 2.9936
Iteration: 2369; Percent complete: 59.2%; Average loss: 3.0093
Iteration: 2370; Percent complete: 59.2%; Average loss: 3.1073
Iteration: 2371; Percent complete: 59.3%; Average loss: 3.1157
Iteration: 2372; Percent complete: 59.3%; Average loss: 3.0576
Iteration: 2373; Percent complete: 59.3%; Average loss: 3.1390
Iteration: 2374; Percent complete: 59.4%; Average loss: 2.9443
Iteration: 2375; Percent complete: 59.4%; Average loss: 2.8409
Iteration: 2376; Percent complete: 59.4%; Average loss: 3.2704
Iteration: 2377; Percent complete: 59.4%; Average loss: 3.0567
Iteration: 2378; Percent complete: 59.5%; Average loss: 3.1371
Iteration: 2379; Percent complete: 59.5%; Average loss: 3.2293
Iteration: 2380; Percent complete: 59.5%; Average loss: 2.8115
Iteration: 2381; Percent complete: 59.5%; Average loss: 2.9999
Iteration: 2382; Percent complete: 59.6%; Average loss: 3.1881
Iteration: 2383; Percent complete: 59.6%; Average loss: 3.0079
Iteration: 2384; Percent complete: 59.6%; Average loss: 3.0950
Iteration: 2385; Percent complete: 59.6%; Average loss: 2.9615
Iteration: 2386; Percent complete: 59.7%; Average loss: 3.2104
Iteration: 2387; Percent complete: 59.7%; Average loss: 3.1512
Iteration: 2388; Percent complete: 59.7%; Average loss: 2.9526
Iteration: 2389; Percent complete: 59.7%; Average loss: 3.4124
Iteration: 2390; Percent complete: 59.8%; Average loss: 3.2284
Iteration: 2391; Percent complete: 59.8%; Average loss: 3.1804
Iteration: 2392; Percent complete: 59.8%; Average loss: 3.1217
Iteration: 2393; Percent complete: 59.8%; Average loss: 3.1370
Iteration: 2394; Percent complete: 59.9%; Average loss: 3.0400
Iteration: 2395; Percent complete: 59.9%; Average loss: 3.2201
Iteration: 2396; Percent complete: 59.9%; Average loss: 3.0447
Iteration: 2397; Percent complete: 59.9%; Average loss: 3.0405
Iteration: 2398; Percent complete: 60.0%; Average loss: 3.1871
Iteration: 2399; Percent complete: 60.0%; Average loss: 3.0780
Iteration: 2400; Percent complete: 60.0%; Average loss: 3.1463
Iteration: 2401; Percent complete: 60.0%; Average loss: 3.1914
Iteration: 2402; Percent complete: 60.1%; Average loss: 3.0494
Iteration: 2403; Percent complete: 60.1%; Average loss: 3.1877
Iteration: 2404; Percent complete: 60.1%; Average loss: 2.9580
Iteration: 2405; Percent complete: 60.1%; Average loss: 3.2757
Iteration: 2406; Percent complete: 60.2%; Average loss: 3.1213
Iteration: 2407; Percent complete: 60.2%; Average loss: 3.1804
Iteration: 2408; Percent complete: 60.2%; Average loss: 3.0901
Iteration: 2409; Percent complete: 60.2%; Average loss: 2.8389
Iteration: 2410; Percent complete: 60.2%; Average loss: 2.8283
Iteration: 2411; Percent complete: 60.3%; Average loss: 3.0324
Iteration: 2412; Percent complete: 60.3%; Average loss: 3.0572
Iteration: 2413; Percent complete: 60.3%; Average loss: 2.9466
Iteration: 2414; Percent complete: 60.4%; Average loss: 3.0054
Iteration: 2415; Percent complete: 60.4%; Average loss: 2.9513
Iteration: 2416; Percent complete: 60.4%; Average loss: 3.0032
Iteration: 2417; Percent complete: 60.4%; Average loss: 2.9406
Iteration: 2418; Percent complete: 60.5%; Average loss: 3.2104
Iteration: 2419; Percent complete: 60.5%; Average loss: 3.0402
Iteration: 2420; Percent complete: 60.5%; Average loss: 3.0324
Iteration: 2421; Percent complete: 60.5%; Average loss: 2.8559
Iteration: 2422; Percent complete: 60.6%; Average loss: 2.7608
Iteration: 2423; Percent complete: 60.6%; Average loss: 2.9907
Iteration: 2424; Percent complete: 60.6%; Average loss: 3.0413
Iteration: 2425; Percent complete: 60.6%; Average loss: 3.1534
Iteration: 2426; Percent complete: 60.7%; Average loss: 3.1721
Iteration: 2427; Percent complete: 60.7%; Average loss: 3.1434
Iteration: 2428; Percent complete: 60.7%; Average loss: 2.8336
Iteration: 2429; Percent complete: 60.7%; Average loss: 3.1004
Iteration: 2430; Percent complete: 60.8%; Average loss: 2.8840
Iteration: 2431; Percent complete: 60.8%; Average loss: 3.1377
Iteration: 2432; Percent complete: 60.8%; Average loss: 3.0134
Iteration: 2433; Percent complete: 60.8%; Average loss: 2.8431
Iteration: 2434; Percent complete: 60.9%; Average loss: 3.1809
Iteration: 2435; Percent complete: 60.9%; Average loss: 2.8075
Iteration: 2436; Percent complete: 60.9%; Average loss: 2.9590
Iteration: 2437; Percent complete: 60.9%; Average loss: 3.1757
Iteration: 2438; Percent complete: 61.0%; Average loss: 3.2321
Iteration: 2439; Percent complete: 61.0%; Average loss: 2.9601
Iteration: 2440; Percent complete: 61.0%; Average loss: 3.2403
Iteration: 2441; Percent complete: 61.0%; Average loss: 3.0491
Iteration: 2442; Percent complete: 61.1%; Average loss: 2.6395
Iteration: 2443; Percent complete: 61.1%; Average loss: 3.2207
Iteration: 2444; Percent complete: 61.1%; Average loss: 3.1482
Iteration: 2445; Percent complete: 61.1%; Average loss: 3.3016
Iteration: 2446; Percent complete: 61.2%; Average loss: 3.0571
Iteration: 2447; Percent complete: 61.2%; Average loss: 2.9910
Iteration: 2448; Percent complete: 61.2%; Average loss: 2.9596
Iteration: 2449; Percent complete: 61.2%; Average loss: 3.0137
Iteration: 2450; Percent complete: 61.3%; Average loss: 3.0443
Iteration: 2451; Percent complete: 61.3%; Average loss: 3.0397
Iteration: 2452; Percent complete: 61.3%; Average loss: 2.9499
Iteration: 2453; Percent complete: 61.3%; Average loss: 2.7156
Iteration: 2454; Percent complete: 61.4%; Average loss: 3.0825
Iteration: 2455; Percent complete: 61.4%; Average loss: 3.0152
Iteration: 2456; Percent complete: 61.4%; Average loss: 3.2544
Iteration: 2457; Percent complete: 61.4%; Average loss: 2.9444
Iteration: 2458; Percent complete: 61.5%; Average loss: 3.0602
Iteration: 2459; Percent complete: 61.5%; Average loss: 2.9448
Iteration: 2460; Percent complete: 61.5%; Average loss: 2.9663
Iteration: 2461; Percent complete: 61.5%; Average loss: 2.7855
Iteration: 2462; Percent complete: 61.6%; Average loss: 3.0480
Iteration: 2463; Percent complete: 61.6%; Average loss: 3.2227
Iteration: 2464; Percent complete: 61.6%; Average loss: 3.2765
Iteration: 2465; Percent complete: 61.6%; Average loss: 3.2093
Iteration: 2466; Percent complete: 61.7%; Average loss: 3.2822
Iteration: 2467; Percent complete: 61.7%; Average loss: 3.0702
Iteration: 2468; Percent complete: 61.7%; Average loss: 3.2657
Iteration: 2469; Percent complete: 61.7%; Average loss: 2.9425
Iteration: 2470; Percent complete: 61.8%; Average loss: 2.7384
Iteration: 2471; Percent complete: 61.8%; Average loss: 2.9861
Iteration: 2472; Percent complete: 61.8%; Average loss: 3.0602
Iteration: 2473; Percent complete: 61.8%; Average loss: 3.2244
Iteration: 2474; Percent complete: 61.9%; Average loss: 2.9202
Iteration: 2475; Percent complete: 61.9%; Average loss: 3.2425
Iteration: 2476; Percent complete: 61.9%; Average loss: 3.2915
Iteration: 2477; Percent complete: 61.9%; Average loss: 3.0142
Iteration: 2478; Percent complete: 62.0%; Average loss: 2.8871
Iteration: 2479; Percent complete: 62.0%; Average loss: 2.7663
Iteration: 2480; Percent complete: 62.0%; Average loss: 2.9936
Iteration: 2481; Percent complete: 62.0%; Average loss: 3.2250
Iteration: 2482; Percent complete: 62.1%; Average loss: 3.1037
Iteration: 2483; Percent complete: 62.1%; Average loss: 2.9617
Iteration: 2484; Percent complete: 62.1%; Average loss: 3.2404
Iteration: 2485; Percent complete: 62.1%; Average loss: 3.1698
Iteration: 2486; Percent complete: 62.2%; Average loss: 2.9993
Iteration: 2487; Percent complete: 62.2%; Average loss: 2.7342
Iteration: 2488; Percent complete: 62.2%; Average loss: 3.0033
Iteration: 2489; Percent complete: 62.2%; Average loss: 2.8206
Iteration: 2490; Percent complete: 62.3%; Average loss: 3.1042
Iteration: 2491; Percent complete: 62.3%; Average loss: 3.0013
Iteration: 2492; Percent complete: 62.3%; Average loss: 3.2417
Iteration: 2493; Percent complete: 62.3%; Average loss: 2.8010
Iteration: 2494; Percent complete: 62.4%; Average loss: 3.0192
Iteration: 2495; Percent complete: 62.4%; Average loss: 3.1349
Iteration: 2496; Percent complete: 62.4%; Average loss: 2.9238
Iteration: 2497; Percent complete: 62.4%; Average loss: 3.1980
Iteration: 2498; Percent complete: 62.5%; Average loss: 3.2101
Iteration: 2499; Percent complete: 62.5%; Average loss: 2.9093
Iteration: 2500; Percent complete: 62.5%; Average loss: 3.0945
Iteration: 2501; Percent complete: 62.5%; Average loss: 3.0209
Iteration: 2502; Percent complete: 62.5%; Average loss: 2.7945
Iteration: 2503; Percent complete: 62.6%; Average loss: 3.2946
Iteration: 2504; Percent complete: 62.6%; Average loss: 3.0115
Iteration: 2505; Percent complete: 62.6%; Average loss: 2.7458
Iteration: 2506; Percent complete: 62.6%; Average loss: 2.7888
Iteration: 2507; Percent complete: 62.7%; Average loss: 3.2634
Iteration: 2508; Percent complete: 62.7%; Average loss: 3.2812
Iteration: 2509; Percent complete: 62.7%; Average loss: 3.2948
Iteration: 2510; Percent complete: 62.7%; Average loss: 3.0400
Iteration: 2511; Percent complete: 62.8%; Average loss: 3.3070
Iteration: 2512; Percent complete: 62.8%; Average loss: 2.9151
Iteration: 2513; Percent complete: 62.8%; Average loss: 3.1706
Iteration: 2514; Percent complete: 62.8%; Average loss: 3.1614
Iteration: 2515; Percent complete: 62.9%; Average loss: 3.0835
Iteration: 2516; Percent complete: 62.9%; Average loss: 3.2329
Iteration: 2517; Percent complete: 62.9%; Average loss: 3.1830
Iteration: 2518; Percent complete: 62.9%; Average loss: 3.0783
Iteration: 2519; Percent complete: 63.0%; Average loss: 3.0394
Iteration: 2520; Percent complete: 63.0%; Average loss: 3.1767
Iteration: 2521; Percent complete: 63.0%; Average loss: 3.0548
Iteration: 2522; Percent complete: 63.0%; Average loss: 2.9976
Iteration: 2523; Percent complete: 63.1%; Average loss: 2.9922
Iteration: 2524; Percent complete: 63.1%; Average loss: 3.1973
Iteration: 2525; Percent complete: 63.1%; Average loss: 2.9199
Iteration: 2526; Percent complete: 63.1%; Average loss: 2.9922
Iteration: 2527; Percent complete: 63.2%; Average loss: 3.2118
Iteration: 2528; Percent complete: 63.2%; Average loss: 2.9983
Iteration: 2529; Percent complete: 63.2%; Average loss: 3.0815
Iteration: 2530; Percent complete: 63.2%; Average loss: 2.9093
Iteration: 2531; Percent complete: 63.3%; Average loss: 2.9142
Iteration: 2532; Percent complete: 63.3%; Average loss: 2.8915
Iteration: 2533; Percent complete: 63.3%; Average loss: 3.1968
Iteration: 2534; Percent complete: 63.3%; Average loss: 3.0434
Iteration: 2535; Percent complete: 63.4%; Average loss: 2.8953
Iteration: 2536; Percent complete: 63.4%; Average loss: 2.9154
Iteration: 2537; Percent complete: 63.4%; Average loss: 3.0055
Iteration: 2538; Percent complete: 63.4%; Average loss: 2.8481
Iteration: 2539; Percent complete: 63.5%; Average loss: 2.8230
Iteration: 2540; Percent complete: 63.5%; Average loss: 2.9267
Iteration: 2541; Percent complete: 63.5%; Average loss: 3.0909
Iteration: 2542; Percent complete: 63.5%; Average loss: 2.9822
Iteration: 2543; Percent complete: 63.6%; Average loss: 3.0804
Iteration: 2544; Percent complete: 63.6%; Average loss: 2.8355
Iteration: 2545; Percent complete: 63.6%; Average loss: 3.0788
Iteration: 2546; Percent complete: 63.6%; Average loss: 2.9315
Iteration: 2547; Percent complete: 63.7%; Average loss: 3.1379
Iteration: 2548; Percent complete: 63.7%; Average loss: 2.8218
Iteration: 2549; Percent complete: 63.7%; Average loss: 2.8885
Iteration: 2550; Percent complete: 63.7%; Average loss: 3.1322
Iteration: 2551; Percent complete: 63.8%; Average loss: 2.6200
Iteration: 2552; Percent complete: 63.8%; Average loss: 2.8752
Iteration: 2553; Percent complete: 63.8%; Average loss: 2.9050
Iteration: 2554; Percent complete: 63.8%; Average loss: 2.8927
Iteration: 2555; Percent complete: 63.9%; Average loss: 2.8916
Iteration: 2556; Percent complete: 63.9%; Average loss: 2.9484
Iteration: 2557; Percent complete: 63.9%; Average loss: 2.9615
Iteration: 2558; Percent complete: 63.9%; Average loss: 2.9408
Iteration: 2559; Percent complete: 64.0%; Average loss: 2.7808
Iteration: 2560; Percent complete: 64.0%; Average loss: 3.1803
Iteration: 2561; Percent complete: 64.0%; Average loss: 3.1940
Iteration: 2562; Percent complete: 64.0%; Average loss: 3.0668
Iteration: 2563; Percent complete: 64.1%; Average loss: 2.8187
Iteration: 2564; Percent complete: 64.1%; Average loss: 2.9569
Iteration: 2565; Percent complete: 64.1%; Average loss: 3.0708
Iteration: 2566; Percent complete: 64.1%; Average loss: 2.9874
Iteration: 2567; Percent complete: 64.2%; Average loss: 2.9541
Iteration: 2568; Percent complete: 64.2%; Average loss: 3.0593
Iteration: 2569; Percent complete: 64.2%; Average loss: 3.1225
Iteration: 2570; Percent complete: 64.2%; Average loss: 3.0964
Iteration: 2571; Percent complete: 64.3%; Average loss: 2.9257
Iteration: 2572; Percent complete: 64.3%; Average loss: 3.0272
Iteration: 2573; Percent complete: 64.3%; Average loss: 3.3137
Iteration: 2574; Percent complete: 64.3%; Average loss: 2.9998
Iteration: 2575; Percent complete: 64.4%; Average loss: 3.1943
Iteration: 2576; Percent complete: 64.4%; Average loss: 3.2153
Iteration: 2577; Percent complete: 64.4%; Average loss: 3.2275
Iteration: 2578; Percent complete: 64.5%; Average loss: 2.9046
Iteration: 2579; Percent complete: 64.5%; Average loss: 3.0404
Iteration: 2580; Percent complete: 64.5%; Average loss: 2.8787
Iteration: 2581; Percent complete: 64.5%; Average loss: 3.1471
Iteration: 2582; Percent complete: 64.5%; Average loss: 3.0865
Iteration: 2583; Percent complete: 64.6%; Average loss: 3.0511
Iteration: 2584; Percent complete: 64.6%; Average loss: 3.1041
Iteration: 2585; Percent complete: 64.6%; Average loss: 2.9949
Iteration: 2586; Percent complete: 64.6%; Average loss: 2.9604
Iteration: 2587; Percent complete: 64.7%; Average loss: 3.0596
Iteration: 2588; Percent complete: 64.7%; Average loss: 2.9542
Iteration: 2589; Percent complete: 64.7%; Average loss: 3.2467
Iteration: 2590; Percent complete: 64.8%; Average loss: 3.1299
Iteration: 2591; Percent complete: 64.8%; Average loss: 2.9331
Iteration: 2592; Percent complete: 64.8%; Average loss: 3.0841
Iteration: 2593; Percent complete: 64.8%; Average loss: 2.7288
Iteration: 2594; Percent complete: 64.8%; Average loss: 3.3515
Iteration: 2595; Percent complete: 64.9%; Average loss: 3.1121
Iteration: 2596; Percent complete: 64.9%; Average loss: 2.9269
Iteration: 2597; Percent complete: 64.9%; Average loss: 3.0045
Iteration: 2598; Percent complete: 65.0%; Average loss: 3.0006
Iteration: 2599; Percent complete: 65.0%; Average loss: 2.8529
Iteration: 2600; Percent complete: 65.0%; Average loss: 2.9734
Iteration: 2601; Percent complete: 65.0%; Average loss: 3.0563
Iteration: 2602; Percent complete: 65.0%; Average loss: 2.8721
Iteration: 2603; Percent complete: 65.1%; Average loss: 3.0588
Iteration: 2604; Percent complete: 65.1%; Average loss: 2.9742
Iteration: 2605; Percent complete: 65.1%; Average loss: 3.2081
Iteration: 2606; Percent complete: 65.1%; Average loss: 3.1114
Iteration: 2607; Percent complete: 65.2%; Average loss: 3.0469
Iteration: 2608; Percent complete: 65.2%; Average loss: 3.0484
Iteration: 2609; Percent complete: 65.2%; Average loss: 2.9718
Iteration: 2610; Percent complete: 65.2%; Average loss: 3.2893
Iteration: 2611; Percent complete: 65.3%; Average loss: 3.1526
Iteration: 2612; Percent complete: 65.3%; Average loss: 3.1204
Iteration: 2613; Percent complete: 65.3%; Average loss: 3.0813
Iteration: 2614; Percent complete: 65.3%; Average loss: 3.0306
Iteration: 2615; Percent complete: 65.4%; Average loss: 3.0568
Iteration: 2616; Percent complete: 65.4%; Average loss: 2.8739
Iteration: 2617; Percent complete: 65.4%; Average loss: 3.0444
Iteration: 2618; Percent complete: 65.5%; Average loss: 2.9057
Iteration: 2619; Percent complete: 65.5%; Average loss: 2.9401
Iteration: 2620; Percent complete: 65.5%; Average loss: 3.0384
Iteration: 2621; Percent complete: 65.5%; Average loss: 3.1561
Iteration: 2622; Percent complete: 65.5%; Average loss: 3.2772
Iteration: 2623; Percent complete: 65.6%; Average loss: 3.0068
Iteration: 2624; Percent complete: 65.6%; Average loss: 3.2597
Iteration: 2625; Percent complete: 65.6%; Average loss: 3.2444
Iteration: 2626; Percent complete: 65.6%; Average loss: 2.9081
Iteration: 2627; Percent complete: 65.7%; Average loss: 3.1436
Iteration: 2628; Percent complete: 65.7%; Average loss: 3.0507
Iteration: 2629; Percent complete: 65.7%; Average loss: 2.9492
Iteration: 2630; Percent complete: 65.8%; Average loss: 3.0157
Iteration: 2631; Percent complete: 65.8%; Average loss: 2.9127
Iteration: 2632; Percent complete: 65.8%; Average loss: 3.1831
Iteration: 2633; Percent complete: 65.8%; Average loss: 2.9690
Iteration: 2634; Percent complete: 65.8%; Average loss: 2.9374
Iteration: 2635; Percent complete: 65.9%; Average loss: 2.8644
Iteration: 2636; Percent complete: 65.9%; Average loss: 3.0774
Iteration: 2637; Percent complete: 65.9%; Average loss: 3.0151
Iteration: 2638; Percent complete: 66.0%; Average loss: 2.6501
Iteration: 2639; Percent complete: 66.0%; Average loss: 3.1207
Iteration: 2640; Percent complete: 66.0%; Average loss: 2.8199
Iteration: 2641; Percent complete: 66.0%; Average loss: 3.1168
Iteration: 2642; Percent complete: 66.0%; Average loss: 2.9063
Iteration: 2643; Percent complete: 66.1%; Average loss: 2.9926
Iteration: 2644; Percent complete: 66.1%; Average loss: 3.0218
Iteration: 2645; Percent complete: 66.1%; Average loss: 2.9950
Iteration: 2646; Percent complete: 66.1%; Average loss: 3.0505
Iteration: 2647; Percent complete: 66.2%; Average loss: 2.8771
Iteration: 2648; Percent complete: 66.2%; Average loss: 2.8477
Iteration: 2649; Percent complete: 66.2%; Average loss: 2.8388
Iteration: 2650; Percent complete: 66.2%; Average loss: 3.2335
Iteration: 2651; Percent complete: 66.3%; Average loss: 2.8540
Iteration: 2652; Percent complete: 66.3%; Average loss: 2.9839
Iteration: 2653; Percent complete: 66.3%; Average loss: 2.8782
Iteration: 2654; Percent complete: 66.3%; Average loss: 3.1031
Iteration: 2655; Percent complete: 66.4%; Average loss: 2.9896
Iteration: 2656; Percent complete: 66.4%; Average loss: 2.7905
Iteration: 2657; Percent complete: 66.4%; Average loss: 2.9167
Iteration: 2658; Percent complete: 66.5%; Average loss: 2.9375
Iteration: 2659; Percent complete: 66.5%; Average loss: 2.9335
Iteration: 2660; Percent complete: 66.5%; Average loss: 3.0635
Iteration: 2661; Percent complete: 66.5%; Average loss: 2.5775
Iteration: 2662; Percent complete: 66.5%; Average loss: 2.8686
Iteration: 2663; Percent complete: 66.6%; Average loss: 2.9458
Iteration: 2664; Percent complete: 66.6%; Average loss: 2.8422
Iteration: 2665; Percent complete: 66.6%; Average loss: 2.8339
Iteration: 2666; Percent complete: 66.6%; Average loss: 2.8582
Iteration: 2667; Percent complete: 66.7%; Average loss: 2.9569
Iteration: 2668; Percent complete: 66.7%; Average loss: 2.9190
Iteration: 2669; Percent complete: 66.7%; Average loss: 3.2001
Iteration: 2670; Percent complete: 66.8%; Average loss: 2.9118
Iteration: 2671; Percent complete: 66.8%; Average loss: 2.8659
Iteration: 2672; Percent complete: 66.8%; Average loss: 2.8999
Iteration: 2673; Percent complete: 66.8%; Average loss: 2.9400
Iteration: 2674; Percent complete: 66.8%; Average loss: 3.1305
Iteration: 2675; Percent complete: 66.9%; Average loss: 2.7871
Iteration: 2676; Percent complete: 66.9%; Average loss: 2.9733
Iteration: 2677; Percent complete: 66.9%; Average loss: 2.9471
Iteration: 2678; Percent complete: 67.0%; Average loss: 2.8011
Iteration: 2679; Percent complete: 67.0%; Average loss: 2.9447
Iteration: 2680; Percent complete: 67.0%; Average loss: 2.8877
Iteration: 2681; Percent complete: 67.0%; Average loss: 3.1808
Iteration: 2682; Percent complete: 67.0%; Average loss: 2.8659
Iteration: 2683; Percent complete: 67.1%; Average loss: 2.7145
Iteration: 2684; Percent complete: 67.1%; Average loss: 2.9636
Iteration: 2685; Percent complete: 67.1%; Average loss: 3.0564
Iteration: 2686; Percent complete: 67.2%; Average loss: 2.8311
Iteration: 2687; Percent complete: 67.2%; Average loss: 2.8670
Iteration: 2688; Percent complete: 67.2%; Average loss: 2.8791
Iteration: 2689; Percent complete: 67.2%; Average loss: 2.7427
Iteration: 2690; Percent complete: 67.2%; Average loss: 2.9900
Iteration: 2691; Percent complete: 67.3%; Average loss: 3.1825
Iteration: 2692; Percent complete: 67.3%; Average loss: 2.9833
Iteration: 2693; Percent complete: 67.3%; Average loss: 2.9273
Iteration: 2694; Percent complete: 67.3%; Average loss: 2.8093
Iteration: 2695; Percent complete: 67.4%; Average loss: 3.3712
Iteration: 2696; Percent complete: 67.4%; Average loss: 2.9688
Iteration: 2697; Percent complete: 67.4%; Average loss: 2.9077
Iteration: 2698; Percent complete: 67.5%; Average loss: 3.0014
Iteration: 2699; Percent complete: 67.5%; Average loss: 3.1913
Iteration: 2700; Percent complete: 67.5%; Average loss: 2.8702
Iteration: 2701; Percent complete: 67.5%; Average loss: 2.7170
Iteration: 2702; Percent complete: 67.5%; Average loss: 3.3498
Iteration: 2703; Percent complete: 67.6%; Average loss: 3.0503
Iteration: 2704; Percent complete: 67.6%; Average loss: 3.1061
Iteration: 2705; Percent complete: 67.6%; Average loss: 3.0807
Iteration: 2706; Percent complete: 67.7%; Average loss: 2.8501
Iteration: 2707; Percent complete: 67.7%; Average loss: 2.8279
Iteration: 2708; Percent complete: 67.7%; Average loss: 3.0655
Iteration: 2709; Percent complete: 67.7%; Average loss: 3.1474
Iteration: 2710; Percent complete: 67.8%; Average loss: 3.2115
Iteration: 2711; Percent complete: 67.8%; Average loss: 2.8347
Iteration: 2712; Percent complete: 67.8%; Average loss: 3.1021
Iteration: 2713; Percent complete: 67.8%; Average loss: 2.8031
Iteration: 2714; Percent complete: 67.8%; Average loss: 3.0383
Iteration: 2715; Percent complete: 67.9%; Average loss: 3.1586
Iteration: 2716; Percent complete: 67.9%; Average loss: 3.0584
Iteration: 2717; Percent complete: 67.9%; Average loss: 2.8757
Iteration: 2718; Percent complete: 68.0%; Average loss: 3.0566
Iteration: 2719; Percent complete: 68.0%; Average loss: 2.8490
Iteration: 2720; Percent complete: 68.0%; Average loss: 3.1005
Iteration: 2721; Percent complete: 68.0%; Average loss: 3.1219
Iteration: 2722; Percent complete: 68.0%; Average loss: 2.9881
Iteration: 2723; Percent complete: 68.1%; Average loss: 2.9561
Iteration: 2724; Percent complete: 68.1%; Average loss: 3.0290
Iteration: 2725; Percent complete: 68.1%; Average loss: 3.2338
Iteration: 2726; Percent complete: 68.2%; Average loss: 3.0133
Iteration: 2727; Percent complete: 68.2%; Average loss: 2.7433
Iteration: 2728; Percent complete: 68.2%; Average loss: 2.8025
Iteration: 2729; Percent complete: 68.2%; Average loss: 2.9332
Iteration: 2730; Percent complete: 68.2%; Average loss: 2.8951
Iteration: 2731; Percent complete: 68.3%; Average loss: 3.0643
Iteration: 2732; Percent complete: 68.3%; Average loss: 2.9233
Iteration: 2733; Percent complete: 68.3%; Average loss: 2.8131
Iteration: 2734; Percent complete: 68.3%; Average loss: 3.1180
Iteration: 2735; Percent complete: 68.4%; Average loss: 2.8864
Iteration: 2736; Percent complete: 68.4%; Average loss: 3.1245
Iteration: 2737; Percent complete: 68.4%; Average loss: 2.6106
Iteration: 2738; Percent complete: 68.5%; Average loss: 3.0455
Iteration: 2739; Percent complete: 68.5%; Average loss: 2.8836
Iteration: 2740; Percent complete: 68.5%; Average loss: 2.9925
Iteration: 2741; Percent complete: 68.5%; Average loss: 3.0251
Iteration: 2742; Percent complete: 68.5%; Average loss: 2.8634
Iteration: 2743; Percent complete: 68.6%; Average loss: 2.9531
Iteration: 2744; Percent complete: 68.6%; Average loss: 2.9708
Iteration: 2745; Percent complete: 68.6%; Average loss: 3.0567
Iteration: 2746; Percent complete: 68.7%; Average loss: 3.2215
Iteration: 2747; Percent complete: 68.7%; Average loss: 3.0460
Iteration: 2748; Percent complete: 68.7%; Average loss: 2.7811
Iteration: 2749; Percent complete: 68.7%; Average loss: 2.9091
Iteration: 2750; Percent complete: 68.8%; Average loss: 3.2275
Iteration: 2751; Percent complete: 68.8%; Average loss: 3.1293
Iteration: 2752; Percent complete: 68.8%; Average loss: 2.8620
Iteration: 2753; Percent complete: 68.8%; Average loss: 2.7823
Iteration: 2754; Percent complete: 68.8%; Average loss: 3.0515
Iteration: 2755; Percent complete: 68.9%; Average loss: 3.0609
Iteration: 2756; Percent complete: 68.9%; Average loss: 2.9417
Iteration: 2757; Percent complete: 68.9%; Average loss: 3.0428
Iteration: 2758; Percent complete: 69.0%; Average loss: 3.0683
Iteration: 2759; Percent complete: 69.0%; Average loss: 2.9202
Iteration: 2760; Percent complete: 69.0%; Average loss: 2.8833
Iteration: 2761; Percent complete: 69.0%; Average loss: 2.8381
Iteration: 2762; Percent complete: 69.0%; Average loss: 3.1469
Iteration: 2763; Percent complete: 69.1%; Average loss: 2.7325
Iteration: 2764; Percent complete: 69.1%; Average loss: 2.9445
Iteration: 2765; Percent complete: 69.1%; Average loss: 2.8382
Iteration: 2766; Percent complete: 69.2%; Average loss: 3.1146
Iteration: 2767; Percent complete: 69.2%; Average loss: 3.0484
Iteration: 2768; Percent complete: 69.2%; Average loss: 2.9146
Iteration: 2769; Percent complete: 69.2%; Average loss: 2.9035
Iteration: 2770; Percent complete: 69.2%; Average loss: 3.0756
Iteration: 2771; Percent complete: 69.3%; Average loss: 2.9754
Iteration: 2772; Percent complete: 69.3%; Average loss: 2.9568
Iteration: 2773; Percent complete: 69.3%; Average loss: 2.8838
Iteration: 2774; Percent complete: 69.3%; Average loss: 2.7631
Iteration: 2775; Percent complete: 69.4%; Average loss: 2.6278
Iteration: 2776; Percent complete: 69.4%; Average loss: 3.0229
Iteration: 2777; Percent complete: 69.4%; Average loss: 2.9787
Iteration: 2778; Percent complete: 69.5%; Average loss: 3.0584
Iteration: 2779; Percent complete: 69.5%; Average loss: 3.1421
Iteration: 2780; Percent complete: 69.5%; Average loss: 3.1993
Iteration: 2781; Percent complete: 69.5%; Average loss: 2.8670
Iteration: 2782; Percent complete: 69.5%; Average loss: 3.1216
Iteration: 2783; Percent complete: 69.6%; Average loss: 2.8222
Iteration: 2784; Percent complete: 69.6%; Average loss: 2.9837
Iteration: 2785; Percent complete: 69.6%; Average loss: 3.1024
Iteration: 2786; Percent complete: 69.7%; Average loss: 2.8955
Iteration: 2787; Percent complete: 69.7%; Average loss: 3.0631
Iteration: 2788; Percent complete: 69.7%; Average loss: 2.5877
Iteration: 2789; Percent complete: 69.7%; Average loss: 2.8582
Iteration: 2790; Percent complete: 69.8%; Average loss: 2.9070
Iteration: 2791; Percent complete: 69.8%; Average loss: 3.0248
Iteration: 2792; Percent complete: 69.8%; Average loss: 2.9282
Iteration: 2793; Percent complete: 69.8%; Average loss: 3.1261
Iteration: 2794; Percent complete: 69.8%; Average loss: 3.1183
Iteration: 2795; Percent complete: 69.9%; Average loss: 2.7375
Iteration: 2796; Percent complete: 69.9%; Average loss: 2.9672
Iteration: 2797; Percent complete: 69.9%; Average loss: 2.7700
Iteration: 2798; Percent complete: 70.0%; Average loss: 3.0889
Iteration: 2799; Percent complete: 70.0%; Average loss: 3.0769
Iteration: 2800; Percent complete: 70.0%; Average loss: 3.0029
Iteration: 2801; Percent complete: 70.0%; Average loss: 2.8429
Iteration: 2802; Percent complete: 70.0%; Average loss: 2.9101
Iteration: 2803; Percent complete: 70.1%; Average loss: 2.8821
Iteration: 2804; Percent complete: 70.1%; Average loss: 3.1384
Iteration: 2805; Percent complete: 70.1%; Average loss: 2.9959
Iteration: 2806; Percent complete: 70.2%; Average loss: 2.7555
Iteration: 2807; Percent complete: 70.2%; Average loss: 2.7339
Iteration: 2808; Percent complete: 70.2%; Average loss: 3.0719
Iteration: 2809; Percent complete: 70.2%; Average loss: 2.8176
Iteration: 2810; Percent complete: 70.2%; Average loss: 2.8249
Iteration: 2811; Percent complete: 70.3%; Average loss: 2.8073
Iteration: 2812; Percent complete: 70.3%; Average loss: 3.0337
Iteration: 2813; Percent complete: 70.3%; Average loss: 2.9036
Iteration: 2814; Percent complete: 70.3%; Average loss: 2.8465
Iteration: 2815; Percent complete: 70.4%; Average loss: 3.2404
Iteration: 2816; Percent complete: 70.4%; Average loss: 2.9863
Iteration: 2817; Percent complete: 70.4%; Average loss: 2.7012
Iteration: 2818; Percent complete: 70.5%; Average loss: 2.9482
Iteration: 2819; Percent complete: 70.5%; Average loss: 2.9732
Iteration: 2820; Percent complete: 70.5%; Average loss: 3.0659
Iteration: 2821; Percent complete: 70.5%; Average loss: 2.7322
Iteration: 2822; Percent complete: 70.5%; Average loss: 3.0915
Iteration: 2823; Percent complete: 70.6%; Average loss: 2.8676
Iteration: 2824; Percent complete: 70.6%; Average loss: 2.8165
Iteration: 2825; Percent complete: 70.6%; Average loss: 2.8995
Iteration: 2826; Percent complete: 70.7%; Average loss: 2.7943
Iteration: 2827; Percent complete: 70.7%; Average loss: 2.9008
Iteration: 2828; Percent complete: 70.7%; Average loss: 2.5490
Iteration: 2829; Percent complete: 70.7%; Average loss: 2.7602
Iteration: 2830; Percent complete: 70.8%; Average loss: 3.1150
Iteration: 2831; Percent complete: 70.8%; Average loss: 2.9713
Iteration: 2832; Percent complete: 70.8%; Average loss: 2.9964
Iteration: 2833; Percent complete: 70.8%; Average loss: 2.7755
Iteration: 2834; Percent complete: 70.9%; Average loss: 2.9012
Iteration: 2835; Percent complete: 70.9%; Average loss: 2.8549
Iteration: 2836; Percent complete: 70.9%; Average loss: 3.1075
Iteration: 2837; Percent complete: 70.9%; Average loss: 3.2281
Iteration: 2838; Percent complete: 71.0%; Average loss: 2.8889
Iteration: 2839; Percent complete: 71.0%; Average loss: 2.9006
Iteration: 2840; Percent complete: 71.0%; Average loss: 3.0294
Iteration: 2841; Percent complete: 71.0%; Average loss: 2.8014
Iteration: 2842; Percent complete: 71.0%; Average loss: 3.0219
Iteration: 2843; Percent complete: 71.1%; Average loss: 2.9144
Iteration: 2844; Percent complete: 71.1%; Average loss: 3.1962
Iteration: 2845; Percent complete: 71.1%; Average loss: 2.8884
Iteration: 2846; Percent complete: 71.2%; Average loss: 3.0594
Iteration: 2847; Percent complete: 71.2%; Average loss: 3.0113
Iteration: 2848; Percent complete: 71.2%; Average loss: 2.8796
Iteration: 2849; Percent complete: 71.2%; Average loss: 2.7503
Iteration: 2850; Percent complete: 71.2%; Average loss: 2.8852
Iteration: 2851; Percent complete: 71.3%; Average loss: 2.9226
Iteration: 2852; Percent complete: 71.3%; Average loss: 2.9714
Iteration: 2853; Percent complete: 71.3%; Average loss: 2.7872
Iteration: 2854; Percent complete: 71.4%; Average loss: 2.9997
Iteration: 2855; Percent complete: 71.4%; Average loss: 2.9088
Iteration: 2856; Percent complete: 71.4%; Average loss: 2.8120
Iteration: 2857; Percent complete: 71.4%; Average loss: 3.1936
Iteration: 2858; Percent complete: 71.5%; Average loss: 2.8718
Iteration: 2859; Percent complete: 71.5%; Average loss: 2.8773
Iteration: 2860; Percent complete: 71.5%; Average loss: 2.8813
Iteration: 2861; Percent complete: 71.5%; Average loss: 3.0355
Iteration: 2862; Percent complete: 71.5%; Average loss: 2.9752
Iteration: 2863; Percent complete: 71.6%; Average loss: 2.9550
Iteration: 2864; Percent complete: 71.6%; Average loss: 3.0850
Iteration: 2865; Percent complete: 71.6%; Average loss: 2.8003
Iteration: 2866; Percent complete: 71.7%; Average loss: 2.6899
Iteration: 2867; Percent complete: 71.7%; Average loss: 2.9209
Iteration: 2868; Percent complete: 71.7%; Average loss: 3.0313
Iteration: 2869; Percent complete: 71.7%; Average loss: 2.8351
Iteration: 2870; Percent complete: 71.8%; Average loss: 2.7691
Iteration: 2871; Percent complete: 71.8%; Average loss: 2.9307
Iteration: 2872; Percent complete: 71.8%; Average loss: 2.6843
Iteration: 2873; Percent complete: 71.8%; Average loss: 3.0012
Iteration: 2874; Percent complete: 71.9%; Average loss: 2.7115
Iteration: 2875; Percent complete: 71.9%; Average loss: 2.7819
Iteration: 2876; Percent complete: 71.9%; Average loss: 2.9593
Iteration: 2877; Percent complete: 71.9%; Average loss: 2.8188
Iteration: 2878; Percent complete: 72.0%; Average loss: 3.0581
Iteration: 2879; Percent complete: 72.0%; Average loss: 2.8300
Iteration: 2880; Percent complete: 72.0%; Average loss: 2.6200
Iteration: 2881; Percent complete: 72.0%; Average loss: 2.7552
Iteration: 2882; Percent complete: 72.0%; Average loss: 2.9735
Iteration: 2883; Percent complete: 72.1%; Average loss: 2.8303
Iteration: 2884; Percent complete: 72.1%; Average loss: 2.8249
Iteration: 2885; Percent complete: 72.1%; Average loss: 2.9640
Iteration: 2886; Percent complete: 72.2%; Average loss: 2.9479
Iteration: 2887; Percent complete: 72.2%; Average loss: 2.7897
Iteration: 2888; Percent complete: 72.2%; Average loss: 2.8793
Iteration: 2889; Percent complete: 72.2%; Average loss: 2.5750
Iteration: 2890; Percent complete: 72.2%; Average loss: 2.8342
Iteration: 2891; Percent complete: 72.3%; Average loss: 2.6810
Iteration: 2892; Percent complete: 72.3%; Average loss: 2.7951
Iteration: 2893; Percent complete: 72.3%; Average loss: 2.9291
Iteration: 2894; Percent complete: 72.4%; Average loss: 2.9821
Iteration: 2895; Percent complete: 72.4%; Average loss: 2.8725
Iteration: 2896; Percent complete: 72.4%; Average loss: 3.2971
Iteration: 2897; Percent complete: 72.4%; Average loss: 2.7488
Iteration: 2898; Percent complete: 72.5%; Average loss: 2.9484
Iteration: 2899; Percent complete: 72.5%; Average loss: 2.8369
Iteration: 2900; Percent complete: 72.5%; Average loss: 2.6144
Iteration: 2901; Percent complete: 72.5%; Average loss: 2.9253
Iteration: 2902; Percent complete: 72.5%; Average loss: 3.0046
Iteration: 2903; Percent complete: 72.6%; Average loss: 2.9422
Iteration: 2904; Percent complete: 72.6%; Average loss: 2.8846
Iteration: 2905; Percent complete: 72.6%; Average loss: 2.8425
Iteration: 2906; Percent complete: 72.7%; Average loss: 3.0663
Iteration: 2907; Percent complete: 72.7%; Average loss: 2.6918
Iteration: 2908; Percent complete: 72.7%; Average loss: 2.8133
Iteration: 2909; Percent complete: 72.7%; Average loss: 3.2100
Iteration: 2910; Percent complete: 72.8%; Average loss: 2.9444
Iteration: 2911; Percent complete: 72.8%; Average loss: 3.0953
Iteration: 2912; Percent complete: 72.8%; Average loss: 2.9543
Iteration: 2913; Percent complete: 72.8%; Average loss: 2.9472
Iteration: 2914; Percent complete: 72.9%; Average loss: 2.8365
Iteration: 2915; Percent complete: 72.9%; Average loss: 2.7778
Iteration: 2916; Percent complete: 72.9%; Average loss: 2.9049
Iteration: 2917; Percent complete: 72.9%; Average loss: 2.8180
Iteration: 2918; Percent complete: 73.0%; Average loss: 3.1242
Iteration: 2919; Percent complete: 73.0%; Average loss: 2.9359
Iteration: 2920; Percent complete: 73.0%; Average loss: 2.9176
Iteration: 2921; Percent complete: 73.0%; Average loss: 2.8559
Iteration: 2922; Percent complete: 73.0%; Average loss: 2.9883
Iteration: 2923; Percent complete: 73.1%; Average loss: 3.2079
Iteration: 2924; Percent complete: 73.1%; Average loss: 2.8902
Iteration: 2925; Percent complete: 73.1%; Average loss: 3.0818
Iteration: 2926; Percent complete: 73.2%; Average loss: 2.6801
Iteration: 2927; Percent complete: 73.2%; Average loss: 2.7828
Iteration: 2928; Percent complete: 73.2%; Average loss: 2.7332
Iteration: 2929; Percent complete: 73.2%; Average loss: 2.8270
Iteration: 2930; Percent complete: 73.2%; Average loss: 2.7692
Iteration: 2931; Percent complete: 73.3%; Average loss: 2.7663
Iteration: 2932; Percent complete: 73.3%; Average loss: 2.7030
Iteration: 2933; Percent complete: 73.3%; Average loss: 2.8221
Iteration: 2934; Percent complete: 73.4%; Average loss: 2.8160
Iteration: 2935; Percent complete: 73.4%; Average loss: 2.8849
Iteration: 2936; Percent complete: 73.4%; Average loss: 2.6993
Iteration: 2937; Percent complete: 73.4%; Average loss: 2.7854
Iteration: 2938; Percent complete: 73.5%; Average loss: 2.9629
Iteration: 2939; Percent complete: 73.5%; Average loss: 2.7540
Iteration: 2940; Percent complete: 73.5%; Average loss: 2.9986
Iteration: 2941; Percent complete: 73.5%; Average loss: 2.7501
Iteration: 2942; Percent complete: 73.6%; Average loss: 2.8975
Iteration: 2943; Percent complete: 73.6%; Average loss: 3.0486
Iteration: 2944; Percent complete: 73.6%; Average loss: 2.7437
Iteration: 2945; Percent complete: 73.6%; Average loss: 2.6671
Iteration: 2946; Percent complete: 73.7%; Average loss: 2.9257
Iteration: 2947; Percent complete: 73.7%; Average loss: 2.7399
Iteration: 2948; Percent complete: 73.7%; Average loss: 3.0557
Iteration: 2949; Percent complete: 73.7%; Average loss: 2.8625
Iteration: 2950; Percent complete: 73.8%; Average loss: 3.0414
Iteration: 2951; Percent complete: 73.8%; Average loss: 2.8131
Iteration: 2952; Percent complete: 73.8%; Average loss: 2.7982
Iteration: 2953; Percent complete: 73.8%; Average loss: 2.7032
Iteration: 2954; Percent complete: 73.9%; Average loss: 2.7208
Iteration: 2955; Percent complete: 73.9%; Average loss: 2.7804
Iteration: 2956; Percent complete: 73.9%; Average loss: 2.7691
Iteration: 2957; Percent complete: 73.9%; Average loss: 3.1423
Iteration: 2958; Percent complete: 74.0%; Average loss: 2.8508
Iteration: 2959; Percent complete: 74.0%; Average loss: 3.0423
Iteration: 2960; Percent complete: 74.0%; Average loss: 2.8893
Iteration: 2961; Percent complete: 74.0%; Average loss: 3.1355
Iteration: 2962; Percent complete: 74.1%; Average loss: 3.0729
Iteration: 2963; Percent complete: 74.1%; Average loss: 2.8485
Iteration: 2964; Percent complete: 74.1%; Average loss: 2.9150
Iteration: 2965; Percent complete: 74.1%; Average loss: 2.8033
Iteration: 2966; Percent complete: 74.2%; Average loss: 2.9452
Iteration: 2967; Percent complete: 74.2%; Average loss: 3.0231
Iteration: 2968; Percent complete: 74.2%; Average loss: 2.6282
Iteration: 2969; Percent complete: 74.2%; Average loss: 3.2411
Iteration: 2970; Percent complete: 74.2%; Average loss: 3.0344
Iteration: 2971; Percent complete: 74.3%; Average loss: 2.8301
Iteration: 2972; Percent complete: 74.3%; Average loss: 2.8675
Iteration: 2973; Percent complete: 74.3%; Average loss: 3.0727
Iteration: 2974; Percent complete: 74.4%; Average loss: 3.1358
Iteration: 2975; Percent complete: 74.4%; Average loss: 2.9231
Iteration: 2976; Percent complete: 74.4%; Average loss: 2.8291
Iteration: 2977; Percent complete: 74.4%; Average loss: 2.8282
Iteration: 2978; Percent complete: 74.5%; Average loss: 2.5569
Iteration: 2979; Percent complete: 74.5%; Average loss: 3.0409
Iteration: 2980; Percent complete: 74.5%; Average loss: 3.0564
Iteration: 2981; Percent complete: 74.5%; Average loss: 2.6698
Iteration: 2982; Percent complete: 74.6%; Average loss: 2.6499
Iteration: 2983; Percent complete: 74.6%; Average loss: 2.8377
Iteration: 2984; Percent complete: 74.6%; Average loss: 2.7785
Iteration: 2985; Percent complete: 74.6%; Average loss: 2.9162
Iteration: 2986; Percent complete: 74.7%; Average loss: 2.7576
Iteration: 2987; Percent complete: 74.7%; Average loss: 3.1482
Iteration: 2988; Percent complete: 74.7%; Average loss: 2.9011
Iteration: 2989; Percent complete: 74.7%; Average loss: 2.9025
Iteration: 2990; Percent complete: 74.8%; Average loss: 3.1078
Iteration: 2991; Percent complete: 74.8%; Average loss: 2.9449
Iteration: 2992; Percent complete: 74.8%; Average loss: 2.9471
Iteration: 2993; Percent complete: 74.8%; Average loss: 2.7315
Iteration: 2994; Percent complete: 74.9%; Average loss: 3.0425
Iteration: 2995; Percent complete: 74.9%; Average loss: 3.0274
Iteration: 2996; Percent complete: 74.9%; Average loss: 2.7390
Iteration: 2997; Percent complete: 74.9%; Average loss: 2.9744
Iteration: 2998; Percent complete: 75.0%; Average loss: 2.6825
Iteration: 2999; Percent complete: 75.0%; Average loss: 2.8708
Iteration: 3000; Percent complete: 75.0%; Average loss: 3.2082
Iteration: 3001; Percent complete: 75.0%; Average loss: 2.8387
Iteration: 3002; Percent complete: 75.0%; Average loss: 2.8657
Iteration: 3003; Percent complete: 75.1%; Average loss: 3.0210
Iteration: 3004; Percent complete: 75.1%; Average loss: 2.8414
Iteration: 3005; Percent complete: 75.1%; Average loss: 2.7026
Iteration: 3006; Percent complete: 75.1%; Average loss: 3.1719
Iteration: 3007; Percent complete: 75.2%; Average loss: 3.0342
Iteration: 3008; Percent complete: 75.2%; Average loss: 3.0683
Iteration: 3009; Percent complete: 75.2%; Average loss: 3.0627
Iteration: 3010; Percent complete: 75.2%; Average loss: 2.7174
Iteration: 3011; Percent complete: 75.3%; Average loss: 2.7418
Iteration: 3012; Percent complete: 75.3%; Average loss: 2.7915
Iteration: 3013; Percent complete: 75.3%; Average loss: 2.8516
Iteration: 3014; Percent complete: 75.3%; Average loss: 2.8170
Iteration: 3015; Percent complete: 75.4%; Average loss: 2.8015
Iteration: 3016; Percent complete: 75.4%; Average loss: 2.9109
Iteration: 3017; Percent complete: 75.4%; Average loss: 2.8118
Iteration: 3018; Percent complete: 75.4%; Average loss: 2.8266
Iteration: 3019; Percent complete: 75.5%; Average loss: 2.8349
Iteration: 3020; Percent complete: 75.5%; Average loss: 2.6932
Iteration: 3021; Percent complete: 75.5%; Average loss: 2.7889
Iteration: 3022; Percent complete: 75.5%; Average loss: 2.9308
Iteration: 3023; Percent complete: 75.6%; Average loss: 2.8192
Iteration: 3024; Percent complete: 75.6%; Average loss: 2.8546
Iteration: 3025; Percent complete: 75.6%; Average loss: 2.8752
Iteration: 3026; Percent complete: 75.6%; Average loss: 2.7222
Iteration: 3027; Percent complete: 75.7%; Average loss: 2.9261
Iteration: 3028; Percent complete: 75.7%; Average loss: 2.9956
Iteration: 3029; Percent complete: 75.7%; Average loss: 2.9192
Iteration: 3030; Percent complete: 75.8%; Average loss: 2.8980
Iteration: 3031; Percent complete: 75.8%; Average loss: 2.9582
Iteration: 3032; Percent complete: 75.8%; Average loss: 2.9902
Iteration: 3033; Percent complete: 75.8%; Average loss: 3.0727
Iteration: 3034; Percent complete: 75.8%; Average loss: 2.8685
Iteration: 3035; Percent complete: 75.9%; Average loss: 2.8932
Iteration: 3036; Percent complete: 75.9%; Average loss: 2.8475
Iteration: 3037; Percent complete: 75.9%; Average loss: 2.8536
Iteration: 3038; Percent complete: 75.9%; Average loss: 2.8495
Iteration: 3039; Percent complete: 76.0%; Average loss: 2.6533
Iteration: 3040; Percent complete: 76.0%; Average loss: 2.9522
Iteration: 3041; Percent complete: 76.0%; Average loss: 2.9095
Iteration: 3042; Percent complete: 76.0%; Average loss: 3.0347
Iteration: 3043; Percent complete: 76.1%; Average loss: 3.0563
Iteration: 3044; Percent complete: 76.1%; Average loss: 2.8898
Iteration: 3045; Percent complete: 76.1%; Average loss: 2.8240
Iteration: 3046; Percent complete: 76.1%; Average loss: 2.7921
Iteration: 3047; Percent complete: 76.2%; Average loss: 2.8315
Iteration: 3048; Percent complete: 76.2%; Average loss: 3.1654
Iteration: 3049; Percent complete: 76.2%; Average loss: 2.8335
Iteration: 3050; Percent complete: 76.2%; Average loss: 3.1241
Iteration: 3051; Percent complete: 76.3%; Average loss: 3.0035
Iteration: 3052; Percent complete: 76.3%; Average loss: 2.9581
Iteration: 3053; Percent complete: 76.3%; Average loss: 2.8892
Iteration: 3054; Percent complete: 76.3%; Average loss: 2.7720
Iteration: 3055; Percent complete: 76.4%; Average loss: 2.7651
Iteration: 3056; Percent complete: 76.4%; Average loss: 2.8374
Iteration: 3057; Percent complete: 76.4%; Average loss: 2.8049
Iteration: 3058; Percent complete: 76.4%; Average loss: 2.7995
Iteration: 3059; Percent complete: 76.5%; Average loss: 2.8616
Iteration: 3060; Percent complete: 76.5%; Average loss: 2.8015
Iteration: 3061; Percent complete: 76.5%; Average loss: 2.7409
Iteration: 3062; Percent complete: 76.5%; Average loss: 2.6437
Iteration: 3063; Percent complete: 76.6%; Average loss: 2.8994
Iteration: 3064; Percent complete: 76.6%; Average loss: 2.8303
Iteration: 3065; Percent complete: 76.6%; Average loss: 2.9336
Iteration: 3066; Percent complete: 76.6%; Average loss: 2.9065
Iteration: 3067; Percent complete: 76.7%; Average loss: 2.9702
Iteration: 3068; Percent complete: 76.7%; Average loss: 3.0744
Iteration: 3069; Percent complete: 76.7%; Average loss: 2.9515
Iteration: 3070; Percent complete: 76.8%; Average loss: 2.9527
Iteration: 3071; Percent complete: 76.8%; Average loss: 3.1636
Iteration: 3072; Percent complete: 76.8%; Average loss: 2.8833
Iteration: 3073; Percent complete: 76.8%; Average loss: 2.9407
Iteration: 3074; Percent complete: 76.8%; Average loss: 2.7562
Iteration: 3075; Percent complete: 76.9%; Average loss: 2.6233
Iteration: 3076; Percent complete: 76.9%; Average loss: 2.9065
Iteration: 3077; Percent complete: 76.9%; Average loss: 2.8851
Iteration: 3078; Percent complete: 77.0%; Average loss: 3.0702
Iteration: 3079; Percent complete: 77.0%; Average loss: 2.7248
Iteration: 3080; Percent complete: 77.0%; Average loss: 2.8626
Iteration: 3081; Percent complete: 77.0%; Average loss: 2.7220
Iteration: 3082; Percent complete: 77.0%; Average loss: 2.7002
Iteration: 3083; Percent complete: 77.1%; Average loss: 2.9211
Iteration: 3084; Percent complete: 77.1%; Average loss: 2.8031
Iteration: 3085; Percent complete: 77.1%; Average loss: 2.5699
Iteration: 3086; Percent complete: 77.1%; Average loss: 2.9353
Iteration: 3087; Percent complete: 77.2%; Average loss: 2.8152
Iteration: 3088; Percent complete: 77.2%; Average loss: 2.9736
Iteration: 3089; Percent complete: 77.2%; Average loss: 2.8059
Iteration: 3090; Percent complete: 77.2%; Average loss: 2.8754
Iteration: 3091; Percent complete: 77.3%; Average loss: 2.7216
Iteration: 3092; Percent complete: 77.3%; Average loss: 2.7969
Iteration: 3093; Percent complete: 77.3%; Average loss: 2.9984
Iteration: 3094; Percent complete: 77.3%; Average loss: 2.7721
Iteration: 3095; Percent complete: 77.4%; Average loss: 2.9955
Iteration: 3096; Percent complete: 77.4%; Average loss: 2.8415
Iteration: 3097; Percent complete: 77.4%; Average loss: 3.1471
Iteration: 3098; Percent complete: 77.5%; Average loss: 3.0201
Iteration: 3099; Percent complete: 77.5%; Average loss: 2.8099
Iteration: 3100; Percent complete: 77.5%; Average loss: 2.6879
Iteration: 3101; Percent complete: 77.5%; Average loss: 3.2993
Iteration: 3102; Percent complete: 77.5%; Average loss: 2.7105
Iteration: 3103; Percent complete: 77.6%; Average loss: 2.8650
Iteration: 3104; Percent complete: 77.6%; Average loss: 2.7679
Iteration: 3105; Percent complete: 77.6%; Average loss: 3.0291
Iteration: 3106; Percent complete: 77.6%; Average loss: 2.9426
Iteration: 3107; Percent complete: 77.7%; Average loss: 3.0912
Iteration: 3108; Percent complete: 77.7%; Average loss: 2.9929
Iteration: 3109; Percent complete: 77.7%; Average loss: 2.8582
Iteration: 3110; Percent complete: 77.8%; Average loss: 2.9638
Iteration: 3111; Percent complete: 77.8%; Average loss: 2.8246
Iteration: 3112; Percent complete: 77.8%; Average loss: 2.8307
Iteration: 3113; Percent complete: 77.8%; Average loss: 2.8995
Iteration: 3114; Percent complete: 77.8%; Average loss: 2.7544
Iteration: 3115; Percent complete: 77.9%; Average loss: 2.8738
Iteration: 3116; Percent complete: 77.9%; Average loss: 2.7534
Iteration: 3117; Percent complete: 77.9%; Average loss: 2.8534
Iteration: 3118; Percent complete: 78.0%; Average loss: 2.9037
Iteration: 3119; Percent complete: 78.0%; Average loss: 2.7779
Iteration: 3120; Percent complete: 78.0%; Average loss: 2.8605
Iteration: 3121; Percent complete: 78.0%; Average loss: 2.9228
Iteration: 3122; Percent complete: 78.0%; Average loss: 2.9239
Iteration: 3123; Percent complete: 78.1%; Average loss: 2.8518
Iteration: 3124; Percent complete: 78.1%; Average loss: 2.7606
Iteration: 3125; Percent complete: 78.1%; Average loss: 2.6916
Iteration: 3126; Percent complete: 78.1%; Average loss: 2.7671
Iteration: 3127; Percent complete: 78.2%; Average loss: 2.6757
Iteration: 3128; Percent complete: 78.2%; Average loss: 2.8178
Iteration: 3129; Percent complete: 78.2%; Average loss: 2.7629
Iteration: 3130; Percent complete: 78.2%; Average loss: 2.6067
Iteration: 3131; Percent complete: 78.3%; Average loss: 2.7772
Iteration: 3132; Percent complete: 78.3%; Average loss: 3.1471
Iteration: 3133; Percent complete: 78.3%; Average loss: 2.8779
Iteration: 3134; Percent complete: 78.3%; Average loss: 2.9312
Iteration: 3135; Percent complete: 78.4%; Average loss: 2.8127
Iteration: 3136; Percent complete: 78.4%; Average loss: 2.7677
Iteration: 3137; Percent complete: 78.4%; Average loss: 2.7769
Iteration: 3138; Percent complete: 78.5%; Average loss: 2.8710
Iteration: 3139; Percent complete: 78.5%; Average loss: 2.9020
Iteration: 3140; Percent complete: 78.5%; Average loss: 3.1006
Iteration: 3141; Percent complete: 78.5%; Average loss: 3.0097
Iteration: 3142; Percent complete: 78.5%; Average loss: 2.8120
Iteration: 3143; Percent complete: 78.6%; Average loss: 2.5472
Iteration: 3144; Percent complete: 78.6%; Average loss: 2.9263
Iteration: 3145; Percent complete: 78.6%; Average loss: 2.8563
Iteration: 3146; Percent complete: 78.6%; Average loss: 2.9272
Iteration: 3147; Percent complete: 78.7%; Average loss: 2.8924
Iteration: 3148; Percent complete: 78.7%; Average loss: 2.8235
Iteration: 3149; Percent complete: 78.7%; Average loss: 2.8562
Iteration: 3150; Percent complete: 78.8%; Average loss: 2.9394
Iteration: 3151; Percent complete: 78.8%; Average loss: 2.6942
Iteration: 3152; Percent complete: 78.8%; Average loss: 3.0842
Iteration: 3153; Percent complete: 78.8%; Average loss: 2.9470
Iteration: 3154; Percent complete: 78.8%; Average loss: 2.8138
Iteration: 3155; Percent complete: 78.9%; Average loss: 2.9214
Iteration: 3156; Percent complete: 78.9%; Average loss: 2.7139
Iteration: 3157; Percent complete: 78.9%; Average loss: 2.9152
Iteration: 3158; Percent complete: 79.0%; Average loss: 2.9027
Iteration: 3159; Percent complete: 79.0%; Average loss: 2.5945
Iteration: 3160; Percent complete: 79.0%; Average loss: 2.9493
Iteration: 3161; Percent complete: 79.0%; Average loss: 2.8459
Iteration: 3162; Percent complete: 79.0%; Average loss: 2.8266
Iteration: 3163; Percent complete: 79.1%; Average loss: 2.7758
Iteration: 3164; Percent complete: 79.1%; Average loss: 2.8414
Iteration: 3165; Percent complete: 79.1%; Average loss: 2.6166
Iteration: 3166; Percent complete: 79.1%; Average loss: 2.7130
Iteration: 3167; Percent complete: 79.2%; Average loss: 2.7000
Iteration: 3168; Percent complete: 79.2%; Average loss: 2.9156
Iteration: 3169; Percent complete: 79.2%; Average loss: 2.7991
Iteration: 3170; Percent complete: 79.2%; Average loss: 2.9208
Iteration: 3171; Percent complete: 79.3%; Average loss: 2.5650
Iteration: 3172; Percent complete: 79.3%; Average loss: 3.0114
Iteration: 3173; Percent complete: 79.3%; Average loss: 2.9536
Iteration: 3174; Percent complete: 79.3%; Average loss: 2.5934
Iteration: 3175; Percent complete: 79.4%; Average loss: 2.8914
Iteration: 3176; Percent complete: 79.4%; Average loss: 2.6875
Iteration: 3177; Percent complete: 79.4%; Average loss: 2.9500
Iteration: 3178; Percent complete: 79.5%; Average loss: 2.7910
Iteration: 3179; Percent complete: 79.5%; Average loss: 2.8909
Iteration: 3180; Percent complete: 79.5%; Average loss: 2.8301
Iteration: 3181; Percent complete: 79.5%; Average loss: 3.1338
Iteration: 3182; Percent complete: 79.5%; Average loss: 2.8655
Iteration: 3183; Percent complete: 79.6%; Average loss: 2.8624
Iteration: 3184; Percent complete: 79.6%; Average loss: 2.8890
Iteration: 3185; Percent complete: 79.6%; Average loss: 2.8654
Iteration: 3186; Percent complete: 79.7%; Average loss: 2.9474
Iteration: 3187; Percent complete: 79.7%; Average loss: 3.0900
Iteration: 3188; Percent complete: 79.7%; Average loss: 2.5822
Iteration: 3189; Percent complete: 79.7%; Average loss: 2.8352
Iteration: 3190; Percent complete: 79.8%; Average loss: 2.8402
Iteration: 3191; Percent complete: 79.8%; Average loss: 2.8413
Iteration: 3192; Percent complete: 79.8%; Average loss: 2.7824
Iteration: 3193; Percent complete: 79.8%; Average loss: 2.8173
Iteration: 3194; Percent complete: 79.8%; Average loss: 3.1326
Iteration: 3195; Percent complete: 79.9%; Average loss: 2.9764
Iteration: 3196; Percent complete: 79.9%; Average loss: 2.6389
Iteration: 3197; Percent complete: 79.9%; Average loss: 2.8409
Iteration: 3198; Percent complete: 80.0%; Average loss: 2.7617
Iteration: 3199; Percent complete: 80.0%; Average loss: 2.8373
Iteration: 3200; Percent complete: 80.0%; Average loss: 3.0859
Iteration: 3201; Percent complete: 80.0%; Average loss: 3.0034
Iteration: 3202; Percent complete: 80.0%; Average loss: 2.8796
Iteration: 3203; Percent complete: 80.1%; Average loss: 2.7115
Iteration: 3204; Percent complete: 80.1%; Average loss: 2.7718
Iteration: 3205; Percent complete: 80.1%; Average loss: 2.9032
Iteration: 3206; Percent complete: 80.2%; Average loss: 2.6438
Iteration: 3207; Percent complete: 80.2%; Average loss: 2.7489
Iteration: 3208; Percent complete: 80.2%; Average loss: 2.7614
Iteration: 3209; Percent complete: 80.2%; Average loss: 2.9291
Iteration: 3210; Percent complete: 80.2%; Average loss: 2.5840
Iteration: 3211; Percent complete: 80.3%; Average loss: 3.0409
Iteration: 3212; Percent complete: 80.3%; Average loss: 2.8967
Iteration: 3213; Percent complete: 80.3%; Average loss: 3.2716
Iteration: 3214; Percent complete: 80.3%; Average loss: 2.9920
Iteration: 3215; Percent complete: 80.4%; Average loss: 2.6893
Iteration: 3216; Percent complete: 80.4%; Average loss: 3.1782
Iteration: 3217; Percent complete: 80.4%; Average loss: 2.7280
Iteration: 3218; Percent complete: 80.5%; Average loss: 2.9373
Iteration: 3219; Percent complete: 80.5%; Average loss: 2.8979
Iteration: 3220; Percent complete: 80.5%; Average loss: 2.7377
Iteration: 3221; Percent complete: 80.5%; Average loss: 3.0119
Iteration: 3222; Percent complete: 80.5%; Average loss: 2.8202
Iteration: 3223; Percent complete: 80.6%; Average loss: 2.8185
Iteration: 3224; Percent complete: 80.6%; Average loss: 2.6396
Iteration: 3225; Percent complete: 80.6%; Average loss: 2.7489
Iteration: 3226; Percent complete: 80.7%; Average loss: 2.5041
Iteration: 3227; Percent complete: 80.7%; Average loss: 2.8028
Iteration: 3228; Percent complete: 80.7%; Average loss: 2.6830
Iteration: 3229; Percent complete: 80.7%; Average loss: 2.6649
Iteration: 3230; Percent complete: 80.8%; Average loss: 2.7884
Iteration: 3231; Percent complete: 80.8%; Average loss: 2.7705
Iteration: 3232; Percent complete: 80.8%; Average loss: 2.9789
Iteration: 3233; Percent complete: 80.8%; Average loss: 3.1359
Iteration: 3234; Percent complete: 80.8%; Average loss: 2.7554
Iteration: 3235; Percent complete: 80.9%; Average loss: 2.7807
Iteration: 3236; Percent complete: 80.9%; Average loss: 2.8846
Iteration: 3237; Percent complete: 80.9%; Average loss: 2.8682
Iteration: 3238; Percent complete: 81.0%; Average loss: 3.0501
Iteration: 3239; Percent complete: 81.0%; Average loss: 2.8653
Iteration: 3240; Percent complete: 81.0%; Average loss: 2.7823
Iteration: 3241; Percent complete: 81.0%; Average loss: 2.6799
Iteration: 3242; Percent complete: 81.0%; Average loss: 2.8702
Iteration: 3243; Percent complete: 81.1%; Average loss: 2.8309
Iteration: 3244; Percent complete: 81.1%; Average loss: 2.9940
Iteration: 3245; Percent complete: 81.1%; Average loss: 2.7904
Iteration: 3246; Percent complete: 81.2%; Average loss: 2.9349
Iteration: 3247; Percent complete: 81.2%; Average loss: 2.9864
Iteration: 3248; Percent complete: 81.2%; Average loss: 2.8700
Iteration: 3249; Percent complete: 81.2%; Average loss: 2.7271
Iteration: 3250; Percent complete: 81.2%; Average loss: 2.6094
Iteration: 3251; Percent complete: 81.3%; Average loss: 2.8626
Iteration: 3252; Percent complete: 81.3%; Average loss: 2.6589
Iteration: 3253; Percent complete: 81.3%; Average loss: 2.9597
Iteration: 3254; Percent complete: 81.3%; Average loss: 2.7122
Iteration: 3255; Percent complete: 81.4%; Average loss: 2.8431
Iteration: 3256; Percent complete: 81.4%; Average loss: 2.7825
Iteration: 3257; Percent complete: 81.4%; Average loss: 2.6902
Iteration: 3258; Percent complete: 81.5%; Average loss: 2.6969
Iteration: 3259; Percent complete: 81.5%; Average loss: 2.8277
Iteration: 3260; Percent complete: 81.5%; Average loss: 2.7161
Iteration: 3261; Percent complete: 81.5%; Average loss: 2.8433
Iteration: 3262; Percent complete: 81.5%; Average loss: 3.0416
Iteration: 3263; Percent complete: 81.6%; Average loss: 2.9083
Iteration: 3264; Percent complete: 81.6%; Average loss: 2.7762
Iteration: 3265; Percent complete: 81.6%; Average loss: 2.4965
Iteration: 3266; Percent complete: 81.7%; Average loss: 2.9169
Iteration: 3267; Percent complete: 81.7%; Average loss: 2.9564
Iteration: 3268; Percent complete: 81.7%; Average loss: 3.0292
Iteration: 3269; Percent complete: 81.7%; Average loss: 2.9701
Iteration: 3270; Percent complete: 81.8%; Average loss: 2.8832
Iteration: 3271; Percent complete: 81.8%; Average loss: 2.9575
Iteration: 3272; Percent complete: 81.8%; Average loss: 2.8389
Iteration: 3273; Percent complete: 81.8%; Average loss: 2.7981
Iteration: 3274; Percent complete: 81.8%; Average loss: 2.8190
Iteration: 3275; Percent complete: 81.9%; Average loss: 3.0272
Iteration: 3276; Percent complete: 81.9%; Average loss: 2.9034
Iteration: 3277; Percent complete: 81.9%; Average loss: 2.6756
Iteration: 3278; Percent complete: 82.0%; Average loss: 2.9176
Iteration: 3279; Percent complete: 82.0%; Average loss: 2.6339
Iteration: 3280; Percent complete: 82.0%; Average loss: 2.7995
Iteration: 3281; Percent complete: 82.0%; Average loss: 3.2274
Iteration: 3282; Percent complete: 82.0%; Average loss: 2.6910
Iteration: 3283; Percent complete: 82.1%; Average loss: 2.9487
Iteration: 3284; Percent complete: 82.1%; Average loss: 2.8561
Iteration: 3285; Percent complete: 82.1%; Average loss: 2.6100
Iteration: 3286; Percent complete: 82.2%; Average loss: 2.6488
Iteration: 3287; Percent complete: 82.2%; Average loss: 2.8579
Iteration: 3288; Percent complete: 82.2%; Average loss: 2.8616
Iteration: 3289; Percent complete: 82.2%; Average loss: 2.8006
Iteration: 3290; Percent complete: 82.2%; Average loss: 2.6943
Iteration: 3291; Percent complete: 82.3%; Average loss: 2.9940
Iteration: 3292; Percent complete: 82.3%; Average loss: 2.6803
Iteration: 3293; Percent complete: 82.3%; Average loss: 2.7050
Iteration: 3294; Percent complete: 82.3%; Average loss: 2.9314
Iteration: 3295; Percent complete: 82.4%; Average loss: 2.8498
Iteration: 3296; Percent complete: 82.4%; Average loss: 2.8859
Iteration: 3297; Percent complete: 82.4%; Average loss: 2.7610
Iteration: 3298; Percent complete: 82.5%; Average loss: 2.6265
Iteration: 3299; Percent complete: 82.5%; Average loss: 2.7696
Iteration: 3300; Percent complete: 82.5%; Average loss: 3.2196
Iteration: 3301; Percent complete: 82.5%; Average loss: 2.8118
Iteration: 3302; Percent complete: 82.5%; Average loss: 2.9336
Iteration: 3303; Percent complete: 82.6%; Average loss: 2.8272
Iteration: 3304; Percent complete: 82.6%; Average loss: 2.6899
Iteration: 3305; Percent complete: 82.6%; Average loss: 2.8440
Iteration: 3306; Percent complete: 82.7%; Average loss: 2.7227
Iteration: 3307; Percent complete: 82.7%; Average loss: 2.7046
Iteration: 3308; Percent complete: 82.7%; Average loss: 2.7418
Iteration: 3309; Percent complete: 82.7%; Average loss: 2.6276
Iteration: 3310; Percent complete: 82.8%; Average loss: 2.7469
Iteration: 3311; Percent complete: 82.8%; Average loss: 2.7682
Iteration: 3312; Percent complete: 82.8%; Average loss: 2.6792
Iteration: 3313; Percent complete: 82.8%; Average loss: 2.9757
Iteration: 3314; Percent complete: 82.8%; Average loss: 2.9162
Iteration: 3315; Percent complete: 82.9%; Average loss: 2.6712
Iteration: 3316; Percent complete: 82.9%; Average loss: 2.6277
Iteration: 3317; Percent complete: 82.9%; Average loss: 2.8288
Iteration: 3318; Percent complete: 83.0%; Average loss: 2.7150
Iteration: 3319; Percent complete: 83.0%; Average loss: 3.0587
Iteration: 3320; Percent complete: 83.0%; Average loss: 2.6348
Iteration: 3321; Percent complete: 83.0%; Average loss: 2.7219
Iteration: 3322; Percent complete: 83.0%; Average loss: 2.9103
Iteration: 3323; Percent complete: 83.1%; Average loss: 3.0035
Iteration: 3324; Percent complete: 83.1%; Average loss: 2.6151
Iteration: 3325; Percent complete: 83.1%; Average loss: 2.5220
Iteration: 3326; Percent complete: 83.2%; Average loss: 2.9258
Iteration: 3327; Percent complete: 83.2%; Average loss: 2.8767
Iteration: 3328; Percent complete: 83.2%; Average loss: 2.6678
Iteration: 3329; Percent complete: 83.2%; Average loss: 2.8069
Iteration: 3330; Percent complete: 83.2%; Average loss: 2.7843
Iteration: 3331; Percent complete: 83.3%; Average loss: 2.7319
Iteration: 3332; Percent complete: 83.3%; Average loss: 2.7884
Iteration: 3333; Percent complete: 83.3%; Average loss: 2.6480
Iteration: 3334; Percent complete: 83.4%; Average loss: 2.7763
Iteration: 3335; Percent complete: 83.4%; Average loss: 2.6084
Iteration: 3336; Percent complete: 83.4%; Average loss: 2.9221
Iteration: 3337; Percent complete: 83.4%; Average loss: 2.7943
Iteration: 3338; Percent complete: 83.5%; Average loss: 2.7513
Iteration: 3339; Percent complete: 83.5%; Average loss: 2.6431
Iteration: 3340; Percent complete: 83.5%; Average loss: 2.7757
Iteration: 3341; Percent complete: 83.5%; Average loss: 2.8108
Iteration: 3342; Percent complete: 83.5%; Average loss: 3.0855
Iteration: 3343; Percent complete: 83.6%; Average loss: 2.9044
Iteration: 3344; Percent complete: 83.6%; Average loss: 2.7227
Iteration: 3345; Percent complete: 83.6%; Average loss: 2.7012
Iteration: 3346; Percent complete: 83.7%; Average loss: 2.6921
Iteration: 3347; Percent complete: 83.7%; Average loss: 2.8282
Iteration: 3348; Percent complete: 83.7%; Average loss: 2.5628
Iteration: 3349; Percent complete: 83.7%; Average loss: 2.8650
Iteration: 3350; Percent complete: 83.8%; Average loss: 2.7695
Iteration: 3351; Percent complete: 83.8%; Average loss: 2.8151
Iteration: 3352; Percent complete: 83.8%; Average loss: 2.8631
Iteration: 3353; Percent complete: 83.8%; Average loss: 2.8717
Iteration: 3354; Percent complete: 83.9%; Average loss: 2.8629
Iteration: 3355; Percent complete: 83.9%; Average loss: 2.7143
Iteration: 3356; Percent complete: 83.9%; Average loss: 2.6674
Iteration: 3357; Percent complete: 83.9%; Average loss: 2.9290
Iteration: 3358; Percent complete: 84.0%; Average loss: 2.9983
Iteration: 3359; Percent complete: 84.0%; Average loss: 2.7086
Iteration: 3360; Percent complete: 84.0%; Average loss: 2.8802
Iteration: 3361; Percent complete: 84.0%; Average loss: 2.7287
Iteration: 3362; Percent complete: 84.0%; Average loss: 2.7770
Iteration: 3363; Percent complete: 84.1%; Average loss: 3.0120
Iteration: 3364; Percent complete: 84.1%; Average loss: 2.7623
Iteration: 3365; Percent complete: 84.1%; Average loss: 2.8580
Iteration: 3366; Percent complete: 84.2%; Average loss: 2.6437
Iteration: 3367; Percent complete: 84.2%; Average loss: 2.9783
Iteration: 3368; Percent complete: 84.2%; Average loss: 2.8190
Iteration: 3369; Percent complete: 84.2%; Average loss: 2.8824
Iteration: 3370; Percent complete: 84.2%; Average loss: 2.7075
Iteration: 3371; Percent complete: 84.3%; Average loss: 2.8653
Iteration: 3372; Percent complete: 84.3%; Average loss: 2.8030
Iteration: 3373; Percent complete: 84.3%; Average loss: 2.7981
Iteration: 3374; Percent complete: 84.4%; Average loss: 2.7518
Iteration: 3375; Percent complete: 84.4%; Average loss: 2.8212
Iteration: 3376; Percent complete: 84.4%; Average loss: 2.6036
Iteration: 3377; Percent complete: 84.4%; Average loss: 2.7546
Iteration: 3378; Percent complete: 84.5%; Average loss: 2.9916
Iteration: 3379; Percent complete: 84.5%; Average loss: 2.5967
Iteration: 3380; Percent complete: 84.5%; Average loss: 2.9150
Iteration: 3381; Percent complete: 84.5%; Average loss: 2.7155
Iteration: 3382; Percent complete: 84.5%; Average loss: 2.9074
Iteration: 3383; Percent complete: 84.6%; Average loss: 2.8523
Iteration: 3384; Percent complete: 84.6%; Average loss: 2.9973
Iteration: 3385; Percent complete: 84.6%; Average loss: 2.6552
Iteration: 3386; Percent complete: 84.7%; Average loss: 2.7179
Iteration: 3387; Percent complete: 84.7%; Average loss: 2.5512
Iteration: 3388; Percent complete: 84.7%; Average loss: 2.7620
Iteration: 3389; Percent complete: 84.7%; Average loss: 2.8854
Iteration: 3390; Percent complete: 84.8%; Average loss: 2.9341
Iteration: 3391; Percent complete: 84.8%; Average loss: 2.8030
Iteration: 3392; Percent complete: 84.8%; Average loss: 2.6980
Iteration: 3393; Percent complete: 84.8%; Average loss: 2.6745
Iteration: 3394; Percent complete: 84.9%; Average loss: 2.6841
Iteration: 3395; Percent complete: 84.9%; Average loss: 2.7483
Iteration: 3396; Percent complete: 84.9%; Average loss: 2.9312
Iteration: 3397; Percent complete: 84.9%; Average loss: 2.7015
Iteration: 3398; Percent complete: 85.0%; Average loss: 2.6685
Iteration: 3399; Percent complete: 85.0%; Average loss: 2.7885
Iteration: 3400; Percent complete: 85.0%; Average loss: 2.9412
Iteration: 3401; Percent complete: 85.0%; Average loss: 2.7664
Iteration: 3402; Percent complete: 85.0%; Average loss: 2.6818
Iteration: 3403; Percent complete: 85.1%; Average loss: 2.6757
Iteration: 3404; Percent complete: 85.1%; Average loss: 2.9216
Iteration: 3405; Percent complete: 85.1%; Average loss: 2.6895
Iteration: 3406; Percent complete: 85.2%; Average loss: 2.6903
Iteration: 3407; Percent complete: 85.2%; Average loss: 2.6268
Iteration: 3408; Percent complete: 85.2%; Average loss: 3.0265
Iteration: 3409; Percent complete: 85.2%; Average loss: 2.9531
Iteration: 3410; Percent complete: 85.2%; Average loss: 2.8270
Iteration: 3411; Percent complete: 85.3%; Average loss: 2.6599
Iteration: 3412; Percent complete: 85.3%; Average loss: 2.5295
Iteration: 3413; Percent complete: 85.3%; Average loss: 2.9167
Iteration: 3414; Percent complete: 85.4%; Average loss: 2.5975
Iteration: 3415; Percent complete: 85.4%; Average loss: 2.8841
Iteration: 3416; Percent complete: 85.4%; Average loss: 2.7643
Iteration: 3417; Percent complete: 85.4%; Average loss: 2.6055
Iteration: 3418; Percent complete: 85.5%; Average loss: 2.8951
Iteration: 3419; Percent complete: 85.5%; Average loss: 2.5922
Iteration: 3420; Percent complete: 85.5%; Average loss: 2.8248
Iteration: 3421; Percent complete: 85.5%; Average loss: 2.7513
Iteration: 3422; Percent complete: 85.5%; Average loss: 2.8304
Iteration: 3423; Percent complete: 85.6%; Average loss: 2.8399
Iteration: 3424; Percent complete: 85.6%; Average loss: 2.5745
Iteration: 3425; Percent complete: 85.6%; Average loss: 2.7103
Iteration: 3426; Percent complete: 85.7%; Average loss: 2.8160
Iteration: 3427; Percent complete: 85.7%; Average loss: 2.7765
Iteration: 3428; Percent complete: 85.7%; Average loss: 2.7978
Iteration: 3429; Percent complete: 85.7%; Average loss: 2.8625
Iteration: 3430; Percent complete: 85.8%; Average loss: 2.9073
Iteration: 3431; Percent complete: 85.8%; Average loss: 2.9445
Iteration: 3432; Percent complete: 85.8%; Average loss: 2.7570
Iteration: 3433; Percent complete: 85.8%; Average loss: 2.8057
Iteration: 3434; Percent complete: 85.9%; Average loss: 2.7427
Iteration: 3435; Percent complete: 85.9%; Average loss: 2.7125
Iteration: 3436; Percent complete: 85.9%; Average loss: 2.6341
Iteration: 3437; Percent complete: 85.9%; Average loss: 2.9316
Iteration: 3438; Percent complete: 86.0%; Average loss: 2.9033
Iteration: 3439; Percent complete: 86.0%; Average loss: 3.0003
Iteration: 3440; Percent complete: 86.0%; Average loss: 2.6847
Iteration: 3441; Percent complete: 86.0%; Average loss: 2.6013
Iteration: 3442; Percent complete: 86.1%; Average loss: 2.8189
Iteration: 3443; Percent complete: 86.1%; Average loss: 2.6040
Iteration: 3444; Percent complete: 86.1%; Average loss: 2.7766
Iteration: 3445; Percent complete: 86.1%; Average loss: 2.9133
Iteration: 3446; Percent complete: 86.2%; Average loss: 2.7985
Iteration: 3447; Percent complete: 86.2%; Average loss: 2.8423
Iteration: 3448; Percent complete: 86.2%; Average loss: 2.9593
Iteration: 3449; Percent complete: 86.2%; Average loss: 2.6038
Iteration: 3450; Percent complete: 86.2%; Average loss: 2.8597
Iteration: 3451; Percent complete: 86.3%; Average loss: 2.6720
Iteration: 3452; Percent complete: 86.3%; Average loss: 2.7052
Iteration: 3453; Percent complete: 86.3%; Average loss: 2.7137
Iteration: 3454; Percent complete: 86.4%; Average loss: 2.7163
Iteration: 3455; Percent complete: 86.4%; Average loss: 3.0485
Iteration: 3456; Percent complete: 86.4%; Average loss: 2.7992
Iteration: 3457; Percent complete: 86.4%; Average loss: 2.8644
Iteration: 3458; Percent complete: 86.5%; Average loss: 2.9348
Iteration: 3459; Percent complete: 86.5%; Average loss: 2.9483
Iteration: 3460; Percent complete: 86.5%; Average loss: 2.7585
Iteration: 3461; Percent complete: 86.5%; Average loss: 2.8758
Iteration: 3462; Percent complete: 86.6%; Average loss: 2.7077
Iteration: 3463; Percent complete: 86.6%; Average loss: 2.7446
Iteration: 3464; Percent complete: 86.6%; Average loss: 2.8195
Iteration: 3465; Percent complete: 86.6%; Average loss: 2.7668
Iteration: 3466; Percent complete: 86.7%; Average loss: 2.7633
Iteration: 3467; Percent complete: 86.7%; Average loss: 2.8313
Iteration: 3468; Percent complete: 86.7%; Average loss: 2.6828
Iteration: 3469; Percent complete: 86.7%; Average loss: 2.6551
Iteration: 3470; Percent complete: 86.8%; Average loss: 2.5049
Iteration: 3471; Percent complete: 86.8%; Average loss: 2.9151
Iteration: 3472; Percent complete: 86.8%; Average loss: 2.9523
Iteration: 3473; Percent complete: 86.8%; Average loss: 2.9186
Iteration: 3474; Percent complete: 86.9%; Average loss: 2.6515
Iteration: 3475; Percent complete: 86.9%; Average loss: 2.8596
Iteration: 3476; Percent complete: 86.9%; Average loss: 2.9992
Iteration: 3477; Percent complete: 86.9%; Average loss: 2.7702
Iteration: 3478; Percent complete: 87.0%; Average loss: 2.5167
Iteration: 3479; Percent complete: 87.0%; Average loss: 2.7721
Iteration: 3480; Percent complete: 87.0%; Average loss: 2.6652
Iteration: 3481; Percent complete: 87.0%; Average loss: 3.0257
Iteration: 3482; Percent complete: 87.1%; Average loss: 2.5915
Iteration: 3483; Percent complete: 87.1%; Average loss: 2.7290
Iteration: 3484; Percent complete: 87.1%; Average loss: 2.6603
Iteration: 3485; Percent complete: 87.1%; Average loss: 2.5286
Iteration: 3486; Percent complete: 87.2%; Average loss: 2.6516
Iteration: 3487; Percent complete: 87.2%; Average loss: 2.8865
Iteration: 3488; Percent complete: 87.2%; Average loss: 2.6548
Iteration: 3489; Percent complete: 87.2%; Average loss: 2.8739
Iteration: 3490; Percent complete: 87.2%; Average loss: 2.7767
Iteration: 3491; Percent complete: 87.3%; Average loss: 3.0980
Iteration: 3492; Percent complete: 87.3%; Average loss: 2.8703
Iteration: 3493; Percent complete: 87.3%; Average loss: 2.7038
Iteration: 3494; Percent complete: 87.4%; Average loss: 2.6305
Iteration: 3495; Percent complete: 87.4%; Average loss: 2.7397
Iteration: 3496; Percent complete: 87.4%; Average loss: 2.9765
Iteration: 3497; Percent complete: 87.4%; Average loss: 2.7751
Iteration: 3498; Percent complete: 87.5%; Average loss: 2.6858
Iteration: 3499; Percent complete: 87.5%; Average loss: 2.7990
Iteration: 3500; Percent complete: 87.5%; Average loss: 2.5910
Iteration: 3501; Percent complete: 87.5%; Average loss: 2.7707
Iteration: 3502; Percent complete: 87.5%; Average loss: 2.7614
Iteration: 3503; Percent complete: 87.6%; Average loss: 2.9014
Iteration: 3504; Percent complete: 87.6%; Average loss: 2.9102
Iteration: 3505; Percent complete: 87.6%; Average loss: 2.5373
Iteration: 3506; Percent complete: 87.6%; Average loss: 2.8169
Iteration: 3507; Percent complete: 87.7%; Average loss: 2.8584
Iteration: 3508; Percent complete: 87.7%; Average loss: 2.8909
Iteration: 3509; Percent complete: 87.7%; Average loss: 2.5841
Iteration: 3510; Percent complete: 87.8%; Average loss: 2.6439
Iteration: 3511; Percent complete: 87.8%; Average loss: 2.5069
Iteration: 3512; Percent complete: 87.8%; Average loss: 2.9684
Iteration: 3513; Percent complete: 87.8%; Average loss: 2.7880
Iteration: 3514; Percent complete: 87.8%; Average loss: 3.0035
Iteration: 3515; Percent complete: 87.9%; Average loss: 2.5418
Iteration: 3516; Percent complete: 87.9%; Average loss: 2.7156
Iteration: 3517; Percent complete: 87.9%; Average loss: 2.7995
Iteration: 3518; Percent complete: 87.9%; Average loss: 2.7236
Iteration: 3519; Percent complete: 88.0%; Average loss: 2.8630
Iteration: 3520; Percent complete: 88.0%; Average loss: 2.9154
Iteration: 3521; Percent complete: 88.0%; Average loss: 2.6003
Iteration: 3522; Percent complete: 88.0%; Average loss: 2.7724
Iteration: 3523; Percent complete: 88.1%; Average loss: 2.6625
Iteration: 3524; Percent complete: 88.1%; Average loss: 2.8004
Iteration: 3525; Percent complete: 88.1%; Average loss: 2.8002
Iteration: 3526; Percent complete: 88.1%; Average loss: 2.4126
Iteration: 3527; Percent complete: 88.2%; Average loss: 2.6690
Iteration: 3528; Percent complete: 88.2%; Average loss: 2.8626
Iteration: 3529; Percent complete: 88.2%; Average loss: 2.6896
Iteration: 3530; Percent complete: 88.2%; Average loss: 2.6770
Iteration: 3531; Percent complete: 88.3%; Average loss: 2.9063
Iteration: 3532; Percent complete: 88.3%; Average loss: 2.6001
Iteration: 3533; Percent complete: 88.3%; Average loss: 2.8803
Iteration: 3534; Percent complete: 88.3%; Average loss: 2.7761
Iteration: 3535; Percent complete: 88.4%; Average loss: 2.5933
Iteration: 3536; Percent complete: 88.4%; Average loss: 2.5006
Iteration: 3537; Percent complete: 88.4%; Average loss: 2.8938
Iteration: 3538; Percent complete: 88.4%; Average loss: 2.8236
Iteration: 3539; Percent complete: 88.5%; Average loss: 2.7206
Iteration: 3540; Percent complete: 88.5%; Average loss: 3.2298
Iteration: 3541; Percent complete: 88.5%; Average loss: 2.5814
Iteration: 3542; Percent complete: 88.5%; Average loss: 2.7391
Iteration: 3543; Percent complete: 88.6%; Average loss: 2.6921
Iteration: 3544; Percent complete: 88.6%; Average loss: 2.8725
Iteration: 3545; Percent complete: 88.6%; Average loss: 2.9024
Iteration: 3546; Percent complete: 88.6%; Average loss: 2.9363
Iteration: 3547; Percent complete: 88.7%; Average loss: 2.6828
Iteration: 3548; Percent complete: 88.7%; Average loss: 2.8667
Iteration: 3549; Percent complete: 88.7%; Average loss: 2.6940
Iteration: 3550; Percent complete: 88.8%; Average loss: 2.6199
Iteration: 3551; Percent complete: 88.8%; Average loss: 2.7815
Iteration: 3552; Percent complete: 88.8%; Average loss: 2.7126
Iteration: 3553; Percent complete: 88.8%; Average loss: 2.7794
Iteration: 3554; Percent complete: 88.8%; Average loss: 2.7452
Iteration: 3555; Percent complete: 88.9%; Average loss: 2.9900
Iteration: 3556; Percent complete: 88.9%; Average loss: 2.8069
Iteration: 3557; Percent complete: 88.9%; Average loss: 2.7556
Iteration: 3558; Percent complete: 88.9%; Average loss: 2.8904
Iteration: 3559; Percent complete: 89.0%; Average loss: 2.7132
Iteration: 3560; Percent complete: 89.0%; Average loss: 2.8419
Iteration: 3561; Percent complete: 89.0%; Average loss: 2.7097
Iteration: 3562; Percent complete: 89.0%; Average loss: 2.8591
Iteration: 3563; Percent complete: 89.1%; Average loss: 2.8414
Iteration: 3564; Percent complete: 89.1%; Average loss: 2.7875
Iteration: 3565; Percent complete: 89.1%; Average loss: 2.4673
Iteration: 3566; Percent complete: 89.1%; Average loss: 2.7571
Iteration: 3567; Percent complete: 89.2%; Average loss: 2.5056
Iteration: 3568; Percent complete: 89.2%; Average loss: 2.5939
Iteration: 3569; Percent complete: 89.2%; Average loss: 2.8868
Iteration: 3570; Percent complete: 89.2%; Average loss: 2.6722
Iteration: 3571; Percent complete: 89.3%; Average loss: 2.8191
Iteration: 3572; Percent complete: 89.3%; Average loss: 2.8139
Iteration: 3573; Percent complete: 89.3%; Average loss: 2.6507
Iteration: 3574; Percent complete: 89.3%; Average loss: 2.6425
Iteration: 3575; Percent complete: 89.4%; Average loss: 2.8636
Iteration: 3576; Percent complete: 89.4%; Average loss: 2.6928
Iteration: 3577; Percent complete: 89.4%; Average loss: 2.6575
Iteration: 3578; Percent complete: 89.5%; Average loss: 2.5855
Iteration: 3579; Percent complete: 89.5%; Average loss: 2.5886
Iteration: 3580; Percent complete: 89.5%; Average loss: 2.4207
Iteration: 3581; Percent complete: 89.5%; Average loss: 2.9810
Iteration: 3582; Percent complete: 89.5%; Average loss: 2.6380
Iteration: 3583; Percent complete: 89.6%; Average loss: 2.7822
Iteration: 3584; Percent complete: 89.6%; Average loss: 2.8390
Iteration: 3585; Percent complete: 89.6%; Average loss: 2.8859
Iteration: 3586; Percent complete: 89.6%; Average loss: 2.7236
Iteration: 3587; Percent complete: 89.7%; Average loss: 2.7999
Iteration: 3588; Percent complete: 89.7%; Average loss: 2.8471
Iteration: 3589; Percent complete: 89.7%; Average loss: 2.7131
Iteration: 3590; Percent complete: 89.8%; Average loss: 2.8400
Iteration: 3591; Percent complete: 89.8%; Average loss: 3.0634
Iteration: 3592; Percent complete: 89.8%; Average loss: 2.5746
Iteration: 3593; Percent complete: 89.8%; Average loss: 2.6852
Iteration: 3594; Percent complete: 89.8%; Average loss: 2.6004
Iteration: 3595; Percent complete: 89.9%; Average loss: 2.6251
Iteration: 3596; Percent complete: 89.9%; Average loss: 2.8565
Iteration: 3597; Percent complete: 89.9%; Average loss: 2.7573
Iteration: 3598; Percent complete: 90.0%; Average loss: 2.7992
Iteration: 3599; Percent complete: 90.0%; Average loss: 2.7738
Iteration: 3600; Percent complete: 90.0%; Average loss: 2.6999
Iteration: 3601; Percent complete: 90.0%; Average loss: 2.6094
Iteration: 3602; Percent complete: 90.0%; Average loss: 2.8199
Iteration: 3603; Percent complete: 90.1%; Average loss: 2.5907
Iteration: 3604; Percent complete: 90.1%; Average loss: 2.9664
Iteration: 3605; Percent complete: 90.1%; Average loss: 2.7373
Iteration: 3606; Percent complete: 90.1%; Average loss: 2.5853
Iteration: 3607; Percent complete: 90.2%; Average loss: 2.7886
Iteration: 3608; Percent complete: 90.2%; Average loss: 2.5139
Iteration: 3609; Percent complete: 90.2%; Average loss: 2.5933
Iteration: 3610; Percent complete: 90.2%; Average loss: 2.5234
Iteration: 3611; Percent complete: 90.3%; Average loss: 2.9331
Iteration: 3612; Percent complete: 90.3%; Average loss: 2.5171
Iteration: 3613; Percent complete: 90.3%; Average loss: 2.6553
Iteration: 3614; Percent complete: 90.3%; Average loss: 2.6586
Iteration: 3615; Percent complete: 90.4%; Average loss: 2.6920
Iteration: 3616; Percent complete: 90.4%; Average loss: 2.7711
Iteration: 3617; Percent complete: 90.4%; Average loss: 2.6194
Iteration: 3618; Percent complete: 90.5%; Average loss: 2.7595
Iteration: 3619; Percent complete: 90.5%; Average loss: 2.4973
Iteration: 3620; Percent complete: 90.5%; Average loss: 2.5918
Iteration: 3621; Percent complete: 90.5%; Average loss: 2.7854
Iteration: 3622; Percent complete: 90.5%; Average loss: 2.6777
Iteration: 3623; Percent complete: 90.6%; Average loss: 2.8420
Iteration: 3624; Percent complete: 90.6%; Average loss: 2.4298
Iteration: 3625; Percent complete: 90.6%; Average loss: 2.5684
Iteration: 3626; Percent complete: 90.6%; Average loss: 2.7109
Iteration: 3627; Percent complete: 90.7%; Average loss: 2.8481
Iteration: 3628; Percent complete: 90.7%; Average loss: 2.7559
Iteration: 3629; Percent complete: 90.7%; Average loss: 2.6707
Iteration: 3630; Percent complete: 90.8%; Average loss: 2.9809
Iteration: 3631; Percent complete: 90.8%; Average loss: 2.8235
Iteration: 3632; Percent complete: 90.8%; Average loss: 2.6473
Iteration: 3633; Percent complete: 90.8%; Average loss: 2.7281
Iteration: 3634; Percent complete: 90.8%; Average loss: 2.7877
Iteration: 3635; Percent complete: 90.9%; Average loss: 2.6518
Iteration: 3636; Percent complete: 90.9%; Average loss: 2.6782
Iteration: 3637; Percent complete: 90.9%; Average loss: 2.6915
Iteration: 3638; Percent complete: 91.0%; Average loss: 2.8316
Iteration: 3639; Percent complete: 91.0%; Average loss: 2.6803
Iteration: 3640; Percent complete: 91.0%; Average loss: 2.5479
Iteration: 3641; Percent complete: 91.0%; Average loss: 2.8644
Iteration: 3642; Percent complete: 91.0%; Average loss: 2.8356
Iteration: 3643; Percent complete: 91.1%; Average loss: 2.9562
Iteration: 3644; Percent complete: 91.1%; Average loss: 2.7831
Iteration: 3645; Percent complete: 91.1%; Average loss: 2.6244
Iteration: 3646; Percent complete: 91.1%; Average loss: 2.7569
Iteration: 3647; Percent complete: 91.2%; Average loss: 2.7773
Iteration: 3648; Percent complete: 91.2%; Average loss: 2.6389
Iteration: 3649; Percent complete: 91.2%; Average loss: 2.7589
Iteration: 3650; Percent complete: 91.2%; Average loss: 2.6253
Iteration: 3651; Percent complete: 91.3%; Average loss: 2.7744
Iteration: 3652; Percent complete: 91.3%; Average loss: 2.7339
Iteration: 3653; Percent complete: 91.3%; Average loss: 2.7126
Iteration: 3654; Percent complete: 91.3%; Average loss: 2.7389
Iteration: 3655; Percent complete: 91.4%; Average loss: 2.6766
Iteration: 3656; Percent complete: 91.4%; Average loss: 2.8717
Iteration: 3657; Percent complete: 91.4%; Average loss: 2.6033
Iteration: 3658; Percent complete: 91.5%; Average loss: 2.7104
Iteration: 3659; Percent complete: 91.5%; Average loss: 2.6176
Iteration: 3660; Percent complete: 91.5%; Average loss: 2.8395
Iteration: 3661; Percent complete: 91.5%; Average loss: 2.9230
Iteration: 3662; Percent complete: 91.5%; Average loss: 2.5923
Iteration: 3663; Percent complete: 91.6%; Average loss: 2.6104
Iteration: 3664; Percent complete: 91.6%; Average loss: 2.7921
Iteration: 3665; Percent complete: 91.6%; Average loss: 2.7637
Iteration: 3666; Percent complete: 91.6%; Average loss: 2.5738
Iteration: 3667; Percent complete: 91.7%; Average loss: 2.6855
Iteration: 3668; Percent complete: 91.7%; Average loss: 2.5085
Iteration: 3669; Percent complete: 91.7%; Average loss: 2.7272
Iteration: 3670; Percent complete: 91.8%; Average loss: 2.5654
Iteration: 3671; Percent complete: 91.8%; Average loss: 2.5880
Iteration: 3672; Percent complete: 91.8%; Average loss: 2.7681
Iteration: 3673; Percent complete: 91.8%; Average loss: 2.6279
Iteration: 3674; Percent complete: 91.8%; Average loss: 2.6873
Iteration: 3675; Percent complete: 91.9%; Average loss: 2.7705
Iteration: 3676; Percent complete: 91.9%; Average loss: 2.9097
Iteration: 3677; Percent complete: 91.9%; Average loss: 2.8153
Iteration: 3678; Percent complete: 92.0%; Average loss: 2.7834
Iteration: 3679; Percent complete: 92.0%; Average loss: 2.7523
Iteration: 3680; Percent complete: 92.0%; Average loss: 2.8429
Iteration: 3681; Percent complete: 92.0%; Average loss: 2.5751
Iteration: 3682; Percent complete: 92.0%; Average loss: 2.6875
Iteration: 3683; Percent complete: 92.1%; Average loss: 2.6956
Iteration: 3684; Percent complete: 92.1%; Average loss: 2.7630
Iteration: 3685; Percent complete: 92.1%; Average loss: 2.8014
Iteration: 3686; Percent complete: 92.2%; Average loss: 2.8136
Iteration: 3687; Percent complete: 92.2%; Average loss: 2.5969
Iteration: 3688; Percent complete: 92.2%; Average loss: 2.5821
Iteration: 3689; Percent complete: 92.2%; Average loss: 2.9021
Iteration: 3690; Percent complete: 92.2%; Average loss: 2.5676
Iteration: 3691; Percent complete: 92.3%; Average loss: 2.6598
Iteration: 3692; Percent complete: 92.3%; Average loss: 2.6707
Iteration: 3693; Percent complete: 92.3%; Average loss: 2.8077
Iteration: 3694; Percent complete: 92.3%; Average loss: 2.7701
Iteration: 3695; Percent complete: 92.4%; Average loss: 2.4167
Iteration: 3696; Percent complete: 92.4%; Average loss: 2.6559
Iteration: 3697; Percent complete: 92.4%; Average loss: 2.5871
Iteration: 3698; Percent complete: 92.5%; Average loss: 2.7281
Iteration: 3699; Percent complete: 92.5%; Average loss: 2.9555
Iteration: 3700; Percent complete: 92.5%; Average loss: 2.7911
Iteration: 3701; Percent complete: 92.5%; Average loss: 2.9315
Iteration: 3702; Percent complete: 92.5%; Average loss: 2.6680
Iteration: 3703; Percent complete: 92.6%; Average loss: 2.4977
Iteration: 3704; Percent complete: 92.6%; Average loss: 2.8827
Iteration: 3705; Percent complete: 92.6%; Average loss: 2.7108
Iteration: 3706; Percent complete: 92.7%; Average loss: 2.6262
Iteration: 3707; Percent complete: 92.7%; Average loss: 2.6591
Iteration: 3708; Percent complete: 92.7%; Average loss: 2.7692
Iteration: 3709; Percent complete: 92.7%; Average loss: 2.7863
Iteration: 3710; Percent complete: 92.8%; Average loss: 2.7351
Iteration: 3711; Percent complete: 92.8%; Average loss: 2.6131
Iteration: 3712; Percent complete: 92.8%; Average loss: 2.8384
Iteration: 3713; Percent complete: 92.8%; Average loss: 2.6141
Iteration: 3714; Percent complete: 92.8%; Average loss: 2.5932
Iteration: 3715; Percent complete: 92.9%; Average loss: 2.7986
Iteration: 3716; Percent complete: 92.9%; Average loss: 2.8372
Iteration: 3717; Percent complete: 92.9%; Average loss: 2.7300
Iteration: 3718; Percent complete: 93.0%; Average loss: 2.7799
Iteration: 3719; Percent complete: 93.0%; Average loss: 2.8090
Iteration: 3720; Percent complete: 93.0%; Average loss: 2.7249
Iteration: 3721; Percent complete: 93.0%; Average loss: 2.7016
Iteration: 3722; Percent complete: 93.0%; Average loss: 2.9068
Iteration: 3723; Percent complete: 93.1%; Average loss: 2.7449
Iteration: 3724; Percent complete: 93.1%; Average loss: 2.6392
Iteration: 3725; Percent complete: 93.1%; Average loss: 3.0301
Iteration: 3726; Percent complete: 93.2%; Average loss: 2.8979
Iteration: 3727; Percent complete: 93.2%; Average loss: 2.7373
Iteration: 3728; Percent complete: 93.2%; Average loss: 2.7689
Iteration: 3729; Percent complete: 93.2%; Average loss: 2.8806
Iteration: 3730; Percent complete: 93.2%; Average loss: 2.5630
Iteration: 3731; Percent complete: 93.3%; Average loss: 2.7795
Iteration: 3732; Percent complete: 93.3%; Average loss: 2.6553
Iteration: 3733; Percent complete: 93.3%; Average loss: 2.5881
Iteration: 3734; Percent complete: 93.3%; Average loss: 2.7051
Iteration: 3735; Percent complete: 93.4%; Average loss: 2.8790
Iteration: 3736; Percent complete: 93.4%; Average loss: 2.5799
Iteration: 3737; Percent complete: 93.4%; Average loss: 2.8610
Iteration: 3738; Percent complete: 93.5%; Average loss: 2.7371
Iteration: 3739; Percent complete: 93.5%; Average loss: 2.7429
Iteration: 3740; Percent complete: 93.5%; Average loss: 2.8262
Iteration: 3741; Percent complete: 93.5%; Average loss: 2.8750
Iteration: 3742; Percent complete: 93.5%; Average loss: 2.5357
Iteration: 3743; Percent complete: 93.6%; Average loss: 2.4983
Iteration: 3744; Percent complete: 93.6%; Average loss: 2.6732
Iteration: 3745; Percent complete: 93.6%; Average loss: 2.5847
Iteration: 3746; Percent complete: 93.7%; Average loss: 2.9117
Iteration: 3747; Percent complete: 93.7%; Average loss: 2.6986
Iteration: 3748; Percent complete: 93.7%; Average loss: 2.8896
Iteration: 3749; Percent complete: 93.7%; Average loss: 2.5478
Iteration: 3750; Percent complete: 93.8%; Average loss: 2.5501
Iteration: 3751; Percent complete: 93.8%; Average loss: 2.5924
Iteration: 3752; Percent complete: 93.8%; Average loss: 2.7611
Iteration: 3753; Percent complete: 93.8%; Average loss: 2.7419
Iteration: 3754; Percent complete: 93.8%; Average loss: 2.8828
Iteration: 3755; Percent complete: 93.9%; Average loss: 2.3884
Iteration: 3756; Percent complete: 93.9%; Average loss: 2.4053
Iteration: 3757; Percent complete: 93.9%; Average loss: 2.7292
Iteration: 3758; Percent complete: 94.0%; Average loss: 2.5851
Iteration: 3759; Percent complete: 94.0%; Average loss: 2.7526
Iteration: 3760; Percent complete: 94.0%; Average loss: 2.7944
Iteration: 3761; Percent complete: 94.0%; Average loss: 2.5844
Iteration: 3762; Percent complete: 94.0%; Average loss: 2.7686
Iteration: 3763; Percent complete: 94.1%; Average loss: 2.7147
Iteration: 3764; Percent complete: 94.1%; Average loss: 2.6028
Iteration: 3765; Percent complete: 94.1%; Average loss: 2.6676
Iteration: 3766; Percent complete: 94.2%; Average loss: 2.7481
Iteration: 3767; Percent complete: 94.2%; Average loss: 3.0132
Iteration: 3768; Percent complete: 94.2%; Average loss: 2.5559
Iteration: 3769; Percent complete: 94.2%; Average loss: 2.7745
Iteration: 3770; Percent complete: 94.2%; Average loss: 2.7262
Iteration: 3771; Percent complete: 94.3%; Average loss: 2.6541
Iteration: 3772; Percent complete: 94.3%; Average loss: 2.5496
Iteration: 3773; Percent complete: 94.3%; Average loss: 2.6537
Iteration: 3774; Percent complete: 94.3%; Average loss: 2.8206
Iteration: 3775; Percent complete: 94.4%; Average loss: 2.5584
Iteration: 3776; Percent complete: 94.4%; Average loss: 2.5980
Iteration: 3777; Percent complete: 94.4%; Average loss: 2.6431
Iteration: 3778; Percent complete: 94.5%; Average loss: 2.6727
Iteration: 3779; Percent complete: 94.5%; Average loss: 2.6639
Iteration: 3780; Percent complete: 94.5%; Average loss: 2.9645
Iteration: 3781; Percent complete: 94.5%; Average loss: 2.8561
Iteration: 3782; Percent complete: 94.5%; Average loss: 2.6237
Iteration: 3783; Percent complete: 94.6%; Average loss: 2.6537
Iteration: 3784; Percent complete: 94.6%; Average loss: 2.6057
Iteration: 3785; Percent complete: 94.6%; Average loss: 2.6020
Iteration: 3786; Percent complete: 94.7%; Average loss: 2.6293
Iteration: 3787; Percent complete: 94.7%; Average loss: 2.7345
Iteration: 3788; Percent complete: 94.7%; Average loss: 2.6071
Iteration: 3789; Percent complete: 94.7%; Average loss: 2.7944
Iteration: 3790; Percent complete: 94.8%; Average loss: 2.6202
Iteration: 3791; Percent complete: 94.8%; Average loss: 2.7309
Iteration: 3792; Percent complete: 94.8%; Average loss: 2.6042
Iteration: 3793; Percent complete: 94.8%; Average loss: 2.5266
Iteration: 3794; Percent complete: 94.8%; Average loss: 2.5282
Iteration: 3795; Percent complete: 94.9%; Average loss: 2.5836
Iteration: 3796; Percent complete: 94.9%; Average loss: 2.8776
Iteration: 3797; Percent complete: 94.9%; Average loss: 2.6501
Iteration: 3798; Percent complete: 95.0%; Average loss: 2.5928
Iteration: 3799; Percent complete: 95.0%; Average loss: 2.6681
Iteration: 3800; Percent complete: 95.0%; Average loss: 2.6215
Iteration: 3801; Percent complete: 95.0%; Average loss: 2.4664
Iteration: 3802; Percent complete: 95.0%; Average loss: 2.7957
Iteration: 3803; Percent complete: 95.1%; Average loss: 2.6901
Iteration: 3804; Percent complete: 95.1%; Average loss: 2.6127
Iteration: 3805; Percent complete: 95.1%; Average loss: 2.8821
Iteration: 3806; Percent complete: 95.2%; Average loss: 2.7026
Iteration: 3807; Percent complete: 95.2%; Average loss: 2.6924
Iteration: 3808; Percent complete: 95.2%; Average loss: 2.6871
Iteration: 3809; Percent complete: 95.2%; Average loss: 2.5928
Iteration: 3810; Percent complete: 95.2%; Average loss: 2.8269
Iteration: 3811; Percent complete: 95.3%; Average loss: 2.7050
Iteration: 3812; Percent complete: 95.3%; Average loss: 2.6487
Iteration: 3813; Percent complete: 95.3%; Average loss: 2.9744
Iteration: 3814; Percent complete: 95.3%; Average loss: 2.5931
Iteration: 3815; Percent complete: 95.4%; Average loss: 2.7340
Iteration: 3816; Percent complete: 95.4%; Average loss: 2.7822
Iteration: 3817; Percent complete: 95.4%; Average loss: 2.4366
Iteration: 3818; Percent complete: 95.5%; Average loss: 2.6381
Iteration: 3819; Percent complete: 95.5%; Average loss: 2.5947
Iteration: 3820; Percent complete: 95.5%; Average loss: 2.6732
Iteration: 3821; Percent complete: 95.5%; Average loss: 2.5724
Iteration: 3822; Percent complete: 95.5%; Average loss: 2.8268
Iteration: 3823; Percent complete: 95.6%; Average loss: 2.6489
Iteration: 3824; Percent complete: 95.6%; Average loss: 2.7899
Iteration: 3825; Percent complete: 95.6%; Average loss: 2.7259
Iteration: 3826; Percent complete: 95.7%; Average loss: 2.4198
Iteration: 3827; Percent complete: 95.7%; Average loss: 2.5990
Iteration: 3828; Percent complete: 95.7%; Average loss: 2.5483
Iteration: 3829; Percent complete: 95.7%; Average loss: 2.7135
Iteration: 3830; Percent complete: 95.8%; Average loss: 2.7487
Iteration: 3831; Percent complete: 95.8%; Average loss: 2.7893
Iteration: 3832; Percent complete: 95.8%; Average loss: 2.6489
Iteration: 3833; Percent complete: 95.8%; Average loss: 2.7226
Iteration: 3834; Percent complete: 95.9%; Average loss: 2.6959
Iteration: 3835; Percent complete: 95.9%; Average loss: 2.7021
Iteration: 3836; Percent complete: 95.9%; Average loss: 2.6007
Iteration: 3837; Percent complete: 95.9%; Average loss: 2.6987
Iteration: 3838; Percent complete: 96.0%; Average loss: 2.7438
Iteration: 3839; Percent complete: 96.0%; Average loss: 2.7632
Iteration: 3840; Percent complete: 96.0%; Average loss: 2.6520
Iteration: 3841; Percent complete: 96.0%; Average loss: 2.6271
Iteration: 3842; Percent complete: 96.0%; Average loss: 2.7914
Iteration: 3843; Percent complete: 96.1%; Average loss: 2.7371
Iteration: 3844; Percent complete: 96.1%; Average loss: 2.5694
Iteration: 3845; Percent complete: 96.1%; Average loss: 2.8943
Iteration: 3846; Percent complete: 96.2%; Average loss: 2.7332
Iteration: 3847; Percent complete: 96.2%; Average loss: 2.6931
Iteration: 3848; Percent complete: 96.2%; Average loss: 2.7756
Iteration: 3849; Percent complete: 96.2%; Average loss: 2.6274
Iteration: 3850; Percent complete: 96.2%; Average loss: 2.3679
Iteration: 3851; Percent complete: 96.3%; Average loss: 2.8340
Iteration: 3852; Percent complete: 96.3%; Average loss: 2.4179
Iteration: 3853; Percent complete: 96.3%; Average loss: 2.7367
Iteration: 3854; Percent complete: 96.4%; Average loss: 2.4997
Iteration: 3855; Percent complete: 96.4%; Average loss: 2.9255
Iteration: 3856; Percent complete: 96.4%; Average loss: 2.5578
Iteration: 3857; Percent complete: 96.4%; Average loss: 2.6465
Iteration: 3858; Percent complete: 96.5%; Average loss: 2.6916
Iteration: 3859; Percent complete: 96.5%; Average loss: 2.6338
Iteration: 3860; Percent complete: 96.5%; Average loss: 2.6908
Iteration: 3861; Percent complete: 96.5%; Average loss: 2.8436
Iteration: 3862; Percent complete: 96.5%; Average loss: 2.7095
Iteration: 3863; Percent complete: 96.6%; Average loss: 2.8340
Iteration: 3864; Percent complete: 96.6%; Average loss: 2.6732
Iteration: 3865; Percent complete: 96.6%; Average loss: 2.7746
Iteration: 3866; Percent complete: 96.7%; Average loss: 2.8526
Iteration: 3867; Percent complete: 96.7%; Average loss: 2.6440
Iteration: 3868; Percent complete: 96.7%; Average loss: 2.7529
Iteration: 3869; Percent complete: 96.7%; Average loss: 2.6131
Iteration: 3870; Percent complete: 96.8%; Average loss: 2.4479
Iteration: 3871; Percent complete: 96.8%; Average loss: 2.9407
Iteration: 3872; Percent complete: 96.8%; Average loss: 2.6677
Iteration: 3873; Percent complete: 96.8%; Average loss: 2.7215
Iteration: 3874; Percent complete: 96.9%; Average loss: 2.7661
Iteration: 3875; Percent complete: 96.9%; Average loss: 2.7203
Iteration: 3876; Percent complete: 96.9%; Average loss: 2.7388
Iteration: 3877; Percent complete: 96.9%; Average loss: 2.8014
Iteration: 3878; Percent complete: 97.0%; Average loss: 2.8451
Iteration: 3879; Percent complete: 97.0%; Average loss: 2.7045
Iteration: 3880; Percent complete: 97.0%; Average loss: 2.8848
Iteration: 3881; Percent complete: 97.0%; Average loss: 2.8625
Iteration: 3882; Percent complete: 97.0%; Average loss: 2.6648
Iteration: 3883; Percent complete: 97.1%; Average loss: 2.7342
Iteration: 3884; Percent complete: 97.1%; Average loss: 2.7482
Iteration: 3885; Percent complete: 97.1%; Average loss: 2.9607
Iteration: 3886; Percent complete: 97.2%; Average loss: 2.5727
Iteration: 3887; Percent complete: 97.2%; Average loss: 2.7757
Iteration: 3888; Percent complete: 97.2%; Average loss: 2.7350
Iteration: 3889; Percent complete: 97.2%; Average loss: 2.5607
Iteration: 3890; Percent complete: 97.2%; Average loss: 2.5259
Iteration: 3891; Percent complete: 97.3%; Average loss: 2.7460
Iteration: 3892; Percent complete: 97.3%; Average loss: 2.4769
Iteration: 3893; Percent complete: 97.3%; Average loss: 2.4874
Iteration: 3894; Percent complete: 97.4%; Average loss: 2.6558
Iteration: 3895; Percent complete: 97.4%; Average loss: 2.6113
Iteration: 3896; Percent complete: 97.4%; Average loss: 2.6713
Iteration: 3897; Percent complete: 97.4%; Average loss: 2.8078
Iteration: 3898; Percent complete: 97.5%; Average loss: 2.8446
Iteration: 3899; Percent complete: 97.5%; Average loss: 2.7151
Iteration: 3900; Percent complete: 97.5%; Average loss: 2.6672
Iteration: 3901; Percent complete: 97.5%; Average loss: 2.6658
Iteration: 3902; Percent complete: 97.5%; Average loss: 2.7657
Iteration: 3903; Percent complete: 97.6%; Average loss: 2.5139
Iteration: 3904; Percent complete: 97.6%; Average loss: 2.8538
Iteration: 3905; Percent complete: 97.6%; Average loss: 2.6762
Iteration: 3906; Percent complete: 97.7%; Average loss: 2.6700
Iteration: 3907; Percent complete: 97.7%; Average loss: 2.7888
Iteration: 3908; Percent complete: 97.7%; Average loss: 2.7187
Iteration: 3909; Percent complete: 97.7%; Average loss: 2.6759
Iteration: 3910; Percent complete: 97.8%; Average loss: 2.8420
Iteration: 3911; Percent complete: 97.8%; Average loss: 2.5767
Iteration: 3912; Percent complete: 97.8%; Average loss: 2.7510
Iteration: 3913; Percent complete: 97.8%; Average loss: 2.9155
Iteration: 3914; Percent complete: 97.9%; Average loss: 2.6964
Iteration: 3915; Percent complete: 97.9%; Average loss: 2.7244
Iteration: 3916; Percent complete: 97.9%; Average loss: 2.7762
Iteration: 3917; Percent complete: 97.9%; Average loss: 2.7486
Iteration: 3918; Percent complete: 98.0%; Average loss: 2.7164
Iteration: 3919; Percent complete: 98.0%; Average loss: 2.6179
Iteration: 3920; Percent complete: 98.0%; Average loss: 2.7760
Iteration: 3921; Percent complete: 98.0%; Average loss: 2.6410
Iteration: 3922; Percent complete: 98.0%; Average loss: 2.6274
Iteration: 3923; Percent complete: 98.1%; Average loss: 2.4793
Iteration: 3924; Percent complete: 98.1%; Average loss: 2.5193
Iteration: 3925; Percent complete: 98.1%; Average loss: 2.5504
Iteration: 3926; Percent complete: 98.2%; Average loss: 2.8548
Iteration: 3927; Percent complete: 98.2%; Average loss: 2.7355
Iteration: 3928; Percent complete: 98.2%; Average loss: 2.7249
Iteration: 3929; Percent complete: 98.2%; Average loss: 2.6959
Iteration: 3930; Percent complete: 98.2%; Average loss: 2.5790
Iteration: 3931; Percent complete: 98.3%; Average loss: 2.7315
Iteration: 3932; Percent complete: 98.3%; Average loss: 2.6424
Iteration: 3933; Percent complete: 98.3%; Average loss: 2.8444
Iteration: 3934; Percent complete: 98.4%; Average loss: 2.8400
Iteration: 3935; Percent complete: 98.4%; Average loss: 2.5783
Iteration: 3936; Percent complete: 98.4%; Average loss: 2.6215
Iteration: 3937; Percent complete: 98.4%; Average loss: 2.5532
Iteration: 3938; Percent complete: 98.5%; Average loss: 2.5501
Iteration: 3939; Percent complete: 98.5%; Average loss: 2.5539
Iteration: 3940; Percent complete: 98.5%; Average loss: 2.8100
Iteration: 3941; Percent complete: 98.5%; Average loss: 2.5861
Iteration: 3942; Percent complete: 98.6%; Average loss: 2.5282
Iteration: 3943; Percent complete: 98.6%; Average loss: 2.6136
Iteration: 3944; Percent complete: 98.6%; Average loss: 2.5912
Iteration: 3945; Percent complete: 98.6%; Average loss: 2.6076
Iteration: 3946; Percent complete: 98.7%; Average loss: 2.9012
Iteration: 3947; Percent complete: 98.7%; Average loss: 2.8429
Iteration: 3948; Percent complete: 98.7%; Average loss: 2.6958
Iteration: 3949; Percent complete: 98.7%; Average loss: 2.6944
Iteration: 3950; Percent complete: 98.8%; Average loss: 2.5562
Iteration: 3951; Percent complete: 98.8%; Average loss: 2.5630
Iteration: 3952; Percent complete: 98.8%; Average loss: 2.6162
Iteration: 3953; Percent complete: 98.8%; Average loss: 2.8443
Iteration: 3954; Percent complete: 98.9%; Average loss: 2.6101
Iteration: 3955; Percent complete: 98.9%; Average loss: 2.6947
Iteration: 3956; Percent complete: 98.9%; Average loss: 2.6750
Iteration: 3957; Percent complete: 98.9%; Average loss: 2.5357
Iteration: 3958; Percent complete: 99.0%; Average loss: 2.5495
Iteration: 3959; Percent complete: 99.0%; Average loss: 2.6968
Iteration: 3960; Percent complete: 99.0%; Average loss: 2.6801
Iteration: 3961; Percent complete: 99.0%; Average loss: 2.9289
Iteration: 3962; Percent complete: 99.1%; Average loss: 2.6188
Iteration: 3963; Percent complete: 99.1%; Average loss: 2.4507
Iteration: 3964; Percent complete: 99.1%; Average loss: 2.5651
Iteration: 3965; Percent complete: 99.1%; Average loss: 2.3198
Iteration: 3966; Percent complete: 99.2%; Average loss: 2.7673
Iteration: 3967; Percent complete: 99.2%; Average loss: 2.6521
Iteration: 3968; Percent complete: 99.2%; Average loss: 2.6129
Iteration: 3969; Percent complete: 99.2%; Average loss: 2.6019
Iteration: 3970; Percent complete: 99.2%; Average loss: 2.8263
Iteration: 3971; Percent complete: 99.3%; Average loss: 2.7993
Iteration: 3972; Percent complete: 99.3%; Average loss: 2.6240
Iteration: 3973; Percent complete: 99.3%; Average loss: 2.4418
Iteration: 3974; Percent complete: 99.4%; Average loss: 2.7453
Iteration: 3975; Percent complete: 99.4%; Average loss: 2.7695
Iteration: 3976; Percent complete: 99.4%; Average loss: 2.6219
Iteration: 3977; Percent complete: 99.4%; Average loss: 2.6266
Iteration: 3978; Percent complete: 99.5%; Average loss: 2.6234
Iteration: 3979; Percent complete: 99.5%; Average loss: 2.7708
Iteration: 3980; Percent complete: 99.5%; Average loss: 2.5785
Iteration: 3981; Percent complete: 99.5%; Average loss: 2.6775
Iteration: 3982; Percent complete: 99.6%; Average loss: 2.5176
Iteration: 3983; Percent complete: 99.6%; Average loss: 2.4804
Iteration: 3984; Percent complete: 99.6%; Average loss: 2.7015
Iteration: 3985; Percent complete: 99.6%; Average loss: 2.6028
Iteration: 3986; Percent complete: 99.7%; Average loss: 2.6169
Iteration: 3987; Percent complete: 99.7%; Average loss: 2.6685
Iteration: 3988; Percent complete: 99.7%; Average loss: 2.6734
Iteration: 3989; Percent complete: 99.7%; Average loss: 2.4207
Iteration: 3990; Percent complete: 99.8%; Average loss: 2.5213
Iteration: 3991; Percent complete: 99.8%; Average loss: 2.6607
Iteration: 3992; Percent complete: 99.8%; Average loss: 2.6188
Iteration: 3993; Percent complete: 99.8%; Average loss: 2.8319
Iteration: 3994; Percent complete: 99.9%; Average loss: 2.5817
Iteration: 3995; Percent complete: 99.9%; Average loss: 2.4979
Iteration: 3996; Percent complete: 99.9%; Average loss: 2.7317
Iteration: 3997; Percent complete: 99.9%; Average loss: 2.5969
Iteration: 3998; Percent complete: 100.0%; Average loss: 2.2275
Iteration: 3999; Percent complete: 100.0%; Average loss: 2.7124
Iteration: 4000; Percent complete: 100.0%; Average loss: 2.5975

</code></pre>
<h3 id="run-evaluation">Run Evaluation</h3>
<h3 id="运行评估">运行评估</h3>
<p>To chat with your model, run the following block. 运行以下部分来与你的模型聊天</p>
<pre><code class="language-py"># Set dropout layers to eval mode
encoder.eval()
decoder.eval()

# Initialize search module
searcher = GreedySearchDecoder(encoder, decoder)

# Begin chatting (uncomment and run the following line to begin)
# evaluateInput(encoder, decoder, searcher, voc)

</code></pre>
<h2 id="结论">结论</h2>
<p>伙计们，这就是这一切。 恭喜，您现在知道构建生成聊天机器人模型的基础知识！ 如果您有兴趣，可以尝试通过调整模型和训练参数以及自定义训练模型的数据来定制聊天机器人的行为。</p>
<p>查看其他教程，了解PyTorch中更酷的深度学习应用程序！</p>
</div>
<hr class="uk-article-divider">
<div class="uk-block uk-block-muted uk-padding-top-remove uk-padding-bottom-remove uk-margin-large-top  book-recommend-wrap">
<div class="uk-margin-top uk-margin-bottom uk-margin-left uk-margin-right">
<div class="uk-margin uk-text-muted "><i class="uk-icon-outdent uk-icon-justify uk-margin-small-right"></i>书籍推荐</div>
<div class="books">
<ul class="uk-book-list">
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/166/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/166/index.html">What the f*ck Python中文版</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/95.html">leisurelicht</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">70页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月26日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 7300个">7300</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/162/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/162/index.html">Python方向综合面试题</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/92.html">jackfrued</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">115页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月26日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 35个">35</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/21/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/21/index.html">笨办法学 Python 3</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/16.html">yammgao</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">63页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年5月3日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 12个">12</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/4/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/redux_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/4/index.html">Redux 中文文档</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/4.html">camsong</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="redux">redux</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">69页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年5月1日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 2786个">2786</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/198/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/storm_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/198/index.html">大数据入门指南</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/113.html">heibaiying</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="storm">storm</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">98页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2021年10月24日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 个"></span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/41/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/android_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/41/index.html">Android官方培訓課程中文版</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/22.html">jasonblog</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="android">android</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">294页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年5月29日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 1个">1</span>
</div>
</div>
</div>
</li>
<hr>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
<nav class="tm-navbar uk-navbar uk-navbar-attached reader-nav">
<div class="uk-float-left uk-margin-small-top">
<a href="javascript:;" title="目录菜单" class="show-menu  uk-icon-hover  uk-icon-align-justify uk-margin-right"></a>
<div data-uk-dropdown="{mode:'click',pos:'bottom-left'}" class="font-setting-wrap">
<a class="uk-icon-hover uk-icon-font uk-margin-right" aria-label="字体设置" href="javascript:;"></a>
<div class="uk-dropdown dropdown-menu">
<div class="dropdown-caret"><span class="caret-outer"></span><span class="caret-inner"></span></div>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-reduce">小字</button>
<button class="uk-button-link button size-2 font-enlarge">大字</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-1 ">宋体</button>
<button class="uk-button-link button size-2 font-2 ">黑体</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-3 color-theme-sun "><i class="uk-icon-sun-o"></i>白天</button>
<button class="uk-button-link button size-3 color-theme-eye "><i class="uk-icon-eye"></i>护眼</button>
<button class="uk-button-link button size-3 color-theme-moon "><i class="uk-icon-moon-o"></i>夜晚</button></div>
</div>
</div>
<a class="logo uk-margin-right" href="../../../../" title="返回首页"><img class="" src="../../../../static/components/images/icon_32.png" /></a>
</div>
<div class="uk-navbar-flip  uk-hidden-small">
<div id="share-box"></div>
</div>
</nav>
<div id="menu-id" class="uk-offcanvas reader-offcanvas">
<div class="uk-offcanvas-bar">
<ul class="book-menu-bar uk-nav uk-nav-offcanvas" data-uk-nav>
<li>
<a href="../../../../book/169/index.html" data-book-page-rel-url="index.html" data-book-page-id="0" title="封面">封面</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/readme.html" data-book-page-rel-url="readme.html" data-book-page-id="0" title="简介">简介</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_getting_started.html" title="起步" data-book-page-rel-url="docs/1.0/tut_getting_started.html" data-book-page-id="11555">起步</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/deep_learning_60min_blitz.html" title="PyTorch 深度学习: 60 分钟极速入门" data-book-page-rel-url="docs/1.0/deep_learning_60min_blitz.html" data-book-page-id="11556">PyTorch 深度学习: 60 分钟极速入门</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/blitz_tensor_tutorial.html" title="什么是 PyTorch？" data-book-page-rel-url="docs/1.0/blitz_tensor_tutorial.html" data-book-page-id="11557">什么是 PyTorch？</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/blitz_autograd_tutorial.html" title="Autograd：自动求导" data-book-page-rel-url="docs/1.0/blitz_autograd_tutorial.html" data-book-page-id="11558">Autograd：自动求导</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/blitz_neural_networks_tutorial.html" title="神经网络" data-book-page-rel-url="docs/1.0/blitz_neural_networks_tutorial.html" data-book-page-id="11559">神经网络</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/blitz_cifar10_tutorial.html" title="训练分类器" data-book-page-rel-url="docs/1.0/blitz_cifar10_tutorial.html" data-book-page-id="11560">训练分类器</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/blitz_data_parallel_tutorial.html" title="可选：数据并行处理" data-book-page-rel-url="docs/1.0/blitz_data_parallel_tutorial.html" data-book-page-id="11561">可选：数据并行处理</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/data_loading_tutorial.html" title="数据加载和处理教程" data-book-page-rel-url="docs/1.0/data_loading_tutorial.html" data-book-page-id="11562">数据加载和处理教程</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/pytorch_with_examples.html" title="用例子学习 PyTorch" data-book-page-rel-url="docs/1.0/pytorch_with_examples.html" data-book-page-id="11563">用例子学习 PyTorch</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/transfer_learning_tutorial.html" title="迁移学习教程" data-book-page-rel-url="docs/1.0/transfer_learning_tutorial.html" data-book-page-id="11564">迁移学习教程</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/deploy_seq2seq_hybrid_frontend_tutorial.html" title="混合前端的 seq2seq 模型部署" data-book-page-rel-url="docs/1.0/deploy_seq2seq_hybrid_frontend_tutorial.html" data-book-page-id="11565">混合前端的 seq2seq 模型部署</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/saving_loading_models.html" title="Saving and Loading Models" data-book-page-rel-url="docs/1.0/saving_loading_models.html" data-book-page-id="11566">Saving and Loading Models</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nn_tutorial.html" title="What is `torch.nn` _really_?" data-book-page-rel-url="docs/1.0/nn_tutorial.html" data-book-page-id="11567">What is `torch.nn` _really_?</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_image.html" title="图像" data-book-page-rel-url="docs/1.0/tut_image.html" data-book-page-id="11568">图像</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/finetuning_torchvision_models_tutorial.html" title="Torchvision 模型微调" data-book-page-rel-url="docs/1.0/finetuning_torchvision_models_tutorial.html" data-book-page-id="11569">Torchvision 模型微调</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/spatial_transformer_tutorial.html" title="空间变换器网络教程" data-book-page-rel-url="docs/1.0/spatial_transformer_tutorial.html" data-book-page-id="11570">空间变换器网络教程</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/neural_style_tutorial.html" title="使用 PyTorch 进行图像风格转换" data-book-page-rel-url="docs/1.0/neural_style_tutorial.html" data-book-page-id="11571">使用 PyTorch 进行图像风格转换</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/fgsm_tutorial.html" title="对抗性示例生成" data-book-page-rel-url="docs/1.0/fgsm_tutorial.html" data-book-page-id="11572">对抗性示例生成</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/super_resolution_with_caffe2.html" title="使用 ONNX 将模型从 PyTorch 传输到 Caffe2 和移动端" data-book-page-rel-url="docs/1.0/super_resolution_with_caffe2.html" data-book-page-id="11573">使用 ONNX 将模型从 PyTorch 传输到 Caffe2 和移动端</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_text.html" title="文本" data-book-page-rel-url="docs/1.0/tut_text.html" data-book-page-id="11574">文本</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/chatbot_tutorial.html" title="聊天机器人教程" data-book-page-rel-url="docs/1.0/chatbot_tutorial.html" data-book-page-id="11575">聊天机器人教程</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/char_rnn_generation_tutorial.html" title="使用字符级别特征的 RNN 网络生成姓氏" data-book-page-rel-url="docs/1.0/char_rnn_generation_tutorial.html" data-book-page-id="11576">使用字符级别特征的 RNN 网络生成姓氏</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/char_rnn_classification_tutorial.html" title="使用字符级别特征的 RNN 网络进行姓氏分类" data-book-page-rel-url="docs/1.0/char_rnn_classification_tutorial.html" data-book-page-id="11577">使用字符级别特征的 RNN 网络进行姓氏分类</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/deep_learning_nlp_tutorial.html" title="Deep Learning for NLP with Pytorch" data-book-page-rel-url="docs/1.0/deep_learning_nlp_tutorial.html" data-book-page-id="11578">Deep Learning for NLP with Pytorch</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nlp_pytorch_tutorial.html" title="PyTorch 介绍" data-book-page-rel-url="docs/1.0/nlp_pytorch_tutorial.html" data-book-page-id="11579">PyTorch 介绍</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nlp_deep_learning_tutorial.html" title="使用 PyTorch 进行深度学习" data-book-page-rel-url="docs/1.0/nlp_deep_learning_tutorial.html" data-book-page-id="11580">使用 PyTorch 进行深度学习</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nlp_word_embeddings_tutorial.html" title="Word Embeddings: Encoding Lexical Semantics" data-book-page-rel-url="docs/1.0/nlp_word_embeddings_tutorial.html" data-book-page-id="11581">Word Embeddings: Encoding Lexical Semantics</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nlp_sequence_models_tutorial.html" title="序列模型和 LSTM 网络" data-book-page-rel-url="docs/1.0/nlp_sequence_models_tutorial.html" data-book-page-id="11582">序列模型和 LSTM 网络</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nlp_advanced_tutorial.html" title="Advanced: Making Dynamic Decisions and the Bi-LSTM CRF" data-book-page-rel-url="docs/1.0/nlp_advanced_tutorial.html" data-book-page-id="11583">Advanced: Making Dynamic Decisions and the Bi-LSTM CRF</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/seq2seq_translation_tutorial.html" title="基于注意力机制的 seq2seq 神经网络翻译" data-book-page-rel-url="docs/1.0/seq2seq_translation_tutorial.html" data-book-page-id="11584">基于注意力机制的 seq2seq 神经网络翻译</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_generative.html" title="生成" data-book-page-rel-url="docs/1.0/tut_generative.html" data-book-page-id="11585">生成</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/dcgan_faces_tutorial.html" title="DCGAN Tutorial" data-book-page-rel-url="docs/1.0/dcgan_faces_tutorial.html" data-book-page-id="11586">DCGAN Tutorial</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_reinforcement_learning.html" title="强化学习" data-book-page-rel-url="docs/1.0/tut_reinforcement_learning.html" data-book-page-id="11587">强化学习</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/reinforcement_q_learning.html" title="Reinforcement Learning (DQN) Tutorial" data-book-page-rel-url="docs/1.0/reinforcement_q_learning.html" data-book-page-id="11588">Reinforcement Learning (DQN) Tutorial</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_extending_pytorch.html" title="扩展 PyTorch" data-book-page-rel-url="docs/1.0/tut_extending_pytorch.html" data-book-page-id="11589">扩展 PyTorch</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/numpy_extensions_tutorial.html" title="用 numpy 和 scipy 创建扩展" data-book-page-rel-url="docs/1.0/numpy_extensions_tutorial.html" data-book-page-id="11590">用 numpy 和 scipy 创建扩展</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/cpp_extension.html" title="Custom C++   and CUDA Extensions" data-book-page-rel-url="docs/1.0/cpp_extension.html" data-book-page-id="11591">Custom C++ and CUDA Extensions</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/torch_script_custom_ops.html" title="Extending TorchScript with Custom C++   Operators" data-book-page-rel-url="docs/1.0/torch_script_custom_ops.html" data-book-page-id="11592">Extending TorchScript with Custom C++ Operators</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_production_usage.html" title="生产性使用" data-book-page-rel-url="docs/1.0/tut_production_usage.html" data-book-page-id="11593">生产性使用</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/dist_tuto.html" title="Writing Distributed Applications with PyTorch" data-book-page-rel-url="docs/1.0/dist_tuto.html" data-book-page-id="11594">Writing Distributed Applications with PyTorch</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/aws_distributed_training_tutorial.html" title="使用 Amazon AWS 进行分布式训练" data-book-page-rel-url="docs/1.0/aws_distributed_training_tutorial.html" data-book-page-id="11595">使用 Amazon AWS 进行分布式训练</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/ONNXLive.html" title="ONNX 现场演示教程" data-book-page-rel-url="docs/1.0/ONNXLive.html" data-book-page-id="11596">ONNX 现场演示教程</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/cpp_export.html" title="在 C++ 中加载 PYTORCH 模型" data-book-page-rel-url="docs/1.0/cpp_export.html" data-book-page-id="11597">在 C++ 中加载 PYTORCH 模型</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_other_language.html" title="其它语言中的 PyTorch" data-book-page-rel-url="docs/1.0/tut_other_language.html" data-book-page-id="11598">其它语言中的 PyTorch</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/cpp_frontend.html" title="使用 PyTorch C++ 前端" data-book-page-rel-url="docs/1.0/cpp_frontend.html" data-book-page-id="11599">使用 PyTorch C++ 前端</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/docs_notes.html" title="注解" data-book-page-rel-url="docs/1.0/docs_notes.html" data-book-page-id="11600">注解</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_autograd.html" title="自动求导机制" data-book-page-rel-url="docs/1.0/notes_autograd.html" data-book-page-id="11601">自动求导机制</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_broadcasting.html" title="广播语义" data-book-page-rel-url="docs/1.0/notes_broadcasting.html" data-book-page-id="11602">广播语义</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_cuda.html" title="CUDA 语义" data-book-page-rel-url="docs/1.0/notes_cuda.html" data-book-page-id="11603">CUDA 语义</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_extending.html" title="Extending PyTorch" data-book-page-rel-url="docs/1.0/notes_extending.html" data-book-page-id="11604">Extending PyTorch</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_faq.html" title="Frequently Asked Questions" data-book-page-rel-url="docs/1.0/notes_faq.html" data-book-page-id="11605">Frequently Asked Questions</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_multiprocessing.html" title="Multiprocessing best practices" data-book-page-rel-url="docs/1.0/notes_multiprocessing.html" data-book-page-id="11606">Multiprocessing best practices</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_randomness.html" title="Reproducibility" data-book-page-rel-url="docs/1.0/notes_randomness.html" data-book-page-id="11607">Reproducibility</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_serialization.html" title="Serialization semantics" data-book-page-rel-url="docs/1.0/notes_serialization.html" data-book-page-id="11608">Serialization semantics</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_windows.html" title="Windows FAQ" data-book-page-rel-url="docs/1.0/notes_windows.html" data-book-page-id="11609">Windows FAQ</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/docs_package_ref.html" title="包参考" data-book-page-rel-url="docs/1.0/docs_package_ref.html" data-book-page-id="11610">包参考</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/torch.html" title="torch" data-book-page-rel-url="docs/1.0/torch.html" data-book-page-id="11611">torch</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tensors.html" title="torch.Tensor" data-book-page-rel-url="docs/1.0/tensors.html" data-book-page-id="11612">torch.Tensor</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tensor_attributes.html" title="Tensor Attributes" data-book-page-rel-url="docs/1.0/tensor_attributes.html" data-book-page-id="11613">Tensor Attributes</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/type_info.html" title="数据类型信息" data-book-page-rel-url="docs/1.0/type_info.html" data-book-page-id="11614">数据类型信息</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/sparse.html" title="torch.sparse" data-book-page-rel-url="docs/1.0/sparse.html" data-book-page-id="11615">torch.sparse</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/cuda.html" title="torch.cuda" data-book-page-rel-url="docs/1.0/cuda.html" data-book-page-id="11616">torch.cuda</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/storage.html" title="torch.Storage" data-book-page-rel-url="docs/1.0/storage.html" data-book-page-id="11617">torch.Storage</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nn.html" title="torch.nn" data-book-page-rel-url="docs/1.0/nn.html" data-book-page-id="11618">torch.nn</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nn_functional.html" title="torch.nn.functional" data-book-page-rel-url="docs/1.0/nn_functional.html" data-book-page-id="11619">torch.nn.functional</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nn_init.html" title="torch.nn.init" data-book-page-rel-url="docs/1.0/nn_init.html" data-book-page-id="11620">torch.nn.init</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/optim.html" title="torch.optim" data-book-page-rel-url="docs/1.0/optim.html" data-book-page-id="11621">torch.optim</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/autograd.html" title="Automatic differentiation package - torch.autograd" data-book-page-rel-url="docs/1.0/autograd.html" data-book-page-id="11622">Automatic differentiation package - torch.autograd</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/distributed.html" title="Distributed communication package - torch.distributed" data-book-page-rel-url="docs/1.0/distributed.html" data-book-page-id="11623">Distributed communication package - torch.distributed</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/distributions.html" title="Probability distributions - torch.distributions" data-book-page-rel-url="docs/1.0/distributions.html" data-book-page-id="11624">Probability distributions - torch.distributions</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/jit.html" title="Torch Script" data-book-page-rel-url="docs/1.0/jit.html" data-book-page-id="11625">Torch Script</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/multiprocessing.html" title="多进程包 - torch.multiprocessing" data-book-page-rel-url="docs/1.0/multiprocessing.html" data-book-page-id="11626">多进程包 - torch.multiprocessing</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/bottleneck.html" title="torch.utils.bottleneck" data-book-page-rel-url="docs/1.0/bottleneck.html" data-book-page-id="11627">torch.utils.bottleneck</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/checkpoint.html" title="torch.utils.checkpoint" data-book-page-rel-url="docs/1.0/checkpoint.html" data-book-page-id="11628">torch.utils.checkpoint</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/docs_cpp_extension.html" title="torch.utils.cpp_extension" data-book-page-rel-url="docs/1.0/docs_cpp_extension.html" data-book-page-id="11629">torch.utils.cpp_extension</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/data.html" title="torch.utils.data" data-book-page-rel-url="docs/1.0/data.html" data-book-page-id="11630">torch.utils.data</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/dlpack.html" title="torch.utils.dlpack" data-book-page-rel-url="docs/1.0/dlpack.html" data-book-page-id="11631">torch.utils.dlpack</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/hub.html" title="torch.hub" data-book-page-rel-url="docs/1.0/hub.html" data-book-page-id="11632">torch.hub</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/model_zoo.html" title="torch.utils.model_zoo" data-book-page-rel-url="docs/1.0/model_zoo.html" data-book-page-id="11633">torch.utils.model_zoo</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/onnx.html" title="torch.onnx" data-book-page-rel-url="docs/1.0/onnx.html" data-book-page-id="11634">torch.onnx</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/distributed_deprecated.html" title="Distributed communication package (deprecated) - torch.distributed.deprecated" data-book-page-rel-url="docs/1.0/distributed_deprecated.html" data-book-page-id="11635">Distributed communication package (deprecated) - torch.distributed.deprecated</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/docs_torchvision_ref.html" title="torchvision 参考" data-book-page-rel-url="docs/1.0/docs_torchvision_ref.html" data-book-page-id="11636">torchvision 参考</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/torchvision_datasets.html" title="torchvision.datasets" data-book-page-rel-url="docs/1.0/torchvision_datasets.html" data-book-page-id="11637">torchvision.datasets</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/torchvision_models.html" title="torchvision.models" data-book-page-rel-url="docs/1.0/torchvision_models.html" data-book-page-id="11638">torchvision.models</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/torchvision_transforms.html" title="torchvision.transforms" data-book-page-rel-url="docs/1.0/torchvision_transforms.html" data-book-page-id="11639">torchvision.transforms</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/torchvision_utils.html" title="torchvision.utils" data-book-page-rel-url="docs/1.0/torchvision_utils.html" data-book-page-id="11640">torchvision.utils</a>
</li>
</ul>
</li>
</ul>
</div>
</div>
<script src="https://cdn.staticfile.net/jquery/1.12.4/jquery.min.js"></script>
<script type="text/javascript" src="../../../../static/components/uikit-2.27.5/js/uikit.reader.js"></script>
<script type="text/javascript" src="../../../../static/components/social-share/social-share.min.js"></script>
<script>(function(){var bp =document.createElement('script');var curProtocol =window.location.protocol.split(':')[0];if (curProtocol ==='https') {bp.src ='https://zz.bdstatic.com/linksubmit/push.js';}
else {bp.src ='http://push.zhanzhang.baidu.com/push.js';}
var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(bp,s);})();</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
<script src="https://cdn.staticfile.net/highlight.js/9.12.0/highlight.min.js"></script>
<script src="https://cdn.staticfile.net/jquery.pjax/2.0.1/jquery.pjax.min.js"></script>
<script src="https://cdn.staticfile.net/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="https://cdn.staticfile.net/uikit/2.27.5/js/components/lightbox.min.js"></script>
<link rel="dns-prefetch" href="../../../..//cdn.mathjax.org" />
<script type="text/x-mathjax-config">
 function initMathJax() {
    var mathId = $("book-content-section")[0];
    MathJax.Hub.Config({
        tex2jax: {skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code','a']},
        showProcessingMessages: false,
        messageStyle: "none"
    });
    MathJax.Hub.Queue(["Typeset",MathJax.Hub,mathId]);
 };
initMathJax();
</script>
<script src='https://cdn.staticfile.net/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML' async></script>
<style>
	.MathJax_Display{display:inline!important;}
</style>
<script type="text/javascript" src="../../../../static/components/js/reader.js"></script>
<script type="text/javascript">var bookId =169;var bookPageId =11575;var bookPageRelUrl ='docs/1.0/chatbot_tutorial.html';</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
</body>
</html>