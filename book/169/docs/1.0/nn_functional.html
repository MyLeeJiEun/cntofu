
<!DOCTYPE html>
<html lang="zh-CN">
<head>
<title>torch.nn.functional-PyTorch 1.0 中文文档 & 教程</title>
<meta content='torch.nn.functional,PyTorch 1.0 中文文档 & 教程' name='keywords'>
<meta content='torch.nn.functional,PyTorch 1.0 中文文档 & 教程' name='description'>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta http-equiv="Content-Language" content="zh-CN" />
<meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1, maximum-scale=1, user-scalable=no"../../../../>
<meta name="applicable-device" content="pc,mobile">
<link rel="shortcut icon" href="../../../../favicon.ico" type="image/x-icon" />
<meta name="renderer" content="webkit">
<link rel="stylesheet" href="../../../../static/components/uikit-2.27.5/css/uikit.custom.css">
<link rel="stylesheet" href="../../../../static/components/social-share/social-share.min.css">
<link rel="stylesheet" href="../../../../static/components/highlight/styles/custom.css">
<link rel="stylesheet" href="../../../../static/components/css/base.css">
<link rel="stylesheet" href="../../../../static/components/css/reader.css">
<link rel="stylesheet" href="../../../../static/components/css/markdown.css">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5313208362165053" crossorigin="anonymous"></script>
</head>
<body>
<div class=" book-main-wrap uk-container uk-container-center uk-margin-top ">
<div class="uk-grid">
<div class="uk-width-1-1 reader-wrap ">
<div class=" bottom-nav uk-clearfix ">
<div class="uk-align-left ">
<a href="../../../../book/169/docs/1.0/nn.html">
<i class="nav-icon-left uk-icon-small  uk-icon-caret-left"></i>
<span class="">torch.nn</span>
</a>
</div>
<div class="uk-align-right ">
<a href="../../../../book/169/docs/1.0/nn_init.html">
<span class="">torch.nn.in..</span>
<i class="nav-icon-right uk-icon-small  uk-icon-caret-right"></i>
</a>
</div>
</div>
<div class="uk-text-center">
<h2 class="book-page-title uk-container-center">
<a href="../../../../book/169/index.html">PyTorch 1.0 中文文档 & 教程</a>
<a target="_blank" rel="nofollow" href="https://github.com/apachecn/pytorch-doc-zh" class="uk-icon-button uk-icon-github" title="github项目地址"></a>
</h2>
</div>
<script type="text/javascript" src="../../../../static/components/js/app_intro.js"></script>
<ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-5313208362165053" data-ad-slot="1328047120"></ins>
<script>(adsbygoogle =window.adsbygoogle ||[]).push({});</script>
<hr class="uk-article-divider">
<div class="book-content-section  md-content-section  uk-margin-bottom">
<p>﻿# torch.nn.functional</p>
<blockquote>
<p>译者：<a href="https://github.com/hijkzzz">hijkzzz</a></p>
</blockquote>
<h2 id="卷积函数">卷积函数</h2>
<h3 id="conv1d">conv1d</h3>
<pre><code class="language-py">torch.nn.functional.conv1d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) → Tensor
</code></pre>
<p>对由多个输入平面组成的输入信号进行一维卷积.</p>
<p>有关详细信息和输出形状, 请参见<a href="#torch.nn.Conv1d" title="torch.nn.Conv1d"><code>Conv1d</code></a>.</p>
<p>注意</p>
<p>在某些情况下, 当使用CUDA后端与CuDNN时, 该操作符可能会选择不确定性算法来提高性能. 如果这不是您希望的, 您可以通过设置<code>torch.backends.cudn .deterministic = True</code>来尝试使操作具有确定性(可能会以性能为代价). 请参阅关于 <a href="notes/randomness.html">Reproducibility</a> 了解背景.</p>
<p>参数:</p>
<ul>
<li><strong>input</strong> – 输入tensor, 形状为 <a href="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bminibatch%7D %5Ctimes %5Ctext%7Bin%5C_channels%7D %5Ctimes iW)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bminibatch%7D %5Ctimes %5Ctext%7Bin%5C_channels%7D %5Ctimes iW)" alt=""></a></li>
<li><strong>weight</strong> – 卷积核, 形状为 <a href="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bout%5C_channels%7D %5Ctimes %5Cfrac%7B%5Ctext%7Bin%5C_channels%7D%7D%7B%5Ctext%7Bgroups%7D%7D %5Ctimes kW)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bout%5C_channels%7D %5Ctimes %5Cfrac%7B%5Ctext%7Bin%5C_channels%7D%7D%7B%5Ctext%7Bgroups%7D%7D %5Ctimes kW)" alt=""></a></li>
<li><strong>bias</strong> – 可选的偏置, 形状为 <a href="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bout%5C_channels%7D)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bout%5C_channels%7D)" alt=""></a>. 默认值: <code>None</code></li>
<li><strong>stride</strong> – 卷积核的步幅, 可以是单个数字或一个元素元组<code>(sW,)</code>. 默认值: 1</li>
<li><strong>padding</strong> – 在输入的两边隐式加零. 可以是单个数字或一个元素元组<code>(padW, )</code>. 默认值: 0</li>
<li><strong>dilation</strong> – 核元素之间的空洞. 可以是单个数字或单元素元组<code>(dW,)</code>. 默认值: 1</li>
<li><strong>groups</strong> – 将输入分成组, <a href="http://latex.codecogs.com/gif.latex?%5Ctext%7Bin%5C_channels%7D" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%5Ctext%7Bin%5C_channels%7D" alt=""></a> 应该可以被组的数目整除. 默认值: 1</li>
</ul>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; filters = torch.randn(33, 16, 3)
&gt;&gt;&gt; inputs = torch.randn(20, 16, 50)
&gt;&gt;&gt; F.conv1d(inputs, filters)

</code></pre>
<h3 id="conv2d">conv2d</h3>
<pre><code class="language-py">torch.nn.functional.conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) → Tensor
</code></pre>
<p>对由多个输入平面组成的输入图像应用二维卷积.</p>
<p>有关详细信息和输出形状, 请参见<a href="#torch.nn.Conv2d" title="torch.nn.Conv2d"><code>Conv2d</code></a>.</p>
<p>注意</p>
<p>在某些情况下, 当使用CUDA后端与CuDNN时, 该操作符可能会选择不确定性算法来提高性能. 如果这不是您希望的, 您可以通过设置<code>torch.backends.cudn .deterministic = True</code>来尝试使操作具有确定性(可能会以性能为代价). 请参阅关于 <a href="notes/randomness.html">Reproducibility</a> 了解背景.</p>
<p>参数:</p>
<ul>
<li><strong>input</strong> – 输入tensor, 形状为 <a href="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bminibatch%7D %5Ctimes %5Ctext%7Bin%5C_channels%7D %5Ctimes iH %5Ctimes iW)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bminibatch%7D %5Ctimes %5Ctext%7Bin%5C_channels%7D %5Ctimes iH %5Ctimes iW)" alt=""></a></li>
<li><strong>weight</strong> – 卷积核, 形状为 <a href="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bout%5C_channels%7D %5Ctimes %5Cfrac%7B%5Ctext%7Bin%5C_channels%7D%7D%7B%5Ctext%7Bgroups%7D%7D %5Ctimes kH %5Ctimes kW)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bout%5C_channels%7D %5Ctimes %5Cfrac%7B%5Ctext%7Bin%5C_channels%7D%7D%7B%5Ctext%7Bgroups%7D%7D %5Ctimes kH %5Ctimes kW)" alt=""></a></li>
<li><strong>bias</strong> – 可选的偏置, 形状为 <a href="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bout%5C_channels%7D)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bout%5C_channels%7D)" alt=""></a>. 默认值: <code>None</code></li>
<li><strong>stride</strong> – 卷积核的步幅, 可以是单个数字或一个元素元组 <code>(sH, sW)</code>. 默认值: 1</li>
<li><strong>padding</strong> – 在输入的两边隐式加零. 可以是单个数字或一个元素元组 <code>(padH, padW)</code>. 默认值: 0</li>
<li><strong>dilation</strong> – 核元素之间的空洞. 可以是单个数字或单元素元组 <code>(dH, dW)</code>. 默认值: 1</li>
<li><strong>groups</strong> – 将输入分成组, <a href="http://latex.codecogs.com/gif.latex?%5Ctext%7Bin%5C_channels%7D" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%5Ctext%7Bin%5C_channels%7D" alt=""></a> 应该可以被组的数目整除. 默认值: 1</li>
</ul>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; # With square kernels and equal stride
&gt;&gt;&gt; filters = torch.randn(8,4,3,3)
&gt;&gt;&gt; inputs = torch.randn(1,4,5,5)
&gt;&gt;&gt; F.conv2d(inputs, filters, padding=1)

</code></pre>
<h3 id="conv3d">conv3d</h3>
<pre><code class="language-py">torch.nn.functional.conv3d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) → Tensor
</code></pre>
<p>对由多个输入平面组成的输入图像应用三维卷积.</p>
<p>有关详细信息和输出形状, 请参见 <a href="#torch.nn.Conv3d" title="torch.nn.Conv3d"><code>Conv3d</code></a>.</p>
<p>注意</p>
<p>在某些情况下, 当使用CUDA后端与CuDNN时, 该操作符可能会选择不确定性算法来提高性能. 如果这不是您希望的, 您可以通过设置<code>torch.backends.cudn .deterministic = True</code>来尝试使操作具有确定性(可能会以性能为代价). 请参阅关于 <a href="notes/randomness.html">Reproducibility</a> 了解背景.</p>
<p>参数:</p>
<ul>
<li><strong>input</strong> – 输入tensor, 形状为 <a href="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bminibatch%7D %5Ctimes %5Ctext%7Bin%5C_channels%7D %5Ctimes iT %5Ctimes iH %5Ctimes iW)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bminibatch%7D %5Ctimes %5Ctext%7Bin%5C_channels%7D %5Ctimes iT %5Ctimes iH %5Ctimes iW)" alt=""></a></li>
<li><strong>weight</strong> – 卷积核, 形状为 <a href="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bout%5C_channels%7D %5Ctimes %5Cfrac%7B%5Ctext%7Bin%5C_channels%7D%7D%7B%5Ctext%7Bgroups%7D%7D %5Ctimes kT %5Ctimes kH %5Ctimes kW)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bout%5C_channels%7D %5Ctimes %5Cfrac%7B%5Ctext%7Bin%5C_channels%7D%7D%7B%5Ctext%7Bgroups%7D%7D %5Ctimes kT %5Ctimes kH %5Ctimes kW)" alt=""></a></li>
<li><strong>bias</strong> – 可选的偏置, 形状为 <a href="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bout%5C_channels%7D)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bout%5C_channels%7D)" alt=""></a>. 默认值: None</li>
<li><strong>stride</strong> – 卷积核的步幅, 可以是单个数字或一个元素元组 <code>(sT, sH, sW)</code>. 默认值: 1</li>
<li><strong>padding</strong> – 在输入的两边隐式加零. 可以是单个数字或一个元素元组 <code>(padT, padH, padW)</code>. 默认值: 0</li>
<li><strong>dilation</strong> – 核元素之间的空洞. 可以是单个数字或单元素元组 <code>(dT, dH, dW)</code>. 默认值: 1</li>
<li><strong>groups</strong> – 将输入分成组, <a href="http://latex.codecogs.com/gif.latex?%5Ctext%7Bin%5C_channels%7D" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%5Ctext%7Bin%5C_channels%7D" alt=""></a> 应该可以被组的数目整除. 默认值: 1</li>
</ul>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; filters = torch.randn(33, 16, 3, 3, 3)
&gt;&gt;&gt; inputs = torch.randn(20, 16, 50, 10, 20)
&gt;&gt;&gt; F.conv3d(inputs, filters)

</code></pre>
<h3 id="conv-transpose1d">conv_transpose1d</h3>
<pre><code class="language-py">torch.nn.functional.conv_transpose1d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) → Tensor
</code></pre>
<p>对由多个输入平面组成的输入信号应用一维转置卷积算子, 有时也称为反卷积.</p>
<p>有关详细信息和输出形状, 请参见 <a href="#torch.nn.ConvTranspose1d" title="torch.nn.ConvTranspose1d"><code>ConvTranspose1d</code></a></p>
<p>注意</p>
<p>在某些情况下, 当使用CUDA后端与CuDNN时, 该操作符可能会选择不确定性算法来提高性能. 如果这不是您希望的, 您可以通过设置<code>torch.backends.cudn .deterministic = True</code>来尝试使操作具有确定性(可能会以性能为代价). 请参阅关于 <a href="notes/randomness.html">Reproducibility</a> 了解背景.</p>
<p>参数:</p>
<ul>
<li><strong>input</strong> – 输入tensor, 形状为 <a href="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bminibatch%7D %5Ctimes %5Ctext%7Bin%5C_channels%7D %5Ctimes iW)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bminibatch%7D %5Ctimes %5Ctext%7Bin%5C_channels%7D %5Ctimes iW)" alt=""></a></li>
<li><strong>weight</strong> – 卷积核, 形状为 <a href="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bin%5C_channels%7D %5Ctimes %5Cfrac%7B%5Ctext%7Bout%5C_channels%7D%7D%7B%5Ctext%7Bgroups%7D%7D %5Ctimes kW)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bin%5C_channels%7D %5Ctimes %5Cfrac%7B%5Ctext%7Bout%5C_channels%7D%7D%7B%5Ctext%7Bgroups%7D%7D %5Ctimes kW)" alt=""></a></li>
<li><strong>bias</strong> – 可选的偏置, 形状为 <a href="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bout%5C_channels%7D)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bout%5C_channels%7D)" alt=""></a>. 默认值: None</li>
<li><strong>stride</strong> – 卷积核的步幅, 可以是单个数字或一个元素元组 <code>(sW,)</code>. 默认值: 1</li>
<li><strong>padding</strong> – 输入中的每个维度的两边都将添加零填充<code>kernel_size - 1 - padding</code>. 可以是单个数字或元组 <code>(padW,)</code>. 默认值: 0</li>
<li><strong>output_padding</strong> – 添加到输出形状中每个维度的一侧的额外大小. 可以是单个数字或元组 <code>(out_padW)</code>. 默认值: 0</li>
<li><strong>groups</strong> – 将输入分成组, <a href="http://latex.codecogs.com/gif.latex?%5Ctext%7Bin%5C_channels%7D" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%5Ctext%7Bin%5C_channels%7D" alt=""></a> 应该可以被组的数目整除. 默认值: 1</li>
<li><strong>dilation</strong> – 核元素之间的空洞. 可以是单个数字或单元素元组 <code>(dW,)</code>. 默认值: 1</li>
</ul>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; inputs = torch.randn(20, 16, 50)
&gt;&gt;&gt; weights = torch.randn(16, 33, 5)
&gt;&gt;&gt; F.conv_transpose1d(inputs, weights)

</code></pre>
<h3 id="conv-transpose2d">conv_transpose2d</h3>
<pre><code class="language-py">torch.nn.functional.conv_transpose2d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) → Tensor
</code></pre>
<p>对由多个输入平面组成的输入图像应用二维转置卷积算子, 有时也称为反卷积.</p>
<p>有关详细信息和输出形状, 请参见 <a href="#torch.nn.ConvTranspose2d" title="torch.nn.ConvTranspose2d"><code>ConvTranspose2d</code></a>.</p>
<p>注意</p>
<p>在某些情况下, 当使用CUDA后端与CuDNN时, 该操作符可能会选择不确定性算法来提高性能. 如果这不是您希望的, 您可以通过设置<code>torch.backends.cudn .deterministic = True</code>来尝试使操作具有确定性(可能会以性能为代价). 请参阅关于 <a href="notes/randomness.html">Reproducibility</a> 了解背景.</p>
<p>参数:</p>
<ul>
<li><strong>input</strong> – 输入tensor, 形状为 <a href="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bminibatch%7D %5Ctimes %5Ctext%7Bin%5C_channels%7D %5Ctimes iH %5Ctimes iW)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bminibatch%7D %5Ctimes %5Ctext%7Bin%5C_channels%7D %5Ctimes iH %5Ctimes iW)" alt=""></a></li>
<li><strong>weight</strong> – 卷积核, 形状为 <a href="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bin%5C_channels%7D %5Ctimes %5Cfrac%7B%5Ctext%7Bout%5C_channels%7D%7D%7B%5Ctext%7Bgroups%7D%7D %5Ctimes kH %5Ctimes kW)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bin%5C_channels%7D %5Ctimes %5Cfrac%7B%5Ctext%7Bout%5C_channels%7D%7D%7B%5Ctext%7Bgroups%7D%7D %5Ctimes kH %5Ctimes kW)" alt=""></a></li>
<li><strong>bias</strong> –可选的偏置, 形状为 <a href="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bout%5C_channels%7D)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bout%5C_channels%7D)" alt=""></a>. 默认值: None</li>
<li><strong>stride</strong> – 卷积核的步幅, 可以是单个数字或一个元素元组 <code>(sH, sW)</code>. 默认值: 1</li>
<li><strong>padding</strong> – 输入中的每个维度的两边都将添加零填充<code>kernel_size - 1 - padding</code>. 可以是单个数字或元组 <code>(padH, padW)</code>. 默认值: 0</li>
<li><strong>output_padding</strong> – 添加到输出形状中每个维度的一侧的额外大小. 可以是单个数字或元组 <code>(out_padH, out_padW)</code>. 默认值: 0</li>
<li><strong>groups</strong> – 将输入分成组, <a href="http://latex.codecogs.com/gif.latex?%5Ctext%7Bin%5C_channels%7D" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%5Ctext%7Bin%5C_channels%7D" alt=""></a> 应该可以被组的数目整除. 默认值: 1</li>
<li><strong>dilation</strong> – 核元素之间的空洞. 可以是单个数字或单元素元组 <code>(dH, dW)</code>. 默认值: 1</li>
</ul>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; # With square kernels and equal stride
&gt;&gt;&gt; inputs = torch.randn(1, 4, 5, 5)
&gt;&gt;&gt; weights = torch.randn(4, 8, 3, 3)
&gt;&gt;&gt; F.conv_transpose2d(inputs, weights, padding=1)

</code></pre>
<h3 id="conv-transpose3d">conv_transpose3d</h3>
<pre><code class="language-py">torch.nn.functional.conv_transpose3d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) → Tensor
</code></pre>
<p>对由多个输入平面组成的输入图像应用一个三维转置卷积算子, 有时也称为反卷积</p>
<p>有关详细信息和输出形状, 请参见 <a href="#torch.nn.ConvTranspose3d" title="torch.nn.ConvTranspose3d"><code>ConvTranspose3d</code></a>.</p>
<p>注意</p>
<p>在某些情况下, 当使用CUDA后端与CuDNN时, 该操作符可能会选择不确定性算法来提高性能. 如果这不是您希望的, 您可以通过设置<code>torch.backends.cudn .deterministic = True</code>来尝试使操作具有确定性(可能会以性能为代价). 请参阅关于 <a href="notes/randomness.html">Reproducibility</a> 了解背景.</p>
<p>参数:</p>
<ul>
<li><strong>input</strong> – 输入tensor, 形状为 <a href="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bminibatch%7D %5Ctimes %5Ctext%7Bin%5C_channels%7D %5Ctimes iT %5Ctimes iH %5Ctimes iW)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bminibatch%7D %5Ctimes %5Ctext%7Bin%5C_channels%7D %5Ctimes iT %5Ctimes iH %5Ctimes iW)" alt=""></a></li>
<li><strong>weight</strong> – 卷积核, 形状为 <a href="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bin%5C_channels%7D %5Ctimes %5Cfrac%7B%5Ctext%7Bout%5C_channels%7D%7D%7B%5Ctext%7Bgroups%7D%7D %5Ctimes kT %5Ctimes kH %5Ctimes kW)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bin%5C_channels%7D %5Ctimes %5Cfrac%7B%5Ctext%7Bout%5C_channels%7D%7D%7B%5Ctext%7Bgroups%7D%7D %5Ctimes kT %5Ctimes kH %5Ctimes kW)" alt=""></a></li>
<li><strong>bias</strong> –可选的偏置, 形状为 <a href="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bout%5C_channels%7D)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bout%5C_channels%7D)" alt=""></a>. 默认值: None</li>
<li><strong>stride</strong> – 卷积核的步幅, 可以是单个数字或一个元素元组 <code>(sT, sH, sW)</code>. 默认值: 1</li>
<li><strong>padding</strong> – 输入中的每个维度的两边都将添加零填充<code>kernel_size - 1 - padding</code>. 可以是单个数字或元组 <code>(padT, padH, padW)</code>. 默认值: 0</li>
<li><strong>output_padding</strong> – 添加到输出形状中每个维度的一侧的额外大小. 可以是单个数字或元组 <code>(out_padT, out_padH, out_padW)</code>. 默认值: 0</li>
<li><strong>groups</strong> – 将输入分成组, <a href="http://latex.codecogs.com/gif.latex?%5Ctext%7Bin%5C_channels%7D" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%5Ctext%7Bin%5C_channels%7D" alt=""></a> 应该可以被组的数目整除. 默认值: 1</li>
<li><strong>dilation</strong> – 核元素之间的空洞. 可以是单个数字或单元素元组 <code>(dT, dH, dW)</code>. 默认值: 1</li>
</ul>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; inputs = torch.randn(20, 16, 50, 10, 20)
&gt;&gt;&gt; weights = torch.randn(16, 33, 3, 3, 3)
&gt;&gt;&gt; F.conv_transpose3d(inputs, weights)

</code></pre>
<h3 id="unfold">unfold</h3>
<pre><code class="language-py">torch.nn.functional.unfold(input, kernel_size, dilation=1, padding=0, stride=1)
</code></pre>
<p>从批量的<code>input</code>tensor中提取滑动局部块.</p>
<p>警告</p>
<p>目前, 仅支持 4-D input tensors (如批量的图像 tensors).</p>
<p>细节请参阅 <a href="#torch.nn.Unfold" title="torch.nn.Unfold"><code>torch.nn.Unfold</code></a></p>
<h3 id="fold">fold</h3>
<pre><code class="language-py">torch.nn.functional.fold(input, output_size, kernel_size, dilation=1, padding=0, stride=1)
</code></pre>
<p>将一组滑动局部块数组合成一个大的tensor.</p>
<p>警告</p>
<p>目前, 仅支持 4-D input tensors (如批量的图像 tensors).</p>
<p>细节请参阅 <a href="#torch.nn.Fold" title="torch.nn.Fold"><code>torch.nn.Fold</code></a></p>
<h2 id="池化函数">池化函数</h2>
<h3 id="avg-pool1d">avg_pool1d</h3>
<pre><code class="language-py">torch.nn.functional.avg_pool1d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) → Tensor
</code></pre>
<p>对由多个输入平面组成的输入信号应用一维平均池化.</p>
<p>有关详细信息和输出形状, 请参见 <a href="#torch.nn.AvgPool1d" title="torch.nn.AvgPool1d"><code>AvgPool1d</code></a>.</p>
<p>参数:</p>
<ul>
<li><strong>input</strong> – 输入tensor, 形状为 <a href="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bminibatch%7D %5Ctimes %5Ctext%7Bin%5C_channels%7D %5Ctimes iW)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bminibatch%7D %5Ctimes %5Ctext%7Bin%5C_channels%7D %5Ctimes iW)" alt=""></a></li>
<li><strong>kernel_size</strong> – 窗口的大小. 可以是单个数字或元组 <a href="http://latex.codecogs.com/gif.latex?(kW%2C)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(kW%2C)" alt=""></a></li>
<li><strong>stride</strong> – 窗户的步幅. 可以是单个数字或元组 <code>(sW,)</code>. 默认值: <code>kernel_size</code></li>
<li><strong>padding</strong> – 在输入的两边隐式加零. 可以是单个数字或一个元素元组 <code>(padW,)</code>. 默认值: 0</li>
<li><strong>ceil_mode</strong> – 如果 <code>True</code>, 将用 <code>ceil</code> 代替 <code>floor</code>计算输出形状. 默认值: <code>False</code></li>
<li><strong>count_include_pad</strong> – 如果 <code>True</code>, 将在平均计算中包括零填充. 默认值: <code>True</code></li>
</ul>
<pre><code class="language-py">例子::
</code></pre>
<pre><code class="language-py">&gt;&gt;&gt; # pool of square window of size=3, stride=2
&gt;&gt;&gt; input = torch.tensor([[[1,2,3,4,5,6,7]]])
&gt;&gt;&gt; F.avg_pool1d(input, kernel_size=3, stride=2)
tensor([[[ 2.,  4.,  6.]]])

</code></pre>
<h3 id="avg-pool2d">avg_pool2d</h3>
<pre><code class="language-py">torch.nn.functional.avg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) → Tensor
</code></pre>
<p>应用2D平均池化操作于 <a href="http://latex.codecogs.com/gif.latex?kH %5Ctimes kW" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?kH %5Ctimes kW" alt=""></a> 区域, 步幅为 <a href="http://latex.codecogs.com/gif.latex?sH %5Ctimes sW" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?sH %5Ctimes sW" alt=""></a> . 输出特征的数量等于输入平面的数量.</p>
<p>有关详细信息和输出形状, 请参见 <a href="#torch.nn.AvgPool2d" title="torch.nn.AvgPool2d"><code>AvgPool2d</code></a>.</p>
<p>参数:</p>
<ul>
<li><strong>input</strong> – input tensor <a href="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bminibatch%7D %5Ctimes %5Ctext%7Bin%5C_channels%7D %5Ctimes iH %5Ctimes iW)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bminibatch%7D %5Ctimes %5Ctext%7Bin%5C_channels%7D %5Ctimes iH %5Ctimes iW)" alt=""></a></li>
<li><strong>kernel_size</strong> – 池化区域的大小, 可以是一个数字或者元组 <a href="http://latex.codecogs.com/gif.latex?(kH %5Ctimes kW)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(kH %5Ctimes kW)" alt=""></a></li>
<li><strong>stride</strong> – 池化步幅, 可以是一个数字或者元组 <code>(sH, sW)</code>. 默认值: <code>kernel_size</code></li>
<li><strong>padding</strong> – 在输入的两边隐式加零. 可以是单个数字或一个元素元组 <code>(padH, padW)</code>. 默认值: 0</li>
<li><strong>ceil_mode</strong> – 如果 <code>True</code>, 将用 <code>ceil</code> 代替 <code>floor</code>计算输出形状. 默认值: <code>False</code></li>
<li><strong>count_include_pad</strong> – 如果 <code>True</code>, 将在平均计算中包括零填充. 默认值: <code>True</code></li>
</ul>
<h3 id="avg-pool3d">avg_pool3d</h3>
<pre><code class="language-py">torch.nn.functional.avg_pool3d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) → Tensor
</code></pre>
<p>应用3D平均池化操作于 <a href="http://latex.codecogs.com/gif.latex?kT %5Ctimes kH %5Ctimes kW" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?kT %5Ctimes kH %5Ctimes kW" alt=""></a> 区域, 步幅为 <a href="http://latex.codecogs.com/gif.latex?sT %5Ctimes sH %5Ctimes sW" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?sT %5Ctimes sH %5Ctimes sW" alt=""></a> . 输出特征的数量等于 <a href="http://latex.codecogs.com/gif.latex?%5Clfloor%5Cfrac%7B%5Ctext%7Binput planes%7D%7D%7BsT%7D%5Crfloor" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%5Clfloor%5Cfrac%7B%5Ctext%7Binput planes%7D%7D%7BsT%7D%5Crfloor" alt=""></a>.</p>
<p>有关详细信息和输出形状, 请参见 <a href="#torch.nn.AvgPool3d" title="torch.nn.AvgPool3d"><code>AvgPool3d</code></a>.</p>
<p>参数:</p>
<ul>
<li><strong>input</strong> – input tensor <a href="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bminibatch%7D %5Ctimes %5Ctext%7Bin%5C_channels%7D %5Ctimes iT %5Ctimes iH %5Ctimes iW)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(%5Ctext%7Bminibatch%7D %5Ctimes %5Ctext%7Bin%5C_channels%7D %5Ctimes iT %5Ctimes iH %5Ctimes iW)" alt=""></a></li>
<li><strong>kernel_size</strong> – 池化区域的大小, 可以是一个数字或者元组 <a href="http://latex.codecogs.com/gif.latex?(kT %5Ctimes kH %5Ctimes kW)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(kT %5Ctimes kH %5Ctimes kW)" alt=""></a></li>
<li><strong>stride</strong> – 池化步幅, 可以是一个数字或者元组 <code>(sT, sH, sW)</code>. 默认值: <code>kernel_size</code></li>
<li><strong>padding</strong> – 在输入的两边隐式加零. 可以是单个数字或一个元素元组 <code>(padT, padH, padW)</code>, 默认值: 0</li>
<li><strong>ceil_mode</strong> – 如果 <code>True</code>, 将用 <code>ceil</code> 代替 <code>floor</code>计算输出形状. 默认值: <code>False</code></li>
<li><strong>count_include_pad</strong> – 如果 <code>True</code>, 将在平均计算中包括零填充. 默认值: <code>True</code></li>
</ul>
<h3 id="max-pool1d">max_pool1d</h3>
<pre><code class="language-py">torch.nn.functional.max_pool1d(*args, **kwargs)
</code></pre>
<p>在由多个输入平面组成的输入信号上应用1D 最大池化.</p>
<p>详情见 <a href="#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code>MaxPool1d</code></a>.</p>
<h3 id="max-pool2d">max_pool2d</h3>
<pre><code class="language-py">torch.nn.functional.max_pool2d(*args, **kwargs)
</code></pre>
<p>在由多个输入平面组成的输入信号上应用2D 最大池化.</p>
<p>详情见 <a href="#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code>MaxPool2d</code></a>.</p>
<h3 id="max-pool3d">max_pool3d</h3>
<pre><code class="language-py">torch.nn.functional.max_pool3d(*args, **kwargs)
</code></pre>
<p>在由多个输入平面组成的输入信号上应用3D 最大池化.</p>
<p>详情见 <a href="#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code>MaxPool3d</code></a>.</p>
<h3 id="max-unpool1d">max_unpool1d</h3>
<pre><code class="language-py">torch.nn.functional.max_unpool1d(input, indices, kernel_size, stride=None, padding=0, output_size=None)
</code></pre>
<p>计算逆 <code>MaxPool1d</code>.</p>
<p>请参见 <a href="#torch.nn.MaxUnpool1d" title="torch.nn.MaxUnpool1d"><code>MaxUnpool1d</code></a>.</p>
<h3 id="max-unpool2d">max_unpool2d</h3>
<pre><code class="language-py">torch.nn.functional.max_unpool2d(input, indices, kernel_size, stride=None, padding=0, output_size=None)
</code></pre>
<p>计算逆 <code>MaxPool2d</code>.</p>
<p>详情见 <a href="#torch.nn.MaxUnpool2d" title="torch.nn.MaxUnpool2d"><code>MaxUnpool2d</code></a>.</p>
<h3 id="max-unpool3d">max_unpool3d</h3>
<pre><code class="language-py">torch.nn.functional.max_unpool3d(input, indices, kernel_size, stride=None, padding=0, output_size=None)
</code></pre>
<p>计算逆 <code>MaxPool3d</code>.</p>
<p>详情见 <a href="#torch.nn.MaxUnpool3d" title="torch.nn.MaxUnpool3d"><code>MaxUnpool3d</code></a>.</p>
<h3 id="lp-pool1d">lp_pool1d</h3>
<pre><code class="language-py">torch.nn.functional.lp_pool1d(input, norm_type, kernel_size, stride=None, ceil_mode=False)
</code></pre>
<p>在由多个输入平面组成的输入信号上应用1D幂平均池化. 如果所有输入的p次方的和为零, 梯度也为零.</p>
<p>详情见 <a href="#torch.nn.LPPool1d" title="torch.nn.LPPool1d"><code>LPPool1d</code></a>.</p>
<h3 id="lp-pool2d">lp_pool2d</h3>
<pre><code class="language-py">torch.nn.functional.lp_pool2d(input, norm_type, kernel_size, stride=None, ceil_mode=False)
</code></pre>
<p>在由多个输入平面组成的输入信号上应用2D幂平均池化. 如果所有输入的p次方的和为零, 梯度也为零.</p>
<p>详情见 <a href="#torch.nn.LPPool2d" title="torch.nn.LPPool2d"><code>LPPool2d</code></a>.</p>
<h3 id="adaptive-max-pool1d">adaptive_max_pool1d</h3>
<pre><code class="language-py">torch.nn.functional.adaptive_max_pool1d(*args, **kwargs)
</code></pre>
<p>在由多个输入平面组成的输入信号上应用1D自适应最大池.</p>
<p>请参见 <a href="#torch.nn.AdaptiveMaxPool1d" title="torch.nn.AdaptiveMaxPool1d"><code>AdaptiveMaxPool1d</code></a>和输出形状.</p>
<p>参数:</p>
<ul>
<li><strong>output_size</strong> – 输出目标大小(单个整数)</li>
<li><strong>return_indices</strong> – 是否返回池化索引. 默认值: <code>False</code></li>
</ul>
<h3 id="adaptive-max-pool2d">adaptive_max_pool2d</h3>
<pre><code class="language-py">torch.nn.functional.adaptive_max_pool2d(*args, **kwargs)
</code></pre>
<p>在由多个输入平面组成的输入信号上应用2D自适应最大池.</p>
<p>请参见 <a href="#torch.nn.AdaptiveMaxPool2d" title="torch.nn.AdaptiveMaxPool2d"><code>AdaptiveMaxPool2d</code></a> 关于详情和输出形状.</p>
<p>参数:</p>
<ul>
<li><strong>output_size</strong> – 输出目标大小(单个整数 or 双整数元组)</li>
<li><strong>return_indices</strong> – 是否返回池化索引. 默认值: <code>False</code></li>
</ul>
<h3 id="adaptive-max-pool3d">adaptive_max_pool3d</h3>
<pre><code class="language-py">torch.nn.functional.adaptive_max_pool3d(*args, **kwargs)
</code></pre>
<p>在由多个输入平面组成的输入信号上应用3D自适应最大池.</p>
<p>请参见 <a href="#torch.nn.AdaptiveMaxPool3d" title="torch.nn.AdaptiveMaxPool3d"><code>AdaptiveMaxPool3d</code></a>和输出形状.</p>
<p>参数:</p>
<ul>
<li><strong>output_size</strong> – 输出目标大小(单个整数 或者 三整数元组)</li>
<li><strong>return_indices</strong> – 是否返回池化索引. 默认值: <code>False</code></li>
</ul>
<h3 id="adaptive-avg-pool1d">adaptive_avg_pool1d</h3>
<pre><code class="language-py">torch.nn.functional.adaptive_avg_pool1d(input, output_size) → Tensor
</code></pre>
<p>在由多个输入平面组成的输入信号上应用1D自适应平均池化.</p>
<p>请参见 <a href="#torch.nn.AdaptiveAvgPool1d" title="torch.nn.AdaptiveAvgPool1d"><code>AdaptiveAvgPool1d</code></a> 关于详情和输出形状.</p>
<p>| 参数:| <strong>output_size</strong> – 输出目标大小(单个整数)</p>
<h3 id="adaptive-avg-pool2d">adaptive_avg_pool2d</h3>
<pre><code class="language-py">torch.nn.functional.adaptive_avg_pool2d(input, output_size)
</code></pre>
<p>在由多个输入平面组成的输入信号上应用2D自适应平均池化.</p>
<p>请参见 <a href="#torch.nn.AdaptiveAvgPool2d" title="torch.nn.AdaptiveAvgPool2d"><code>AdaptiveAvgPool2d</code></a> 关于详情和输出形状.</p>
<p>| 参数:| <strong>output_size</strong> – 输出目标大小(单个整数 或者双整数元组)</p>
<h3 id="adaptive-avg-pool3d">adaptive_avg_pool3d</h3>
<pre><code class="language-py">torch.nn.functional.adaptive_avg_pool3d(input, output_size)
</code></pre>
<p>在由多个输入平面组成的输入信号上应用3D自适应平均池化.</p>
<p>请参见 <a href="#torch.nn.AdaptiveAvgPool3d" title="torch.nn.AdaptiveAvgPool3d"><code>AdaptiveAvgPool3d</code></a> 关于详情和输出形状.</p>
<p>| 参数:| <strong>output_size</strong> – 输出目标大小(单个整数 或者三整数元组)</p>
<h2 id="非线性激活函数">非线性激活函数</h2>
<h3 id="threshold">threshold</h3>
<pre><code class="language-py">torch.nn.functional.threshold(input, threshold, value, inplace=False)
</code></pre>
<p>为 input Tensor 的每个元素设置阈值.</p>
<p>请参见 <a href="#torch.nn.Threshold" title="torch.nn.Threshold"><code>Threshold</code></a>.</p>
<pre><code class="language-py">torch.nn.functional.threshold_(input, threshold, value) → Tensor
</code></pre>
<p>原地版本的 <a href="#torch.nn.functional.threshold" title="torch.nn.functional.threshold"><code>threshold()</code></a>.</p>
<h3 id="relu">relu</h3>
<pre><code class="language-py">torch.nn.functional.relu(input, inplace=False) → Tensor
</code></pre>
<p>逐元素应用整流线性单元函数. 请参见 <a href="#torch.nn.ReLU" title="torch.nn.ReLU"><code>ReLU</code></a>.</p>
<pre><code class="language-py">torch.nn.functional.relu_(input) → Tensor
</code></pre>
<p>原地版本的 <a href="#torch.nn.functional.relu" title="torch.nn.functional.relu"><code>relu()</code></a>.</p>
<h3 id="hardtanh">hardtanh</h3>
<pre><code class="language-py">torch.nn.functional.hardtanh(input, min_val=-1., max_val=1., inplace=False) → Tensor
</code></pre>
<p>逐元素应用hardtanh函数. 请参见 <a href="#torch.nn.Hardtanh" title="torch.nn.Hardtanh"><code>Hardtanh</code></a>.</p>
<pre><code class="language-py">torch.nn.functional.hardtanh_(input, min_val=-1., max_val=1.) → Tensor
</code></pre>
<p>原地版本的 <a href="#torch.nn.functional.hardtanh" title="torch.nn.functional.hardtanh"><code>hardtanh()</code></a>.</p>
<h3 id="relu6">relu6</h3>
<pre><code class="language-py">torch.nn.functional.relu6(input, inplace=False) → Tensor
</code></pre>
<p>逐元素应用函数 <a href="http://latex.codecogs.com/gif.latex?%5Ctext%7BReLU6%7D(x) %3D %5Cmin(%5Cmax(0%2Cx)%2C 6)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%5Ctext%7BReLU6%7D(x) %3D %5Cmin(%5Cmax(0%2Cx)%2C 6)" alt=""></a>.</p>
<p>请参见 <a href="#torch.nn.ReLU6" title="torch.nn.ReLU6"><code>ReLU6</code></a>.</p>
<h3 id="elu">elu</h3>
<pre><code class="language-py">torch.nn.functional.elu(input, alpha=1.0, inplace=False)
</code></pre>
<p>逐元素应用, <a href="http://latex.codecogs.com/gif.latex?%5Ctext%7BELU%7D(x) %3D %5Cmax(0%2Cx) %2B %5Cmin(0%2C %5Calpha * (%5Cexp(x) - 1))" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%5Ctext%7BELU%7D(x) %3D %5Cmax(0%2Cx) %2B %5Cmin(0%2C %5Calpha * (%5Cexp(x) - 1))" alt=""></a>.</p>
<p>请参见 <a href="#torch.nn.ELU" title="torch.nn.ELU"><code>ELU</code></a>.</p>
<pre><code class="language-py">torch.nn.functional.elu_(input, alpha=1.) → Tensor
</code></pre>
<p>原地版本的 <a href="#torch.nn.functional.elu" title="torch.nn.functional.elu"><code>elu()</code></a>.</p>
<h3 id="selu">selu</h3>
<pre><code class="language-py">torch.nn.functional.selu(input, inplace=False) → Tensor
</code></pre>
<p>逐元素应用, <a href="http://latex.codecogs.com/gif.latex?%5Ctext%7BSELU%7D(x) %3D scale * (%5Cmax(0%2Cx) %2B %5Cmin(0%2C %5Calpha * (%5Cexp(x) - 1)))" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%5Ctext%7BSELU%7D(x) %3D scale * (%5Cmax(0%2Cx) %2B %5Cmin(0%2C %5Calpha * (%5Cexp(x) - 1)))" alt=""></a>, with <a href="http://latex.codecogs.com/gif.latex?%5Calpha%3D1.6732632423543772848170429916717" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%5Calpha%3D1.6732632423543772848170429916717" alt=""></a> and <a href="http://latex.codecogs.com/gif.latex?scale%3D1.0507009873554804934193349852946" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?scale%3D1.0507009873554804934193349852946" alt=""></a>.</p>
<p>请参见 <a href="#torch.nn.SELU" title="torch.nn.SELU"><code>SELU</code></a>.</p>
<h3 id="celu">celu</h3>
<pre><code class="language-py">torch.nn.functional.celu(input, alpha=1., inplace=False) → Tensor
</code></pre>
<p>逐元素应用, <a href="http://latex.codecogs.com/gif.latex?%5Ctext%7BCELU%7D(x) %3D %5Cmax(0%2Cx) %2B %5Cmin(0%2C %5Calpha * (%5Cexp(x%2F%5Calpha) - 1))" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%5Ctext%7BCELU%7D(x) %3D %5Cmax(0%2Cx) %2B %5Cmin(0%2C %5Calpha * (%5Cexp(x%2F%5Calpha) - 1))" alt=""></a>.</p>
<p>请参见 <a href="#torch.nn.CELU" title="torch.nn.CELU"><code>CELU</code></a>.</p>
<h3 id="leaky-relu">leaky_relu</h3>
<pre><code class="language-py">torch.nn.functional.leaky_relu(input, negative_slope=0.01, inplace=False) → Tensor
</code></pre>
<p>逐元素应用, <a href="http://latex.codecogs.com/gif.latex?%5Ctext%7BLeakyReLU%7D(x) %3D %5Cmax(0%2C x) %2B %5Ctext%7Bnegative%5C_slope%7D * %5Cmin(0%2C x)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%5Ctext%7BLeakyReLU%7D(x) %3D %5Cmax(0%2C x) %2B %5Ctext%7Bnegative%5C_slope%7D * %5Cmin(0%2C x)" alt=""></a></p>
<p>请参见 <a href="#torch.nn.LeakyReLU" title="torch.nn.LeakyReLU"><code>LeakyReLU</code></a>.</p>
<pre><code class="language-py">torch.nn.functional.leaky_relu_(input, negative_slope=0.01) → Tensor
</code></pre>
<p>原地版本的 <a href="#torch.nn.functional.leaky_relu" title="torch.nn.functional.leaky_relu"><code>leaky_relu()</code></a>.</p>
<h3 id="prelu">prelu</h3>
<pre><code class="language-py">torch.nn.functional.prelu(input, weight) → Tensor
</code></pre>
<p>逐元素应用函数 <a href="http://latex.codecogs.com/gif.latex?%5Ctext%7BPReLU%7D(x) %3D %5Cmax(0%2Cx) %2B %5Ctext%7Bweight%7D * %5Cmin(0%2Cx)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%5Ctext%7BPReLU%7D(x) %3D %5Cmax(0%2Cx) %2B %5Ctext%7Bweight%7D * %5Cmin(0%2Cx)" alt=""></a> where weight is a learnable parameter.</p>
<p>请参见 <a href="#torch.nn.PReLU" title="torch.nn.PReLU"><code>PReLU</code></a>.</p>
<h3 id="rrelu">rrelu</h3>
<pre><code class="language-py">torch.nn.functional.rrelu(input, lower=1./8, upper=1./3, training=False, inplace=False) → Tensor
</code></pre>
<p>随机的 leaky ReLU.</p>
<p>请参见 <a href="#torch.nn.RReLU" title="torch.nn.RReLU"><code>RReLU</code></a>.</p>
<pre><code class="language-py">torch.nn.functional.rrelu_(input, lower=1./8, upper=1./3, training=False) → Tensor
</code></pre>
<p>原地版本的 <a href="#torch.nn.functional.rrelu" title="torch.nn.functional.rrelu"><code>rrelu()</code></a>.</p>
<h3 id="glu">glu</h3>
<pre><code class="language-py">torch.nn.functional.glu(input, dim=-1) → Tensor
</code></pre>
<p>门控线性单元. 计算:</p>
<p><a href="http://latex.codecogs.com/gif.latex?%0D%0AH %3D A %5Ctimes %5Csigma(B)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%0D%0AH %3D A %5Ctimes %5Csigma(B)" alt=""></a></p>
<p>其中<code>inpuy</code>沿<code>dim</code>分成两半, 形成<code>A</code>和<code>B</code>.</p>
<p>见 <a href="https://arxiv.org/abs/1612.08083">Language Modeling with Gated Convolutional Networks</a>.</p>
<p>参数:</p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input tensor</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – 维度用于分割 input</li>
</ul>
<h3 id="logsigmoid">logsigmoid</h3>
<pre><code class="language-py">torch.nn.functional.logsigmoid(input) → Tensor
</code></pre>
<p>逐元素应用 <a href="http://latex.codecogs.com/gif.latex?%5Ctext%7BLogSigmoid%7D(x) %3D %5Clog %5Cleft(%5Cfrac%7B1%7D%7B1 %2B %5Cexp(-x_i)%7D%5Cright)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%5Ctext%7BLogSigmoid%7D(x) %3D %5Clog %5Cleft(%5Cfrac%7B1%7D%7B1 %2B %5Cexp(-x_i)%7D%5Cright)" alt=""></a></p>
<p>请参见 <a href="#torch.nn.LogSigmoid" title="torch.nn.LogSigmoid"><code>LogSigmoid</code></a>.</p>
<h3 id="hardshrink">hardshrink</h3>
<pre><code class="language-py">torch.nn.functional.hardshrink(input, lambd=0.5) → Tensor
</code></pre>
<p>逐元素应用hardshrink函数</p>
<p>请参见 <a href="#torch.nn.Hardshrink" title="torch.nn.Hardshrink"><code>Hardshrink</code></a>.</p>
<h3 id="tanhshrink">tanhshrink</h3>
<pre><code class="language-py">torch.nn.functional.tanhshrink(input) → Tensor
</code></pre>
<p>逐元素应用, <a href="http://latex.codecogs.com/gif.latex?%5Ctext%7BTanhshrink%7D(x) %3D x - %5Ctext%7BTanh%7D(x)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%5Ctext%7BTanhshrink%7D(x) %3D x - %5Ctext%7BTanh%7D(x)" alt=""></a></p>
<p>请参见 <a href="#torch.nn.Tanhshrink" title="torch.nn.Tanhshrink"><code>Tanhshrink</code></a>.</p>
<h3 id="softsign">softsign</h3>
<pre><code class="language-py">torch.nn.functional.softsign(input) → Tensor
</code></pre>
<p>逐元素应用, the function <a href="http://latex.codecogs.com/gif.latex?%5Ctext%7BSoftSign%7D(x) %3D %5Cfrac%7Bx%7D%7B1 %2B %7Cx%7C%7D" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%5Ctext%7BSoftSign%7D(x) %3D %5Cfrac%7Bx%7D%7B1 %2B %7Cx%7C%7D" alt=""></a></p>
<p>请参见 <a href="#torch.nn.Softsign" title="torch.nn.Softsign"><code>Softsign</code></a>.</p>
<h3 id="softplus">softplus</h3>
<pre><code class="language-py">torch.nn.functional.softplus(input, beta=1, threshold=20) → Tensor
</code></pre>
<h3 id="softmin">softmin</h3>
<pre><code class="language-py">torch.nn.functional.softmin(input, dim=None, _stacklevel=3, dtype=None)
</code></pre>
<p>应用 softmin 函数.</p>
<p>注意 <a href="http://latex.codecogs.com/gif.latex?%5Ctext%7BSoftmin%7D(x) %3D %5Ctext%7BSoftmax%7D(-x)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%5Ctext%7BSoftmin%7D(x) %3D %5Ctext%7BSoftmax%7D(-x)" alt=""></a>. See softmax definition for mathematical formula.</p>
<p>请参见 <a href="#torch.nn.Softmin" title="torch.nn.Softmin"><code>Softmin</code></a>.</p>
<p>参数:</p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – 计算softmin的维度(因此dim上的每个切片的和为1).</li>
<li><strong>dtype</strong> (<code>torch.dtype</code>, 可选的) – 返回tenosr的期望数据类型.</li>
</ul>
<p>:param如果指定, 输入张量在执行::param操作之前被转换为<code>dtype</code>. 这对于防止数据类型溢出非常有用. 默认值: None.</p>
<h3 id="softmax">softmax</h3>
<pre><code class="language-py">torch.nn.functional.softmax(input, dim=None, _stacklevel=3, dtype=None)
</code></pre>
<p>应用 softmax 函数.</p>
<p>Softmax定义为:</p>
<p><a href="http://latex.codecogs.com/gif.latex?%5Ctext%7BSoftmax%7D(x_%7Bi%7D) %3D %5Cfrac%7Bexp(x_i)%7D%7B%5Csum_j exp(x_j)%7D" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%5Ctext%7BSoftmax%7D(x_%7Bi%7D) %3D %5Cfrac%7Bexp(x_i)%7D%7B%5Csum_j exp(x_j)%7D" alt=""></a></p>
<p>它应用于dim上的所有切片, 并将对它们进行重新缩放, 使元素位于<code>(0,1)</code>范围内, 和为1.</p>
<p>请参见 <a href="#torch.nn.Softmax" title="torch.nn.Softmax"><code>Softmax</code></a>.</p>
<p>参数:</p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – 将计算softmax的维度.</li>
<li><strong>dtype</strong> (<code>torch.dtype</code>, 可选的) – 返回tenosr的期望数据类型.</li>
</ul>
<p>:param如果指定, 输入张量在执行::param操作之前被转换为<code>dtype</code>. 这对于防止数据类型溢出非常有用. 默认值: None.</p>
<p>注意</p>
<p>此函数不能直接用于<code>NLLLoss</code>, <code>NLLLoss</code>期望在<code>Softmax</code>和它自己之间计算对数. 改用<code>log_softmax</code> (它速度更快, 数值性能更好).</p>
<h3 id="softshrink">softshrink</h3>
<pre><code class="language-py">torch.nn.functional.softshrink(input, lambd=0.5) → Tensor
</code></pre>
<p>逐元素应用 soft shrinkage 函数</p>
<p>请参见 <a href="#torch.nn.Softshrink" title="torch.nn.Softshrink"><code>Softshrink</code></a>.</p>
<h3 id="gumbel-softmax">gumbel_softmax</h3>
<pre><code class="language-py">torch.nn.functional.gumbel_softmax(logits, tau=1.0, hard=False, eps=1e-10)
</code></pre>
<p>采样自Gumbel-Softmax分布, 并可选地离散化.</p>
<p>参数:</p>
<ul>
<li><strong>logits</strong> – <code>[batch_size, num_features]</code> 非规范化对数概率</li>
<li><strong>tau</strong> – 非负的温度标量</li>
<li><strong>hard</strong> – 如果 <code>True</code>, 返回的样本将会离散为 one-hot 向量, 但将是可微分的似乎它是autograd中的soft sample</li>
</ul>
<p>| 返回值: | 从 Gumbel-Softmax 分布采样的 tensor, 形状为 <code>batch_size x num_features</code> . 如果 <code>hard=True</code>, 返回值是 one-hot 编码, 否则, 它们就是特征和为1的概率分布</p>
<p>约束:</p>
<ul>
<li>目前仅支持 2D 输入 <code>logits</code> tensor , 形状为 <code>batch_size x num_features</code></li>
</ul>
<p>基于 <a href="https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical VAE.ipynb">https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical VAE.ipynb</a> , (MIT license)</p>
<h3 id="log-softmax">log_softmax</h3>
<pre><code class="language-py">torch.nn.functional.log_softmax(input, dim=None, _stacklevel=3, dtype=None)
</code></pre>
<p>应用 softmax 跟随对数运算.</p>
<p>虽然在数学上等价于log(softmax(x)), 但分别执行这两个操作比较慢, 而且在数值上不稳定. 这个函数使用另一种公式来正确计算输出和梯度.</p>
<p>请参见 <a href="#torch.nn.LogSoftmax" title="torch.nn.LogSoftmax"><code>LogSoftmax</code></a>.</p>
<p>参数:</p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – A dimension along which log_softmax will be computed.</li>
<li><strong>dtype</strong> (<code>torch.dtype</code>, 可选的) – 返回tenosr的期望数据类型.</li>
</ul>
<p>:param如果指定, 输入张量在执行::param操作之前被转换为<code>dtype</code>. 这对于防止数据类型溢出非常有用. 默认值: None.</p>
<h3 id="tanh">tanh</h3>
<pre><code class="language-py">torch.nn.functional.tanh(input) → Tensor
</code></pre>
<p>逐元素应用, <a href="http://latex.codecogs.com/gif.latex?%5Ctext%7BTanh%7D(x) %3D %5Ctanh(x) %3D %5Cfrac%7B%5Cexp(x) - %5Cexp(-x)%7D%7B%5Cexp(x) %2B %5Cexp(-x)%7D" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%5Ctext%7BTanh%7D(x) %3D %5Ctanh(x) %3D %5Cfrac%7B%5Cexp(x) - %5Cexp(-x)%7D%7B%5Cexp(x) %2B %5Cexp(-x)%7D" alt=""></a></p>
<p>请参见 <a href="#torch.nn.Tanh" title="torch.nn.Tanh"><code>Tanh</code></a>.</p>
<h3 id="sigmoid">sigmoid</h3>
<pre><code class="language-py">torch.nn.functional.sigmoid(input) → Tensor
</code></pre>
<p>逐元素应用函数 <a href="http://latex.codecogs.com/gif.latex?%5Ctext%7BSigmoid%7D(x) %3D %5Cfrac%7B1%7D%7B1 %2B %5Cexp(-x)%7D" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%5Ctext%7BSigmoid%7D(x) %3D %5Cfrac%7B1%7D%7B1 %2B %5Cexp(-x)%7D" alt=""></a></p>
<p>请参见 <a href="#torch.nn.Sigmoid" title="torch.nn.Sigmoid"><code>Sigmoid</code></a>.</p>
<h2 id="规范化函数">规范化函数</h2>
<h3 id="batch-norm">batch_norm</h3>
<pre><code class="language-py">torch.nn.functional.batch_norm(input, running_mean, running_var, weight=None, bias=None, training=False, momentum=0.1, eps=1e-05)
</code></pre>
<p>对一批数据中的每个通道应用批量标准化.</p>
<p>请参见 <a href="#torch.nn.BatchNorm1d" title="torch.nn.BatchNorm1d"><code>BatchNorm1d</code></a>, <a href="#torch.nn.BatchNorm2d" title="torch.nn.BatchNorm2d"><code>BatchNorm2d</code></a>, <a href="#torch.nn.BatchNorm3d" title="torch.nn.BatchNorm3d"><code>BatchNorm3d</code></a>.</p>
<h3 id="instance-norm">instance_norm</h3>
<pre><code class="language-py">torch.nn.functional.instance_norm(input, running_mean=None, running_var=None, weight=None, bias=None, use_input_stats=True, momentum=0.1, eps=1e-05)
</code></pre>
<p>对批中每个数据样本中的每个通道应用实例规范化.</p>
<p>请参见 <a href="#torch.nn.InstanceNorm1d" title="torch.nn.InstanceNorm1d"><code>InstanceNorm1d</code></a>, <a href="#torch.nn.InstanceNorm2d" title="torch.nn.InstanceNorm2d"><code>InstanceNorm2d</code></a>, <a href="#torch.nn.InstanceNorm3d" title="torch.nn.InstanceNorm3d"><code>InstanceNorm3d</code></a>.</p>
<h3 id="layer-norm">layer_norm</h3>
<pre><code class="language-py">torch.nn.functional.layer_norm(input, normalized_shape, weight=None, bias=None, eps=1e-05)
</code></pre>
<p>对最后特定数量的维度应用layer规范化.</p>
<p>请参见 <a href="#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code>LayerNorm</code></a>.</p>
<h3 id="local-response-norm">local_response_norm</h3>
<pre><code class="language-py">torch.nn.functional.local_response_norm(input, size, alpha=0.0001, beta=0.75, k=1.0)
</code></pre>
<p>对由多个输入平面组成的输入信号进行局部响应归一化, 其中通道占据第二维. 跨通道应用标准化.</p>
<p>请参见 <a href="#torch.nn.LocalResponseNorm" title="torch.nn.LocalResponseNorm"><code>LocalResponseNorm</code></a>.</p>
<h3 id="normalize">normalize</h3>
<pre><code class="language-py">torch.nn.functional.normalize(input, p=2, dim=1, eps=1e-12, out=None)
</code></pre>
<p>执行 <a href="http://latex.codecogs.com/gif.latex?L_p" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?L_p" alt=""></a> 规范化对于指定维度.</p>
<p>对于一个 tensor <code>input</code> 尺寸为 <a href="http://latex.codecogs.com/gif.latex?(n_0%2C ...%2C n_%7Bdim%7D%2C ...%2C n_k)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(n_0%2C ...%2C n_%7Bdim%7D%2C ...%2C n_k)" alt=""></a>, 每一 <a href="http://latex.codecogs.com/gif.latex?n_%7Bdim%7D" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?n_%7Bdim%7D" alt=""></a> -元素向量<a href="http://latex.codecogs.com/gif.latex?v" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?v" alt=""></a> 沿着维度 <code>dim</code> 被转换为</p>
<p><a href="http://latex.codecogs.com/gif.latex?%0D%0Av %3D %5Cfrac%7Bv%7D%7B%5Cmax(%5ClVert v %5CrVert_p%2C %5Cepsilon)%7D.%0D%0A%0D%0A" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%0D%0Av %3D %5Cfrac%7Bv%7D%7B%5Cmax(%5ClVert v %5CrVert_p%2C %5Cepsilon)%7D.%0D%0A%0D%0A" alt=""></a></p>
<p>对于默认参数, 它使用沿维度<a href="http://latex.codecogs.com/gif.latex?1" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?1" alt=""></a>的欧几里得范数进行标准化.</p>
<p>参数:</p>
<ul>
<li><strong>input</strong> – 任意形状的输入 tensor</li>
<li><strong>p</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – 范数公式中的指数值. 默认值: 2</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – 进行规约的维度. 默认值: 1</li>
<li><strong>eps</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – 避免除以零的小值. 默认值: 1e-12</li>
<li><strong>out</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>可选的</em>) – 输出 tensor. 如果 <code>out</code> 被设置, 此操作不可微分.</li>
</ul>
<h2 id="线性函数">线性函数</h2>
<h3 id="linear">linear</h3>
<pre><code class="language-py">torch.nn.functional.linear(input, weight, bias=None)
</code></pre>
<p>对传入数据应用线性转换: <a href="http://latex.codecogs.com/gif.latex?y %3D xA%5ET %2B b" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?y %3D xA%5ET %2B b" alt=""></a>.</p>
<p>形状:</p>
<blockquote>
<ul>
<li>Input: <a href="http://latex.codecogs.com/gif.latex?(N%2C *%2C in%5C_features)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(N%2C *%2C in%5C_features)" alt=""></a> <code>*</code> 表示任意数量的附加维度</li>
<li>Weight: <a href="http://latex.codecogs.com/gif.latex?(out%5C_features%2C in%5C_features)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(out%5C_features%2C in%5C_features)" alt=""></a></li>
<li>Bias: <a href="http://latex.codecogs.com/gif.latex?(out%5C_features)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(out%5C_features)" alt=""></a></li>
<li>Output: <a href="http://latex.codecogs.com/gif.latex?(N%2C *%2C out%5C_features)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(N%2C *%2C out%5C_features)" alt=""></a></li>
</ul>
</blockquote>
<h3 id="bilinear">bilinear</h3>
<pre><code class="language-py">torch.nn.functional.bilinear(input1, input2, weight, bias=None)
</code></pre>
<h2 id="dropout-函数">Dropout 函数</h2>
<h3 id="dropout">dropout</h3>
<pre><code class="language-py">torch.nn.functional.dropout(input, p=0.5, training=True, inplace=False)
</code></pre>
<p>在训练过程中, 使用伯努利分布的样本, 随机地用概率<code>p</code>将输入张量的一些元素归零.</p>
<p>请参见 <a href="#torch.nn.Dropout" title="torch.nn.Dropout"><code>Dropout</code></a>.</p>
<p>参数:</p>
<ul>
<li><strong>p</strong> – 清零概率. 默认值: 0.5</li>
<li><strong>training</strong> – 如果 <code>True</code> 使用 dropout. 默认值: <code>True</code></li>
<li><strong>inplace</strong> – 如果设置为 <code>True</code>, 将会原地操作. 默认值: <code>False</code></li>
</ul>
<h3 id="alpha-dropout">alpha_dropout</h3>
<pre><code class="language-py">torch.nn.functional.alpha_dropout(input, p=0.5, training=False, inplace=False)
</code></pre>
<p>应用 alpha dropout.</p>
<p>请参见 <a href="#torch.nn.AlphaDropout" title="torch.nn.AlphaDropout"><code>AlphaDropout</code></a>.</p>
<h3 id="dropout2d">dropout2d</h3>
<pre><code class="language-py">torch.nn.functional.dropout2d(input, p=0.5, training=True, inplace=False)
</code></pre>
<p>随机归零input tensor的整个通道 (一个通道是一个 2D 特征图, 例如, <a href="http://latex.codecogs.com/gif.latex?j" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?j" alt=""></a>-th channel of the <a href="http://latex.codecogs.com/gif.latex?i" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?i" alt=""></a>-th sample in the batched input is a 2D tensor <a href="http://latex.codecogs.com/gif.latex?%5Ctext%7Binput%7D%5Bi%2C j%5D" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%5Ctext%7Binput%7D%5Bi%2C j%5D" alt=""></a>)). 每次前向传递时, 每个信道都将被独立清零. 用概率 <code>p</code> 从 Bernoulli 分布采样.</p>
<p>请参见 <a href="#torch.nn.Dropout2d" title="torch.nn.Dropout2d"><code>Dropout2d</code></a>.</p>
<p>参数:</p>
<ul>
<li><strong>p</strong> – 通道清零的概率. 默认值: 0.5</li>
<li><strong>training</strong> – 使用 dropout 如果设为 <code>True</code>. 默认值: <code>True</code></li>
<li><strong>inplace</strong> – 如果设置为 <code>True</code>, 将会做原地操作. 默认值: <code>False</code></li>
</ul>
<h3 id="dropout3d">dropout3d</h3>
<pre><code class="language-py">torch.nn.functional.dropout3d(input, p=0.5, training=True, inplace=False)
</code></pre>
<p>随机归零input tensor的整个通道 (一个通道是一个 3D 特征图, 例如, the <a href="http://latex.codecogs.com/gif.latex?j" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?j" alt=""></a>-th channel of the <a href="http://latex.codecogs.com/gif.latex?i" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?i" alt=""></a>-th sample in the batched input is a 3D tensor <a href="http://latex.codecogs.com/gif.latex?%5Ctext%7Binput%7D%5Bi%2C j%5D" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%5Ctext%7Binput%7D%5Bi%2C j%5D" alt=""></a>). 每次前向传递时, 每个信道都将被独立清零. 用概率 <code>p</code> 从 Bernoulli 分布采样.</p>
<p>请参见 <a href="#torch.nn.Dropout3d" title="torch.nn.Dropout3d"><code>Dropout3d</code></a>.</p>
<p>参数:</p>
<ul>
<li><strong>p</strong> – 通道清零的概率. 默认值: 0.5</li>
<li><strong>training</strong> – 使用 dropout 如果设为 <code>True</code>. 默认值: <code>True</code></li>
<li><strong>inplace</strong> – 如果设置为 <code>True</code>, 将会做原地操作. 默认值: <code>False</code></li>
</ul>
<h2 id="稀疏函数">稀疏函数</h2>
<h3 id="embedding">embedding</h3>
<pre><code class="language-py">torch.nn.functional.embedding(input, weight, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False)
</code></pre>
<p>一个简单的查找表, 查找固定字典中的embedding(嵌入)内容和大小.</p>
<p>这个模块通常用于使用索引检索单词嵌入. 模块的输入是索引列表和嵌入矩阵, 输出是相应的单词嵌入.</p>
<p>请参见 <a href="#torch.nn.Embedding" title="torch.nn.Embedding"><code>torch.nn.Embedding</code></a>.</p>
<p>参数:</p>
<ul>
<li><strong>input</strong> (<em>LongTensor</em>) – 包含嵌入矩阵中的索引的tensor</li>
<li><strong>weight</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 嵌入矩阵的行数等于可能的最大索引数+ 1, 列数等于嵌入大小</li>
<li><strong>padding_idx</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>,</em> <em>可选的</em>) – 如果给定, 每当遇到索引时, 在<code>padding_idx</code> (初始化为零)用嵌入向量填充输出.</li>
<li><strong>max_norm</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>,</em> <em>可选的</em>) – 如果给定, 则将范数大于<code>max_norm</code>的每个嵌入向量重新规范化, 得到范数<code>max_norm</code>. 注意:这将修改适当的<code>weight</code>.</li>
<li><strong>norm_type</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>,</em> <em>可选的</em>) – 用于计算<code>max_norm</code>选项的p范数的p. 默认 <code>2</code>.</li>
<li><strong>scale_grad_by_freq</strong> (<em>boolean</em>_,_ <em>可选的</em>) – 如果给定, 这将通过小批处理中单词频率的倒数来缩放梯度. 默认 <code>False</code>.</li>
<li><strong>sparse</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>,</em> <em>可选的</em>) – If <code>True</code>, 梯度 w.r.t. <code>weight</code> 将会是一个稀疏 tensor. 请看 <a href="#torch.nn.Embedding" title="torch.nn.Embedding"><code>torch.nn.Embedding</code></a>有关稀疏梯度的更多详细信息.</li>
</ul>
<p>形状:</p>
<blockquote>
<ul>
<li>Input: 任意形状LongTensor, 包含要提取的索引的</li>
<li>Weight: 浮点型嵌入矩阵, 形状为 (V, embedding_dim), V = maximum index + 1 并且 embedding_dim = the embedding size</li>
<li>Output: <code>(*, embedding_dim)</code>, <code>*</code> 是输入形状</li>
</ul>
</blockquote>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; # a batch of 2 samples of 4 indices each
&gt;&gt;&gt; input = torch.tensor([[1,2,4,5],[4,3,2,9]])
&gt;&gt;&gt; # an embedding matrix containing 10 tensors of size 3
&gt;&gt;&gt; embedding_matrix = torch.rand(10, 3)
&gt;&gt;&gt; F.embedding(input, embedding_matrix)
tensor([[[ 0.8490,  0.9625,  0.6753],
 [ 0.9666,  0.7761,  0.6108],
 [ 0.6246,  0.9751,  0.3618],
 [ 0.4161,  0.2419,  0.7383]],

 [[ 0.6246,  0.9751,  0.3618],
 [ 0.0237,  0.7794,  0.0528],
 [ 0.9666,  0.7761,  0.6108],
 [ 0.3385,  0.8612,  0.1867]]])

&gt;&gt;&gt; # example with padding_idx
&gt;&gt;&gt; weights = torch.rand(10, 3)
&gt;&gt;&gt; weights[0, :].zero_()
&gt;&gt;&gt; embedding_matrix = weights
&gt;&gt;&gt; input = torch.tensor([[0,2,0,5]])
&gt;&gt;&gt; F.embedding(input, embedding_matrix, padding_idx=0)
tensor([[[ 0.0000,  0.0000,  0.0000],
 [ 0.5609,  0.5384,  0.8720],
 [ 0.0000,  0.0000,  0.0000],
 [ 0.6262,  0.2438,  0.7471]]])

</code></pre>
<h3 id="embedding-bag">embedding_bag</h3>
<pre><code class="language-py">torch.nn.functional.embedding_bag(input, weight, offsets=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, mode='mean', sparse=False)
</code></pre>
<p>计算嵌入<code>bags</code>的和、平均值或最大值, 而不实例化中间嵌入. .</p>
<p>请参见 <a href="#torch.nn.EmbeddingBag" title="torch.nn.EmbeddingBag"><code>torch.nn.EmbeddingBag</code></a></p>
<p>参数:</p>
<ul>
<li><strong>input</strong> (<em>LongTensor</em>) – 包含索引<code>bags</code>的张量</li>
<li><strong>weight</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 嵌入矩阵的行数等于可能的最大索引数+ 1, 列数等于嵌入大小</li>
<li><strong>offsets</strong> (<em>LongTensor</em>_,_ <em>可选的</em>) – 仅当<code>input</code>为1D时使用. <code>offsets</code>确定输入中每个<code>bag</code>(序列)的起始索引位置</li>
<li><strong>max_norm</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>,</em> <em>可选的</em>) – 如果给定, 范数大于<code>max_norm</code>的每个嵌入向量将被重新规格化为范数<code>max_norm</code>. 注意:这将就地修改<code>weight</code></li>
<li><strong>norm_type</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>,</em> <em>可选的</em>) – The <code>p</code> in the <code>p</code>-norm to compute for the <code>max_norm</code> option. 默认 <code>2</code>.</li>
<li><strong>scale_grad_by_freq</strong> (<em>boolean</em>_,_ <em>可选的</em>) – 如果给定, 这将通过小批处理中单词频率的倒数来缩放梯度. 默认假. 注意:当<code>mode="max"</code>时不支持此选项.</li>
<li><strong>mode</strong> (<em>string</em>_,_ <em>可选的</em>) – <code>"sum"</code>, <code>"mean"</code> or <code>"max"</code>. 指定reduce<code>bag</code>的方法. 默认值: <code>"mean"</code></li>
<li><strong>sparse</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>,</em> <em>可选的</em>) – 如果<code>True</code>, 梯度w.r.t.权值就是一个稀疏张量.请参见 <a href="#torch.nn.Embedding" title="torch.nn.Embedding"><code>torch.nn.Embedding</code></a> 关于稀疏梯度. 注意: 此选项不支持 <code>mode="max"</code>.</li>
</ul>
<p>形状:</p>
<blockquote>
<ul>
<li> <p><code>input</code> (LongTensor) 和 <code>offsets</code> (LongTensor, 可选的)</p>
<ul>
<li> <p>如果 <code>input</code> 是 2D的, 形状为 <code>B x N</code>,</p> <p>它将被视为每个固定长度<code>N</code>的<code>B</code>个bag(序列), 这将根据模式以某种方式返回<code>B</code>个聚合值. 在本例中, <code>offsets</code>被忽略, 并且要求为<code>None</code></p> </li>
<li> <p>如果 <code>input</code> 是 1D的, 形状为 <code>N</code>,</p> <p>it will be treated as a concatenation of multiple bags (sequences). <code>offsets</code> is required to be a 1D tensor containing the starting index positions of each bag in <code>input</code>. Therefore, for <code>offsets</code> of shape <code>B</code>, <code>input</code> will be viewed as having <code>B</code> bags. Empty bags (i.e., having 0-length) will have returned vectors filled by zeros. 它将被视为多个<code>bag</code>(序列)的串联. <code>offsets</code>必须是一个一维tensor, 其中包含<code>input</code>中每个<code>bag</code>的起始索引位置. 因此, 对于形状<code>B</code>的偏移量, 输入将被视为有<code>B</code>个bag. 空bags( 即, 具有0长度)将返回由0填充的向量</p> </li>
</ul> </li>
<li> <p><code>weight</code> (Tensor): 模块的可学习权重, 形状 <code>(num_embeddings x embedding_dim)</code></p> </li>
<li> <p><code>output</code>: 聚合的嵌入值, 形状 <code>B x embedding_dim</code></p> </li>
</ul>
</blockquote>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; # an Embedding module containing 10 tensors of size 3
&gt;&gt;&gt; embedding_matrix = torch.rand(10, 3)
&gt;&gt;&gt; # a batch of 2 samples of 4 indices each
&gt;&gt;&gt; input = torch.tensor([1,2,4,5,4,3,2,9])
&gt;&gt;&gt; offsets = torch.tensor([0,4])
&gt;&gt;&gt; F.embedding_bag(embedding_matrix, input, offsets)
tensor([[ 0.3397,  0.3552,  0.5545],
 [ 0.5893,  0.4386,  0.5882]])

</code></pre>
<h2 id="距离函数">距离函数</h2>
<h3 id="pairwise-distance">pairwise_distance</h3>
<pre><code class="language-py">torch.nn.functional.pairwise_distance(x1, x2, p=2.0, eps=1e-06, keepdim=False)
</code></pre>
<p>请参见 <a href="#torch.nn.PairwiseDistance" title="torch.nn.PairwiseDistance"><code>torch.nn.PairwiseDistance</code></a></p>
<h3 id="cosine-similarity">cosine_similarity</h3>
<pre><code class="language-py">torch.nn.functional.cosine_similarity(x1, x2, dim=1, eps=1e-8) → Tensor
</code></pre>
<p>返回x1和x2之间的余弦相似度, 沿dim计算</p>
<p><a href="http://latex.codecogs.com/gif.latex?%0D%0A%5Ctext%7Bsimilarity%7D %3D %5Cdfrac%7Bx_1 %5Ccdot x_2%7D%7B%5Cmax(%5CVert x_1 %5CVert _2 %5Ccdot %5CVert x_2 %5CVert _2%2C %5Cepsilon)%7D%0D%0A%0D%0A" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%0D%0A%5Ctext%7Bsimilarity%7D %3D %5Cdfrac%7Bx_1 %5Ccdot x_2%7D%7B%5Cmax(%5CVert x_1 %5CVert _2 %5Ccdot %5CVert x_2 %5CVert _2%2C %5Cepsilon)%7D%0D%0A%0D%0A" alt=""></a></p>
<p>参数:</p>
<ul>
<li><strong>x1</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 第一个input.</li>
<li><strong>x2</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 第二个input (大小和 x1 匹配).</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>,</em> <em>可选的</em>) – 维度. 默认值: 1</li>
<li><strong>eps</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>,</em> <em>可选的</em>) – 非常小的值避免除以0. 默认值: 1e-8</li>
</ul>
<pre><code class="language-py">形状:
</code></pre>
<ul>
<li>Input: <a href="http://latex.codecogs.com/gif.latex?(%5Cast_1%2C D%2C %5Cast_2)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(%5Cast_1%2C D%2C %5Cast_2)" alt=""></a> where D is at position <code>dim</code>.</li>
<li>Output: <a href="http://latex.codecogs.com/gif.latex?(%5Cast_1%2C %5Cast_2)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(%5Cast_1%2C %5Cast_2)" alt=""></a> where 1 is at position <code>dim</code>.</li>
</ul>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; input1 = torch.randn(100, 128)
&gt;&gt;&gt; input2 = torch.randn(100, 128)
&gt;&gt;&gt; output = F.cosine_similarity(input1, input2)
&gt;&gt;&gt; print(output)

</code></pre>
<h3 id="pdist">pdist</h3>
<pre><code class="language-py">torch.nn.functional.pdist(input, p=2) → Tensor
</code></pre>
<p>计算输入中每​​对行向量之间的p范数距离. 这与<code>torch.norm(input[:, None] - input, dim=2, p=p)</code>的上三角形部分（不包括对角线）相同. 如果行是连续的, 则此函数将更快</p>
<p>如果输入具有形状 <a href="http://latex.codecogs.com/gif.latex?N %5Ctimes M" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?N %5Ctimes M" alt=""></a> 则输出将具有形状 <a href="http://latex.codecogs.com/gif.latex?%5Cfrac%7B1%7D%7B2%7D N (N - 1)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%5Cfrac%7B1%7D%7B2%7D N (N - 1)" alt=""></a>.</p>
<p>这个函数相当于 <code>scipy.spatial.distance.pdist(input, ‘minkowski’, p=p)</code> 如果 <a href="http://latex.codecogs.com/gif.latex?p %5Cin (0%2C %5Cinfty)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?p %5Cin (0%2C %5Cinfty)" alt=""></a>. 当 <a href="http://latex.codecogs.com/gif.latex?p %3D 0" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?p %3D 0" alt=""></a> 它等价于 <code>scipy.spatial.distance.pdist(input, ‘hamming’) * M</code>. 当 <a href="http://latex.codecogs.com/gif.latex?p %3D %5Cinfty" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?p %3D %5Cinfty" alt=""></a>, 最相近的scipy函数是 <code>scipy.spatial.distance.pdist(xn, lambda x, y: np.abs(x - y).max())</code>.</p>
<p>参数:</p>
<ul>
<li><strong>input</strong> – 输入tensor, 形状为 <a href="http://latex.codecogs.com/gif.latex?N %5Ctimes M" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?N %5Ctimes M" alt=""></a>.</li>
<li><strong>p</strong> – 计算每个向量对之间的p范数距离的p值 <a href="http://latex.codecogs.com/gif.latex?%5Cin %5B0%2C %5Cinfty%5D" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%5Cin %5B0%2C %5Cinfty%5D" alt=""></a>.</li>
</ul>
<h2 id="损失函数">损失函数</h2>
<h3 id="binary-cross-entropy">binary_cross_entropy</h3>
<pre><code class="language-py">torch.nn.functional.binary_cross_entropy(input, target, weight=None, size_average=None, reduce=None, reduction='mean')
</code></pre>
<p>计算目标和输出之间二进制交叉熵的函数.</p>
<p>请参见 <a href="#torch.nn.BCELoss" title="torch.nn.BCELoss"><code>BCELoss</code></a>.</p>
<p>参数:</p>
<ul>
<li><strong>input</strong> – 任意形状的tensor</li>
<li><strong>target</strong> – 与输入形状相同的tensor</li>
<li><strong>weight</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>可选的</em>) – 手动重新调整weight, 如果提供, 它重复来匹配输入张量的形状</li>
<li><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>,</em> <em>可选的</em>) – 废弃的 (见 <code>reduction</code>). 默认情况下, 批处理中的每个损失元素的平均损失. 注意, 对于某些损失, 每个样本有多个元素. 如果<code>size_average</code>设置为<code>False</code>, 则对每个小批的损失进行汇总. reduce为False时忽略. 默认值: <code>True</code></li>
<li><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>,</em> <em>可选的</em>) – 废弃的 (见 <code>reduction</code>). 默认情况下, 根据size_average, 对每个小批量的观察结果的损失进行平均或求和. 当reduce为False时, 返回每批元素的损失并忽略<code>size_average</code>. 默认值: <code>True</code></li>
<li><strong>reduction</strong> (<em>string</em>_,_ <em>可选的</em>) – 指定要应用于输出的<code>reduction</code>：'none'| 'mean'| 'sum'. 'none'：没有reduction, 'mean'：输出的总和将除以输出中的元素数量 'sum'：输出将被求和. 注意：<code>size_average</code>和<code>reduce</code>正在被弃用, 同时, 指定这两个args中的任何一个都将覆盖reduce. 默认值：'mean', 默认值: ‘mean’</li>
</ul>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; input = torch.randn((3, 2), requires_grad=True)
&gt;&gt;&gt; target = torch.rand((3, 2), requires_grad=False)
&gt;&gt;&gt; loss = F.binary_cross_entropy(F.sigmoid(input), target)
&gt;&gt;&gt; loss.backward()

</code></pre>
<h3 id="binary-cross-entropy-with-logits">binary_cross_entropy_with_logits</h3>
<pre><code class="language-py">torch.nn.functional.binary_cross_entropy_with_logits(input, target, weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None)
</code></pre>
<p>计算目标和输出logits之间的二进制交叉熵的函数.</p>
<p>请参见 <a href="#torch.nn.BCEWithLogitsLoss" title="torch.nn.BCEWithLogitsLoss"><code>BCEWithLogitsLoss</code></a>.</p>
<p>参数:</p>
<ul>
<li><strong>input</strong> – 任意形状的tensor</li>
<li><strong>target</strong> – 与输入形状相同的tensor</li>
<li><strong>weight</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>可选的</em>) – 手动重新调整weight, 如果提供, 它重复来匹配输入张量的形状</li>
<li><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>,</em> <em>可选的</em>) – 废弃的 (见 <code>reduction</code>). 默认情况下, 批处理中的每个损失元素的平均损失. 注意, 对于某些损失, 每个样本有多个元素. 如果<code>size_average</code>设置为<code>False</code>, 则对每个小批的损失进行汇总. reduce为False时忽略. 默认值: <code>True</code></li>
<li><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>,</em> <em>可选的</em>) – 废弃的 (见 <code>reduction</code>). 默认情况下, 根据size_average, 对每个小批量的观察结果的损失进行平均或求和. 当reduce为False时, 返回每批元素的损失并忽略<code>size_average</code>. 默认值: <code>True</code></li>
<li><strong>reduction</strong> (<em>string</em>_,_ <em>可选的</em>) – 指定要应用于输出的<code>reduction</code>：'none'| 'mean'| 'sum'. 'none'：没有reduction, 'mean'：输出的总和将除以输出中的元素数量 'sum'：输出将被求和. 注意：<code>size_average</code>和<code>reduce</code>正在被弃用, 同时, 指定这两个args中的任何一个都将覆盖reduce. 默认值：'mean', 默认值: ‘mean’</li>
<li><strong>pos_weight</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>可选的</em>) – 正例样本的权重. 必须是长度等于类数的向量.</li>
</ul>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; input = torch.randn(3, requires_grad=True)
&gt;&gt;&gt; target = torch.empty(3).random_(2)
&gt;&gt;&gt; loss = F.binary_cross_entropy_with_logits(input, target)
&gt;&gt;&gt; loss.backward()

</code></pre>
<h3 id="poisson-nll-loss">poisson_nll_loss</h3>
<pre><code class="language-py">torch.nn.functional.poisson_nll_loss(input, target, log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction='mean')
</code></pre>
<p>泊松负对数似然损失.</p>
<p>请参见 <a href="#torch.nn.PoissonNLLLoss" title="torch.nn.PoissonNLLLoss"><code>PoissonNLLLoss</code></a>.</p>
<p>参数:</p>
<ul>
<li><strong>input</strong> – 潜在泊松分布的期望.</li>
<li><strong>target</strong> – 随机抽样 <a href="http://latex.codecogs.com/gif.latex?target %5Csim %5Ctext%7BPoisson%7D(input)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?target %5Csim %5Ctext%7BPoisson%7D(input)" alt=""></a>.</li>
<li><strong>log_input</strong> – 如果为<code>True</code>, 则损失计算为 <a href="http://latex.codecogs.com/gif.latex?%5Cexp(%5Ctext%7Binput%7D) - %5Ctext%7Btarget%7D * %5Ctext%7Binput%7D" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%5Cexp(%5Ctext%7Binput%7D) - %5Ctext%7Btarget%7D * %5Ctext%7Binput%7D" alt=""></a>, 如果为<code>False</code>, 则损失计算为 <a href="http://latex.codecogs.com/gif.latex?%5Ctext%7Binput%7D - %5Ctext%7Btarget%7D * %5Clog(%5Ctext%7Binput%7D%2B%5Ctext%7Beps%7D)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%5Ctext%7Binput%7D - %5Ctext%7Btarget%7D * %5Clog(%5Ctext%7Binput%7D%2B%5Ctext%7Beps%7D)" alt=""></a>. 默认值: <code>True</code></li>
<li><strong>full</strong> – 是否计算全部损失, 即. 加入Stirling近似项. 默认值: <code>False</code> <a href="http://latex.codecogs.com/gif.latex?%5Ctext%7Btarget%7D * %5Clog(%5Ctext%7Btarget%7D) - %5Ctext%7Btarget%7D %2B 0.5 * %5Clog(2 * %5Cpi * %5Ctext%7Btarget%7D)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%5Ctext%7Btarget%7D * %5Clog(%5Ctext%7Btarget%7D) - %5Ctext%7Btarget%7D %2B 0.5 * %5Clog(2 * %5Cpi * %5Ctext%7Btarget%7D)" alt=""></a>.</li>
<li><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>,</em> <em>可选的</em>) – 废弃的 (见 <code>reduction</code>). 默认情况下, 批处理中的每个损失元素的平均损失. 注意, 对于某些损失, 每个样本有多个元素. 如果<code>size_average</code>设置为<code>False</code>, 则对每个小批的损失进行汇总. reduce为False时忽略. 默认值: <code>True</code></li>
<li><strong>eps</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>,</em> <em>可选的</em>) – 一个小值避免求值 <a href="http://latex.codecogs.com/gif.latex?%5Clog(0)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%5Clog(0)" alt=""></a> when <code>log_input</code>=<code>False</code>. 默认值: 1e-8</li>
<li><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>,</em> <em>可选的</em>) – 废弃的 (见 <code>reduction</code>). 默认情况下, 根据size_average, 对每个小批量的观察结果的损失进行平均或求和. 当reduce为False时, 返回每批元素的损失并忽略<code>size_average</code>. 默认值: <code>True</code></li>
<li><strong>reduction</strong> (<em>string</em>_,_ <em>可选的</em>) – 指定要应用于输出的<code>reduction</code>：'none'| 'mean'| 'sum'. 'none'：没有reduction, 'mean'：输出的总和将除以输出中的元素数量 'sum'：输出将被求和. 注意：<code>size_average</code>和<code>reduce</code>正在被弃用, 同时, 指定这两个args中的任何一个都将覆盖reduce. 默认值：'mean', 默认值: ‘mean’</li>
</ul>
<h3 id="cosine-embedding-loss">cosine_embedding_loss</h3>
<pre><code class="language-py">torch.nn.functional.cosine_embedding_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') → Tensor
</code></pre>
<p>请参见 <a href="#torch.nn.CosineEmbeddingLoss" title="torch.nn.CosineEmbeddingLoss"><code>CosineEmbeddingLoss</code></a>.</p>
<h3 id="cross-entropy">cross_entropy</h3>
<pre><code class="language-py">torch.nn.functional.cross_entropy(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')
</code></pre>
<p>此函数结合了 <code>log_softmax</code> 和 <code>nll_loss</code>.</p>
<p>请参见 <a href="#torch.nn.CrossEntropyLoss" title="torch.nn.CrossEntropyLoss"><code>CrossEntropyLoss</code></a>.</p>
<p>参数:</p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – <a href="http://latex.codecogs.com/gif.latex?(N%2C C)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(N%2C C)" alt=""></a> where <code>C = number of classes</code> or <a href="http://latex.codecogs.com/gif.latex?(N%2C C%2C H%2C W)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(N%2C C%2C H%2C W)" alt=""></a> in case of 2D Loss, or <a href="http://latex.codecogs.com/gif.latex?(N%2C C%2C d_1%2C d_2%2C ...%2C d_K)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(N%2C C%2C d_1%2C d_2%2C ...%2C d_K)" alt=""></a> where <a href="http://latex.codecogs.com/gif.latex?K %3E 1" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?K %3E 1" alt=""></a> in the case of K-dimensional loss.</li>
<li><strong>target</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – <a href="http://latex.codecogs.com/gif.latex?(N)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(N)" alt=""></a> where each value is <a href="http://latex.codecogs.com/gif.latex?0 %5Cleq %5Ctext%7Btargets%7D%5Bi%5D %5Cleq C-1" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?0 %5Cleq %5Ctext%7Btargets%7D%5Bi%5D %5Cleq C-1" alt=""></a>, or <a href="http://latex.codecogs.com/gif.latex?(N%2C d_1%2C d_2%2C ...%2C d_K)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(N%2C d_1%2C d_2%2C ...%2C d_K)" alt=""></a> where <a href="http://latex.codecogs.com/gif.latex?K %5Cgeq 1" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?K %5Cgeq 1" alt=""></a> for K-dimensional loss.</li>
<li><strong>weight</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>可选的</em>) – 给每个类别的手动重定权重. 如果给定, 必须是大小为<code>C</code>的tensor</li>
<li><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>,</em> <em>可选的</em>) – 废弃的 (见 <code>reduction</code>). 默认情况下, 批处理中的每个损失元素的平均损失. 注意, 对于某些损失, 每个样本有多个元素. 如果<code>size_average</code>设置为<code>False</code>, 则对每个小批的损失进行汇总. reduce为False时忽略. 默认值: <code>True</code></li>
<li><strong>ignore_index</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>,</em> <em>可选的</em>) – Specifies a target value that is ignored and does not contribute to the input gradient. When <code>size_average</code> is <code>True</code>, the loss is averaged over non-ignored targets. 默认值: -100</li>
<li><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>,</em> <em>可选的</em>) – 废弃的 (见 <code>reduction</code>). 默认情况下, 根据size_average, 对每个小批量的观察结果的损失进行平均或求和. 当reduce为False时, 返回每批元素的损失并忽略<code>size_average</code>. 默认值: <code>True</code></li>
<li><strong>reduction</strong> (<em>string</em>_,_ <em>可选的</em>) – 指定要应用于输出的<code>reduction</code>：'none'| 'mean'| 'sum'. 'none'：没有reduction, 'mean'：输出的总和将除以输出中的元素数量 'sum'：输出将被求和. 注意：<code>size_average</code>和<code>reduce</code>正在被弃用, 同时, 指定这两个args中的任何一个都将覆盖reduce. 默认值：'mean', 默认值: ‘mean’</li>
</ul>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)
&gt;&gt;&gt; target = torch.randint(5, (3,), dtype=torch.int64)
&gt;&gt;&gt; loss = F.cross_entropy(input, target)
&gt;&gt;&gt; loss.backward()

</code></pre>
<h3 id="ctc-loss">ctc_loss</h3>
<pre><code class="language-py">torch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank=0, reduction='mean')
</code></pre>
<p>Connectionist Temporal Classification损失.</p>
<p>请参见 <a href="#torch.nn.CTCLoss" title="torch.nn.CTCLoss"><code>CTCLoss</code></a>.</p>
<p>注意</p>
<p>在某些情况下, 当使用CUDA后端与CuDNN时, 该操作符可能会选择不确定性算法来提高性能. 如果这不是您希望的, 您可以通过设置<code>torch.backends.cudn .deterministic = True</code>来尝试使操作具有确定性(可能会以性能为代价). 请参阅关于 <a href="notes/randomness.html">Reproducibility</a> 了解背景.</p>
<p>注意</p>
<p>当使用CUDA后端时, 此操作可能会导致不确定的向后行为, 并且不容易关闭. 请参阅关于<a href="notes/randomness.html">Reproducibility</a>的注释.</p>
<p>参数:</p>
<ul>
<li><strong>log_probs</strong> – <a href="http://latex.codecogs.com/gif.latex?(T%2C N%2C C)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(T%2C N%2C C)" alt=""></a> where <code>C = number of characters in alphabet including blank</code>, <code>T = input length</code>, and <code>N = batch size</code>. The logarithmized probabilities of the outputs (e.g. obtained with <a href="#torch.nn.functional.log_softmax" title="torch.nn.functional.log_softmax"><code>torch.nn.functional.log_softmax()</code></a>).</li>
<li><strong>targets</strong> – <a href="http://latex.codecogs.com/gif.latex?(N%2C S)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(N%2C S)" alt=""></a> or <code>(sum(target_lengths))</code>. Targets (cannot be blank). In the second form, the targets are assumed to be concatenated.</li>
<li><strong>input_lengths</strong> – <a href="http://latex.codecogs.com/gif.latex?(N)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(N)" alt=""></a>. Lengths of the inputs (must each be <a href="http://latex.codecogs.com/gif.latex?%5Cleq T" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%5Cleq T" alt=""></a>)</li>
<li><strong>target_lengths</strong> – <a href="http://latex.codecogs.com/gif.latex?(N)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(N)" alt=""></a>. Lengths of the targets</li>
<li><strong>blank</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>,</em> <em>可选的</em>) – Blank label. 默认 <a href="http://latex.codecogs.com/gif.latex?0" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?0" alt=""></a>.</li>
<li><strong>reduction</strong> (<em>string</em>_,_ <em>可选的</em>) - 指定要应用于输出的<code>reduction</code>：'none'| 'mean'| 'sum'. 'none'：不会应用<code>reduce</code>, 'mean'：输出损失将除以目标长度, 然后得到批次的平均值. 默认值：'mean'</li>
</ul>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; log_probs = torch.randn(50, 16, 20).log_softmax(2).detach().requires_grad_()
&gt;&gt;&gt; targets = torch.randint(1, 20, (16, 30), dtype=torch.long)
&gt;&gt;&gt; input_lengths = torch.full((16,), 50, dtype=torch.long)
&gt;&gt;&gt; target_lengths = torch.randint(10,30,(16,), dtype=torch.long)
&gt;&gt;&gt; loss = F.ctc_loss(log_probs, targets, input_lengths, target_lengths)
&gt;&gt;&gt; loss.backward()

</code></pre>
<h3 id="hinge-embedding-loss">hinge_embedding_loss</h3>
<pre><code class="language-py">torch.nn.functional.hinge_embedding_loss(input, target, margin=1.0, size_average=None, reduce=None, reduction='mean') → Tensor
</code></pre>
<p>请参见 <a href="#torch.nn.HingeEmbeddingLoss" title="torch.nn.HingeEmbeddingLoss"><code>HingeEmbeddingLoss</code></a>.</p>
<h3 id="kl-div">kl_div</h3>
<pre><code class="language-py">torch.nn.functional.kl_div(input, target, size_average=None, reduce=None, reduction='mean')
</code></pre>
<p><a href="https://en.wikipedia.org/wiki/Kullback-Leibler_divergence">Kullback-Leibler divergence</a> 损失.</p>
<p>请参见 <a href="#torch.nn.KLDivLoss" title="torch.nn.KLDivLoss"><code>KLDivLoss</code></a>.</p>
<p>参数:</p>
<ul>
<li><strong>input</strong> – Tensor of arbitrary shape</li>
<li><strong>target</strong> – Tensor of the same shape as input</li>
<li><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>,</em> <em>可选的</em>) – 废弃的 (见 <code>reduction</code>). 默认情况下, 批处理中的每个损失元素的平均损失. 注意, 对于某些损失, 每个样本有多个元素. 如果<code>size_average</code>设置为<code>False</code>, 则对每个小批的损失进行汇总. reduce为False时忽略. 默认值: <code>True</code></li>
<li><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>,</em> <em>可选的</em>) – 废弃的 (见 <code>reduction</code>). 默认情况下, 根据size_average, 对每个小批量的观察结果的损失进行平均或求和. 当reduce为False时, 返回每批元素的损失并忽略<code>size_average</code>. 默认值: <code>True</code></li>
<li><strong>reduction</strong> (<em>string</em>_,_ <em>可选的</em>) – 指定要应用于输出的缩减：'none'| 'batchmean'| 'sum'| 'mean'. 'none'：不会应用<code>reduction</code> 'batchmean'：输出的总和将除以batchsize 'sum'：输出将被加总 'mean'：输出将除以输出中的元素数 默认值：'mean'</li>
</ul>
<p>:param . .注::<code>size average</code>和<code>reduce</code>正在被弃用, :同时, 指定这两个arg中的一个将覆盖reduce. :param . .注意::<code>reduce = mean</code>不返回真实的kl散度值, 请使用:<code>reduce = batchmean</code>, 它符合kl的数学定义.</p>
<blockquote>
<p>在下一个主要版本中, “mean”将被修改为与“batchmean”相同.</p>
</blockquote>
<h3 id="l1-loss">l1_loss</h3>
<pre><code class="language-py">torch.nn.functional.l1_loss(input, target, size_average=None, reduce=None, reduction='mean') → Tensor
</code></pre>
<p>函数的作用是求元素的绝对值的平均值.</p>
<p>请参见 <a href="#torch.nn.L1Loss" title="torch.nn.L1Loss"><code>L1Loss</code></a>.</p>
<h3 id="mse-loss">mse_loss</h3>
<pre><code class="language-py">torch.nn.functional.mse_loss(input, target, size_average=None, reduce=None, reduction='mean') → Tensor
</code></pre>
<p>测量元素的均方误差.</p>
<p>请参见 <a href="#torch.nn.MSELoss" title="torch.nn.MSELoss"><code>MSELoss</code></a>.</p>
<h3 id="margin-ranking-loss">margin_ranking_loss</h3>
<pre><code class="language-py">torch.nn.functional.margin_ranking_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') → Tensor
</code></pre>
<p>请参见 <a href="#torch.nn.MarginRankingLoss" title="torch.nn.MarginRankingLoss"><code>MarginRankingLoss</code></a>.</p>
<h3 id="multilabel-margin-loss">multilabel_margin_loss</h3>
<pre><code class="language-py">torch.nn.functional.multilabel_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') → Tensor
</code></pre>
<p>请参见 <a href="#torch.nn.MultiLabelMarginLoss" title="torch.nn.MultiLabelMarginLoss"><code>MultiLabelMarginLoss</code></a>.</p>
<h3 id="multilabel-soft-margin-loss">multilabel_soft_margin_loss</h3>
<pre><code class="language-py">torch.nn.functional.multilabel_soft_margin_loss(input, target, weight=None, size_average=None) → Tensor
</code></pre>
<p>请参见 <a href="#torch.nn.MultiLabelSoftMarginLoss" title="torch.nn.MultiLabelSoftMarginLoss"><code>MultiLabelSoftMarginLoss</code></a>.</p>
<h3 id="multi-margin-loss">multi_margin_loss</h3>
<pre><code class="language-py">torch.nn.functional.multi_margin_loss(input, target, p=1, margin=1.0, weight=None, size_average=None, reduce=None, reduction='mean')
</code></pre>
<pre><code class="language-py">multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,
</code></pre>
<p>reduce=None, reduction=’mean’) -&gt; Tensor</p>
<p>请参见 <a href="#torch.nn.MultiMarginLoss" title="torch.nn.MultiMarginLoss"><code>MultiMarginLoss</code></a>.</p>
<h3 id="nll-loss">nll_loss</h3>
<pre><code class="language-py">torch.nn.functional.nll_loss(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')
</code></pre>
<p>负的对数似然函数.</p>
<p>请参见 <a href="#torch.nn.NLLLoss" title="torch.nn.NLLLoss"><code>NLLLoss</code></a>.</p>
<p>参数:</p>
<ul>
<li><strong>input</strong> – <a href="http://latex.codecogs.com/gif.latex?(N%2C C)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(N%2C C)" alt=""></a> <code>C = 类别的数量</code> 或者 <a href="http://latex.codecogs.com/gif.latex?(N%2C C%2C H%2C W)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(N%2C C%2C H%2C W)" alt=""></a> 在二维损失的情况下, 或者 <a href="http://latex.codecogs.com/gif.latex?(N%2C C%2C d_1%2C d_2%2C ...%2C d_K)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(N%2C C%2C d_1%2C d_2%2C ...%2C d_K)" alt=""></a> <a href="http://latex.codecogs.com/gif.latex?K %3E 1" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?K %3E 1" alt=""></a> 在K维损失的情况下.</li>
<li><strong>target</strong> – <a href="http://latex.codecogs.com/gif.latex?(N)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(N)" alt=""></a> 每个值是 <a href="http://latex.codecogs.com/gif.latex?0 %5Cleq %5Ctext%7Btargets%7D%5Bi%5D %5Cleq C-1" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?0 %5Cleq %5Ctext%7Btargets%7D%5Bi%5D %5Cleq C-1" alt=""></a>, 或者 <a href="http://latex.codecogs.com/gif.latex?(N%2C d_1%2C d_2%2C ...%2C d_K)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(N%2C d_1%2C d_2%2C ...%2C d_K)" alt=""></a> <a href="http://latex.codecogs.com/gif.latex?K %5Cgeq 1" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?K %5Cgeq 1" alt=""></a> K维损失.</li>
<li><strong>weight</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>,</em> <em>可选的</em>) – 给每个类别的手动重定权重. 如果给定, 必须是大小为<code>C</code>的tensor</li>
<li><strong>size_average</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>,</em> <em>可选的</em>) – 废弃的 (见 <code>reduction</code>). 默认情况下, 批处理中的每个损失元素的平均损失. 注意, 对于某些损失, 每个样本有多个元素. 如果<code>size_average</code>设置为<code>False</code>, 则对每个小批的损失进行汇总. reduce为False时忽略. 默认值: <code>True</code></li>
<li><strong>ignore_index</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>,</em> <em>可选的</em>) – 指定一个被忽略的目标值, 该值不会影响输入梯度. 当<code>size_average</code>为<code>True</code>时, 损耗在未忽略的目标上平均. 默认值: -100</li>
<li><strong>reduce</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>,</em> <em>可选的</em>) – 废弃的 (见 <code>reduction</code>). 默认情况下, 根据size_average, 对每个小批量的观察结果的损失进行平均或求和. 当reduce为False时, 返回每批元素的损失并忽略<code>size_average</code>. 默认值: <code>True</code></li>
<li><strong>reduction</strong> (<em>string</em>_,_ <em>可选的</em>) – 指定要应用于输出的<code>reduction</code>：'none'| 'mean'| 'sum'. 'none'：没有reduction, 'mean'：输出的总和将除以输出中的元素数量 'sum'：输出将被求和. 注意：<code>size_average</code>和<code>reduce</code>正在被弃用, 同时, 指定这两个args中的任何一个都将覆盖reduce. 默认值：'mean', 默认值: ‘mean’</li>
</ul>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; # input is of size N x C = 3 x 5
&gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)
&gt;&gt;&gt; # each element in target has to have 0 &lt;= value &lt; C
&gt;&gt;&gt; target = torch.tensor([1, 0, 4])
&gt;&gt;&gt; output = F.nll_loss(F.log_softmax(input), target)
&gt;&gt;&gt; output.backward()

</code></pre>
<h3 id="smooth-l1-loss">smooth_l1_loss</h3>
<pre><code class="language-py">torch.nn.functional.smooth_l1_loss(input, target, size_average=None, reduce=None, reduction='mean')
</code></pre>
<p>如果绝对元素误差低于1, 则使用平方项, 否则使用L1项的函数.</p>
<p>请参见 <a href="#torch.nn.SmoothL1Loss" title="torch.nn.SmoothL1Loss"><code>SmoothL1Loss</code></a>.</p>
<h3 id="soft-margin-loss">soft_margin_loss</h3>
<pre><code class="language-py">torch.nn.functional.soft_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') → Tensor
</code></pre>
<p>请参见 <a href="#torch.nn.SoftMarginLoss" title="torch.nn.SoftMarginLoss"><code>SoftMarginLoss</code></a>.</p>
<h3 id="triplet-margin-loss">triplet_margin_loss</h3>
<pre><code class="language-py">torch.nn.functional.triplet_margin_loss(anchor, positive, negative, margin=1.0, p=2, eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean')
</code></pre>
<p>请参见 <a href="#torch.nn.TripletMarginLoss" title="torch.nn.TripletMarginLoss"><code>TripletMarginLoss</code></a></p>
<h2 id="视觉函数">视觉函数</h2>
<h3 id="pixel-shuffle">pixel_shuffle</h3>
<pre><code class="language-py">torch.nn.functional.pixel_shuffle()
</code></pre>
<p>重新排列tensor中的元素, 从形状 <a href="http://latex.codecogs.com/gif.latex?(*%2C C %5Ctimes r%5E2%2C H%2C W)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(*%2C C %5Ctimes r%5E2%2C H%2C W)" alt=""></a> 到 <a href="http://latex.codecogs.com/gif.latex?(C%2C H %5Ctimes r%2C W %5Ctimes r)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(C%2C H %5Ctimes r%2C W %5Ctimes r)" alt=""></a>.</p>
<p>请参见 <a href="#torch.nn.PixelShuffle" title="torch.nn.PixelShuffle"><code>PixelShuffle</code></a>.</p>
<p>参数:</p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) –输入 tensor</li>
<li><strong>upscale_factor</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – 提高空间解析度的参数</li>
</ul>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; input = torch.randn(1, 9, 4, 4)
&gt;&gt;&gt; output = torch.nn.functional.pixel_shuffle(input, 3)
&gt;&gt;&gt; print(output.size())
torch.Size([1, 1, 12, 12])

</code></pre>
<h3 id="pad">pad</h3>
<pre><code class="language-py">torch.nn.functional.pad(input, pad, mode='constant', value=0)
</code></pre>
<p>填充 tensor.</p>
<pre><code class="language-py">Pading size:
</code></pre>
<p>要填充的维度数为 <a href="http://latex.codecogs.com/gif.latex?%5Cleft%5Clfloor%5Cfrac%7B%5Ctext%7Blen(pad)%7D%7D%7B2%7D%5Cright%5Crfloor" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%5Cleft%5Clfloor%5Cfrac%7B%5Ctext%7Blen(pad)%7D%7D%7B2%7D%5Cright%5Crfloor" alt=""></a>填充的维度从最后一个维度开始向前移动. 例如, 填充输入tensor的最后一个维度, 所以 <cite>pad</cite> 形如 <cite>(padLeft, padRight)</cite>; 填充最后 2 个维度, 使用 <cite>(padLeft, padRight, padTop, padBottom)</cite>; 填充最后 3 个维度, 使用 <cite>(padLeft, padRight, padTop, padBottom, padFront, padBack)</cite>.</p>
<pre><code class="language-py">Padding mode:
</code></pre>
<p>请参见 <a href="#torch.nn.ConstantPad2d" title="torch.nn.ConstantPad2d"><code>torch.nn.ConstantPad2d</code></a>, <a href="#torch.nn.ReflectionPad2d" title="torch.nn.ReflectionPad2d"><code>torch.nn.ReflectionPad2d</code></a>, and <a href="#torch.nn.ReplicationPad2d" title="torch.nn.ReplicationPad2d"><code>torch.nn.ReplicationPad2d</code></a> 有关每个填充模式如何工作的具体示例. Constant padding 已经实现于任意维度. 复制填充用于填充5D输入张量的最后3个维度, 或4D输入张量的最后2个维度, 或3D输入张量的最后一个维度. 反射填充仅用于填充4D输入张量的最后两个维度, 或者3D输入张量的最后一个维度.</p>
<p>注意</p>
<p>当使用CUDA后端时, 此操作可能会导致不确定的向后行为, 并且不容易关闭. 请参阅关于<a href="notes/randomness.html">Reproducibility</a>的注释.</p>
<p>参数:</p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – <code>Nd</code> tensor</li>
<li><strong>pad</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – m-elem tuple, where <a href="http://latex.codecogs.com/gif.latex?%5Cfrac%7Bm%7D%7B2%7D %5Cleq" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?%5Cfrac%7Bm%7D%7B2%7D %5Cleq" alt=""></a> input dimensions and <a href="http://latex.codecogs.com/gif.latex?m" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?m" alt=""></a> is even.</li>
<li><strong>mode</strong> – ‘constant’, ‘reflect’ or ‘replicate’. 默认值: ‘constant’</li>
<li><strong>value</strong> – fill value for ‘constant’ padding. 默认值: 0</li>
</ul>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; t4d = torch.empty(3, 3, 4, 2)
&gt;&gt;&gt; p1d = (1, 1) # pad last dim by 1 on each side
&gt;&gt;&gt; out = F.pad(t4d, p1d, "constant", 0)  # effectively zero padding
&gt;&gt;&gt; print(out.data.size())
torch.Size([3, 3, 4, 4])
&gt;&gt;&gt; p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)
&gt;&gt;&gt; out = F.pad(t4d, p2d, "constant", 0)
&gt;&gt;&gt; print(out.data.size())
torch.Size([3, 3, 8, 4])
&gt;&gt;&gt; t4d = torch.empty(3, 3, 4, 2)
&gt;&gt;&gt; p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)
&gt;&gt;&gt; out = F.pad(t4d, p3d, "constant", 0)
&gt;&gt;&gt; print(out.data.size())
torch.Size([3, 9, 7, 3])

</code></pre>
<h3 id="interpolate">interpolate</h3>
<pre><code class="language-py">torch.nn.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None)
</code></pre>
<p>向下/向上采样输入到给定的<code>size</code>或给定的scale_facto</p>
<p>由 <code>mode</code> 指定插值的算法.</p>
<p>目前支持时间, 空间和体积上采样, 即预期输入为3-D, 4-D或5-D形状.</p>
<p>输入维度形式: <code>mini-batch x channels x [可选的 depth] x [可选的 height] x width</code>.</p>
<p>可用于上采样的模式是: <code>nearest</code>, <code>linear</code> (仅3D), <code>bilinear</code> (仅4D), <code>trilinear</code> (仅5D), <code>area</code></p>
<p>参数:</p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 tensor</li>
<li><strong>size</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a> <em>or</em> <em>Tuple</em>_[<em><a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a></em>] or_ <em>Tuple</em>_[<em><a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a></em>,_ <a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>] or</em> <em>Tuple</em>_[<em><a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a></em>,_ <a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>,</em> <a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>]</em>) – 输出 size.</li>
<li><strong>scale_factor</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a> <em>or</em> <em>Tuple</em>_[<em><a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a></em>]_) – 空间大小的乘数. 如果是元组, 则必须匹配输入大小.</li>
<li><strong>mode</strong> (<em>string</em>) – 上采样算法: ‘nearest’ | ‘linear’ | ‘bilinear’ | ‘trilinear’ | ‘area’. 默认值: ‘nearest’</li>
<li><strong>align_corners</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>,</em> <em>可选的</em>) – 如果为True, 则输入和输出张量的角像素对齐, 从而保留这些像素的值. 仅在 <code>mode</code> 是 <code>linear</code>, <code>bilinear</code>, 或者 <code>trilinear</code> 时生效. 默认值: False</li>
</ul>
<p>警告</p>
<p><code>align_corners = True</code>时, 线性插值模式(<code>linear</code>, <code>bilinear</code>, and <code>trilinear</code>)不会按比例对齐输出和输入像素, 因此输出值可能取决于输入大小. 这是0.3.1版之前这些模式的默认行为.此后, 默认行为为<code>align_corners = False</code>. 有关这如何影响输出的具体示例, 请参见上例.</p>
<p>注意</p>
<p>当使用CUDA后端时, 此操作可能会导致不确定的向后行为, 并且不容易关闭. 请参阅关于<a href="notes/randomness.html">Reproducibility</a>的注释.</p>
<h3 id="upsample">upsample</h3>
<pre><code class="language-py">torch.nn.functional.upsample(input, size=None, scale_factor=None, mode='nearest', align_corners=None)
</code></pre>
<p>将输入采样到给定<code>size</code>或给定的<code>scale_factor</code></p>
<p>警告</p>
<p>此函数已被弃用, 取而代之的是 <a href="#torch.nn.functional.interpolate" title="torch.nn.functional.interpolate"><code>torch.nn.functional.interpolate()</code></a>. 等价于 <code>nn.functional.interpolate(...)</code>.</p>
<p>注意</p>
<p>当使用CUDA后端时, 此操作可能会导致不确定的向后行为, 并且不容易关闭. 请参阅关于<a href="notes/randomness.html">Reproducibility</a>的注释.</p>
<p>用于上采样的算法由 <code>mode</code> 确定.</p>
<p>目前支持时间, 空间和体积上采样, 即预期输入为3-D, 4-D或5-D形状.</p>
<p>输入维度形式: <code>mini-batch x channels x [可选的 depth] x [可选的 height] x width</code>.</p>
<p>可用于上采样的模式是: <code>nearest</code>, <code>linear</code> (仅3D), <code>bilinear</code> (仅4D), <code>trilinear</code> (仅5D)</p>
<p>参数:</p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入 tensor</li>
<li><strong>size</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a> <em>or</em> <em>Tuple</em>_[<em><a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a></em>] or_ <em>Tuple</em>_[<em><a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a></em>,_ <a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>] or</em> <em>Tuple</em>_[<em><a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a></em>,_ <a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>,</em> <a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>]</em>) – 输出 size.</li>
<li><strong>scale_factor</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – 空间大小的乘数. 必须是整数.</li>
<li><strong>mode</strong> (<em>string</em>) – 上采样算法: ‘nearest’ | ‘linear’| ‘bilinear’ | ‘trilinear’. 默认值: ‘nearest’</li>
<li><strong>align_corners</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>,</em> <em>可选的</em>) – 如果为True, 则输入和输出张量的角像素对齐, 从而保留这些像素的值. 仅在 <code>mode</code> 是 <code>linear</code>, <code>bilinear</code>, 或者 <code>trilinear</code> 时生效. 默认值: False</li>
</ul>
<p>警告</p>
<p><code>align_corners = True</code>时, 线性插值模式(<code>linear</code>, <code>bilinear</code>, and <code>trilinear</code>)不会按比例对齐输出和输入像素, 因此输出值可能取决于输入大小. 这是0.3.1版之前这些模式的默认行为.此后, 默认行为为<code>align_corners = False</code>. 有关这如何影响输出的具体示例, 请参见 <a href="#torch.nn.Upsample" title="torch.nn.Upsample"><code>Upsample</code></a></p>
<h3 id="upsample-nearest">upsample_nearest</h3>
<pre><code class="language-py">torch.nn.functional.upsample_nearest(input, size=None, scale_factor=None)
</code></pre>
<p>使用最近邻居的像素值对输入进行上采样.</p>
<p>警告</p>
<p>不推荐使用此函数, 而使用 <a href="#torch.nn.functional.interpolate" title="torch.nn.functional.interpolate"><code>torch.nn.functional.interpolate()</code></a>. 等价于h <code>nn.functional.interpolate(..., mode='nearest')</code>.</p>
<p>目前支持空间和体积上采样 (即 inputs 是 4 或者 5 维的).</p>
<p>参数:</p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input</li>
<li><strong>size</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a> <em>or</em> <em>Tuple</em>_[<em><a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a></em>,_ <a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>] or</em> <em>Tuple</em>_[<em><a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a></em>,_ <a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>,</em> <a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>]</em>) – output spatia size.</li>
<li><strong>scale_factor</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – multiplier for spatial size. Has to be an integer.</li>
</ul>
<p>注意</p>
<p>当使用CUDA后端时, 此操作可能会导致不确定的向后行为, 并且不容易关闭. 请参阅关于<a href="notes/randomness.html">Reproducibility</a>的注释.</p>
<h3 id="upsample-bilinear">upsample_bilinear</h3>
<pre><code class="language-py">torch.nn.functional.upsample_bilinear(input, size=None, scale_factor=None)
</code></pre>
<p>使用双线性上采样对输入进行上采样.</p>
<p>警告</p>
<p>不推荐使用此函数, 而使用 <a href="#torch.nn.functional.interpolate" title="torch.nn.functional.interpolate"><code>torch.nn.functional.interpolate()</code></a>. 等价于 <code>nn.functional.interpolate(..., mode='bilinear', align_corners=True)</code>.</p>
<p>期望输入是空间的 (4D). 用 <code>upsample_trilinear</code> 对体积 (5D) 输入.</p>
<p>参数:</p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input</li>
<li><strong>size</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a> <em>or</em> <em>Tuple</em>_[<em><a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a></em>,_ <a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>]</em>) – output spatial size.</li>
<li><strong>scale_factor</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a> <em>or</em> <em>Tuple</em>_[<em><a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a></em>,_ <a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>]</em>) – multiplier for spatial size</li>
</ul>
<p>注意</p>
<p>当使用CUDA后端时, 此操作可能会导致不确定的向后行为, 并且不容易关闭. 请参阅关于<a href="notes/randomness.html">Reproducibility</a>的注释.</p>
<h3 id="grid-sample">grid_sample</h3>
<pre><code class="language-py">torch.nn.functional.grid_sample(input, grid, mode='bilinear', padding_mode='zeros')
</code></pre>
<p>给定<code>input</code> 和流场 <code>grid</code>, 使用 <code>input</code> 和 <code>grid</code> 中的像素位置计算<code>output</code>.</p>
<p>目前, 仅支持 spatial (4-D) 和 volumetric (5-D) <code>input</code>.</p>
<p>在 spatial (4-D) 的情况下, 对于 <code>input</code> 形如 <a href="http://latex.codecogs.com/gif.latex?(N%2C C%2C H_%5Ctext%7Bin%7D%2C W_%5Ctext%7Bin%7D)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(N%2C C%2C H_%5Ctext%7Bin%7D%2C W_%5Ctext%7Bin%7D)" alt=""></a> 和 <code>grid</code> 形如 <a href="http://latex.codecogs.com/gif.latex?(N%2C H_%5Ctext%7Bout%7D%2C W_%5Ctext%7Bout%7D%2C 2)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(N%2C H_%5Ctext%7Bout%7D%2C W_%5Ctext%7Bout%7D%2C 2)" alt=""></a>, 输出的形状为 <a href="http://latex.codecogs.com/gif.latex?(N%2C C%2C H_%5Ctext%7Bout%7D%2C W_%5Ctext%7Bout%7D)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(N%2C C%2C H_%5Ctext%7Bout%7D%2C W_%5Ctext%7Bout%7D)" alt=""></a>.</p>
<p>对于每个输出位置 <code>output[n, :, h, w]</code>, 大小为2的向量 <code>grid[n, h, w]</code> 指定 <code>input</code> 的像素位置 <code>x</code> 和 <code>y</code>, 用于插值输出值 <code>output[n, :, h, w]</code>. 对于 5D 的 inputs, <code>grid[n, d, h, w]</code> 指定 <code>x</code>, <code>y</code>, <code>z</code> 像素位置用于插值 <code>output[n, :, d, h, w]</code>. <code>mode</code> 参数指定 <code>nearest</code> or <code>bilinear</code> 插值方法.</p>
<p><code>grid</code> 大多数值应该处于 <code>[-1, 1]</code>. 这是因为像素位置由<code>input</code> 空间维度标准化.例如, 值 <code>x = -1, y = -1</code> 是 <code>input</code> 的左上角, 值 <code>x = 1, y = 1</code> 是 <code>input</code> 的右下角.</p>
<p>如果 <code>grid</code> 有 <code>[-1, 1]</code> 之外的值, 那些坐标将由 <code>padding_mode</code> 定义. 选项如下</p>
<blockquote>
<ul>
<li><code>padding_mode="zeros"</code>: 用 <code>0</code> 代替边界外的值,</li>
<li><code>padding_mode="border"</code>: 用 border 值代替,</li>
<li><code>padding_mode="reflection"</code>: 对于超出边界的值, 用反射的值. 对于距离边界较远的位置, 它会一直被反射, 直到到达边界, 例如(归一化)像素位置<code>x = -3.5</code>被<code>-1</code>反射, 变成<code>x' = 2.5</code>, 然后被边界1反射, 变成<code>x'' = -0.5</code>.</li>
</ul>
</blockquote>
<p>注意</p>
<p>该功能常用于空间变换网络的构建.</p>
<p>注意</p>
<p>当使用CUDA后端时, 此操作可能会导致不确定的向后行为, 并且不容易关闭. 请参阅关于<a href="notes/randomness.html">Reproducibility</a>的注释.</p>
<p>参数:</p>
<ul>
<li><strong>input</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input of shape <a href="http://latex.codecogs.com/gif.latex?(N%2C C%2C H_%5Ctext%7Bin%7D%2C W_%5Ctext%7Bin%7D)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(N%2C C%2C H_%5Ctext%7Bin%7D%2C W_%5Ctext%7Bin%7D)" alt=""></a> (4-D case) or <a href="http://latex.codecogs.com/gif.latex?(N%2C C%2C D_%5Ctext%7Bin%7D%2C H_%5Ctext%7Bin%7D%2C W_%5Ctext%7Bin%7D)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(N%2C C%2C D_%5Ctext%7Bin%7D%2C H_%5Ctext%7Bin%7D%2C W_%5Ctext%7Bin%7D)" alt=""></a> (5-D case)</li>
<li><strong>grid</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – flow-field of shape <a href="http://latex.codecogs.com/gif.latex?(N%2C H_%5Ctext%7Bout%7D%2C W_%5Ctext%7Bout%7D%2C 2)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(N%2C H_%5Ctext%7Bout%7D%2C W_%5Ctext%7Bout%7D%2C 2)" alt=""></a> (4-D case) or <a href="http://latex.codecogs.com/gif.latex?(N%2C D_%5Ctext%7Bout%7D%2C H_%5Ctext%7Bout%7D%2C W_%5Ctext%7Bout%7D%2C 3)" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?(N%2C D_%5Ctext%7Bout%7D%2C H_%5Ctext%7Bout%7D%2C W_%5Ctext%7Bout%7D%2C 3)" alt=""></a> (5-D case)</li>
<li><strong>mode</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – interpolation mode to calculate output values ‘bilinear’ | ‘nearest’. 默认值: ‘bilinear’</li>
<li><strong>padding_mode</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – padding mode for outside grid values ‘zeros’ | ‘border’ | ‘reflection’. 默认值: ‘zeros’</li>
</ul>
<p>| 返回值: | output Tensor</p>
<p>| 返回类型: | output (<a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>)</p>
<h3 id="affine-grid">affine_grid</h3>
<pre><code class="language-py">torch.nn.functional.affine_grid(theta, size)
</code></pre>
<p>在给定一批仿射矩阵<code>theta</code>的情况下生成二维流场. 通常与<a href="#torch.nn.functional.grid_sample" title="torch.nn.functional.grid_sample"><code>grid_sample()</code></a>一起使用以实现<code>空间变换器网络</code>.</p>
<p>参数:</p>
<ul>
<li><strong>theta</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入的仿射矩阵 (<a href="http://latex.codecogs.com/gif.latex?N %5Ctimes 2 %5Ctimes 3" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?N %5Ctimes 2 %5Ctimes 3" alt=""></a>)</li>
<li><strong>size</strong> (<em>torch.Size</em>) – 目标输出图像大小 (<a href="http://latex.codecogs.com/gif.latex?N %5Ctimes C %5Ctimes H %5Ctimes W" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?N %5Ctimes C %5Ctimes H %5Ctimes W" alt=""></a>) 例子: torch.Size((32, 3, 24, 24))</li>
</ul>
<p>| 返回值: | 输出tensor, 形状为 (<a href="http://latex.codecogs.com/gif.latex?N %5Ctimes H %5Ctimes W %5Ctimes 2" data-uk-lightbox><img src="http://latex.codecogs.com/gif.latex?N %5Ctimes H %5Ctimes W %5Ctimes 2" alt=""></a>)</p>
<p>| 返回类型: | output (<a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>)</p>
<h2 id="数据并行函数-multi-gpu-distributed">数据并行函数 (multi-GPU, distributed)</h2>
<h3 id="data-parallel">data_parallel</h3>
<pre><code class="language-py">torch.nn.parallel.data_parallel(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None)
</code></pre>
<p>在设备id中给定的gpu上并行计算模块(输入).</p>
<p>这是DataParallel模块的函数版本.</p>
<p>参数:</p>
<ul>
<li><strong>module</strong> (<a href="#torch.nn.Module" title="torch.nn.Module"><em>Module</em></a>) – 要并行评估的模块</li>
<li><strong>inputs</strong> (<em>tensor</em>) – 模块的输入</li>
<li><strong>device_ids</strong> (<em>list of python:int</em> <em>or</em> <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a>) – 用于复制模块的GPU id</li>
<li><strong>output_device</strong> (<em>list of python:int</em> <em>or</em> <a href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a>) – 输出的GPU位置使用 -1表示CPU. (默认值: device_ids[0])</li>
</ul>
<p>| 返回值: | 一个tensor, 包含位于输出设备上的模块(输入)的结果</p>
</div>
<hr class="uk-article-divider">
<div class="uk-block uk-block-muted uk-padding-top-remove uk-padding-bottom-remove uk-margin-large-top  book-recommend-wrap">
<div class="uk-margin-top uk-margin-bottom uk-margin-left uk-margin-right">
<div class="uk-margin uk-text-muted "><i class="uk-icon-outdent uk-icon-justify uk-margin-small-right"></i>书籍推荐</div>
<div class="books">
<ul class="uk-book-list">
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/68/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/68/index.html">Python 资源大全中文版</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/19.html">伯乐在线</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">1页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月6日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 10237个">10237</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/166/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/166/index.html">What the f*ck Python中文版</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/95.html">leisurelicht</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">70页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月26日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 7300个">7300</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/127/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/127/index.html">aiohttp 中文文档</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/68.html">HuberTRoy</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">124页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年7月8日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 34个">34</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/163/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/git_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/163/index.html">Git的奇技淫巧</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/92.html">jackfrued</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="git">git</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">77页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月26日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 28个">28</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/187/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/code_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/187/index.html">软件开发的工程化</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/105.html">azl397985856</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="code">code</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月26日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 112个">112</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/22/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/cplusplus_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/22/index.html">计算与推断思维</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/17.html">Kivy Developers From China</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="cplusplus">cplusplus</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">19页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年5月3日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 243个">243</span>
</div>
</div>
</div>
</li>
<hr>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
<nav class="tm-navbar uk-navbar uk-navbar-attached reader-nav">
<div class="uk-float-left uk-margin-small-top">
<a href="javascript:;" title="目录菜单" class="show-menu  uk-icon-hover  uk-icon-align-justify uk-margin-right"></a>
<div data-uk-dropdown="{mode:'click',pos:'bottom-left'}" class="font-setting-wrap">
<a class="uk-icon-hover uk-icon-font uk-margin-right" aria-label="字体设置" href="javascript:;"></a>
<div class="uk-dropdown dropdown-menu">
<div class="dropdown-caret"><span class="caret-outer"></span><span class="caret-inner"></span></div>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-reduce">小字</button>
<button class="uk-button-link button size-2 font-enlarge">大字</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-1 ">宋体</button>
<button class="uk-button-link button size-2 font-2 ">黑体</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-3 color-theme-sun "><i class="uk-icon-sun-o"></i>白天</button>
<button class="uk-button-link button size-3 color-theme-eye "><i class="uk-icon-eye"></i>护眼</button>
<button class="uk-button-link button size-3 color-theme-moon "><i class="uk-icon-moon-o"></i>夜晚</button></div>
</div>
</div>
<a class="logo uk-margin-right" href="../../../../" title="返回首页"><img class="" src="../../../../static/components/images/icon_32.png" /></a>
</div>
<div class="uk-navbar-flip  uk-hidden-small">
<div id="share-box"></div>
</div>
</nav>
<div id="menu-id" class="uk-offcanvas reader-offcanvas">
<div class="uk-offcanvas-bar">
<ul class="book-menu-bar uk-nav uk-nav-offcanvas" data-uk-nav>
<li>
<a href="../../../../book/169/index.html" data-book-page-rel-url="index.html" data-book-page-id="0" title="封面">封面</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/readme.html" data-book-page-rel-url="readme.html" data-book-page-id="0" title="简介">简介</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_getting_started.html" title="起步" data-book-page-rel-url="docs/1.0/tut_getting_started.html" data-book-page-id="11555">起步</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/deep_learning_60min_blitz.html" title="PyTorch 深度学习: 60 分钟极速入门" data-book-page-rel-url="docs/1.0/deep_learning_60min_blitz.html" data-book-page-id="11556">PyTorch 深度学习: 60 分钟极速入门</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/blitz_tensor_tutorial.html" title="什么是 PyTorch？" data-book-page-rel-url="docs/1.0/blitz_tensor_tutorial.html" data-book-page-id="11557">什么是 PyTorch？</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/blitz_autograd_tutorial.html" title="Autograd：自动求导" data-book-page-rel-url="docs/1.0/blitz_autograd_tutorial.html" data-book-page-id="11558">Autograd：自动求导</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/blitz_neural_networks_tutorial.html" title="神经网络" data-book-page-rel-url="docs/1.0/blitz_neural_networks_tutorial.html" data-book-page-id="11559">神经网络</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/blitz_cifar10_tutorial.html" title="训练分类器" data-book-page-rel-url="docs/1.0/blitz_cifar10_tutorial.html" data-book-page-id="11560">训练分类器</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/blitz_data_parallel_tutorial.html" title="可选：数据并行处理" data-book-page-rel-url="docs/1.0/blitz_data_parallel_tutorial.html" data-book-page-id="11561">可选：数据并行处理</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/data_loading_tutorial.html" title="数据加载和处理教程" data-book-page-rel-url="docs/1.0/data_loading_tutorial.html" data-book-page-id="11562">数据加载和处理教程</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/pytorch_with_examples.html" title="用例子学习 PyTorch" data-book-page-rel-url="docs/1.0/pytorch_with_examples.html" data-book-page-id="11563">用例子学习 PyTorch</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/transfer_learning_tutorial.html" title="迁移学习教程" data-book-page-rel-url="docs/1.0/transfer_learning_tutorial.html" data-book-page-id="11564">迁移学习教程</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/deploy_seq2seq_hybrid_frontend_tutorial.html" title="混合前端的 seq2seq 模型部署" data-book-page-rel-url="docs/1.0/deploy_seq2seq_hybrid_frontend_tutorial.html" data-book-page-id="11565">混合前端的 seq2seq 模型部署</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/saving_loading_models.html" title="Saving and Loading Models" data-book-page-rel-url="docs/1.0/saving_loading_models.html" data-book-page-id="11566">Saving and Loading Models</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nn_tutorial.html" title="What is `torch.nn` _really_?" data-book-page-rel-url="docs/1.0/nn_tutorial.html" data-book-page-id="11567">What is `torch.nn` _really_?</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_image.html" title="图像" data-book-page-rel-url="docs/1.0/tut_image.html" data-book-page-id="11568">图像</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/finetuning_torchvision_models_tutorial.html" title="Torchvision 模型微调" data-book-page-rel-url="docs/1.0/finetuning_torchvision_models_tutorial.html" data-book-page-id="11569">Torchvision 模型微调</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/spatial_transformer_tutorial.html" title="空间变换器网络教程" data-book-page-rel-url="docs/1.0/spatial_transformer_tutorial.html" data-book-page-id="11570">空间变换器网络教程</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/neural_style_tutorial.html" title="使用 PyTorch 进行图像风格转换" data-book-page-rel-url="docs/1.0/neural_style_tutorial.html" data-book-page-id="11571">使用 PyTorch 进行图像风格转换</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/fgsm_tutorial.html" title="对抗性示例生成" data-book-page-rel-url="docs/1.0/fgsm_tutorial.html" data-book-page-id="11572">对抗性示例生成</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/super_resolution_with_caffe2.html" title="使用 ONNX 将模型从 PyTorch 传输到 Caffe2 和移动端" data-book-page-rel-url="docs/1.0/super_resolution_with_caffe2.html" data-book-page-id="11573">使用 ONNX 将模型从 PyTorch 传输到 Caffe2 和移动端</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_text.html" title="文本" data-book-page-rel-url="docs/1.0/tut_text.html" data-book-page-id="11574">文本</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/chatbot_tutorial.html" title="聊天机器人教程" data-book-page-rel-url="docs/1.0/chatbot_tutorial.html" data-book-page-id="11575">聊天机器人教程</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/char_rnn_generation_tutorial.html" title="使用字符级别特征的 RNN 网络生成姓氏" data-book-page-rel-url="docs/1.0/char_rnn_generation_tutorial.html" data-book-page-id="11576">使用字符级别特征的 RNN 网络生成姓氏</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/char_rnn_classification_tutorial.html" title="使用字符级别特征的 RNN 网络进行姓氏分类" data-book-page-rel-url="docs/1.0/char_rnn_classification_tutorial.html" data-book-page-id="11577">使用字符级别特征的 RNN 网络进行姓氏分类</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/deep_learning_nlp_tutorial.html" title="Deep Learning for NLP with Pytorch" data-book-page-rel-url="docs/1.0/deep_learning_nlp_tutorial.html" data-book-page-id="11578">Deep Learning for NLP with Pytorch</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nlp_pytorch_tutorial.html" title="PyTorch 介绍" data-book-page-rel-url="docs/1.0/nlp_pytorch_tutorial.html" data-book-page-id="11579">PyTorch 介绍</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nlp_deep_learning_tutorial.html" title="使用 PyTorch 进行深度学习" data-book-page-rel-url="docs/1.0/nlp_deep_learning_tutorial.html" data-book-page-id="11580">使用 PyTorch 进行深度学习</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nlp_word_embeddings_tutorial.html" title="Word Embeddings: Encoding Lexical Semantics" data-book-page-rel-url="docs/1.0/nlp_word_embeddings_tutorial.html" data-book-page-id="11581">Word Embeddings: Encoding Lexical Semantics</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nlp_sequence_models_tutorial.html" title="序列模型和 LSTM 网络" data-book-page-rel-url="docs/1.0/nlp_sequence_models_tutorial.html" data-book-page-id="11582">序列模型和 LSTM 网络</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nlp_advanced_tutorial.html" title="Advanced: Making Dynamic Decisions and the Bi-LSTM CRF" data-book-page-rel-url="docs/1.0/nlp_advanced_tutorial.html" data-book-page-id="11583">Advanced: Making Dynamic Decisions and the Bi-LSTM CRF</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/seq2seq_translation_tutorial.html" title="基于注意力机制的 seq2seq 神经网络翻译" data-book-page-rel-url="docs/1.0/seq2seq_translation_tutorial.html" data-book-page-id="11584">基于注意力机制的 seq2seq 神经网络翻译</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_generative.html" title="生成" data-book-page-rel-url="docs/1.0/tut_generative.html" data-book-page-id="11585">生成</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/dcgan_faces_tutorial.html" title="DCGAN Tutorial" data-book-page-rel-url="docs/1.0/dcgan_faces_tutorial.html" data-book-page-id="11586">DCGAN Tutorial</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_reinforcement_learning.html" title="强化学习" data-book-page-rel-url="docs/1.0/tut_reinforcement_learning.html" data-book-page-id="11587">强化学习</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/reinforcement_q_learning.html" title="Reinforcement Learning (DQN) Tutorial" data-book-page-rel-url="docs/1.0/reinforcement_q_learning.html" data-book-page-id="11588">Reinforcement Learning (DQN) Tutorial</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_extending_pytorch.html" title="扩展 PyTorch" data-book-page-rel-url="docs/1.0/tut_extending_pytorch.html" data-book-page-id="11589">扩展 PyTorch</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/numpy_extensions_tutorial.html" title="用 numpy 和 scipy 创建扩展" data-book-page-rel-url="docs/1.0/numpy_extensions_tutorial.html" data-book-page-id="11590">用 numpy 和 scipy 创建扩展</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/cpp_extension.html" title="Custom C++   and CUDA Extensions" data-book-page-rel-url="docs/1.0/cpp_extension.html" data-book-page-id="11591">Custom C++ and CUDA Extensions</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/torch_script_custom_ops.html" title="Extending TorchScript with Custom C++   Operators" data-book-page-rel-url="docs/1.0/torch_script_custom_ops.html" data-book-page-id="11592">Extending TorchScript with Custom C++ Operators</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_production_usage.html" title="生产性使用" data-book-page-rel-url="docs/1.0/tut_production_usage.html" data-book-page-id="11593">生产性使用</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/dist_tuto.html" title="Writing Distributed Applications with PyTorch" data-book-page-rel-url="docs/1.0/dist_tuto.html" data-book-page-id="11594">Writing Distributed Applications with PyTorch</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/aws_distributed_training_tutorial.html" title="使用 Amazon AWS 进行分布式训练" data-book-page-rel-url="docs/1.0/aws_distributed_training_tutorial.html" data-book-page-id="11595">使用 Amazon AWS 进行分布式训练</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/ONNXLive.html" title="ONNX 现场演示教程" data-book-page-rel-url="docs/1.0/ONNXLive.html" data-book-page-id="11596">ONNX 现场演示教程</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/cpp_export.html" title="在 C++ 中加载 PYTORCH 模型" data-book-page-rel-url="docs/1.0/cpp_export.html" data-book-page-id="11597">在 C++ 中加载 PYTORCH 模型</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_other_language.html" title="其它语言中的 PyTorch" data-book-page-rel-url="docs/1.0/tut_other_language.html" data-book-page-id="11598">其它语言中的 PyTorch</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/cpp_frontend.html" title="使用 PyTorch C++ 前端" data-book-page-rel-url="docs/1.0/cpp_frontend.html" data-book-page-id="11599">使用 PyTorch C++ 前端</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/docs_notes.html" title="注解" data-book-page-rel-url="docs/1.0/docs_notes.html" data-book-page-id="11600">注解</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_autograd.html" title="自动求导机制" data-book-page-rel-url="docs/1.0/notes_autograd.html" data-book-page-id="11601">自动求导机制</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_broadcasting.html" title="广播语义" data-book-page-rel-url="docs/1.0/notes_broadcasting.html" data-book-page-id="11602">广播语义</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_cuda.html" title="CUDA 语义" data-book-page-rel-url="docs/1.0/notes_cuda.html" data-book-page-id="11603">CUDA 语义</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_extending.html" title="Extending PyTorch" data-book-page-rel-url="docs/1.0/notes_extending.html" data-book-page-id="11604">Extending PyTorch</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_faq.html" title="Frequently Asked Questions" data-book-page-rel-url="docs/1.0/notes_faq.html" data-book-page-id="11605">Frequently Asked Questions</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_multiprocessing.html" title="Multiprocessing best practices" data-book-page-rel-url="docs/1.0/notes_multiprocessing.html" data-book-page-id="11606">Multiprocessing best practices</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_randomness.html" title="Reproducibility" data-book-page-rel-url="docs/1.0/notes_randomness.html" data-book-page-id="11607">Reproducibility</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_serialization.html" title="Serialization semantics" data-book-page-rel-url="docs/1.0/notes_serialization.html" data-book-page-id="11608">Serialization semantics</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_windows.html" title="Windows FAQ" data-book-page-rel-url="docs/1.0/notes_windows.html" data-book-page-id="11609">Windows FAQ</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/docs_package_ref.html" title="包参考" data-book-page-rel-url="docs/1.0/docs_package_ref.html" data-book-page-id="11610">包参考</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/torch.html" title="torch" data-book-page-rel-url="docs/1.0/torch.html" data-book-page-id="11611">torch</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tensors.html" title="torch.Tensor" data-book-page-rel-url="docs/1.0/tensors.html" data-book-page-id="11612">torch.Tensor</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tensor_attributes.html" title="Tensor Attributes" data-book-page-rel-url="docs/1.0/tensor_attributes.html" data-book-page-id="11613">Tensor Attributes</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/type_info.html" title="数据类型信息" data-book-page-rel-url="docs/1.0/type_info.html" data-book-page-id="11614">数据类型信息</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/sparse.html" title="torch.sparse" data-book-page-rel-url="docs/1.0/sparse.html" data-book-page-id="11615">torch.sparse</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/cuda.html" title="torch.cuda" data-book-page-rel-url="docs/1.0/cuda.html" data-book-page-id="11616">torch.cuda</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/storage.html" title="torch.Storage" data-book-page-rel-url="docs/1.0/storage.html" data-book-page-id="11617">torch.Storage</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nn.html" title="torch.nn" data-book-page-rel-url="docs/1.0/nn.html" data-book-page-id="11618">torch.nn</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nn_functional.html" title="torch.nn.functional" data-book-page-rel-url="docs/1.0/nn_functional.html" data-book-page-id="11619">torch.nn.functional</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nn_init.html" title="torch.nn.init" data-book-page-rel-url="docs/1.0/nn_init.html" data-book-page-id="11620">torch.nn.init</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/optim.html" title="torch.optim" data-book-page-rel-url="docs/1.0/optim.html" data-book-page-id="11621">torch.optim</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/autograd.html" title="Automatic differentiation package - torch.autograd" data-book-page-rel-url="docs/1.0/autograd.html" data-book-page-id="11622">Automatic differentiation package - torch.autograd</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/distributed.html" title="Distributed communication package - torch.distributed" data-book-page-rel-url="docs/1.0/distributed.html" data-book-page-id="11623">Distributed communication package - torch.distributed</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/distributions.html" title="Probability distributions - torch.distributions" data-book-page-rel-url="docs/1.0/distributions.html" data-book-page-id="11624">Probability distributions - torch.distributions</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/jit.html" title="Torch Script" data-book-page-rel-url="docs/1.0/jit.html" data-book-page-id="11625">Torch Script</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/multiprocessing.html" title="多进程包 - torch.multiprocessing" data-book-page-rel-url="docs/1.0/multiprocessing.html" data-book-page-id="11626">多进程包 - torch.multiprocessing</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/bottleneck.html" title="torch.utils.bottleneck" data-book-page-rel-url="docs/1.0/bottleneck.html" data-book-page-id="11627">torch.utils.bottleneck</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/checkpoint.html" title="torch.utils.checkpoint" data-book-page-rel-url="docs/1.0/checkpoint.html" data-book-page-id="11628">torch.utils.checkpoint</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/docs_cpp_extension.html" title="torch.utils.cpp_extension" data-book-page-rel-url="docs/1.0/docs_cpp_extension.html" data-book-page-id="11629">torch.utils.cpp_extension</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/data.html" title="torch.utils.data" data-book-page-rel-url="docs/1.0/data.html" data-book-page-id="11630">torch.utils.data</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/dlpack.html" title="torch.utils.dlpack" data-book-page-rel-url="docs/1.0/dlpack.html" data-book-page-id="11631">torch.utils.dlpack</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/hub.html" title="torch.hub" data-book-page-rel-url="docs/1.0/hub.html" data-book-page-id="11632">torch.hub</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/model_zoo.html" title="torch.utils.model_zoo" data-book-page-rel-url="docs/1.0/model_zoo.html" data-book-page-id="11633">torch.utils.model_zoo</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/onnx.html" title="torch.onnx" data-book-page-rel-url="docs/1.0/onnx.html" data-book-page-id="11634">torch.onnx</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/distributed_deprecated.html" title="Distributed communication package (deprecated) - torch.distributed.deprecated" data-book-page-rel-url="docs/1.0/distributed_deprecated.html" data-book-page-id="11635">Distributed communication package (deprecated) - torch.distributed.deprecated</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/docs_torchvision_ref.html" title="torchvision 参考" data-book-page-rel-url="docs/1.0/docs_torchvision_ref.html" data-book-page-id="11636">torchvision 参考</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/torchvision_datasets.html" title="torchvision.datasets" data-book-page-rel-url="docs/1.0/torchvision_datasets.html" data-book-page-id="11637">torchvision.datasets</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/torchvision_models.html" title="torchvision.models" data-book-page-rel-url="docs/1.0/torchvision_models.html" data-book-page-id="11638">torchvision.models</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/torchvision_transforms.html" title="torchvision.transforms" data-book-page-rel-url="docs/1.0/torchvision_transforms.html" data-book-page-id="11639">torchvision.transforms</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/torchvision_utils.html" title="torchvision.utils" data-book-page-rel-url="docs/1.0/torchvision_utils.html" data-book-page-id="11640">torchvision.utils</a>
</li>
</ul>
</li>
</ul>
</div>
</div>
<script src="https://cdn.staticfile.net/jquery/1.12.4/jquery.min.js"></script>
<script type="text/javascript" src="../../../../static/components/uikit-2.27.5/js/uikit.reader.js"></script>
<script type="text/javascript" src="../../../../static/components/social-share/social-share.min.js"></script>
<script>(function(){var bp =document.createElement('script');var curProtocol =window.location.protocol.split(':')[0];if (curProtocol ==='https') {bp.src ='https://zz.bdstatic.com/linksubmit/push.js';}
else {bp.src ='http://push.zhanzhang.baidu.com/push.js';}
var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(bp,s);})();</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
<script src="https://cdn.staticfile.net/highlight.js/9.12.0/highlight.min.js"></script>
<script src="https://cdn.staticfile.net/jquery.pjax/2.0.1/jquery.pjax.min.js"></script>
<script src="https://cdn.staticfile.net/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="https://cdn.staticfile.net/uikit/2.27.5/js/components/lightbox.min.js"></script>
<link rel="dns-prefetch" href="../../../..//cdn.mathjax.org" />
<script type="text/x-mathjax-config">
 function initMathJax() {
    var mathId = $("book-content-section")[0];
    MathJax.Hub.Config({
        tex2jax: {skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code','a']},
        showProcessingMessages: false,
        messageStyle: "none"
    });
    MathJax.Hub.Queue(["Typeset",MathJax.Hub,mathId]);
 };
initMathJax();
</script>
<script src='https://cdn.staticfile.net/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML' async></script>
<style>
	.MathJax_Display{display:inline!important;}
</style>
<script type="text/javascript" src="../../../../static/components/js/reader.js"></script>
<script type="text/javascript">var bookId =169;var bookPageId =11619;var bookPageRelUrl ='docs/1.0/nn_functional.html';</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
</body>
</html>