
<!DOCTYPE html>
<html lang="zh-CN">
<head>
<title>Probability distributions - torch.distributions-PyTorch 1.0 中文文档 & 教程</title>
<meta content='Probability distributions - torch.distributions,PyTorch 1.0 中文文档 & 教程' name='keywords'>
<meta content='Probability distributions - torch.distributions,PyTorch 1.0 中文文档 & 教程' name='description'>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta http-equiv="Content-Language" content="zh-CN" />
<meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1, maximum-scale=1, user-scalable=no"../../../../>
<meta name="applicable-device" content="pc,mobile">
<link rel="shortcut icon" href="../../../../favicon.ico" type="image/x-icon" />
<meta name="renderer" content="webkit">
<link rel="stylesheet" href="../../../../static/components/uikit-2.27.5/css/uikit.custom.css">
<link rel="stylesheet" href="../../../../static/components/social-share/social-share.min.css">
<link rel="stylesheet" href="../../../../static/components/highlight/styles/custom.css">
<link rel="stylesheet" href="../../../../static/components/css/base.css">
<link rel="stylesheet" href="../../../../static/components/css/reader.css">
<link rel="stylesheet" href="../../../../static/components/css/markdown.css">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5313208362165053" crossorigin="anonymous"></script>
</head>
<body>
<div class=" book-main-wrap uk-container uk-container-center uk-margin-top ">
<div class="uk-grid">
<div class="uk-width-1-1 reader-wrap ">
<div class=" bottom-nav uk-clearfix ">
<div class="uk-align-left ">
<a href="../../../../book/169/docs/1.0/distributed.html">
<i class="nav-icon-left uk-icon-small  uk-icon-caret-left"></i>
<span class="">Distributed..</span>
</a>
</div>
<div class="uk-align-right ">
<a href="../../../../book/169/docs/1.0/jit.html">
<span class="">Torch Script</span>
<i class="nav-icon-right uk-icon-small  uk-icon-caret-right"></i>
</a>
</div>
</div>
<div class="uk-text-center">
<h2 class="book-page-title uk-container-center">
<a href="../../../../book/169/index.html">PyTorch 1.0 中文文档 & 教程</a>
<a target="_blank" rel="nofollow" href="https://github.com/apachecn/pytorch-doc-zh" class="uk-icon-button uk-icon-github" title="github项目地址"></a>
</h2>
</div>
<script type="text/javascript" src="../../../../static/components/js/app_intro.js"></script>
<ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-5313208362165053" data-ad-slot="1328047120"></ins>
<script>(adsbygoogle =window.adsbygoogle ||[]).push({});</script>
<hr class="uk-article-divider">
<div class="book-content-section  md-content-section  uk-margin-bottom">
<h1 id="概率分布---torchdistributions">概率分布 - torch.distributions</h1>
<blockquote>
<p>译者：<a href="https://github.com/hijkzzz">hijkzzz</a></p>
</blockquote>
<p><code>distributions</code> 包含可参数化的概率分布和采样函数. 这允许构造用于优化的随机计算图和随机梯度估计器. 这个包一般遵循 <a href="https://arxiv.org/abs/1711.10604">TensorFlow Distributions</a> 包的设计.</p>
<p>通常, 不可能直接通过随机样本反向传播. 但是, 有两种主要方法可创建可以反向传播的代理函数. 即得分函数估计器/似然比估计器/REINFORCE和pathwise derivative估计器. REINFORCE通常被视为强化学习中策略梯度方法的基础, 并且pathwise derivative估计器常见于变分自动编码器中的重新参数化技巧. 得分函数仅需要样本的值 <a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/cb804637f7fdaaf91569cfe4f047b418.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/cb804637f7fdaaf91569cfe4f047b418.jpg" alt=""></a>, pathwise derivative 需要导数 <a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/385dbaaac9dd8aad33acc31ac64d2f27.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/385dbaaac9dd8aad33acc31ac64d2f27.jpg" alt=""></a>. 接下来的部分将在一个强化学习示例中讨论这两个问题. 有关详细信息, 请参阅 <a href="https://arxiv.org/abs/1506.05254">Gradient Estimation Using Stochastic Computation Graphs</a> .</p>
<h2 id="得分函数">得分函数</h2>
<p>当概率密度函数相对于其参数可微分时, 我们只需要<code>sample()</code>和<code>log_prob()</code>来实现REINFORCE:</p>
<p><a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/b50e881c13615b1d9aa00ad0c9cdfa99.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/b50e881c13615b1d9aa00ad0c9cdfa99.jpg" alt=""></a></p>
<p><a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/51b8359f970d2bfe2ad4cdc3ac1aed3c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/51b8359f970d2bfe2ad4cdc3ac1aed3c.jpg" alt=""></a> 是参数, <a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/82005cc2e0087e2a52c7e43df4a19a00.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/82005cc2e0087e2a52c7e43df4a19a00.jpg" alt=""></a> 是学习速率, <a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/f9f040e861365a0560b2552b4e4e17da.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/f9f040e861365a0560b2552b4e4e17da.jpg" alt=""></a> 是奖励 并且 <a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/2e84bb32ea0808870a16b888aeaf8d0d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/2e84bb32ea0808870a16b888aeaf8d0d.jpg" alt=""></a> 是在状态 <a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/0492c0bfd615cb5e61c847ece512ff51.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/0492c0bfd615cb5e61c847ece512ff51.jpg" alt=""></a> 以及给定策略 <a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/5f3ddae3395c04f9346a3ac1d327ae2a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/5f3ddae3395c04f9346a3ac1d327ae2a.jpg" alt=""></a>执行动作 <a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/070b1af5eca3a5c5d72884b536090f17.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/070b1af5eca3a5c5d72884b536090f17.jpg" alt=""></a> 的概率.</p>
<p>在实践中, 我们将从网络输出中采样一个动作, 将这个动作应用于一个环境中, 然后使用<code>log_prob</code>构造一个等效的损失函数. 请注意, 我们使用负数是因为优化器使用梯度下降, 而上面的规则假设梯度上升. 有了确定的策略, REINFORCE的实现代码如下:</p>
<pre><code class="language-py">probs = policy_network(state)
# Note that this is equivalent to what used to be called multinomial
m = Categorical(probs)
action = m.sample()
next_state, reward = env.step(action)
loss = -m.log_prob(action) * reward
loss.backward()

</code></pre>
<h2 id="pathwise-derivative">Pathwise derivative</h2>
<p>实现这些随机/策略梯度的另一种方法是使用来自<code>rsample()</code>方法的重新参数化技巧, 其中参数化随机变量可以通过无参数随机变量的参数确定性函数构造. 因此, 重新参数化的样本变得可微分. 实现Pathwise derivative的代码如下:</p>
<pre><code class="language-py">params = policy_network(state)
m = Normal(*params)
# Any distribution with .has_rsample == True could work based on the application
action = m.rsample()
next_state, reward = env.step(action)  # Assuming that reward is differentiable
loss = -reward
loss.backward()

</code></pre>
<h2 id="分布">分布</h2>
<pre><code class="language-py">class torch.distributions.distribution.Distribution(batch_shape=torch.Size([]), event_shape=torch.Size([]), validate_args=None)
</code></pre>
<p>基类: <a href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.7)"><code>object</code></a></p>
<p>Distribution是概率分布的抽象基类.</p>
<pre><code class="language-py">arg_constraints
</code></pre>
<p>从参数名称返回字典到 <a href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><code>Constraint</code></a> 对象（应该满足这个分布的每个参数）.不是张量的arg不需要出现在这个字典中.</p>
<pre><code class="language-py">batch_shape
</code></pre>
<p>返回批量参数的形状.</p>
<pre><code class="language-py">cdf(value)
</code></pre>
<p>返回<code>value</code>处的累积密度/质量函数估计.</p>
<p>| 参数: | <strong>value</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – |</p>
<pre><code class="language-py">entropy()
</code></pre>
<p>返回分布的熵, 批量的形状为 batch_shape.</p>
<p>| 返回值: | Tensor 形状为 batch_shape. |</p>
<pre><code class="language-py">enumerate_support(expand=True)
</code></pre>
<p>返回包含离散分布支持的所有值的张量. 结果将在维度0上枚举, 所以结果的形状将是 <code>(cardinality,) + batch_shape + event_shape</code> (对于单变量分布 <code>event_shape = ()</code>).</p>
<p>注意, 这在lock-step中枚举了所有批处理张量<code>[[0, 0], [1, 1], …]</code>. 当 <code>expand=False</code>, 枚举沿着维度 0进行, 但是剩下的批处理维度是单维度, <code>[[0], [1], ..</code>.</p>
<p>遍历整个笛卡尔积的使用 <code>itertools.product(m.enumerate_support())</code>.</p>
<p>| 参数: | <strong>expand</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – 是否扩展对批处理dim的支持以匹配分布的 <code>batch_shape</code>. |</p>
<p>| 返回值: | 张量在维上0迭代. |</p>
<pre><code class="language-py">event_shape
</code></pre>
<p>返回单个样本的形状 (非批量).</p>
<pre><code class="language-py">expand(batch_shape, _instance=None)
</code></pre>
<p>返回一个新的分布实例(或填充派生类提供的现有实例), 其批处理维度扩展为 <code>batch_shape</code>. 这个方法调用 <a href="tensors.html#torch.Tensor.expand" title="torch.Tensor.expand"><code>expand</code></a> 在分布的参数上. 因此, 这不会为扩展的分布实例分配新的内存. 此外, 第一次创建实例时, 这不会在中重复任何参数检查或参数广播在 <code>__init__.py</code>.</p>
<p>参数:</p>
<ul>
<li><strong>batch_shape</strong> (<em>torch.Size</em>) – 所需的扩展尺寸.</li>
<li><strong>_instance</strong> – 由需要重写<code>.expand</code>的子类提供的新实例.</li>
</ul>
<p>| 返回值: | 批处理维度扩展为<code>batch_size</code>的新分布实例. |</p>
<pre><code class="language-py">icdf(value)
</code></pre>
<p>返回按<code>value</code>计算的反向累积密度/质量函数.</p>
<p>| 参数: | <strong>value</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – |</p>
<pre><code class="language-py">log_prob(value)
</code></pre>
<p>返回按<code>value</code>计算的概率密度/质量函数的对数.</p>
<p>| 参数: | <strong>value</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – |</p>
<pre><code class="language-py">mean
</code></pre>
<p>返回分布的平均值.</p>
<pre><code class="language-py">perplexity()
</code></pre>
<p>返回分布的困惑度, 批量的关于 batch_shape.</p>
<p>| 返回值: | 形状为 batch_shape 的张量. |</p>
<pre><code class="language-py">rsample(sample_shape=torch.Size([]))
</code></pre>
<p>如果分布的参数是批量的, 则生成sample_shape形状的重新参数化样本或sample_shape形状的批量重新参数化样本.</p>
<pre><code class="language-py">sample(sample_shape=torch.Size([]))
</code></pre>
<p>如果分布的参数是批量的, 则生成sample_shape形状的样本或sample_shape形状的批量样本.</p>
<pre><code class="language-py">sample_n(n)
</code></pre>
<p>如果分布参数是分批的, 则生成n个样本或n批样本.</p>
<pre><code class="language-py">stddev
</code></pre>
<p>返回分布的标准差.</p>
<pre><code class="language-py">support
</code></pre>
<p>返回<a href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><code>Constraint</code></a> 对象表示该分布的支持.</p>
<pre><code class="language-py">variance
</code></pre>
<p>返回分布的方差.</p>
<h2 id="exponentialfamily">ExponentialFamily</h2>
<pre><code class="language-py">class torch.distributions.exp_family.ExponentialFamily(batch_shape=torch.Size([]), event_shape=torch.Size([]), validate_args=None)
</code></pre>
<p>基类: <a href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p>
<p>指数族是指数族概率分布的抽象基类, 其概率质量/密度函数的形式定义如下</p>
<p><a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/0c8313886f5c82dfae90e21b65152815.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/0c8313886f5c82dfae90e21b65152815.jpg" alt=""></a></p>
<p><a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/51b8359f970d2bfe2ad4cdc3ac1aed3c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/51b8359f970d2bfe2ad4cdc3ac1aed3c.jpg" alt=""></a> 表示自然参数, <a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/e705d3772de12f4df3b0cd75af5110a1.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/e705d3772de12f4df3b0cd75af5110a1.jpg" alt=""></a> 表示充分统计量, <a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/f876c4d8353c747436006e70fb6c4f5d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/f876c4d8353c747436006e70fb6c4f5d.jpg" alt=""></a> 是给定族的对数归一化函数 <a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/d3b6af2f20ffbc8480c6ee97c42958b2.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/d3b6af2f20ffbc8480c6ee97c42958b2.jpg" alt=""></a> 是carrier measure.</p>
<p>注意</p>
<p>该类是<code>Distribution</code>类与指数族分布之间的中介, 主要用于检验<code>.entropy()</code>和解析KL散度方法的正确性. 我们使用这个类来计算熵和KL散度使用AD框架和Bregman散度 (出自: Frank Nielsen and Richard Nock, Entropies and Cross-entropies of Exponential Families).</p>
<pre><code class="language-py">entropy()
</code></pre>
<p>利用对数归一化器的Bregman散度计算熵的方法.</p>
<h2 id="bernoulli">Bernoulli</h2>
<pre><code class="language-py">class torch.distributions.bernoulli.Bernoulli(probs=None, logits=None, validate_args=None)
</code></pre>
<p>基类: <a href="#torch.distributions.exp_family.ExponentialFamily" title="torch.distributions.exp_family.ExponentialFamily"><code>torch.distributions.exp_family.ExponentialFamily</code></a></p>
<p>创建参数化的伯努利分布, 根据 <a href="#torch.distributions.bernoulli.Bernoulli.probs" title="torch.distributions.bernoulli.Bernoulli.probs"><code>probs</code></a> 或者 <a href="#torch.distributions.bernoulli.Bernoulli.logits" title="torch.distributions.bernoulli.Bernoulli.logits"><code>logits</code></a> (但不是同时都有).</p>
<p>样本是二值的 (0 或者 1). 取值 <code>1</code> 伴随概率 <code>p</code> , 或者 <code>0</code> 伴随概率 <code>1 - p</code>.</p>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = Bernoulli(torch.tensor([0.3]))
&gt;&gt;&gt; m.sample()  # 30% chance 1; 70% chance 0
tensor([ 0.])

</code></pre>
<p>参数:</p>
<ul>
<li><strong>probs</strong> (<em>Number</em>_,_ <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the probabilty of sampling <code>1</code></li>
<li><strong>logits</strong> (<em>Number</em>_,_ <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the log-odds of sampling <code>1</code></li>
</ul>
<pre><code class="language-py">arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)}
</code></pre>
<pre><code class="language-py">entropy()
</code></pre>
<pre><code class="language-py">enumerate_support(expand=True)
</code></pre>
<pre><code class="language-py">expand(batch_shape, _instance=None)
</code></pre>
<pre><code class="language-py">has_enumerate_support = True
</code></pre>
<pre><code class="language-py">log_prob(value)
</code></pre>
<pre><code class="language-py">logits
</code></pre>
<pre><code class="language-py">mean
</code></pre>
<pre><code class="language-py">param_shape
</code></pre>
<pre><code class="language-py">probs
</code></pre>
<pre><code class="language-py">sample(sample_shape=torch.Size([]))
</code></pre>
<pre><code class="language-py">support = Boolean()
</code></pre>
<pre><code class="language-py">variance
</code></pre>
<h2 id="beta">Beta</h2>
<pre><code class="language-py">class torch.distributions.beta.Beta(concentration1, concentration0, validate_args=None)
</code></pre>
<p>基类: <a href="#torch.distributions.exp_family.ExponentialFamily" title="torch.distributions.exp_family.ExponentialFamily"><code>torch.distributions.exp_family.ExponentialFamily</code></a></p>
<p>Beta 分布, 参数为 <a href="#torch.distributions.beta.Beta.concentration1" title="torch.distributions.beta.Beta.concentration1"><code>concentration1</code></a> 和 <a href="#torch.distributions.beta.Beta.concentration0" title="torch.distributions.beta.Beta.concentration0"><code>concentration0</code></a>.</p>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = Beta(torch.tensor([0.5]), torch.tensor([0.5]))
&gt;&gt;&gt; m.sample()  # Beta distributed with concentration concentration1 and concentration0
tensor([ 0.1046])

</code></pre>
<p>参数:</p>
<ul>
<li><strong>concentration1</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a> <em>or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 分布的第一个浓度参数（通常称为alpha）</li>
<li><strong>concentration0</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a> <em>or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 分布的第二个浓度参数(通常称为beta)</li>
</ul>
<pre><code class="language-py">arg_constraints = {'concentration0': GreaterThan(lower_bound=0.0), 'concentration1': GreaterThan(lower_bound=0.0)}
</code></pre>
<pre><code class="language-py">concentration0
</code></pre>
<pre><code class="language-py">concentration1
</code></pre>
<pre><code class="language-py">entropy()
</code></pre>
<pre><code class="language-py">expand(batch_shape, _instance=None)
</code></pre>
<pre><code class="language-py">has_rsample = True
</code></pre>
<pre><code class="language-py">log_prob(value)
</code></pre>
<pre><code class="language-py">mean
</code></pre>
<pre><code class="language-py">rsample(sample_shape=())
</code></pre>
<pre><code class="language-py">support = Interval(lower_bound=0.0, upper_bound=1.0)
</code></pre>
<pre><code class="language-py">variance
</code></pre>
<h2 id="binomial">Binomial</h2>
<pre><code class="language-py">class torch.distributions.binomial.Binomial(total_count=1, probs=None, logits=None, validate_args=None)
</code></pre>
<p>基类: <a href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p>
<p>创建一个Binomial 分布, 参数为 <code>total_count</code> 和 <a href="#torch.distributions.binomial.Binomial.probs" title="torch.distributions.binomial.Binomial.probs"><code>probs</code></a> 或者 <a href="#torch.distributions.binomial.Binomial.logits" title="torch.distributions.binomial.Binomial.logits"><code>logits</code></a> (但不是同时都有使用). <code>total_count</code> 必须和 [<code>probs</code>] 之间可广播(#torch.distributions.binomial.Binomial.probs "torch.distributions.binomial.Binomial.probs")/<a href="#torch.distributions.binomial.Binomial.logits" title="torch.distributions.binomial.Binomial.logits"><code>logits</code></a>.</p>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = Binomial(100, torch.tensor([0 , .2, .8, 1]))
&gt;&gt;&gt; x = m.sample()
tensor([   0.,   22.,   71.,  100.])

&gt;&gt;&gt; m = Binomial(torch.tensor([[5.], [10.]]), torch.tensor([0.5, 0.8]))
&gt;&gt;&gt; x = m.sample()
tensor([[ 4.,  5.],
 [ 7.,  6.]])

</code></pre>
<p>参数:</p>
<ul>
<li><strong>total_count</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a> <em>or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 伯努利试验次数</li>
<li><strong>probs</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 事件概率</li>
<li><strong>logits</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 事件 log-odds</li>
</ul>
<pre><code class="language-py">arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0), 'total_count': IntegerGreaterThan(lower_bound=0)}
</code></pre>
<pre><code class="language-py">enumerate_support(expand=True)
</code></pre>
<pre><code class="language-py">expand(batch_shape, _instance=None)
</code></pre>
<pre><code class="language-py">has_enumerate_support = True
</code></pre>
<pre><code class="language-py">log_prob(value)
</code></pre>
<pre><code class="language-py">logits
</code></pre>
<pre><code class="language-py">mean
</code></pre>
<pre><code class="language-py">param_shape
</code></pre>
<pre><code class="language-py">probs
</code></pre>
<pre><code class="language-py">sample(sample_shape=torch.Size([]))
</code></pre>
<pre><code class="language-py">support
</code></pre>
<pre><code class="language-py">variance
</code></pre>
<h2 id="categorical">Categorical</h2>
<pre><code class="language-py">class torch.distributions.categorical.Categorical(probs=None, logits=None, validate_args=None)
</code></pre>
<p>基类: <a href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p>
<p>创建一个 categorical 分布, 参数为 <a href="#torch.distributions.categorical.Categorical.probs" title="torch.distributions.categorical.Categorical.probs"><code>probs</code></a> 或者 <a href="#torch.distributions.categorical.Categorical.logits" title="torch.distributions.categorical.Categorical.logits"><code>logits</code></a> (但不是同时都有).</p>
<p>注意</p>
<p>它等价于从 <a href="torch.html#torch.multinomial" title="torch.multinomial"><code>torch.multinomial()</code></a> 的采样.</p>
<p>样本是整数来自<a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/7c6904e60a8ff7044a079e10eaee1f57.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/7c6904e60a8ff7044a079e10eaee1f57.jpg" alt=""></a> <code>K</code> 是 <code>probs.size(-1)</code>.</p>
<p>如果 <a href="#torch.distributions.categorical.Categorical.probs" title="torch.distributions.categorical.Categorical.probs"><code>probs</code></a> 是 1D 的, 长度为<code>K</code>, 每个元素是在该索引处对类进行抽样的相对概率.</p>
<p>如果 <a href="#torch.distributions.categorical.Categorical.probs" title="torch.distributions.categorical.Categorical.probs"><code>probs</code></a> 是 2D 的, 它被视为一组相对概率向量.</p>
<p>注意</p>
<p><a href="#torch.distributions.categorical.Categorical.probs" title="torch.distributions.categorical.Categorical.probs"><code>probs</code></a> 必须是非负的、有限的并且具有非零和, 并且它将被归一化为和为1.</p>
<p>请参阅: <a href="torch.html#torch.multinomial" title="torch.multinomial"><code>torch.multinomial()</code></a></p>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = Categorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))
&gt;&gt;&gt; m.sample()  # equal probability of 0, 1, 2, 3
tensor(3)

</code></pre>
<p>参数:</p>
<ul>
<li><strong>probs</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – event probabilities</li>
<li><strong>logits</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – event log probabilities</li>
</ul>
<pre><code class="language-py">arg_constraints = {'logits': Real(), 'probs': Simplex()}
</code></pre>
<pre><code class="language-py">entropy()
</code></pre>
<pre><code class="language-py">enumerate_support(expand=True)
</code></pre>
<pre><code class="language-py">expand(batch_shape, _instance=None)
</code></pre>
<pre><code class="language-py">has_enumerate_support = True
</code></pre>
<pre><code class="language-py">log_prob(value)
</code></pre>
<pre><code class="language-py">logits
</code></pre>
<pre><code class="language-py">mean
</code></pre>
<pre><code class="language-py">param_shape
</code></pre>
<pre><code class="language-py">probs
</code></pre>
<pre><code class="language-py">sample(sample_shape=torch.Size([]))
</code></pre>
<pre><code class="language-py">support
</code></pre>
<pre><code class="language-py">variance
</code></pre>
<h2 id="cauchy">Cauchy</h2>
<pre><code class="language-py">class torch.distributions.cauchy.Cauchy(loc, scale, validate_args=None)
</code></pre>
<p>基类: <a href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p>
<p>样本来自柯西(洛伦兹)分布. 均值为0的独立正态分布随机变量之比服从柯西分布.</p>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = Cauchy(torch.tensor([0.0]), torch.tensor([1.0]))
&gt;&gt;&gt; m.sample()  # sample from a Cauchy distribution with loc=0 and scale=1
tensor([ 2.3214])

</code></pre>
<p>参数:</p>
<ul>
<li><strong>loc</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a> <em>or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 分布的模态或中值.</li>
<li><strong>scale</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a> <em>or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – half width at half maximum.</li>
</ul>
<pre><code class="language-py">arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}
</code></pre>
<pre><code class="language-py">cdf(value)
</code></pre>
<pre><code class="language-py">entropy()
</code></pre>
<pre><code class="language-py">expand(batch_shape, _instance=None)
</code></pre>
<pre><code class="language-py">has_rsample = True
</code></pre>
<pre><code class="language-py">icdf(value)
</code></pre>
<pre><code class="language-py">log_prob(value)
</code></pre>
<pre><code class="language-py">mean
</code></pre>
<pre><code class="language-py">rsample(sample_shape=torch.Size([]))
</code></pre>
<pre><code class="language-py">support = Real()
</code></pre>
<pre><code class="language-py">variance
</code></pre>
<h2 id="chi2">Chi2</h2>
<pre><code class="language-py">class torch.distributions.chi2.Chi2(df, validate_args=None)
</code></pre>
<p>基类: <a href="#torch.distributions.gamma.Gamma" title="torch.distributions.gamma.Gamma"><code>torch.distributions.gamma.Gamma</code></a></p>
<p>创建由形状参数<a href="#torch.distributions.chi2.Chi2.df" title="torch.distributions.chi2.Chi2.df"><code>df</code></a>参数化的Chi2分布. 这完全等同于 <code>Gamma(alpha=0.5*df, beta=0.5)</code></p>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = Chi2(torch.tensor([1.0]))
&gt;&gt;&gt; m.sample()  # Chi2 distributed with shape df=1
tensor([ 0.1046])

</code></pre>
<p>| 参数: | <strong>df</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a> <em>or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 分布的形状参数 |</p>
<pre><code class="language-py">arg_constraints = {'df': GreaterThan(lower_bound=0.0)}
</code></pre>
<pre><code class="language-py">df
</code></pre>
<pre><code class="language-py">expand(batch_shape, _instance=None)
</code></pre>
<h2 id="dirichlet">Dirichlet</h2>
<pre><code class="language-py">class torch.distributions.dirichlet.Dirichlet(concentration, validate_args=None)
</code></pre>
<p>基类: <a href="#torch.distributions.exp_family.ExponentialFamily" title="torch.distributions.exp_family.ExponentialFamily"><code>torch.distributions.exp_family.ExponentialFamily</code></a></p>
<p>创建一个 Dirichlet 分布, 参数为<code>concentration</code>.</p>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = Dirichlet(torch.tensor([0.5, 0.5]))
&gt;&gt;&gt; m.sample()  # Dirichlet distributed with concentrarion concentration
tensor([ 0.1046,  0.8954])

</code></pre>
<p>| 参数: | <strong>concentration</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 分布的浓度参数（通常称为alpha） |</p>
<pre><code class="language-py">arg_constraints = {'concentration': GreaterThan(lower_bound=0.0)}
</code></pre>
<pre><code class="language-py">entropy()
</code></pre>
<pre><code class="language-py">expand(batch_shape, _instance=None)
</code></pre>
<pre><code class="language-py">has_rsample = True
</code></pre>
<pre><code class="language-py">log_prob(value)
</code></pre>
<pre><code class="language-py">mean
</code></pre>
<pre><code class="language-py">rsample(sample_shape=())
</code></pre>
<pre><code class="language-py">support = Simplex()
</code></pre>
<pre><code class="language-py">variance
</code></pre>
<h2 id="exponential">Exponential</h2>
<pre><code class="language-py">class torch.distributions.exponential.Exponential(rate, validate_args=None)
</code></pre>
<p>基类: <a href="#torch.distributions.exp_family.ExponentialFamily" title="torch.distributions.exp_family.ExponentialFamily"><code>torch.distributions.exp_family.ExponentialFamily</code></a></p>
<p>创建由<code>rate</code>参数化的指数分布.</p>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = Exponential(torch.tensor([1.0]))
&gt;&gt;&gt; m.sample()  # Exponential distributed with rate=1
tensor([ 0.1046])

</code></pre>
<p>| 参数: | <strong>rate</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a> <em>or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – rate = 1 / 分布的scale |</p>
<pre><code class="language-py">arg_constraints = {'rate': GreaterThan(lower_bound=0.0)}
</code></pre>
<pre><code class="language-py">cdf(value)
</code></pre>
<pre><code class="language-py">entropy()
</code></pre>
<pre><code class="language-py">expand(batch_shape, _instance=None)
</code></pre>
<pre><code class="language-py">has_rsample = True
</code></pre>
<pre><code class="language-py">icdf(value)
</code></pre>
<pre><code class="language-py">log_prob(value)
</code></pre>
<pre><code class="language-py">mean
</code></pre>
<pre><code class="language-py">rsample(sample_shape=torch.Size([]))
</code></pre>
<pre><code class="language-py">stddev
</code></pre>
<pre><code class="language-py">support = GreaterThan(lower_bound=0.0)
</code></pre>
<pre><code class="language-py">variance
</code></pre>
<h2 id="fishersnedecor">FisherSnedecor</h2>
<pre><code class="language-py">class torch.distributions.fishersnedecor.FisherSnedecor(df1, df2, validate_args=None)
</code></pre>
<p>基类: <a href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p>
<p>创建由<code>df1</code>和<code>df2</code>参数化的Fisher-Snedecor分布</p>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = FisherSnedecor(torch.tensor([1.0]), torch.tensor([2.0]))
&gt;&gt;&gt; m.sample()  # Fisher-Snedecor-distributed with df1=1 and df2=2
tensor([ 0.2453])

</code></pre>
<p>参数:</p>
<ul>
<li><strong>df1</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a> <em>or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 自由度参数1</li>
<li><strong>df2</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a> <em>or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 自由度参数2</li>
</ul>
<pre><code class="language-py">arg_constraints = {'df1': GreaterThan(lower_bound=0.0), 'df2': GreaterThan(lower_bound=0.0)}
</code></pre>
<pre><code class="language-py">expand(batch_shape, _instance=None)
</code></pre>
<pre><code class="language-py">has_rsample = True
</code></pre>
<pre><code class="language-py">log_prob(value)
</code></pre>
<pre><code class="language-py">mean
</code></pre>
<pre><code class="language-py">rsample(sample_shape=torch.Size([]))
</code></pre>
<pre><code class="language-py">support = GreaterThan(lower_bound=0.0)
</code></pre>
<pre><code class="language-py">variance
</code></pre>
<h2 id="gamma">Gamma</h2>
<pre><code class="language-py">class torch.distributions.gamma.Gamma(concentration, rate, validate_args=None)
</code></pre>
<p>基类: <a href="#torch.distributions.exp_family.ExponentialFamily" title="torch.distributions.exp_family.ExponentialFamily"><code>torch.distributions.exp_family.ExponentialFamily</code></a></p>
<p>创建由<code>concentration</code>和<code>rate</code>参数化的伽马分布. .</p>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = Gamma(torch.tensor([1.0]), torch.tensor([1.0]))
&gt;&gt;&gt; m.sample()  # Gamma distributed with concentration=1 and rate=1
tensor([ 0.1046])

</code></pre>
<p>参数:</p>
<ul>
<li><strong>concentration</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a> <em>or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 分布的形状参数（通常称为alpha）</li>
<li><strong>rate</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a> <em>or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – rate = 1 / 分布scale (通常称为beta )</li>
</ul>
<pre><code class="language-py">arg_constraints = {'concentration': GreaterThan(lower_bound=0.0), 'rate': GreaterThan(lower_bound=0.0)}
</code></pre>
<pre><code class="language-py">entropy()
</code></pre>
<pre><code class="language-py">expand(batch_shape, _instance=None)
</code></pre>
<pre><code class="language-py">has_rsample = True
</code></pre>
<pre><code class="language-py">log_prob(value)
</code></pre>
<pre><code class="language-py">mean
</code></pre>
<pre><code class="language-py">rsample(sample_shape=torch.Size([]))
</code></pre>
<pre><code class="language-py">support = GreaterThan(lower_bound=0.0)
</code></pre>
<pre><code class="language-py">variance
</code></pre>
<h2 id="geometric">Geometric</h2>
<pre><code class="language-py">class torch.distributions.geometric.Geometric(probs=None, logits=None, validate_args=None)
</code></pre>
<p>基类: <a href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p>
<p>创建由<code>probs</code>参数化的几何分布, 其中<code>probs</code>是伯努利试验成功的概率. 它表示概率在 <a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/10396db36bab7b7242cfe94f04374444.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/10396db36bab7b7242cfe94f04374444.jpg" alt=""></a> 次伯努利试验中, 前 <a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/a1c2f8d5b1226e67bdb44b12a6ddf18b.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/a1c2f8d5b1226e67bdb44b12a6ddf18b.jpg" alt=""></a> 试验失败, 然后成功.</p>
<p>样本是非负整数 [0, <a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/06485c2c6e992cf346fdfe033a86a10d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/06485c2c6e992cf346fdfe033a86a10d.jpg" alt=""></a>).</p>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = Geometric(torch.tensor([0.3]))
&gt;&gt;&gt; m.sample()  # underlying Bernoulli has 30% chance 1; 70% chance 0
tensor([ 2.])

</code></pre>
<p>参数:</p>
<ul>
<li><strong>probs</strong> (<em>Number</em>_,_ <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 抽样<code>1</code>的概率 . 必须是在范围 (0, 1]</li>
<li><strong>logits</strong> (<em>Number</em>_,_ <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 抽样 <code>1</code>的log-odds.</li>
</ul>
<pre><code class="language-py">arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)}
</code></pre>
<pre><code class="language-py">entropy()
</code></pre>
<pre><code class="language-py">expand(batch_shape, _instance=None)
</code></pre>
<pre><code class="language-py">log_prob(value)
</code></pre>
<pre><code class="language-py">logits
</code></pre>
<pre><code class="language-py">mean
</code></pre>
<pre><code class="language-py">probs
</code></pre>
<pre><code class="language-py">sample(sample_shape=torch.Size([]))
</code></pre>
<pre><code class="language-py">support = IntegerGreaterThan(lower_bound=0)
</code></pre>
<pre><code class="language-py">variance
</code></pre>
<h2 id="gumbel">Gumbel</h2>
<pre><code class="language-py">class torch.distributions.gumbel.Gumbel(loc, scale, validate_args=None)
</code></pre>
<p>基类: <a href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code>torch.distributions.transformed_distribution.TransformedDistribution</code></a></p>
<p>来自Gumbel分布的样本.</p>
<p>Examples:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = Gumbel(torch.tensor([1.0]), torch.tensor([2.0]))
&gt;&gt;&gt; m.sample()  # sample from Gumbel distribution with loc=1, scale=2
tensor([ 1.0124])

</code></pre>
<p>参数:</p>
<ul>
<li><strong>loc</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a> <em>or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 分布的位置参数</li>
<li><strong>scale</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a> <em>or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 分布的scale 参数</li>
</ul>
<pre><code class="language-py">arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}
</code></pre>
<pre><code class="language-py">entropy()
</code></pre>
<pre><code class="language-py">expand(batch_shape, _instance=None)
</code></pre>
<pre><code class="language-py">mean
</code></pre>
<pre><code class="language-py">stddev
</code></pre>
<pre><code class="language-py">support = Real()
</code></pre>
<pre><code class="language-py">variance
</code></pre>
<h2 id="halfcauchy">HalfCauchy</h2>
<pre><code class="language-py">class torch.distributions.half_cauchy.HalfCauchy(scale, validate_args=None)
</code></pre>
<p>基类: <a href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code>torch.distributions.transformed_distribution.TransformedDistribution</code></a></p>
<p>创建<code>scale</code>参数化的半正态分布:</p>
<pre><code class="language-py">X ~ Cauchy(0, scale)
Y = |X| ~ HalfCauchy(scale)

</code></pre>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = HalfCauchy(torch.tensor([1.0]))
&gt;&gt;&gt; m.sample()  # half-cauchy distributed with scale=1
tensor([ 2.3214])

</code></pre>
<p>| 参数: | <strong>scale</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a> <em>or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 完全柯西分布的scale |</p>
<pre><code class="language-py">arg_constraints = {'scale': GreaterThan(lower_bound=0.0)}
</code></pre>
<pre><code class="language-py">cdf(value)
</code></pre>
<pre><code class="language-py">entropy()
</code></pre>
<pre><code class="language-py">expand(batch_shape, _instance=None)
</code></pre>
<pre><code class="language-py">has_rsample = True
</code></pre>
<pre><code class="language-py">icdf(prob)
</code></pre>
<pre><code class="language-py">log_prob(value)
</code></pre>
<pre><code class="language-py">mean
</code></pre>
<pre><code class="language-py">scale
</code></pre>
<pre><code class="language-py">support = GreaterThan(lower_bound=0.0)
</code></pre>
<pre><code class="language-py">variance
</code></pre>
<h2 id="halfnormal">HalfNormal</h2>
<pre><code class="language-py">class torch.distributions.half_normal.HalfNormal(scale, validate_args=None)
</code></pre>
<p>基类: <a href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code>torch.distributions.transformed_distribution.TransformedDistribution</code></a></p>
<p>创建按<code>scale</code>参数化的半正态分布:</p>
<pre><code class="language-py">X ~ Normal(0, scale)
Y = |X| ~ HalfNormal(scale)

</code></pre>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = HalfNormal(torch.tensor([1.0]))
&gt;&gt;&gt; m.sample()  # half-normal distributed with scale=1
tensor([ 0.1046])

</code></pre>
<p>| 参数: | <strong>scale</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a> <em>or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 完全正态分布的scale |</p>
<pre><code class="language-py">arg_constraints = {'scale': GreaterThan(lower_bound=0.0)}
</code></pre>
<pre><code class="language-py">cdf(value)
</code></pre>
<pre><code class="language-py">entropy()
</code></pre>
<pre><code class="language-py">expand(batch_shape, _instance=None)
</code></pre>
<pre><code class="language-py">has_rsample = True
</code></pre>
<pre><code class="language-py">icdf(prob)
</code></pre>
<pre><code class="language-py">log_prob(value)
</code></pre>
<pre><code class="language-py">mean
</code></pre>
<pre><code class="language-py">scale
</code></pre>
<pre><code class="language-py">support = GreaterThan(lower_bound=0.0)
</code></pre>
<pre><code class="language-py">variance
</code></pre>
<h2 id="independent">Independent</h2>
<pre><code class="language-py">class torch.distributions.independent.Independent(base_distribution, reinterpreted_batch_ndims, validate_args=None)
</code></pre>
<p>基类: <a href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p>
<p>重新解释一些分布的批量 dims 作为 event dims.</p>
<p>这主要用于改变<a href="#torch.distributions.independent.Independent.log_prob" title="torch.distributions.independent.Independent.log_prob"><code>log_prob()</code></a>结果的形状.例如, 要创建与多元正态分布形状相同的对角正态分布(因此它们是可互换的), 您可以这样做:</p>
<pre><code class="language-py">&gt;&gt;&gt; loc = torch.zeros(3)
&gt;&gt;&gt; scale = torch.ones(3)
&gt;&gt;&gt; mvn = MultivariateNormal(loc, scale_tril=torch.diag(scale))
&gt;&gt;&gt; [mvn.batch_shape, mvn.event_shape]
[torch.Size(()), torch.Size((3,))]
&gt;&gt;&gt; normal = Normal(loc, scale)
&gt;&gt;&gt; [normal.batch_shape, normal.event_shape]
[torch.Size((3,)), torch.Size(())]
&gt;&gt;&gt; diagn = Independent(normal, 1)
&gt;&gt;&gt; [diagn.batch_shape, diagn.event_shape]
[torch.Size(()), torch.Size((3,))]

</code></pre>
<p>参数:</p>
<ul>
<li><strong>base_distribution</strong> (<a href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><em>torch.distributions.distribution.Distribution</em></a>) – 基础分布</li>
<li><strong>reinterpreted_batch_ndims</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) –要重解释的批量dims的数量</li>
</ul>
<pre><code class="language-py">arg_constraints = {}
</code></pre>
<pre><code class="language-py">entropy()
</code></pre>
<pre><code class="language-py">enumerate_support(expand=True)
</code></pre>
<pre><code class="language-py">expand(batch_shape, _instance=None)
</code></pre>
<pre><code class="language-py">has_enumerate_support
</code></pre>
<pre><code class="language-py">has_rsample
</code></pre>
<pre><code class="language-py">log_prob(value)
</code></pre>
<pre><code class="language-py">mean
</code></pre>
<pre><code class="language-py">rsample(sample_shape=torch.Size([]))
</code></pre>
<pre><code class="language-py">sample(sample_shape=torch.Size([]))
</code></pre>
<pre><code class="language-py">support
</code></pre>
<pre><code class="language-py">variance
</code></pre>
<h2 id="laplace">Laplace</h2>
<pre><code class="language-py">class torch.distributions.laplace.Laplace(loc, scale, validate_args=None)
</code></pre>
<p>基类: <a href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p>
<p>创建参数化的拉普拉斯分布, 参数是 <code>loc</code> 和 :attr:’scale’.</p>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = Laplace(torch.tensor([0.0]), torch.tensor([1.0]))
&gt;&gt;&gt; m.sample()  # Laplace distributed with loc=0, scale=1
tensor([ 0.1046])

</code></pre>
<p>参数:</p>
<ul>
<li><strong>loc</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a> <em>or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 分布均值</li>
<li><strong>scale</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a> <em>or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 分布scale</li>
</ul>
<pre><code class="language-py">arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}
</code></pre>
<pre><code class="language-py">cdf(value)
</code></pre>
<pre><code class="language-py">entropy()
</code></pre>
<pre><code class="language-py">expand(batch_shape, _instance=None)
</code></pre>
<pre><code class="language-py">has_rsample = True
</code></pre>
<pre><code class="language-py">icdf(value)
</code></pre>
<pre><code class="language-py">log_prob(value)
</code></pre>
<pre><code class="language-py">mean
</code></pre>
<pre><code class="language-py">rsample(sample_shape=torch.Size([]))
</code></pre>
<pre><code class="language-py">stddev
</code></pre>
<pre><code class="language-py">support = Real()
</code></pre>
<pre><code class="language-py">variance
</code></pre>
<h2 id="lognormal">LogNormal</h2>
<pre><code class="language-py">class torch.distributions.log_normal.LogNormal(loc, scale, validate_args=None)
</code></pre>
<p>基类: <a href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code>torch.distributions.transformed_distribution.TransformedDistribution</code></a></p>
<p>创建参数化的对数正态分布, 参数为 <a href="#torch.distributions.log_normal.LogNormal.loc" title="torch.distributions.log_normal.LogNormal.loc"><code>loc</code></a> 和 <a href="#torch.distributions.log_normal.LogNormal.scale" title="torch.distributions.log_normal.LogNormal.scale"><code>scale</code></a>:</p>
<pre><code class="language-py">X ~ Normal(loc, scale)
Y = exp(X) ~ LogNormal(loc, scale)

</code></pre>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = LogNormal(torch.tensor([0.0]), torch.tensor([1.0]))
&gt;&gt;&gt; m.sample()  # log-normal distributed with mean=0 and stddev=1
tensor([ 0.1046])

</code></pre>
<p>参数:</p>
<ul>
<li><strong>loc</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a> <em>or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 分布对数平均值</li>
<li><strong>scale</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a> <em>or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 分布对数的标准差</li>
</ul>
<pre><code class="language-py">arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}
</code></pre>
<pre><code class="language-py">entropy()
</code></pre>
<pre><code class="language-py">expand(batch_shape, _instance=None)
</code></pre>
<pre><code class="language-py">has_rsample = True
</code></pre>
<pre><code class="language-py">loc
</code></pre>
<pre><code class="language-py">mean
</code></pre>
<pre><code class="language-py">scale
</code></pre>
<pre><code class="language-py">support = GreaterThan(lower_bound=0.0)
</code></pre>
<pre><code class="language-py">variance
</code></pre>
<h2 id="lowrankmultivariatenormal">LowRankMultivariateNormal</h2>
<pre><code class="language-py">class torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal(loc, cov_factor, cov_diag, validate_args=None)
</code></pre>
<p>基类: <a href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p>
<p>使用由<code>cov_factor</code>和<code>cov_diag</code>参数化的低秩形式的协方差矩阵创建多元正态分布:</p>
<pre><code class="language-py">covariance_matrix = cov_factor @ cov_factor.T + cov_diag

</code></pre>
<p>Example</p>
<pre><code class="language-py">&gt;&gt;&gt; m = LowRankMultivariateNormal(torch.zeros(2), torch.tensor([1, 0]), torch.tensor([1, 1]))
&gt;&gt;&gt; m.sample()  # normally distributed with mean=`[0,0]`, cov_factor=`[1,0]`, cov_diag=`[1,1]`
tensor([-0.2102, -0.5429])

</code></pre>
<p>参数:</p>
<ul>
<li><strong>loc</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 分布的均值, 形状为 <code>batch_shape + event_shape</code></li>
<li><strong>cov_factor</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 协方差矩阵低秩形式的因子部分, 形状为 <code>batch_shape + event_shape + (rank,)</code></li>
<li><strong>cov_diag</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 协方差矩阵的低秩形式的对角部分, 形状为 <code>batch_shape + event_shape</code></li>
</ul>
<p>注意</p>
<p>避免了协方差矩阵的行列式和逆的计算, 当 <code>cov_factor.shape[1] &lt;&lt; cov_factor.shape[0]</code> 由于 <a href="https://en.wikipedia.org/wiki/Woodbury_matrix_identity">Woodbury matrix identity</a> 和 <a href="https://en.wikipedia.org/wiki/Matrix_determinant_lemma">matrix determinant lemma</a>. 由于这些公式, 我们只需要计算小尺寸“capacitance”矩阵的行列式和逆:</p>
<pre><code class="language-py">capacitance = I + cov_factor.T @ inv(cov_diag) @ cov_factor

</code></pre>
<pre><code class="language-py">arg_constraints = {'cov_diag': GreaterThan(lower_bound=0.0), 'cov_factor': Real(), 'loc': Real()}
</code></pre>
<pre><code class="language-py">covariance_matrix
</code></pre>
<pre><code class="language-py">entropy()
</code></pre>
<pre><code class="language-py">expand(batch_shape, _instance=None)
</code></pre>
<pre><code class="language-py">has_rsample = True
</code></pre>
<pre><code class="language-py">log_prob(value)
</code></pre>
<pre><code class="language-py">mean
</code></pre>
<pre><code class="language-py">precision_matrix
</code></pre>
<pre><code class="language-py">rsample(sample_shape=torch.Size([]))
</code></pre>
<pre><code class="language-py">scale_tril
</code></pre>
<pre><code class="language-py">support = Real()
</code></pre>
<pre><code class="language-py">variance
</code></pre>
<h2 id="multinomial">Multinomial</h2>
<pre><code class="language-py">class torch.distributions.multinomial.Multinomial(total_count=1, probs=None, logits=None, validate_args=None)
</code></pre>
<p>基类: <a href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p>
<p>创建由<code>total_count</code>和<code>probs</code>或<code>logits</code>（但不是两者）参数化的多项式分布. <code>probs</code>的最内层维度是对类别的索引. 所有其他维度索引批次.</p>
<p>注意 <code>total_count</code> 不需要指定, 当只有 <a href="#torch.distributions.multinomial.Multinomial.log_prob" title="torch.distributions.multinomial.Multinomial.log_prob"><code>log_prob()</code></a> 被调用</p>
<p>注意</p>
<p><a href="#torch.distributions.multinomial.Multinomial.probs" title="torch.distributions.multinomial.Multinomial.probs"><code>probs</code></a> 必须是非负的、有限的并且具有非零和, 并且它将被归一化为和为1.</p>
<ul>
<li><a href="#torch.distributions.multinomial.Multinomial.sample" title="torch.distributions.multinomial.Multinomial.sample"><code>sample()</code></a> 所有参数和样本都需要一个共享的<code>total_count</code>.</li>
<li><a href="#torch.distributions.multinomial.Multinomial.log_prob" title="torch.distributions.multinomial.Multinomial.log_prob"><code>log_prob()</code></a> 允许每个参数和样本使用不同的<code>total_count</code>.</li>
</ul>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = Multinomial(100, torch.tensor([ 1., 1., 1., 1.]))
&gt;&gt;&gt; x = m.sample()  # equal probability of 0, 1, 2, 3
tensor([ 21.,  24.,  30.,  25.])

&gt;&gt;&gt; Multinomial(probs=torch.tensor([1., 1., 1., 1.])).log_prob(x)
tensor([-4.1338])

</code></pre>
<p>参数:</p>
<ul>
<li><strong>total_count</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – 试验次数</li>
<li><strong>probs</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 事件概率</li>
<li><strong>logits</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 事件对数概率</li>
</ul>
<pre><code class="language-py">arg_constraints = {'logits': Real(), 'probs': Simplex()}
</code></pre>
<pre><code class="language-py">expand(batch_shape, _instance=None)
</code></pre>
<pre><code class="language-py">log_prob(value)
</code></pre>
<pre><code class="language-py">logits
</code></pre>
<pre><code class="language-py">mean
</code></pre>
<pre><code class="language-py">param_shape
</code></pre>
<pre><code class="language-py">probs
</code></pre>
<pre><code class="language-py">sample(sample_shape=torch.Size([]))
</code></pre>
<pre><code class="language-py">support
</code></pre>
<pre><code class="language-py">variance
</code></pre>
<h2 id="multivariatenormal">MultivariateNormal</h2>
<pre><code class="language-py">class torch.distributions.multivariate_normal.MultivariateNormal(loc, covariance_matrix=None, precision_matrix=None, scale_tril=None, validate_args=None)
</code></pre>
<p>基类: <a href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p>
<p>创建由均值向量和协方差矩阵参数化的多元正态(也称为高斯)分布.</p>
<p>多元正态分布可以用正定协方差矩阵<a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/ea86c11eaef9af2b4d699b88c2474ffd.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/ea86c11eaef9af2b4d699b88c2474ffd.jpg" alt=""></a>来参数化或者一个正定的精度矩阵 <a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/1949bfcc1decf198a2ff50b6e25f4cf6.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/1949bfcc1decf198a2ff50b6e25f4cf6.jpg" alt=""></a> 或者是一个正对角项的下三角矩阵 <a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/f4996f1b5056dd364eab16f975b808ff.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/f4996f1b5056dd364eab16f975b808ff.jpg" alt=""></a>, 例如 <a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/6749b6afc75abfc8e0652ac8e5c0b8d8.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/6749b6afc75abfc8e0652ac8e5c0b8d8.jpg" alt=""></a>. 这个三角矩阵可以通过协方差的Cholesky分解得到.</p>
<p>例子</p>
<pre><code class="language-py">&gt;&gt;&gt; m = MultivariateNormal(torch.zeros(2), torch.eye(2))
&gt;&gt;&gt; m.sample()  # normally distributed with mean=`[0,0]` and covariance_matrix=`I`
tensor([-0.2102, -0.5429])

</code></pre>
<p>参数:</p>
<ul>
<li><strong>loc</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 分布的均值</li>
<li><strong>covariance_matrix</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 正定协方差矩阵</li>
<li><strong>precision_matrix</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 正定精度矩阵</li>
<li><strong>scale_tril</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 具有正值对角线的下三角协方差因子</li>
</ul>
<p>注意</p>
<p>仅仅一个 <a href="#torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix" title="torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix"><code>covariance_matrix</code></a> 或者 <a href="#torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix" title="torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix"><code>precision_matrix</code></a> 或者 <a href="#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril" title="torch.distributions.multivariate_normal.MultivariateNormal.scale_tril"><code>scale_tril</code></a> 可被指定.</p>
<p>使用 <a href="#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril" title="torch.distributions.multivariate_normal.MultivariateNormal.scale_tril"><code>scale_tril</code></a> 会更有效率: 内部的所有计算都基于 <a href="#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril" title="torch.distributions.multivariate_normal.MultivariateNormal.scale_tril"><code>scale_tril</code></a>. 如果 <a href="#torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix" title="torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix"><code>covariance_matrix</code></a> 或者 <a href="#torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix" title="torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix"><code>precision_matrix</code></a> 已经被传入, 它仅用于使用Cholesky分解计算相应的下三角矩阵.</p>
<pre><code class="language-py">arg_constraints = {'covariance_matrix': PositiveDefinite(), 'loc': RealVector(), 'precision_matrix': PositiveDefinite(), 'scale_tril': LowerCholesky()}
</code></pre>
<pre><code class="language-py">covariance_matrix
</code></pre>
<pre><code class="language-py">entropy()
</code></pre>
<pre><code class="language-py">expand(batch_shape, _instance=None)
</code></pre>
<pre><code class="language-py">has_rsample = True
</code></pre>
<pre><code class="language-py">log_prob(value)
</code></pre>
<pre><code class="language-py">mean
</code></pre>
<pre><code class="language-py">precision_matrix
</code></pre>
<pre><code class="language-py">rsample(sample_shape=torch.Size([]))
</code></pre>
<pre><code class="language-py">scale_tril
</code></pre>
<pre><code class="language-py">support = Real()
</code></pre>
<pre><code class="language-py">variance
</code></pre>
<h2 id="negativebinomial">NegativeBinomial</h2>
<pre><code class="language-py">class torch.distributions.negative_binomial.NegativeBinomial(total_count, probs=None, logits=None, validate_args=None)
</code></pre>
<p>基类: <a href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p>
<p>创建一个负二项分布, 即在达到<code>total_count</code>失败之前所需的独立相同伯努利试验的数量的分布. 每次伯努利试验成功的概率都是<code>probs</code>.</p>
<p>参数:</p>
<ul>
<li><strong>total_count</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a> <em>or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 非负数伯努利试验停止的次数, 虽然分布仍然对实数有效</li>
<li><strong>probs</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 事件概率, 区间为 [0, 1)</li>
<li><strong>logits</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 事件对数几率 - 成功概率的几率</li>
</ul>
<pre><code class="language-py">arg_constraints = {'logits': Real(), 'probs': HalfOpenInterval(lower_bound=0.0, upper_bound=1.0), 'total_count': GreaterThanEq(lower_bound=0)}
</code></pre>
<pre><code class="language-py">expand(batch_shape, _instance=None)
</code></pre>
<pre><code class="language-py">log_prob(value)
</code></pre>
<pre><code class="language-py">logits
</code></pre>
<pre><code class="language-py">mean
</code></pre>
<pre><code class="language-py">param_shape
</code></pre>
<pre><code class="language-py">probs
</code></pre>
<pre><code class="language-py">sample(sample_shape=torch.Size([]))
</code></pre>
<pre><code class="language-py">support = IntegerGreaterThan(lower_bound=0)
</code></pre>
<pre><code class="language-py">variance
</code></pre>
<h2 id="normal">Normal</h2>
<pre><code class="language-py">class torch.distributions.normal.Normal(loc, scale, validate_args=None)
</code></pre>
<p>基类: <a href="#torch.distributions.exp_family.ExponentialFamily" title="torch.distributions.exp_family.ExponentialFamily"><code>torch.distributions.exp_family.ExponentialFamily</code></a></p>
<p>创建由<code>loc</code>和<code>scale</code>参数化的正态（也称为高斯）分布</p>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = Normal(torch.tensor([0.0]), torch.tensor([1.0]))
&gt;&gt;&gt; m.sample()  # normally distributed with loc=0 and scale=1
tensor([ 0.1046])

</code></pre>
<p>参数:</p>
<ul>
<li><strong>loc</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a> <em>or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 均值 (也被称为 mu)</li>
<li><strong>scale</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a> <em>or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 标准差(也被称为) sigma)</li>
</ul>
<pre><code class="language-py">arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}
</code></pre>
<pre><code class="language-py">cdf(value)
</code></pre>
<pre><code class="language-py">entropy()
</code></pre>
<pre><code class="language-py">expand(batch_shape, _instance=None)
</code></pre>
<pre><code class="language-py">has_rsample = True
</code></pre>
<pre><code class="language-py">icdf(value)
</code></pre>
<pre><code class="language-py">log_prob(value)
</code></pre>
<pre><code class="language-py">mean
</code></pre>
<pre><code class="language-py">rsample(sample_shape=torch.Size([]))
</code></pre>
<pre><code class="language-py">sample(sample_shape=torch.Size([]))
</code></pre>
<pre><code class="language-py">stddev
</code></pre>
<pre><code class="language-py">support = Real()
</code></pre>
<pre><code class="language-py">variance
</code></pre>
<h2 id="onehotcategorical">OneHotCategorical</h2>
<pre><code class="language-py">class torch.distributions.one_hot_categorical.OneHotCategorical(probs=None, logits=None, validate_args=None)
</code></pre>
<p>基类: <a href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p>
<p>创建一个由<code>probs</code>或l<code>ogits</code>参数化的One Hot Categorical 分布</p>
<p>样本是大小为 <code>probs.size(-1)</code>热编码向量.</p>
<p>注意</p>
<p><code>probs</code>必须是非负的, 有限的并且具有非零和, 并且它将被归一化为总和为1.</p>
<p>请参见: <code>torch.distributions.Categorical()</code> 对于指定 <a href="#torch.distributions.one_hot_categorical.OneHotCategorical.probs" title="torch.distributions.one_hot_categorical.OneHotCategorical.probs"><code>probs</code></a> 和 <a href="#torch.distributions.one_hot_categorical.OneHotCategorical.logits" title="torch.distributions.one_hot_categorical.OneHotCategorical.logits"><code>logits</code></a>.</p>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = OneHotCategorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))
&gt;&gt;&gt; m.sample()  # equal probability of 0, 1, 2, 3
tensor([ 0.,  0.,  0.,  1.])

</code></pre>
<p>参数:</p>
<ul>
<li><strong>probs</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – event probabilities</li>
<li><strong>logits</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – event log probabilities</li>
</ul>
<pre><code class="language-py">arg_constraints = {'logits': Real(), 'probs': Simplex()}
</code></pre>
<pre><code class="language-py">entropy()
</code></pre>
<pre><code class="language-py">enumerate_support(expand=True)
</code></pre>
<pre><code class="language-py">expand(batch_shape, _instance=None)
</code></pre>
<pre><code class="language-py">has_enumerate_support = True
</code></pre>
<pre><code class="language-py">log_prob(value)
</code></pre>
<pre><code class="language-py">logits
</code></pre>
<pre><code class="language-py">mean
</code></pre>
<pre><code class="language-py">param_shape
</code></pre>
<pre><code class="language-py">probs
</code></pre>
<pre><code class="language-py">sample(sample_shape=torch.Size([]))
</code></pre>
<pre><code class="language-py">support = Simplex()
</code></pre>
<pre><code class="language-py">variance
</code></pre>
<h2 id="pareto">Pareto</h2>
<pre><code class="language-py">class torch.distributions.pareto.Pareto(scale, alpha, validate_args=None)
</code></pre>
<p>基类: <a href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code>torch.distributions.transformed_distribution.TransformedDistribution</code></a></p>
<p>来自Pareto Type 1分布的样本.</p>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = Pareto(torch.tensor([1.0]), torch.tensor([1.0]))
&gt;&gt;&gt; m.sample()  # sample from a Pareto distribution with scale=1 and alpha=1
tensor([ 1.5623])

</code></pre>
<p>参数:</p>
<ul>
<li><strong>scale</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a> <em>or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 分布的Scale</li>
<li><strong>alpha</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a> <em>or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 分布的Shape</li>
</ul>
<pre><code class="language-py">arg_constraints = {'alpha': GreaterThan(lower_bound=0.0), 'scale': GreaterThan(lower_bound=0.0)}
</code></pre>
<pre><code class="language-py">entropy()
</code></pre>
<pre><code class="language-py">expand(batch_shape, _instance=None)
</code></pre>
<pre><code class="language-py">mean
</code></pre>
<pre><code class="language-py">support
</code></pre>
<pre><code class="language-py">variance
</code></pre>
<h2 id="poisson">Poisson</h2>
<pre><code class="language-py">class torch.distributions.poisson.Poisson(rate, validate_args=None)
</code></pre>
<p>基类: <a href="#torch.distributions.exp_family.ExponentialFamily" title="torch.distributions.exp_family.ExponentialFamily"><code>torch.distributions.exp_family.ExponentialFamily</code></a></p>
<p>创建按<code>rate</code>参数化的泊松分布</p>
<p>样本是非负整数, pmf是</p>
<p><a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/32c47de57300c954795486fea3201bdc.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/32c47de57300c954795486fea3201bdc.jpg" alt=""></a></p>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = Poisson(torch.tensor([4]))
&gt;&gt;&gt; m.sample()
tensor([ 3.])

</code></pre>
<p>| 参数: | <strong>rate</strong> (<em>Number</em>_,_ <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – rate 参数 |</p>
<pre><code class="language-py">arg_constraints = {'rate': GreaterThan(lower_bound=0.0)}
</code></pre>
<pre><code class="language-py">expand(batch_shape, _instance=None)
</code></pre>
<pre><code class="language-py">log_prob(value)
</code></pre>
<pre><code class="language-py">mean
</code></pre>
<pre><code class="language-py">sample(sample_shape=torch.Size([]))
</code></pre>
<pre><code class="language-py">support = IntegerGreaterThan(lower_bound=0)
</code></pre>
<pre><code class="language-py">variance
</code></pre>
<h2 id="relaxedbernoulli">RelaxedBernoulli</h2>
<pre><code class="language-py">class torch.distributions.relaxed_bernoulli.RelaxedBernoulli(temperature, probs=None, logits=None, validate_args=None)
</code></pre>
<p>基类: <a href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code>torch.distributions.transformed_distribution.TransformedDistribution</code></a></p>
<p>创建一个RelaxedBernoulli分布, 通过<a href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature" title="torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature"><code>temperature</code></a>参数化, 以及<code>probs</code>或<code>logits</code>（但不是两者）. 这是伯努利分布的松弛版本, 因此值在（0,1）中, 并且具有可重参数化的样本.</p>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = RelaxedBernoulli(torch.tensor([2.2]),
 torch.tensor([0.1, 0.2, 0.3, 0.99]))
&gt;&gt;&gt; m.sample()
tensor([ 0.2951,  0.3442,  0.8918,  0.9021])

</code></pre>
<p>参数:</p>
<ul>
<li><strong>temperature</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 松弛 temperature</li>
<li><strong>probs</strong> (<em>Number</em>_,_ <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) –采样 <code>1</code> 的概率</li>
<li><strong>logits</strong> (<em>Number</em>_,_ <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 采样 <code>1</code> 的对数概率</li>
</ul>
<pre><code class="language-py">arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)}
</code></pre>
<pre><code class="language-py">expand(batch_shape, _instance=None)
</code></pre>
<pre><code class="language-py">has_rsample = True
</code></pre>
<pre><code class="language-py">logits
</code></pre>
<pre><code class="language-py">probs
</code></pre>
<pre><code class="language-py">support = Interval(lower_bound=0.0, upper_bound=1.0)
</code></pre>
<pre><code class="language-py">temperature
</code></pre>
<h2 id="relaxedonehotcategorical">RelaxedOneHotCategorical</h2>
<pre><code class="language-py">class torch.distributions.relaxed_categorical.RelaxedOneHotCategorical(temperature, probs=None, logits=None, validate_args=None)
</code></pre>
<p>基类: <a href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code>torch.distributions.transformed_distribution.TransformedDistribution</code></a></p>
<p>创建一个由温度参数化的<code>RelaxedOneHotCategorical</code>分布, 以及<code>probs</code>或<code>logits</code>. 这是<code>OneHotCategorical</code>分布的松弛版本, 因此它的样本是单一的, 并且可以重参数化.</p>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = RelaxedOneHotCategorical(torch.tensor([2.2]),
 torch.tensor([0.1, 0.2, 0.3, 0.4]))
&gt;&gt;&gt; m.sample()
tensor([ 0.1294,  0.2324,  0.3859,  0.2523])

</code></pre>
<p>参数:</p>
<ul>
<li><strong>temperature</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 松弛 temperature</li>
<li><strong>probs</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 事件概率</li>
<li><strong>logits</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) –对数事件概率.</li>
</ul>
<pre><code class="language-py">arg_constraints = {'logits': Real(), 'probs': Simplex()}
</code></pre>
<pre><code class="language-py">expand(batch_shape, _instance=None)
</code></pre>
<pre><code class="language-py">has_rsample = True
</code></pre>
<pre><code class="language-py">logits
</code></pre>
<pre><code class="language-py">probs
</code></pre>
<pre><code class="language-py">support = Simplex()
</code></pre>
<pre><code class="language-py">temperature
</code></pre>
<h2 id="studentt">StudentT</h2>
<pre><code class="language-py">class torch.distributions.studentT.StudentT(df, loc=0.0, scale=1.0, validate_args=None)
</code></pre>
<p>基类: <a href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p>
<p>根据自由度<code>df</code>, 平均<code>loc</code>和<code>scale</code>创建学生t分布.</p>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = StudentT(torch.tensor([2.0]))
&gt;&gt;&gt; m.sample()  # Student's t-distributed with degrees of freedom=2
tensor([ 0.1046])

</code></pre>
<p>参数:</p>
<ul>
<li><strong>df</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a> <em>or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 自由度</li>
<li><strong>loc</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a> <em>or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 均值</li>
<li><strong>scale</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a> <em>or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 分布的scale</li>
</ul>
<pre><code class="language-py">arg_constraints = {'df': GreaterThan(lower_bound=0.0), 'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}
</code></pre>
<pre><code class="language-py">entropy()
</code></pre>
<pre><code class="language-py">expand(batch_shape, _instance=None)
</code></pre>
<pre><code class="language-py">has_rsample = True
</code></pre>
<pre><code class="language-py">log_prob(value)
</code></pre>
<pre><code class="language-py">mean
</code></pre>
<pre><code class="language-py">rsample(sample_shape=torch.Size([]))
</code></pre>
<pre><code class="language-py">support = Real()
</code></pre>
<pre><code class="language-py">variance
</code></pre>
<h2 id="transformeddistribution">TransformedDistribution</h2>
<pre><code class="language-py">class torch.distributions.transformed_distribution.TransformedDistribution(base_distribution, transforms, validate_args=None)
</code></pre>
<p>基类: <a href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p>
<p>Distribution类的扩展, 它将一系列变换应用于基本分布. 假设f是所应用变换的组成:</p>
<pre><code class="language-py">X ~ BaseDistribution
Y = f(X) ~ TransformedDistribution(BaseDistribution, f)
log p(Y) = log p(X) + log |det (dX/dY)|

</code></pre>
<p>注意 <code>.event_shape</code> of a <a href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code>TransformedDistribution</code></a> 是其基本分布及其变换的最大形状, 因为变换可以引入事件之间的相关性.</p>
<p>一个使用例子 <a href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code>TransformedDistribution</code></a>:</p>
<pre><code class="language-py"># Building a Logistic Distribution
# X ~ Uniform(0, 1)
# f = a + b * logit(X)
# Y ~ f(X) ~ Logistic(a, b)
base_distribution = Uniform(0, 1)
transforms = [SigmoidTransform().inv, AffineTransform(loc=a, scale=b)]
logistic = TransformedDistribution(base_distribution, transforms)

</code></pre>
<p>有关更多示例, 请查看有关实现 <a href="#torch.distributions.gumbel.Gumbel" title="torch.distributions.gumbel.Gumbel"><code>Gumbel</code></a>, <a href="#torch.distributions.half_cauchy.HalfCauchy" title="torch.distributions.half_cauchy.HalfCauchy"><code>HalfCauchy</code></a>, <a href="#torch.distributions.half_normal.HalfNormal" title="torch.distributions.half_normal.HalfNormal"><code>HalfNormal</code></a>, <a href="#torch.distributions.log_normal.LogNormal" title="torch.distributions.log_normal.LogNormal"><code>LogNormal</code></a>, <a href="#torch.distributions.pareto.Pareto" title="torch.distributions.pareto.Pareto"><code>Pareto</code></a>, <a href="#torch.distributions.weibull.Weibull" title="torch.distributions.weibull.Weibull"><code>Weibull</code></a>, <a href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli" title="torch.distributions.relaxed_bernoulli.RelaxedBernoulli"><code>RelaxedBernoulli</code></a> 和 <a href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical" title="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical"><code>RelaxedOneHotCategorical</code></a></p>
<pre><code class="language-py">arg_constraints = {}
</code></pre>
<pre><code class="language-py">cdf(value)
</code></pre>
<p>通过逆变换和计算基分布的分数来计算累积分布函数.</p>
<pre><code class="language-py">expand(batch_shape, _instance=None)
</code></pre>
<pre><code class="language-py">has_rsample
</code></pre>
<pre><code class="language-py">icdf(value)
</code></pre>
<p>使用transform(s)计算逆累积分布函数, 并计算基分布的分数.</p>
<pre><code class="language-py">log_prob(value)
</code></pre>
<p>通过反转变换并使用基本分布的分数和日志abs det jacobian计算分数来对样本进行评分</p>
<pre><code class="language-py">rsample(sample_shape=torch.Size([]))
</code></pre>
<p>如果分布参数是批处理的, 则生成sample_shape形状的重新参数化样本或sample_shape形状的重新参数化样本批次. 首先从基本分布中采样, 并对列表中的每个变换应用<code>transform()</code></p>
<pre><code class="language-py">sample(sample_shape=torch.Size([]))
</code></pre>
<p>如果分布参数是批处理的, 则生成sample_shape形样本或sample_shape形样本批处理. 首先从基本分布中采样, 并对列表中的每个变换应用<code>transform()</code>.</p>
<pre><code class="language-py">support
</code></pre>
<h2 id="uniform">Uniform</h2>
<pre><code class="language-py">class torch.distributions.uniform.Uniform(low, high, validate_args=None)
</code></pre>
<p>基类: <a href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p>
<p>从半开区间<code>[low, high)</code>生成均匀分布的随机样本</p>
<p>例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; m = Uniform(torch.tensor([0.0]), torch.tensor([5.0]))
&gt;&gt;&gt; m.sample()  # uniformly distributed in the range [0.0, 5.0)
tensor([ 2.3418])

</code></pre>
<p>参数:</p>
<ul>
<li><strong>low</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a> <em>or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 下限（含）.</li>
<li><strong>high</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a> <em>or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 上限(排除).</li>
</ul>
<pre><code class="language-py">arg_constraints = {'high': Dependent(), 'low': Dependent()}
</code></pre>
<pre><code class="language-py">cdf(value)
</code></pre>
<pre><code class="language-py">entropy()
</code></pre>
<pre><code class="language-py">expand(batch_shape, _instance=None)
</code></pre>
<pre><code class="language-py">has_rsample = True
</code></pre>
<pre><code class="language-py">icdf(value)
</code></pre>
<pre><code class="language-py">log_prob(value)
</code></pre>
<pre><code class="language-py">mean
</code></pre>
<pre><code class="language-py">rsample(sample_shape=torch.Size([]))
</code></pre>
<pre><code class="language-py">stddev
</code></pre>
<pre><code class="language-py">support
</code></pre>
<pre><code class="language-py">variance
</code></pre>
<h2 id="weibull">Weibull</h2>
<pre><code class="language-py">class torch.distributions.weibull.Weibull(scale, concentration, validate_args=None)
</code></pre>
<p>基类: <a href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code>torch.distributions.transformed_distribution.TransformedDistribution</code></a></p>
<p>来自双参数Weibull分布的样本.</p>
<p>Example</p>
<pre><code class="language-py">&gt;&gt;&gt; m = Weibull(torch.tensor([1.0]), torch.tensor([1.0]))
&gt;&gt;&gt; m.sample()  # sample from a Weibull distribution with scale=1, concentration=1
tensor([ 0.4784])

</code></pre>
<p>参数:</p>
<ul>
<li><strong>scale</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a> <em>or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Scale (lambda).</li>
<li><strong>concentration</strong> (<a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a> <em>or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Concentration (k/shape).</li>
</ul>
<pre><code class="language-py">arg_constraints = {'concentration': GreaterThan(lower_bound=0.0), 'scale': GreaterThan(lower_bound=0.0)}
</code></pre>
<pre><code class="language-py">entropy()
</code></pre>
<pre><code class="language-py">expand(batch_shape, _instance=None)
</code></pre>
<pre><code class="language-py">mean
</code></pre>
<pre><code class="language-py">support = GreaterThan(lower_bound=0.0)
</code></pre>
<pre><code class="language-py">variance
</code></pre>
<h2 id="kl-divergence"><code>KL Divergence</code></h2>
<pre><code class="language-py">torch.distributions.kl.kl_divergence(p, q)
</code></pre>
<p>计算Kullback-Leibler散度 <a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/739a8e4cd0597805c3e4daf35c0fc7c6.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/739a8e4cd0597805c3e4daf35c0fc7c6.jpg" alt=""></a> 对于两个分布.</p>
<p><a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/ff8dcec3abe559720f8b0b464d2471b2.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/ff8dcec3abe559720f8b0b464d2471b2.jpg" alt=""></a></p>
<p>参数:</p>
<ul>
<li><strong>p</strong> (<a href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><em>Distribution</em></a>) – <code>Distribution</code> 对象.</li>
<li><strong>q</strong> (<a href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><em>Distribution</em></a>) – <code>Distribution</code> 对象.</li>
</ul>
<p>| 返回值: | 批量的 KL 散度, 形状为 <code>batch_shape</code>. |</p>
<p>| 返回类型： | <a href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a> |</p>
<p>| 异常: | <a href="https://docs.python.org/3/library/exceptions.html#NotImplementedError" title="(in Python v3.7)"><code>NotImplementedError</code></a> – 如果分布类型尚未通过注册 <a href="#torch.distributions.kl.register_kl" title="torch.distributions.kl.register_kl"><code>register_kl()</code></a>. |</p>
<pre><code class="language-py">torch.distributions.kl.register_kl(type_p, type_q)
</code></pre>
<p>装饰器注册<a href="#torch.distributions.kl.kl_divergence" title="torch.distributions.kl.kl_divergence"><code>kl_divergence()</code></a>的成对函数</p>
<pre><code class="language-py">@register_kl(Normal, Normal)
def kl_normal_normal(p, q):
    # insert implementation here

</code></pre>
<p>Lookup返回由子类排序的最具体(type,type)匹配. 如果匹配不明确, 则会引发<code>RuntimeWarning</code>. 例如, 解决模棱两可的情况</p>
<pre><code class="language-py">@register_kl(BaseP, DerivedQ)
def kl_version1(p, q): ...
@register_kl(DerivedP, BaseQ)
def kl_version2(p, q): ...

</code></pre>
<p>你应该注册第三个最具体的实现, 例如:</p>
<pre><code class="language-py">register_kl(DerivedP, DerivedQ)(kl_version1)  # Break the tie.

</code></pre>
<p>参数:</p>
<ul>
<li><strong>type_p</strong> (<a href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.7)"><em>type</em></a>) – 子类 <code>Distribution</code>.</li>
<li><strong>type_q</strong> (<a href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.7)"><em>type</em></a>) – 子类 <code>Distribution</code>.</li>
</ul>
<h2 id="transforms"><code>Transforms</code></h2>
<pre><code class="language-py">class torch.distributions.transforms.Transform(cache_size=0)
</code></pre>
<p>有可计算的log det jacobians进行可逆变换的抽象类. 它们主要用于 <code>torch.distributions.TransformedDistribution</code>.</p>
<p>缓存对于其反转昂贵或数值不稳定的变换很有用. 请注意, 必须注意记忆值, 因为可以颠倒自动记录图. 例如, 以下操作有或没有缓存:</p>
<pre><code class="language-py">y = t(x)
t.log_abs_det_jacobian(x, y).backward()  # x will receive gradients.

</code></pre>
<p>但是, 由于依赖性反转, 缓存时会出现以下错误:</p>
<pre><code class="language-py">y = t(x)
z = t.inv(y)
grad(z.sum(), [y])  # error because z is x

</code></pre>
<p>派生类应该实现<code>_call()</code>或<code>_inverse()</code>中的一个或两个. 设置<code>bijective=True</code>的派生类也应该实现<code>log_abs_det_jacobian()</code></p>
<p>| 参数: | <strong>cache_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – 缓存大小. 如果为零, 则不进行缓存. 如果是, 则缓存最新的单个值. 仅支持0和1 |</p>
<p>| Variables: |</p>
<ul>
<li><strong>domain</strong> (<a href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><code>Constraint</code></a>) – 表示该变换有效输入的约束.</li>
<li><strong>codomain</strong> (<a href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><code>Constraint</code></a>) – 表示此转换的有效输出的约束, 这些输出是逆变换的输入.</li>
<li><strong>bijective</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – 这个变换是否是双射的. 变换 <code>t</code> 是双射的 如果 <code>t.inv(t(x)) == x</code> 并且 <code>t(t.inv(y)) == y</code> 对于每一个 <code>x</code> 和 <code>y</code>. 不是双射的变形应该至少保持较弱的伪逆属性 <code>t(t.inv(t(x)) == t(x)</code> and <code>t.inv(t(t.inv(y))) == t.inv(y)</code>.</li>
<li><strong>sign</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a> <em>or</em> <a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 对于双射单变量变换, 它应该是+1或-1, 这取决于变换是单调递增还是递减.</li>
<li><strong>event_dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – 变换event_shape中相关的维数. 这对于逐点变换应该是0, 对于在矢量上共同作用的变换是1, 对于在矩阵上共同作用的变换是2, 等等.</li>
</ul>
<pre><code class="language-py">inv
</code></pre>
<p>返回逆<a href="#torch.distributions.transforms.Transform" title="torch.distributions.transforms.Transform"><code>Transform</code></a>. 满足 <code>t.inv.inv is t</code>.</p>
<pre><code class="language-py">sign
</code></pre>
<p>如果适用, 返回雅可比行列式的符号. 一般来说, 这只适用于双射变换.</p>
<pre><code class="language-py">log_abs_det_jacobian(x, y)
</code></pre>
<p>计算 log det jacobian <code>log |dy/dx|</code> 给定输入和输出.</p>
<pre><code class="language-py">class torch.distributions.transforms.ComposeTransform(parts)
</code></pre>
<p>在一个链中组合多个转换. 正在组合的转换负责缓存.</p>
<p>| 参数: | <strong>parts</strong> (list of <a href="#torch.distributions.transforms.Transform" title="torch.distributions.transforms.Transform"><code>Transform</code></a>) – 列表 transforms. |</p>
<pre><code class="language-py">class torch.distributions.transforms.ExpTransform(cache_size=0)
</code></pre>
<p>转换通过映射 <a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/ec8d939394f24908d017d86153e312ea.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/ec8d939394f24908d017d86153e312ea.jpg" alt=""></a>.</p>
<pre><code class="language-py">class torch.distributions.transforms.PowerTransform(exponent, cache_size=0)
</code></pre>
<p>转换通过映射 <a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/2062af7179e0c19c3599816de6768cee.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/2062af7179e0c19c3599816de6768cee.jpg" alt=""></a>.</p>
<pre><code class="language-py">class torch.distributions.transforms.SigmoidTransform(cache_size=0)
</code></pre>
<p>转换通过映射 <a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/749abef3418941161a1c6ff80d9eae76.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/749abef3418941161a1c6ff80d9eae76.jpg" alt=""></a> and <a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/6feb73eb74f2267e5caa87d9693362cb.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/6feb73eb74f2267e5caa87d9693362cb.jpg" alt=""></a>.</p>
<pre><code class="language-py">class torch.distributions.transforms.AbsTransform(cache_size=0)
</code></pre>
<p>转换通过映射 <a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/dca0dc2e17c81b7ec261e70549de5507.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/dca0dc2e17c81b7ec261e70549de5507.jpg" alt=""></a>.</p>
<pre><code class="language-py">class torch.distributions.transforms.AffineTransform(loc, scale, event_dim=0, cache_size=0)
</code></pre>
<p>通过逐点仿射映射<a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/e1df459e7ff26d682fc956b62868f7c4.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/e1df459e7ff26d682fc956b62868f7c4.jpg" alt=""></a>进行转换 .</p>
<p>参数:</p>
<ul>
<li><strong>loc</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a> <em>or</em> <a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – Location.</li>
<li><strong>scale</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a> <em>or</em> <a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – Scale.</li>
<li><strong>event_dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – 可选的 <code>event_shape</code> 大小. T对于单变量随机变量, 该值应为零, 对于矢量分布, 1应为零, 对于矩阵的分布, 应为2.</li>
</ul>
<pre><code class="language-py">class torch.distributions.transforms.SoftmaxTransform(cache_size=0)
</code></pre>
<p>从无约束空间到单纯形的转换, 通过 <a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/ec8d939394f24908d017d86153e312ea.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/ec8d939394f24908d017d86153e312ea.jpg" alt=""></a> 然后归一化.</p>
<p>这不是双射的, 不能用于HMC. 然而, 这主要是协调的（除了最终的归一化）, 因此适合于坐标方式的优化算法.</p>
<pre><code class="language-py">class torch.distributions.transforms.StickBreakingTransform(cache_size=0)
</code></pre>
<p>将无约束空间通过 stick-breaking 过程转化为一个额外维度的单纯形.</p>
<p>这种变换是<code>Dirichlet</code>分布的破棒构造中的迭代sigmoid变换:第一个逻辑通过sigmoid变换成第一个概率和所有其他概率, 然后这个过程重复出现.</p>
<p>这是双射的, 适合在HMC中使用; 然而, 它将坐标混合在一起, 不太适合优化.</p>
<pre><code class="language-py">class torch.distributions.transforms.LowerCholeskyTransform(cache_size=0)
</code></pre>
<p>将无约束矩阵转换为具有非负对角项的下三角矩阵.</p>
<p>这对于根据Cholesky分解来参数化正定矩阵是有用的.</p>
<h2 id="constraints"><code>Constraints</code></h2>
<p>The following constraints are implemented:</p>
<ul>
<li><code>constraints.boolean</code></li>
<li><code>constraints.dependent</code></li>
<li><code>constraints.greater_than(lower_bound)</code></li>
<li><code>constraints.integer_interval(lower_bound, upper_bound)</code></li>
<li><code>constraints.interval(lower_bound, upper_bound)</code></li>
<li><code>constraints.lower_cholesky</code></li>
<li><code>constraints.lower_triangular</code></li>
<li><code>constraints.nonnegative_integer</code></li>
<li><code>constraints.positive</code></li>
<li><code>constraints.positive_definite</code></li>
<li><code>constraints.positive_integer</code></li>
<li><code>constraints.real</code></li>
<li><code>constraints.real_vector</code></li>
<li><code>constraints.simplex</code></li>
<li><code>constraints.unit_interval</code></li>
</ul>
<pre><code class="language-py">class torch.distributions.constraints.Constraint
</code></pre>
<p>constraints 的抽象基类.</p>
<p>constraint对象表示变量有效的区域, 例如, 其中可以优化变量</p>
<pre><code class="language-py">check(value)
</code></pre>
<p>返回一个字节张量 <code>sample_shape + batch_shape</code> 指示值中的每个事件是否满足此约束.</p>
<pre><code class="language-py">torch.distributions.constraints.dependent_property
</code></pre>
<p>alias of <code>torch.distributions.constraints._DependentProperty</code></p>
<pre><code class="language-py">torch.distributions.constraints.integer_interval
</code></pre>
<p>alias of <code>torch.distributions.constraints._IntegerInterval</code></p>
<pre><code class="language-py">torch.distributions.constraints.greater_than
</code></pre>
<p>alias of <code>torch.distributions.constraints._GreaterThan</code></p>
<pre><code class="language-py">torch.distributions.constraints.greater_than_eq
</code></pre>
<p>alias of <code>torch.distributions.constraints._GreaterThanEq</code></p>
<pre><code class="language-py">torch.distributions.constraints.less_than
</code></pre>
<p>alias of <code>torch.distributions.constraints._LessThan</code></p>
<pre><code class="language-py">torch.distributions.constraints.interval
</code></pre>
<p>alias of <code>torch.distributions.constraints._Interval</code></p>
<pre><code class="language-py">torch.distributions.constraints.half_open_interval
</code></pre>
<p>alias of <code>torch.distributions.constraints._HalfOpenInterval</code></p>
<h2 id="constraint-registry"><code>Constraint Registry</code></h2>
<p>PyTorch 提供两个全局 <a href="#torch.distributions.constraint_registry.ConstraintRegistry" title="torch.distributions.constraint_registry.ConstraintRegistry"><code>ConstraintRegistry</code></a> 对象 , 链接 <a href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><code>Constraint</code></a> 对象到 <a href="#torch.distributions.transforms.Transform" title="torch.distributions.transforms.Transform"><code>Transform</code></a> 对象. 这些对象既有输入约束, 也有返回变换, 但是它们对双射性有不同的保证.</p>
<ol>
<li><code>biject_to(constraint)</code> 查找一个双射的 <a href="#torch.distributions.transforms.Transform" title="torch.distributions.transforms.Transform"><code>Transform</code></a> 从 <code>constraints.real</code> 到给定的 <code>constraint</code>. 返回的转换保证具有 <code>.bijective = True</code> 并且应该实现了 <code>.log_abs_det_jacobian()</code>.</li>
<li><code>transform_to(constraint)</code> 查找一个不一定是双射的 <a href="#torch.distributions.transforms.Transform" title="torch.distributions.transforms.Transform"><code>Transform</code></a> 从 <code>constraints.real</code> 到给定的 <code>constraint</code>. 返回的转换不保证实现 <code>.log_abs_det_jacobian()</code>.</li>
</ol>
<p><code>transform_to()</code>注册表对于对概率分布的约束参数执行无约束优化非常有用, 这些参数由每个分布的<code>.arg_constraints</code>指示. 这些变换通常会过度参数化空间以避免旋转; 因此, 它们更适合像Adam那样的坐标优化算法</p>
<pre><code class="language-py">loc = torch.zeros(100, requires_grad=True)
unconstrained = torch.zeros(100, requires_grad=True)
scale = transform_to(Normal.arg_constraints['scale'])(unconstrained)
loss = -Normal(loc, scale).log_prob(data).sum()

</code></pre>
<p><code>biject_to()</code> 注册表对于Hamiltonian Monte Carlo非常有用, 其中来自具有约束. <code>.support</code>的概率分布的样本在无约束空间中传播, 并且算法通常是旋转不变的</p>
<pre><code class="language-py">dist = Exponential(rate)
unconstrained = torch.zeros(100, requires_grad=True)
sample = biject_to(dist.support)(unconstrained)
potential_energy = -dist.log_prob(sample).sum()

</code></pre>
<p>注意</p>
<p>一个 <code>transform_to</code> 和 <code>biject_to</code> 不同的例子是 <code>constraints.simplex</code>: <code>transform_to(constraints.simplex)</code> 返回一个 <a href="#torch.distributions.transforms.SoftmaxTransform" title="torch.distributions.transforms.SoftmaxTransform"><code>SoftmaxTransform</code></a> 简单地对其输入进行指数化和归一化; 这是一种廉价且主要是坐标的操作, 适用于像SVI这样的算法. 相反, <code>biject_to(constraints.simplex)</code> 返回一个 <a href="#torch.distributions.transforms.StickBreakingTransform" title="torch.distributions.transforms.StickBreakingTransform"><code>StickBreakingTransform</code></a> 将其输入生成一个较小维度的空间; 这是一种更昂贵的数值更少的数值稳定的变换, 但对于像HM​​C这样的算法是必需的.</p>
<p><code>biject_to</code> 和 <code>transform_to</code> 对象可以通过用户定义的约束进行扩展, 并使用<code>.register()</code>方法进行转换, 作为单例约束的函数</p>
<pre><code class="language-py">transform_to.register(my_constraint, my_transform)

</code></pre>
<p>或作为参数化约束的装饰器:</p>
<pre><code class="language-py">@transform_to.register(MyConstraintClass)
def my_factory(constraint):
    assert isinstance(constraint, MyConstraintClass)
    return MyTransform(constraint.param1, constraint.param2)

</code></pre>
<p>您可以通过创建新的<a href="#torch.distributions.constraint_registry.ConstraintRegistry" title="torch.distributions.constraint_registry.ConstraintRegistry"><code>ConstraintRegistry</code></a>创建自己的注册表.</p>
<pre><code class="language-py">class torch.distributions.constraint_registry.ConstraintRegistry
</code></pre>
<p>注册表, 将约束链接到转换.</p>
<pre><code class="language-py">register(constraint, factory=None)
</code></pre>
<p>在此注册表注册一个 <a href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><code>Constraint</code></a> 子类. 用法:</p>
<pre><code class="language-py">@my_registry.register(MyConstraintClass)
def construct_transform(constraint):
    assert isinstance(constraint, MyConstraint)
    return MyTransform(constraint.arg_constraints)

</code></pre>
<p>参数:</p>
<ul>
<li><strong>constraint</strong> (subclass of <a href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><code>Constraint</code></a>) – [<code>Constraint</code>]的子类(#torch.distributions.constraints.Constraint "torch.distributions.constraints.Constraint"), 或者派生类的对象.</li>
<li><strong>factory</strong> (<em>callable</em>) – 可调用对象, 输入 constraint 对象返回 <a href="#torch.distributions.transforms.Transform" title="torch.distributions.transforms.Transform"><code>Transform</code></a> 对象.</li>
</ul>
</div>
<hr class="uk-article-divider">
<div class="uk-block uk-block-muted uk-padding-top-remove uk-padding-bottom-remove uk-margin-large-top  book-recommend-wrap">
<div class="uk-margin-top uk-margin-bottom uk-margin-left uk-margin-right">
<div class="uk-margin uk-text-muted "><i class="uk-icon-outdent uk-icon-justify uk-margin-small-right"></i>书籍推荐</div>
<div class="books">
<ul class="uk-book-list">
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/68/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/68/index.html">Python 资源大全中文版</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/19.html">伯乐在线</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">1页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月6日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 10237个">10237</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/154/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/154/index.html">Python 学习总结</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/86.html">itroger</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">11页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月12日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 0个">0</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/166/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/166/index.html">What the f*ck Python中文版</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/95.html">leisurelicht</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">70页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月26日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 7300个">7300</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/46/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/linux_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/46/index.html">软件开发平台及语言笔记大全(超详细)</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/22.html">jasonblog</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="android">android</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="linux">linux</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="java">java</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="cplusplus">cplusplus</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">1,399页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年5月30日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 16个">16</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/98/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/visualstudio_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/98/index.html">Microsoft Visual Studio Code 中文手册</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/60.html">likebeta</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="visualstudio">visualstudio</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">66页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月29日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 0个">0</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/120/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/spark_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/120/index.html">Openstack用户指南（简体中文版）</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/62.html">tzivanmoe</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="spark">spark</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">47页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年7月1日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 0个">0</span>
</div>
</div>
</div>
</li>
<hr>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
<nav class="tm-navbar uk-navbar uk-navbar-attached reader-nav">
<div class="uk-float-left uk-margin-small-top">
<a href="javascript:;" title="目录菜单" class="show-menu  uk-icon-hover  uk-icon-align-justify uk-margin-right"></a>
<div data-uk-dropdown="{mode:'click',pos:'bottom-left'}" class="font-setting-wrap">
<a class="uk-icon-hover uk-icon-font uk-margin-right" aria-label="字体设置" href="javascript:;"></a>
<div class="uk-dropdown dropdown-menu">
<div class="dropdown-caret"><span class="caret-outer"></span><span class="caret-inner"></span></div>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-reduce">小字</button>
<button class="uk-button-link button size-2 font-enlarge">大字</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-1 ">宋体</button>
<button class="uk-button-link button size-2 font-2 ">黑体</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-3 color-theme-sun "><i class="uk-icon-sun-o"></i>白天</button>
<button class="uk-button-link button size-3 color-theme-eye "><i class="uk-icon-eye"></i>护眼</button>
<button class="uk-button-link button size-3 color-theme-moon "><i class="uk-icon-moon-o"></i>夜晚</button></div>
</div>
</div>
<a class="logo uk-margin-right" href="../../../../" title="返回首页"><img class="" src="../../../../static/components/images/icon_32.png" /></a>
</div>
<div class="uk-navbar-flip  uk-hidden-small">
<div id="share-box"></div>
</div>
</nav>
<div id="menu-id" class="uk-offcanvas reader-offcanvas">
<div class="uk-offcanvas-bar">
<ul class="book-menu-bar uk-nav uk-nav-offcanvas" data-uk-nav>
<li>
<a href="../../../../book/169/index.html" data-book-page-rel-url="index.html" data-book-page-id="0" title="封面">封面</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/readme.html" data-book-page-rel-url="readme.html" data-book-page-id="0" title="简介">简介</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_getting_started.html" title="起步" data-book-page-rel-url="docs/1.0/tut_getting_started.html" data-book-page-id="11555">起步</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/deep_learning_60min_blitz.html" title="PyTorch 深度学习: 60 分钟极速入门" data-book-page-rel-url="docs/1.0/deep_learning_60min_blitz.html" data-book-page-id="11556">PyTorch 深度学习: 60 分钟极速入门</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/blitz_tensor_tutorial.html" title="什么是 PyTorch？" data-book-page-rel-url="docs/1.0/blitz_tensor_tutorial.html" data-book-page-id="11557">什么是 PyTorch？</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/blitz_autograd_tutorial.html" title="Autograd：自动求导" data-book-page-rel-url="docs/1.0/blitz_autograd_tutorial.html" data-book-page-id="11558">Autograd：自动求导</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/blitz_neural_networks_tutorial.html" title="神经网络" data-book-page-rel-url="docs/1.0/blitz_neural_networks_tutorial.html" data-book-page-id="11559">神经网络</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/blitz_cifar10_tutorial.html" title="训练分类器" data-book-page-rel-url="docs/1.0/blitz_cifar10_tutorial.html" data-book-page-id="11560">训练分类器</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/blitz_data_parallel_tutorial.html" title="可选：数据并行处理" data-book-page-rel-url="docs/1.0/blitz_data_parallel_tutorial.html" data-book-page-id="11561">可选：数据并行处理</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/data_loading_tutorial.html" title="数据加载和处理教程" data-book-page-rel-url="docs/1.0/data_loading_tutorial.html" data-book-page-id="11562">数据加载和处理教程</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/pytorch_with_examples.html" title="用例子学习 PyTorch" data-book-page-rel-url="docs/1.0/pytorch_with_examples.html" data-book-page-id="11563">用例子学习 PyTorch</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/transfer_learning_tutorial.html" title="迁移学习教程" data-book-page-rel-url="docs/1.0/transfer_learning_tutorial.html" data-book-page-id="11564">迁移学习教程</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/deploy_seq2seq_hybrid_frontend_tutorial.html" title="混合前端的 seq2seq 模型部署" data-book-page-rel-url="docs/1.0/deploy_seq2seq_hybrid_frontend_tutorial.html" data-book-page-id="11565">混合前端的 seq2seq 模型部署</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/saving_loading_models.html" title="Saving and Loading Models" data-book-page-rel-url="docs/1.0/saving_loading_models.html" data-book-page-id="11566">Saving and Loading Models</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nn_tutorial.html" title="What is `torch.nn` _really_?" data-book-page-rel-url="docs/1.0/nn_tutorial.html" data-book-page-id="11567">What is `torch.nn` _really_?</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_image.html" title="图像" data-book-page-rel-url="docs/1.0/tut_image.html" data-book-page-id="11568">图像</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/finetuning_torchvision_models_tutorial.html" title="Torchvision 模型微调" data-book-page-rel-url="docs/1.0/finetuning_torchvision_models_tutorial.html" data-book-page-id="11569">Torchvision 模型微调</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/spatial_transformer_tutorial.html" title="空间变换器网络教程" data-book-page-rel-url="docs/1.0/spatial_transformer_tutorial.html" data-book-page-id="11570">空间变换器网络教程</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/neural_style_tutorial.html" title="使用 PyTorch 进行图像风格转换" data-book-page-rel-url="docs/1.0/neural_style_tutorial.html" data-book-page-id="11571">使用 PyTorch 进行图像风格转换</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/fgsm_tutorial.html" title="对抗性示例生成" data-book-page-rel-url="docs/1.0/fgsm_tutorial.html" data-book-page-id="11572">对抗性示例生成</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/super_resolution_with_caffe2.html" title="使用 ONNX 将模型从 PyTorch 传输到 Caffe2 和移动端" data-book-page-rel-url="docs/1.0/super_resolution_with_caffe2.html" data-book-page-id="11573">使用 ONNX 将模型从 PyTorch 传输到 Caffe2 和移动端</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_text.html" title="文本" data-book-page-rel-url="docs/1.0/tut_text.html" data-book-page-id="11574">文本</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/chatbot_tutorial.html" title="聊天机器人教程" data-book-page-rel-url="docs/1.0/chatbot_tutorial.html" data-book-page-id="11575">聊天机器人教程</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/char_rnn_generation_tutorial.html" title="使用字符级别特征的 RNN 网络生成姓氏" data-book-page-rel-url="docs/1.0/char_rnn_generation_tutorial.html" data-book-page-id="11576">使用字符级别特征的 RNN 网络生成姓氏</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/char_rnn_classification_tutorial.html" title="使用字符级别特征的 RNN 网络进行姓氏分类" data-book-page-rel-url="docs/1.0/char_rnn_classification_tutorial.html" data-book-page-id="11577">使用字符级别特征的 RNN 网络进行姓氏分类</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/deep_learning_nlp_tutorial.html" title="Deep Learning for NLP with Pytorch" data-book-page-rel-url="docs/1.0/deep_learning_nlp_tutorial.html" data-book-page-id="11578">Deep Learning for NLP with Pytorch</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nlp_pytorch_tutorial.html" title="PyTorch 介绍" data-book-page-rel-url="docs/1.0/nlp_pytorch_tutorial.html" data-book-page-id="11579">PyTorch 介绍</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nlp_deep_learning_tutorial.html" title="使用 PyTorch 进行深度学习" data-book-page-rel-url="docs/1.0/nlp_deep_learning_tutorial.html" data-book-page-id="11580">使用 PyTorch 进行深度学习</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nlp_word_embeddings_tutorial.html" title="Word Embeddings: Encoding Lexical Semantics" data-book-page-rel-url="docs/1.0/nlp_word_embeddings_tutorial.html" data-book-page-id="11581">Word Embeddings: Encoding Lexical Semantics</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nlp_sequence_models_tutorial.html" title="序列模型和 LSTM 网络" data-book-page-rel-url="docs/1.0/nlp_sequence_models_tutorial.html" data-book-page-id="11582">序列模型和 LSTM 网络</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nlp_advanced_tutorial.html" title="Advanced: Making Dynamic Decisions and the Bi-LSTM CRF" data-book-page-rel-url="docs/1.0/nlp_advanced_tutorial.html" data-book-page-id="11583">Advanced: Making Dynamic Decisions and the Bi-LSTM CRF</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/seq2seq_translation_tutorial.html" title="基于注意力机制的 seq2seq 神经网络翻译" data-book-page-rel-url="docs/1.0/seq2seq_translation_tutorial.html" data-book-page-id="11584">基于注意力机制的 seq2seq 神经网络翻译</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_generative.html" title="生成" data-book-page-rel-url="docs/1.0/tut_generative.html" data-book-page-id="11585">生成</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/dcgan_faces_tutorial.html" title="DCGAN Tutorial" data-book-page-rel-url="docs/1.0/dcgan_faces_tutorial.html" data-book-page-id="11586">DCGAN Tutorial</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_reinforcement_learning.html" title="强化学习" data-book-page-rel-url="docs/1.0/tut_reinforcement_learning.html" data-book-page-id="11587">强化学习</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/reinforcement_q_learning.html" title="Reinforcement Learning (DQN) Tutorial" data-book-page-rel-url="docs/1.0/reinforcement_q_learning.html" data-book-page-id="11588">Reinforcement Learning (DQN) Tutorial</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_extending_pytorch.html" title="扩展 PyTorch" data-book-page-rel-url="docs/1.0/tut_extending_pytorch.html" data-book-page-id="11589">扩展 PyTorch</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/numpy_extensions_tutorial.html" title="用 numpy 和 scipy 创建扩展" data-book-page-rel-url="docs/1.0/numpy_extensions_tutorial.html" data-book-page-id="11590">用 numpy 和 scipy 创建扩展</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/cpp_extension.html" title="Custom C++   and CUDA Extensions" data-book-page-rel-url="docs/1.0/cpp_extension.html" data-book-page-id="11591">Custom C++ and CUDA Extensions</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/torch_script_custom_ops.html" title="Extending TorchScript with Custom C++   Operators" data-book-page-rel-url="docs/1.0/torch_script_custom_ops.html" data-book-page-id="11592">Extending TorchScript with Custom C++ Operators</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_production_usage.html" title="生产性使用" data-book-page-rel-url="docs/1.0/tut_production_usage.html" data-book-page-id="11593">生产性使用</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/dist_tuto.html" title="Writing Distributed Applications with PyTorch" data-book-page-rel-url="docs/1.0/dist_tuto.html" data-book-page-id="11594">Writing Distributed Applications with PyTorch</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/aws_distributed_training_tutorial.html" title="使用 Amazon AWS 进行分布式训练" data-book-page-rel-url="docs/1.0/aws_distributed_training_tutorial.html" data-book-page-id="11595">使用 Amazon AWS 进行分布式训练</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/ONNXLive.html" title="ONNX 现场演示教程" data-book-page-rel-url="docs/1.0/ONNXLive.html" data-book-page-id="11596">ONNX 现场演示教程</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/cpp_export.html" title="在 C++ 中加载 PYTORCH 模型" data-book-page-rel-url="docs/1.0/cpp_export.html" data-book-page-id="11597">在 C++ 中加载 PYTORCH 模型</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_other_language.html" title="其它语言中的 PyTorch" data-book-page-rel-url="docs/1.0/tut_other_language.html" data-book-page-id="11598">其它语言中的 PyTorch</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/cpp_frontend.html" title="使用 PyTorch C++ 前端" data-book-page-rel-url="docs/1.0/cpp_frontend.html" data-book-page-id="11599">使用 PyTorch C++ 前端</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/docs_notes.html" title="注解" data-book-page-rel-url="docs/1.0/docs_notes.html" data-book-page-id="11600">注解</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_autograd.html" title="自动求导机制" data-book-page-rel-url="docs/1.0/notes_autograd.html" data-book-page-id="11601">自动求导机制</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_broadcasting.html" title="广播语义" data-book-page-rel-url="docs/1.0/notes_broadcasting.html" data-book-page-id="11602">广播语义</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_cuda.html" title="CUDA 语义" data-book-page-rel-url="docs/1.0/notes_cuda.html" data-book-page-id="11603">CUDA 语义</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_extending.html" title="Extending PyTorch" data-book-page-rel-url="docs/1.0/notes_extending.html" data-book-page-id="11604">Extending PyTorch</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_faq.html" title="Frequently Asked Questions" data-book-page-rel-url="docs/1.0/notes_faq.html" data-book-page-id="11605">Frequently Asked Questions</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_multiprocessing.html" title="Multiprocessing best practices" data-book-page-rel-url="docs/1.0/notes_multiprocessing.html" data-book-page-id="11606">Multiprocessing best practices</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_randomness.html" title="Reproducibility" data-book-page-rel-url="docs/1.0/notes_randomness.html" data-book-page-id="11607">Reproducibility</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_serialization.html" title="Serialization semantics" data-book-page-rel-url="docs/1.0/notes_serialization.html" data-book-page-id="11608">Serialization semantics</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_windows.html" title="Windows FAQ" data-book-page-rel-url="docs/1.0/notes_windows.html" data-book-page-id="11609">Windows FAQ</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/docs_package_ref.html" title="包参考" data-book-page-rel-url="docs/1.0/docs_package_ref.html" data-book-page-id="11610">包参考</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/torch.html" title="torch" data-book-page-rel-url="docs/1.0/torch.html" data-book-page-id="11611">torch</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tensors.html" title="torch.Tensor" data-book-page-rel-url="docs/1.0/tensors.html" data-book-page-id="11612">torch.Tensor</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tensor_attributes.html" title="Tensor Attributes" data-book-page-rel-url="docs/1.0/tensor_attributes.html" data-book-page-id="11613">Tensor Attributes</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/type_info.html" title="数据类型信息" data-book-page-rel-url="docs/1.0/type_info.html" data-book-page-id="11614">数据类型信息</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/sparse.html" title="torch.sparse" data-book-page-rel-url="docs/1.0/sparse.html" data-book-page-id="11615">torch.sparse</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/cuda.html" title="torch.cuda" data-book-page-rel-url="docs/1.0/cuda.html" data-book-page-id="11616">torch.cuda</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/storage.html" title="torch.Storage" data-book-page-rel-url="docs/1.0/storage.html" data-book-page-id="11617">torch.Storage</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nn.html" title="torch.nn" data-book-page-rel-url="docs/1.0/nn.html" data-book-page-id="11618">torch.nn</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nn_functional.html" title="torch.nn.functional" data-book-page-rel-url="docs/1.0/nn_functional.html" data-book-page-id="11619">torch.nn.functional</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nn_init.html" title="torch.nn.init" data-book-page-rel-url="docs/1.0/nn_init.html" data-book-page-id="11620">torch.nn.init</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/optim.html" title="torch.optim" data-book-page-rel-url="docs/1.0/optim.html" data-book-page-id="11621">torch.optim</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/autograd.html" title="Automatic differentiation package - torch.autograd" data-book-page-rel-url="docs/1.0/autograd.html" data-book-page-id="11622">Automatic differentiation package - torch.autograd</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/distributed.html" title="Distributed communication package - torch.distributed" data-book-page-rel-url="docs/1.0/distributed.html" data-book-page-id="11623">Distributed communication package - torch.distributed</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/distributions.html" title="Probability distributions - torch.distributions" data-book-page-rel-url="docs/1.0/distributions.html" data-book-page-id="11624">Probability distributions - torch.distributions</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/jit.html" title="Torch Script" data-book-page-rel-url="docs/1.0/jit.html" data-book-page-id="11625">Torch Script</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/multiprocessing.html" title="多进程包 - torch.multiprocessing" data-book-page-rel-url="docs/1.0/multiprocessing.html" data-book-page-id="11626">多进程包 - torch.multiprocessing</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/bottleneck.html" title="torch.utils.bottleneck" data-book-page-rel-url="docs/1.0/bottleneck.html" data-book-page-id="11627">torch.utils.bottleneck</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/checkpoint.html" title="torch.utils.checkpoint" data-book-page-rel-url="docs/1.0/checkpoint.html" data-book-page-id="11628">torch.utils.checkpoint</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/docs_cpp_extension.html" title="torch.utils.cpp_extension" data-book-page-rel-url="docs/1.0/docs_cpp_extension.html" data-book-page-id="11629">torch.utils.cpp_extension</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/data.html" title="torch.utils.data" data-book-page-rel-url="docs/1.0/data.html" data-book-page-id="11630">torch.utils.data</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/dlpack.html" title="torch.utils.dlpack" data-book-page-rel-url="docs/1.0/dlpack.html" data-book-page-id="11631">torch.utils.dlpack</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/hub.html" title="torch.hub" data-book-page-rel-url="docs/1.0/hub.html" data-book-page-id="11632">torch.hub</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/model_zoo.html" title="torch.utils.model_zoo" data-book-page-rel-url="docs/1.0/model_zoo.html" data-book-page-id="11633">torch.utils.model_zoo</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/onnx.html" title="torch.onnx" data-book-page-rel-url="docs/1.0/onnx.html" data-book-page-id="11634">torch.onnx</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/distributed_deprecated.html" title="Distributed communication package (deprecated) - torch.distributed.deprecated" data-book-page-rel-url="docs/1.0/distributed_deprecated.html" data-book-page-id="11635">Distributed communication package (deprecated) - torch.distributed.deprecated</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/docs_torchvision_ref.html" title="torchvision 参考" data-book-page-rel-url="docs/1.0/docs_torchvision_ref.html" data-book-page-id="11636">torchvision 参考</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/torchvision_datasets.html" title="torchvision.datasets" data-book-page-rel-url="docs/1.0/torchvision_datasets.html" data-book-page-id="11637">torchvision.datasets</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/torchvision_models.html" title="torchvision.models" data-book-page-rel-url="docs/1.0/torchvision_models.html" data-book-page-id="11638">torchvision.models</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/torchvision_transforms.html" title="torchvision.transforms" data-book-page-rel-url="docs/1.0/torchvision_transforms.html" data-book-page-id="11639">torchvision.transforms</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/torchvision_utils.html" title="torchvision.utils" data-book-page-rel-url="docs/1.0/torchvision_utils.html" data-book-page-id="11640">torchvision.utils</a>
</li>
</ul>
</li>
</ul>
</div>
</div>
<script src="https://cdn.staticfile.net/jquery/1.12.4/jquery.min.js"></script>
<script type="text/javascript" src="../../../../static/components/uikit-2.27.5/js/uikit.reader.js"></script>
<script type="text/javascript" src="../../../../static/components/social-share/social-share.min.js"></script>
<script>(function(){var bp =document.createElement('script');var curProtocol =window.location.protocol.split(':')[0];if (curProtocol ==='https') {bp.src ='https://zz.bdstatic.com/linksubmit/push.js';}
else {bp.src ='http://push.zhanzhang.baidu.com/push.js';}
var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(bp,s);})();</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
<script src="https://cdn.staticfile.net/highlight.js/9.12.0/highlight.min.js"></script>
<script src="https://cdn.staticfile.net/jquery.pjax/2.0.1/jquery.pjax.min.js"></script>
<script src="https://cdn.staticfile.net/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="https://cdn.staticfile.net/uikit/2.27.5/js/components/lightbox.min.js"></script>
<link rel="dns-prefetch" href="../../../..//cdn.mathjax.org" />
<script type="text/x-mathjax-config">
 function initMathJax() {
    var mathId = $("book-content-section")[0];
    MathJax.Hub.Config({
        tex2jax: {skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code','a']},
        showProcessingMessages: false,
        messageStyle: "none"
    });
    MathJax.Hub.Queue(["Typeset",MathJax.Hub,mathId]);
 };
initMathJax();
</script>
<script src='https://cdn.staticfile.net/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML' async></script>
<style>
	.MathJax_Display{display:inline!important;}
</style>
<script type="text/javascript" src="../../../../static/components/js/reader.js"></script>
<script type="text/javascript">var bookId =169;var bookPageId =11624;var bookPageRelUrl ='docs/1.0/distributions.html';</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
</body>
</html>