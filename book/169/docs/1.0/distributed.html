
<!DOCTYPE html>
<html lang="zh-CN">
<head>
<title>Distributed communication package - torch.distributed-PyTorch 1.0 中文文档 & 教程</title>
<meta content='Distributed communication package - torch.distributed,PyTorch 1.0 中文文档 & 教程' name='keywords'>
<meta content='Distributed communication package - torch.distributed,PyTorch 1.0 中文文档 & 教程' name='description'>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta http-equiv="Content-Language" content="zh-CN" />
<meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1, maximum-scale=1, user-scalable=no"../../../../>
<meta name="applicable-device" content="pc,mobile">
<link rel="shortcut icon" href="../../../../favicon.ico" type="image/x-icon" />
<meta name="renderer" content="webkit">
<link rel="stylesheet" href="../../../../static/components/uikit-2.27.5/css/uikit.custom.css">
<link rel="stylesheet" href="../../../../static/components/social-share/social-share.min.css">
<link rel="stylesheet" href="../../../../static/components/highlight/styles/custom.css">
<link rel="stylesheet" href="../../../../static/components/css/base.css">
<link rel="stylesheet" href="../../../../static/components/css/reader.css">
<link rel="stylesheet" href="../../../../static/components/css/markdown.css">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5313208362165053" crossorigin="anonymous"></script>
</head>
<body>
<div class=" book-main-wrap uk-container uk-container-center uk-margin-top ">
<div class="uk-grid">
<div class="uk-width-1-1 reader-wrap ">
<div class=" bottom-nav uk-clearfix ">
<div class="uk-align-left ">
<a href="../../../../book/169/docs/1.0/autograd.html">
<i class="nav-icon-left uk-icon-small  uk-icon-caret-left"></i>
<span class="">Automatic d..</span>
</a>
</div>
<div class="uk-align-right ">
<a href="../../../../book/169/docs/1.0/distributions.html">
<span class="">Probability..</span>
<i class="nav-icon-right uk-icon-small  uk-icon-caret-right"></i>
</a>
</div>
</div>
<div class="uk-text-center">
<h2 class="book-page-title uk-container-center">
<a href="../../../../book/169/index.html">PyTorch 1.0 中文文档 & 教程</a>
<a target="_blank" rel="nofollow" href="https://github.com/apachecn/pytorch-doc-zh" class="uk-icon-button uk-icon-github" title="github项目地址"></a>
</h2>
</div>
<script type="text/javascript" src="../../../../static/components/js/app_intro.js"></script>
<ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-5313208362165053" data-ad-slot="1328047120"></ins>
<script>(adsbygoogle =window.adsbygoogle ||[]).push({});</script>
<hr class="uk-article-divider">
<div class="book-content-section  md-content-section  uk-margin-bottom">
<h1 id="分布式通信包----torchdistributed">分布式通信包 - torch.distributed</h1>
<blockquote>
<p>译者：<a href="https://github.com/univeryinli">univeryinli</a></p>
</blockquote>
<h2 id="后端">后端</h2>
<p><code>torch.distributed</code> 支持三个后端，每个后端具有不同的功能。下表显示哪些功能可用于CPU/CUDA张量。仅当用于构建PyTorch的实现支持时，MPI才支持CUDA。</p>
<table>
<thead>
<tr>
<th>后端</th>
<th><code>gloo</code></th>
<th><code>mpi</code></th>
<th><code>nccl</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>设备</td>
<td>CPU</td>
<td>GPU</td>
<td>CPU</td>
<td>GPU</td>
<td>CPU</td>
<td>GPU</td>
</tr>
<tr>
<td>---</td>
<td>---</td>
<td>---</td>
<td>---</td>
<td>---</td>
<td>---</td>
<td>---</td>
</tr>
<tr>
<td>发送</td>
<td>✓</td>
<td>✘</td>
<td>✓</td>
<td>?</td>
<td>✘</td>
<td>✘</td>
</tr>
<tr>
<td>接收</td>
<td>✓</td>
<td>✘</td>
<td>✓</td>
<td>?</td>
<td>✘</td>
<td>✘</td>
</tr>
<tr>
<td>广播</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>?</td>
<td>✘</td>
<td>✓</td>
</tr>
<tr>
<td>all_reduce</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>?</td>
<td>✘</td>
<td>✓</td>
</tr>
<tr>
<td>reduce</td>
<td>✓</td>
<td>✘</td>
<td>✓</td>
<td>?</td>
<td>✘</td>
<td>✓</td>
</tr>
<tr>
<td>all_gather</td>
<td>✓</td>
<td>✘</td>
<td>✓</td>
<td>?</td>
<td>✘</td>
<td>✓</td>
</tr>
<tr>
<td>收集</td>
<td>✓</td>
<td>✘</td>
<td>✓</td>
<td>?</td>
<td>✘</td>
<td>✘</td>
</tr>
<tr>
<td>分散</td>
<td>✓</td>
<td>✘</td>
<td>✓</td>
<td>?</td>
<td>✘</td>
<td>✘</td>
</tr>
<tr>
<td>屏障</td>
<td>✓</td>
<td>✘</td>
<td>✓</td>
<td>?</td>
<td>✘</td>
<td>✓</td>
</tr>
</tbody>
</table>
<h3 id="pytorch附带的后端">PyTorch附带的后端</h3>
<p>目前PyTorch分发版仅支持Linux。默认情况下，Gloo和NCCL后端构建并包含在PyTorch的分布之中（仅在使用CUDA构建时为NCCL）。MPI是一个可选的后端，只有从源代码构建PyTorch时才能包含它。（例如，在安装了MPI的主机上构建PyTorch）</p>
<h3 id="哪个后端使用">哪个后端使用？</h3>
<p>在过去，我们经常被问到：“我应该使用哪个后端？”。</p>
<ul>
<li>经验法则
<ul>
<li>使用NCCL后端进行分布式 <strong>GPU</strong> 训练。</li>
<li>使用Gloo后端进行分布式 <strong>CPU</strong> 训练。</li>
</ul> </li>
<li>具有InfiniBand互连的GPU主机
<ul>
<li>使用NCCL，因为它是目前唯一支持InfiniBand和GPUDirect的后端。</li>
</ul> </li>
<li>GPU主机与以太网互连
<ul>
<li>使用NCCL，因为它目前提供最佳的分布式GPU训练性能，特别是对于多进程单节点或多节点分布式训练。如果您遇到NCCL的任何问题，请使用Gloo作为后备选项。（请注意，Gloo目前运行速度比GPU的NCCL慢。）</li>
</ul> </li>
<li>具有InfiniBand互连的CPU主机
<ul>
<li>如果您的InfiniBand在IB上已启用IP，请使用Gloo，否则请使用MPI。我们计划在即将发布的版本中为Gloo添加InfiniBand支持。</li>
</ul> </li>
<li>具有以太网互连的CPU主机
<ul>
<li>除非您有特殊原因要使用MPI，否则请使用Gloo。</li>
</ul> </li>
</ul>
<h3 id="常见的环境变量">常见的环境变量</h3>
<h4 id="选择要使用的网络接口">选择要使用的网络接口</h4>
<p>默认情况下，NCCL和Gloo后端都会尝试查找用于通信的网络接口。但是，从我们的经验来看，并不总能保证这一点。因此，如果您在后端遇到任何问题而无法找到正确的网络接口。您可以尝试设置以下环境变量（每个变量适用于其各自的后端）：</p>
<ul>
<li><strong>NCCL_SOCKET_IFNAME</strong>, 比如 <code>export NCCL_SOCKET_IFNAME=eth0</code></li>
<li><strong>GLOO_SOCKET_IFNAME</strong>, 比如 <code>export GLOO_SOCKET_IFNAME=eth0</code></li>
</ul>
<h4 id="其他nccl环境变量">其他NCCL环境变量</h4>
<p>NCCL还提供了许多用于微调目的的环境变量</p>
<p>常用的包括以下用于调试目的：</p>
<ul>
<li><code>export NCCL_DEBUG=INFO</code></li>
<li><code>export NCCL_DEBUG_SUBSYS=ALL</code></li>
</ul>
<p>有关NCCL环境变量的完整列表，请参阅<a href="https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/env.html">NVIDIA NCCL的官方文档</a></p>
<h2 id="基本">基本</h2>
<p><code>torch.distributed</code>包为在一台或多台机器上运行的多个计算节点上的多进程并行性提供PyTorch支持和通信原语。类 <a href="nn.html#torch.nn.parallel.DistributedDataParallel" title="torch.nn.parallel.DistributedDataParallel"><code>torch.nn.parallel.DistributedDataParallel()</code></a>基于此功能构建，以提供同步分布式训练作为包装器任何PyTorch模型。这与 <a href="multiprocessing.html">Multiprocessing package - torch.multiprocessing</a> 和 <a href="nn.html#torch.nn.DataParallel" title="torch.nn.DataParallel"><code>torch.nn.DataParallel()</code></a> 因为它支持多个联网的机器，并且用户必须为每个进程显式启动主训练脚本的单独副本。</p>
<p>在单机同步的情况下，<code>torch.distributed</code> 或者 <a href="nn.html#torch.nn.parallel.DistributedDataParallel" title="torch.nn.parallel.DistributedDataParallel"><code>torch.nn.parallel.DistributedDataParallel()</code></a> 与其他数据并行方法相比，包装器仍然具有优势，包含 <a href="nn.html#torch.nn.DataParallel" title="torch.nn.DataParallel"><code>torch.nn.DataParallel()</code></a>:</p>
<ul>
<li>每个进程都维护自己的优化器，并在每次迭代时执行完整的优化步骤。虽然这可能看起来是多余的，但由于梯度已经聚集在一起并且在整个过程中平均，因此对于每个过程都是相同的，这意味着不需要参数广播步骤，减少了在节点之间传输张量所花费的时间。</li>
<li>每个进程都包含一个独立的Python解释器，消除了额外的解释器开销和来自单个Python进程驱动多个执行线程，模型副本或GPU的“GIL-thrashing”。这对于大量使用Python运行时的模型尤其重要，包括具有循环层或许多小组件的模型。</li>
</ul>
<h2 id="初始化">初始化</h2>
<p>这个包在调用其他的方法之前，需要使用 <a href="#torch.distributed.init_process_group" title="torch.distributed.init_process_group"><code>torch.distributed.init_process_group()</code></a> 函数进行初始化。这将阻止所有进程加入。</p>
<pre><code class="language-py">torch.distributed.init_process_group(backend, init_method='env://', timeout=datetime.timedelta(seconds=1800), **kwargs)
</code></pre>
<p>初始化默认的分布式进程组，这也将初始化分布式程序包</p>
<p>参数:</p>
<ul>
<li><strong>backend</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a> <em>or</em> <a href="#torch.distributed.Backend" title="torch.distributed.Backend"><em>Backend</em></a>) – 后端使用。根据构建时配置，有效值包括 <code>mpi</code>，<code>gloo</code>和<code>nccl</code>。该字段应该以小写字符串形式给出(例如<code>"gloo"</code>)，也可以通过<a href="#torch.distributed.Backend" title="torch.distributed.Backend"><code>Backend</code></a>访问属性(例如<code>Backend.GLOO</code>)。</li>
<li><strong>init_method</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a><em>,</em> <em>optional</em>) – 指定如何初始化进程组的URL。</li>
<li><strong>world_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>,</em> <em>optional</em>) – 参与作业的进程数。</li>
<li><strong>rank</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>,</em> <em>optional</em>) – 当前流程的排名。</li>
<li><strong>timeout</strong> (<em>timedelta</em>_,_ <em>optional</em>) – 针对进程组执行的操作超时，默认值等于30分钟，这仅适用于<code>gloo</code>后端。</li>
<li><strong>group_name</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a><em>,</em> <em>optional</em>_,_ <em>deprecated</em>) – 团队名字。</li>
</ul>
<p>要启用<code>backend == Backend.MPI</code>，PyTorch需要在支持MPI的系统上从源构建，这同样适用于NCCL。</p>
<pre><code class="language-py">class torch.distributed.Backend
</code></pre>
<p>类似枚举的可用后端类：GLOO，NCCL和MPI。</p>
<p>这个类的值是小写字符串，例如“gloo”。它们可以作为属性访问，例如<code>Backend.NCCL</code>。</p>
<p>可以直接调用此类来解析字符串，例如，<code>Backend（backend_str）</code>将检查<code>backend_str</code>是否有效，如果是，则返回解析的小写字符串。它也接受大写字符串，例如``Backend（“GLOO”）<code>return</code>“gloo”`。 注意</p>
<p>条目<code>Backend.UNDEFINED</code>存在但仅用作某些字段的初始值。用户既不应直接使用也不应假设存在。</p>
<pre><code class="language-py">torch.distributed.get_backend(group=&lt;object object&gt;)
</code></pre>
<p>返回给定进程组的后端</p>
<table>
<thead>
<tr>
<th>参数:</th>
<th><strong>group</strong> (<em>ProcessGroup</em>_,_ <em>optional</em>) – 要处理的进程组。默认值是常规主进程组。如果指定了另一个特定组，则调用进程必须是<code>group</code>的一部分。</th>
</tr>
</thead>
<tbody>
<tr>
<td>返回:</td>
<td>给定进程组的后端作为小写字符串</td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
</tbody>
</table>
<pre><code class="language-py">torch.distributed.get_rank(group=&lt;object object&gt;)
</code></pre>
<p>返回当前进程组的排名</p>
<p>Rank是分配给分布式进程组中每个进程的唯一标识符。它们总是从0到<code>world_size</code>的连续整数。</p>
<table>
<thead>
<tr>
<th>参数:</th>
<th><strong>group</strong> (<em>ProcessGroup</em>_,_ <em>optional</em>) – 要处理的进程组</th>
</tr>
</thead>
<tbody>
<tr>
<td>返回:</td>
<td>进程组-1的等级，如果不是该组的一部分</td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
</tbody>
</table>
<pre><code class="language-py">torch.distributed.get_world_size(group=&lt;object object&gt;)
</code></pre>
<p>返回当前进程组中的进程数</p>
<table>
<thead>
<tr>
<th>参数:</th>
<th><strong>group</strong> (<em>ProcessGroup</em>_,_ <em>optional</em>) – 要处理的进程组</th>
</tr>
</thead>
<tbody>
<tr>
<td>返回:</td>
<td>进程组-1的世界大小，如果不是该组的一部分</td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
</tbody>
</table>
<pre><code class="language-py">torch.distributed.is_initialized()
</code></pre>
<p>检查是否已初始化默认进程组</p>
<pre><code class="language-py">torch.distributed.is_mpi_available()
</code></pre>
<p>检查MPI是否可用</p>
<pre><code class="language-py">torch.distributed.is_nccl_available()
</code></pre>
<p>检查NCCL是否可用</p>
<hr>
<p>目前支持三种初始化方法：</p>
<h3 id="tcp初始化">TCP初始化</h3>
<p>有两种方法可以使用TCP进行初始化，这两种方法都需要从所有进程可以访问的网络地址和所需的<code>world_size</code>。第一种方法需要指定属于rank 0进程的地址。此初始化方法要求所有进程都具有手动指定的排名。</p>
<p>请注意，最新的分布式软件包中不再支持多播地址。<code>group_name</code>也被弃用了。</p>
<pre><code class="language-py">import torch.distributed as dist

# 使用其中一台机器的地址
dist.init_process_group(backend, init_method='tcp://10.1.1.20:23456',
                        rank=args.rank, world_size=4)

</code></pre>
<h3 id="共享文件系统初始化">共享文件系统初始化</h3>
<p>另一种初始化方法使用一个文件系统，该文件系统与组中的所有机器共享和可见，以及所需的<code>world_size</code>。URL应以<code>file：//</code>开头，并包含共享文件系统上不存在的文件（在现有目录中）的路径。如果文件不存在，文件系统初始化将自动创建该文件，但不会删除该文件。因此，下一步初始化 <a href="#torch.distributed.init_process_group" title="torch.distributed.init_process_group"><code>init_process_group()</code></a> 在相同的文件路径发生之前您有责任确保清理文件。</p>
<p>请注意，在最新的分布式软件包中不再支持自动排名分配，并且也不推荐使用<code>group_name</code>。</p>
<p>警告</p>
<p>此方法假定文件系统支持使用<code>fcntl</code>进行锁定 - 大多数本地系统和NFS都支持它。</p>
<p>警告</p>
<p>此方法将始终创建该文件，并尽力在程序结束时清理并删除该文件。换句话说，每次进行初始化都需要创建一个全新的空文件，以便初始化成功。如果再次使用先前初始化使用的相同文件（不会被清除），则这是意外行为，并且经常会导致死锁和故障。因此，即使此方法将尽力清理文件，如果自动删除不成功，您有责任确保在训练结束时删除该文件以防止同一文件被删除 下次再次使用。如果你打算在相同的文件系统路径下多次调用 <a href="#torch.distributed.init_process_group" title="torch.distributed.init_process_group"><code>init_process_group()</code></a> 的时候，就显得尤为重要了。换一种说法，如果那个文件没有被移除并且你再次调用 <a href="#torch.distributed.init_process_group" title="torch.distributed.init_process_group"><code>init_process_group()</code></a>，那么失败是可想而知的。这里的经验法则是，每当调用<a href="#torch.distributed.init_process_group" title="torch.distributed.init_process_group"><code>init_process_group()</code></a>的时候，确保文件不存在或为空。</p>
<pre><code class="language-py">import torch.distributed as dist

# 应始终指定等级
dist.init_process_group(backend, init_method='file:///mnt/nfs/sharedfile',
                        world_size=4, rank=args.rank)

</code></pre>
<h3 id="环境变量初始化">环境变量初始化</h3>
<p>此方法将从环境变量中读取配置，从而可以完全自定义信息的获取方式。要设置的变量是：</p>
<ul>
<li><code>MASTER_PORT</code> - 需要; 必须是机器上的自由端口，等级为0。</li>
<li><code>MASTER_ADDR</code> - 要求（0级除外）; 等级0节点的地址。</li>
<li><code>WORLD_SIZE</code> - 需要; 可以在这里设置，也可以在调用init函数时设置。</li>
<li><code>RANK</code> - 需要; 可以在这里设置，也可以在调用init函数时设置。</li>
</ul>
<p>等级为0的机器将用于设置所有连接。</p>
<p>这是默认方法，意味着不必指定<code>init_method</code>（或者可以是<code>env：//</code>）。</p>
<h2 id="组">组</h2>
<p>默认情况下，集合体在默认组（也称为世界）上运行，并要求所有进程都进入分布式函数调用。但是，一些工作负载可以从更细粒度的通信中受益。这是分布式群体发挥作用的地方。<a href="#torch.distributed.new_group" title="torch.distributed.new_group"><code>new_group()</code></a> 函数可用于创建新组，具有所有进程的任意子集。它返回一个不透明的组句柄，可以作为所有集合体的“group”参数给出（集合体是分布式函数，用于在某些众所周知的编程模式中交换信息）。</p>
<p>目前<code>torch.distributed</code>不支持创建具有不同后端的组。换一种说法，每一个正在被创建的组都会用相同的后端，只要你在 <a href="#torch.distributed.init_process_group" title="torch.distributed.init_process_group"><code>init_process_group()</code></a> 里面声明清楚。</p>
<pre><code class="language-py">torch.distributed.new_group(ranks=None, timeout=datetime.timedelta(seconds=1800))
</code></pre>
<p>创建一个新的分布式组</p>
<p>此功能要求主组中的所有进程（即属于分布式作业的所有进程）都进入此功能，即使它们不是该组的成员也是如此。此外，应在所有进程中以相同的顺序创建组。</p>
<p>参数:</p>
<ul>
<li><strong>ranks</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a><em>[</em><a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>]</em>) – 小组成员的等级列表。</li>
<li><strong>timeout</strong> (<em>timedelta</em>_,_ <em>optional</em>) – 针对进程组执行的操作超时，默认值等于30分钟，这仅适用于<code>gloo</code>后端。</li>
</ul>
<table>
<thead>
<tr>
<th>返回:</th>
<th>分布式组的句柄，可以给予集体调用</th>
</tr>
</thead>
<tbody></tbody>
</table>
<h2 id="点对点通信">点对点通信</h2>
<pre><code class="language-py">torch.distributed.send(tensor, dst, group=&lt;object object&gt;, tag=0)
</code></pre>
<p>同步发送张量</p>
<p>参数:</p>
<ul>
<li><strong>tensor</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 准备发送的张量。</li>
<li><strong>dst</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – 目的地排名。</li>
<li><strong>group</strong> (<em>ProcessGroup</em>_,_ <em>optional</em>) – 要处理的进程组。</li>
<li><strong>tag</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>,</em> <em>optional</em>) – 标记以匹配发送与远程接收。</li>
</ul>
<pre><code class="language-py">torch.distributed.recv(tensor, src=None, group=&lt;object object&gt;, tag=0)
</code></pre>
<p>同步接收张量</p>
<p>参数：</p>
<ul>
<li><strong>tensor</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 张量填充接收的数据。</li>
<li><strong>src</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>,</em> <em>optional</em>) – 来源排名。如果未指定，将从任何流程收到。</li>
<li><strong>group</strong> (<em>ProcessGroup</em>_,_ <em>optional</em>) – 要处理的进程组。</li>
<li><strong>tag</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>,</em> <em>optional</em>) – 标记以匹配接收与远程发送。</li>
</ul>
<table>
<thead>
<tr>
<th>返回:</th>
<th>发件人排名-1，如果不是该组的一部分</th>
</tr>
</thead>
<tbody></tbody>
</table>
<p><a href="#torch.distributed.isend" title="torch.distributed.isend"><code>isend()</code></a> 和 <a href="#torch.distributed.irecv" title="torch.distributed.irecv"><code>irecv()</code></a> 使用时返回分布式请求对象。通常，此对象的类型未指定，因为它们永远不应手动创建，但它们保证支持两种方法：</p>
<ul>
<li><code>is_completed()</code> - 如果操作已完成，则返回True。</li>
<li><code>wait()</code> - 将阻止该过程，直到操作完成，<code>is_completed（）</code>保证一旦返回就返回True。</li>
</ul>
<pre><code class="language-py">torch.distributed.isend(tensor, dst, group=&lt;object object&gt;, tag=0)
</code></pre>
<p>异步发送张量</p>
<p>参数:</p>
<ul>
<li><strong>tensor</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 准本发送的张量。</li>
<li><strong>dst</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – 目的地排名。</li>
<li><strong>group</strong> (<em>ProcessGroup</em>_,_ <em>optional</em>) – 要处理的进程组。</li>
<li><strong>tag</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>,</em> <em>optional</em>) – 标记以匹配发送与远程接收。</li>
</ul>
<table>
<thead>
<tr>
<th>返回:</th>
<th>分布式请求对象。没有，如果不是该组的一部分</th>
</tr>
</thead>
<tbody></tbody>
</table>
<pre><code class="language-py">torch.distributed.irecv(tensor, src, group=&lt;object object&gt;, tag=0)
</code></pre>
<p>异步接收张量</p>
<p>参数:</p>
<ul>
<li><strong>tensor</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 张量填充接收的数据。</li>
<li><strong>src</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – 来源排名。</li>
<li><strong>group</strong> (<em>ProcessGroup</em>_,_ <em>optional</em>) – 要处理的进程组。</li>
<li><strong>tag</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>,</em> <em>optional</em>) – 标记以匹配接收与远程发送。</li>
</ul>
<table>
<thead>
<tr>
<th>返回:</th>
<th>分布式请求对象。没有，如果不是该组的一部分</th>
</tr>
</thead>
<tbody></tbody>
</table>
<h2 id="同步和异步集合操作">同步和异步集合操作</h2>
<p>每个集合操作函数都支持以下两种操作：</p>
<p>同步操作 - 默认模式，当<code>async_op</code>设置为False时。当函数返回时，保证执行集合操作（如果它是CUDA操作，则不一定完成，因为所有CUDA操作都是异步的），并且可以调用任何进一步的函数调用，这取决于集合操作的数据。在同步模式下，集合函数不返回任何内容。</p>
<p>asynchronous operation - 当<code>async_op</code>设置为True时。集合操作函数返回分布式请求对象。通常，您不需要手动创建它，并且保证支持两种方法：</p>
<ul>
<li><code>is_completed()</code> - 如果操作已完成，则返回True。</li>
<li><code>wait()</code> - 将阻止该过程，直到操作完成。</li>
</ul>
<h2 id="集体职能">集体职能</h2>
<pre><code class="language-py">torch.distributed.broadcast(tensor, src, group=&lt;object object&gt;, async_op=False)
</code></pre>
<p>将张量广播到整个群体</p>
<p><code>tensor</code>必须在参与集合体的所有进程中具有相同数量的元素。</p>
<p>参数:</p>
<ul>
<li><strong>tensor</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 如果<code>src</code>是当前进程的等级，则发送的数据，否则用于保存接收数据的张量。</li>
<li><strong>src</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – 来源排名。</li>
<li><strong>group</strong> (<em>ProcessGroup</em>_,_ <em>optional</em>) – 要处理的进程组。</li>
<li><strong>async_op</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>,</em> <em>optional</em>) – 这个操作是否应该是异步操作。</li>
</ul>
<table>
<thead>
<tr>
<th>返回:</th>
<th>异步工作句柄，如果async_op设置为True。无，如果不是async_op或不是组的一部分</th>
</tr>
</thead>
<tbody></tbody>
</table>
<pre><code class="language-py">torch.distributed.all_reduce(tensor, op=ReduceOp.SUM, group=&lt;object object&gt;, async_op=False)
</code></pre>
<p>减少所有机器上的张量数据，以便获得最终结果</p>
<p>调用<code>tensor</code>之后在所有进程中将按位相同。</p>
<p>参数:</p>
<ul>
<li><strong>tensor</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 集体的输入和输出。该功能就地运行。</li>
<li><strong>op</strong> (<em>optional</em>) – 来自<code>torch.distributed.ReduceOp</code>枚举的值之一。指定用于逐元素减少的操作。</li>
<li><strong>group</strong> (<em>ProcessGroup</em>_,_ <em>optional</em>) – 要处理的进程组。</li>
<li><strong>async_op</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>,</em> <em>optional</em>) – 这个操作是否应该是异步操作。</li>
</ul>
<table>
<thead>
<tr>
<th>返回:</th>
<th>异步工作句柄，如果async_op设置为True。无，如果不是async_op或不是组的一部分</th>
</tr>
</thead>
<tbody></tbody>
</table>
<pre><code class="language-py">torch.distributed.reduce(tensor, dst, op=ReduceOp.SUM, group=&lt;object object&gt;, async_op=False)
</code></pre>
<p>减少所有机器的张量数据</p>
<p>只有排名为“dst”的进程才会收到最终结果。</p>
<p>参数:</p>
<ul>
<li><strong>tensor</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 集体的输入和输出。该功能就地运行。</li>
<li><strong>dst</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – 目的地排名。</li>
<li><strong>op</strong> (<em>optional</em>) – 来自<code>torch.distributed.ReduceOp</code>枚举的值之一。指定用于逐元素减少的操作。</li>
<li><strong>group</strong> (<em>ProcessGroup</em>_,_ <em>optional</em>) – 要处理的进程组。</li>
<li><strong>async_op</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>,</em> <em>optional</em>) – 这个操作是否应该是异步操作。</li>
</ul>
<table>
<thead>
<tr>
<th>返回:</th>
<th>异步工作句柄，如果async_op设置为True。无，如果不是async_op或不是组的一部分</th>
</tr>
</thead>
<tbody></tbody>
</table>
<pre><code class="language-py">torch.distributed.all_gather(tensor_list, tensor, group=&lt;object object&gt;, async_op=False)
</code></pre>
<p>从列表中收集整个组的张量</p>
<p>参数：</p>
<ul>
<li><strong>tensor_list</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a><em>[</em><a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em>) – 输出列表。它应包含正确大小的张量，用于集合的输出。</li>
<li><strong>tensor</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 从当前进程广播的张量。</li>
<li><strong>group</strong> (<em>ProcessGroup</em>_,_ <em>optional</em>) – 要处理的进程组。</li>
<li><strong>async_op</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>,</em> <em>optional</em>) – 这个操作是否应该是异步操作。</li>
</ul>
<table>
<thead>
<tr>
<th>返回:</th>
<th>异步工作句柄，如果async_op设置为True。无，如果不是async_op或不是组的一部分</th>
</tr>
</thead>
<tbody></tbody>
</table>
<pre><code class="language-py">torch.distributed.gather(tensor, gather_list, dst, group=&lt;object object&gt;, async_op=False)
</code></pre>
<p>在一个过程中收集张量列表</p>
<p>参数：</p>
<ul>
<li><strong>tensor</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输入张量。</li>
<li><strong>gather_list</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a><em>[</em><a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em>) – 用于接收数据的适当大小的张量列表。仅在接收过程中需要。</li>
<li><strong>dst</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – 目的地排名。除接收数据的进程外，在所有进程中都是必需的。</li>
<li><strong>group</strong> (<em>ProcessGroup</em>_,_ <em>optional</em>) – 要处理的进程组。</li>
<li><strong>async_op</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>,</em> <em>optional</em>) – 这个操作是否应该是异步操作。</li>
</ul>
<table>
<thead>
<tr>
<th>返回:</th>
<th>异步工作句柄，如果async_op设置为True。无，如果不是async_op或不是组的一部分</th>
</tr>
</thead>
<tbody></tbody>
</table>
<pre><code class="language-py">torch.distributed.scatter(tensor, scatter_list, src, group=&lt;object object&gt;, async_op=False)
</code></pre>
<p>将张量列表分散到组中的所有进程</p>
<p>每个进程只接收一个张量并将其数据存储在<code>tensor</code>参数中。</p>
<p>参数：</p>
<ul>
<li><strong>tensor</strong> (<a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – 输出张量。</li>
<li><strong>scatter_list</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a><em>[</em><a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em>) – 要分散的张量列表。仅在发送数据的过程中需要。</li>
<li><strong>src</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – 来源排名。除发送数据的进程外，在所有进程中都是必需的。</li>
<li><strong>group</strong> (<em>ProcessGroup</em>_,_ <em>optional</em>) – 要处理的进程组。</li>
<li><strong>async_op</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>,</em> <em>optional</em>) – 这个操作是否应该是异步操作。</li>
</ul>
<table>
<thead>
<tr>
<th>返回:</th>
<th>异步工作句柄，如果async_op设置为True。如果不是async_op或不是组的一部分，无</th>
</tr>
</thead>
<tbody></tbody>
</table>
<pre><code class="language-py">torch.distributed.barrier(group=&lt;object object&gt;, async_op=False)
</code></pre>
<p>同步所有进程</p>
<p>如果async_op为False，或者在wait（）上调用异步工作句柄，则此集合会阻止进程直到整个组进入此函数。</p>
<p>参数：</p>
<ul>
<li><strong>group</strong> (<em>ProcessGroup</em>_,_ <em>optional</em>) – 要处理的进程组。</li>
<li><strong>async_op</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>,</em> <em>optional</em>) – 这个操作是否应该是异步操作。</li>
</ul>
<table>
<thead>
<tr>
<th>返回:</th>
<th>异步工作句柄，如果async_op设置为True。无，如果不是async_op或不是组的一部分</th>
</tr>
</thead>
<tbody></tbody>
</table>
<pre><code class="language-py">class torch.distributed.ReduceOp
</code></pre>
<p>类似枚举的可用减少操作类：<code>SUM</code>，<code>PRODUCT</code>，<code>MIN</code>和<code>MAX</code>。</p>
<p>该类的值可以作为属性访问，例如，<code>ReduceOp.SUM</code>。它们用于指定减少集群的战略，例如 <a href="#torch.distributed.reduce" title="torch.distributed.reduce"><code>reduce()</code></a>, <a href="#torch.distributed.all_reduce_multigpu" title="torch.distributed.all_reduce_multigpu"><code>all_reduce_multigpu()</code></a>。</p>
<p>成员：</p>
<blockquote>
<p>SUM</p>
<p>PRODUCT</p>
<p>MIN</p>
<p>MAX</p>
</blockquote>
<pre><code class="language-py">class torch.distributed.reduce_op
</code></pre>
<p>用于还原操作的不再使用的枚举类：<code>SUM</code>，<code>PRODUCT</code>，<code>MIN</code>和<code>MAX</code>。</p>
<p>建议使用<a href="#torch.distributed.ReduceOp" title="torch.distributed.ReduceOp"><code>ReduceOp</code></a> 代替。</p>
<h2 id="多gpu集群功能">多GPU集群功能</h2>
<p>如果每个节点上有多个GPU，则在使用NCCL和Gloo后端时，<a href="#torch.distributed.broadcast_multigpu" title="torch.distributed.broadcast_multigpu"><code>broadcast_multigpu()</code></a> <a href="#torch.distributed.all_reduce_multigpu" title="torch.distributed.all_reduce_multigpu"><code>all_reduce_multigpu()</code></a> <a href="#torch.distributed.reduce_multigpu" title="torch.distributed.reduce_multigpu"><code>reduce_multigpu()</code></a> 和 <a href="#torch.distributed.all_gather_multigpu" title="torch.distributed.all_gather_multigpu"><code>all_gather_multigpu()</code></a> 支持每个节点内多个GPU之间的分布式集合操作。这些功能可以潜在地提高整体分布式训练性能，并通过传递张量列表轻松使用。传递的张量列表中的每个张量需要位于调用该函数的主机的单独GPU设备上。请注意，张量列表的长度在所有分布式进程中需要相同。另请注意，目前只有NCCL后端支持多GPU集合功能。</p>
<p>例如，如果我们用于分布式训练的系统有2个节点，每个节点有8个GPU。在16个GPU中的每一个上，都有一个我们希望减少的张量，以下代码可以作为参考：</p>
<p>代码在节点0上运行</p>
<pre><code class="language-py">import torch
import torch.distributed as dist

dist.init_process_group(backend="nccl",
                        init_method="file:///distributed_test",
                        world_size=2,
                        rank=0)
tensor_list = []
for dev_idx in range(torch.cuda.device_count()):
    tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx))

dist.all_reduce_multigpu(tensor_list)

</code></pre>
<p>代码在节点1上运行</p>
<pre><code class="language-py">import torch
import torch.distributed as dist

dist.init_process_group(backend="nccl",
                        init_method="file:///distributed_test",
                        world_size=2,
                        rank=1)
tensor_list = []
for dev_idx in range(torch.cuda.device_count()):
    tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx))

dist.all_reduce_multigpu(tensor_list)

</code></pre>
<p>调用结束后，两个节点上的所有16个张量都将具有16的全减值。</p>
<pre><code class="language-py">torch.distributed.broadcast_multigpu(tensor_list, src, group=&lt;object object&gt;, async_op=False, src_tensor=0)
</code></pre>
<p>使用每个节点多个GPU张量将张量广播到整个组</p>
<p><code>tensor</code>必须在参与集合体的所有进程的所有GPU中具有相同数量的元素。列表中的每个张量必须位于不同的GPU上。</p>
<p>目前仅支持nccl和gloo后端张量应该只是GPU张量</p>
<p>参数：</p>
<ul>
<li><strong>tensor_list</strong> (<em>List</em>_[<em><a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></em>]_) – 参与集群操作行动的张量。如果<code>src</code>是排名，那么<code>tensor_list`（`tensor_list [src_tensor]`）的`src_tensor</code>元素将被广播到src进程中的所有其他张量（在不同的GPU上）以及<code>tensor_list中的所有张量</code>其他非src进程。您还需要确保调用此函数的所有分布式进程的<code>len（tensor_list）</code>是相同的。</li>
<li><strong>src</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – 源排行。</li>
<li><strong>group</strong> (<em>ProcessGroup</em>_,_ <em>optional</em>) – 要被处理的进程组。</li>
<li><strong>async_op</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>,</em> <em>optional</em>) – 这个操作是否应该是异步操作。</li>
<li><strong>src_tensor</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>,</em> <em>optional</em>) – 源张量等级在<code>tensor_list</code>内。</li>
</ul>
<table>
<thead>
<tr>
<th>返回:</th>
<th>异步工作句柄，如果async_op设置为True。无，如果不是async_op或不是组的一部分</th>
</tr>
</thead>
<tbody></tbody>
</table>
<pre><code class="language-py">torch.distributed.all_reduce_multigpu(tensor_list, op=ReduceOp.SUM, group=&lt;object object&gt;, async_op=False)
</code></pre>
<p>减少所有机器上的张量数据，以便获得最终结果。此功能可减少每个节点上的多个张量，而每个张量位于不同的GPU上。因此，张量列表中的输入张量需要是GPU张量。此外，张量列表中的每个张量都需要驻留在不同的GPU上。</p>
<p>在调用之后，<code>tensor_list</code>中的所有<code>tensor</code>在所有进程中都是按位相同的。</p>
<p>目前仅支持nccl和gloo后端，张量应仅为GPU张量。</p>
<p>参数：</p>
<ul>
<li><strong>list</strong> (<em>tensor</em>) – 集体的输入和输出张量列表。该功能就地运行，并要求每个张量在不同的GPU上为GPU张量。您还需要确保调用此函数的所有分布式进程的<code>len（tensor_list）</code>是相同的。</li>
<li><strong>op</strong> (<em>optional</em>) – 来自<code>torch.distributed.ReduceOp</code>枚举的值之一，并且指定一个逐元素减少的操作。</li>
<li><strong>group</strong> (<em>ProcessGroup</em>_,_ <em>optional</em>) – 要处理的进程组。</li>
<li><strong>async_op</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>,</em> <em>optional</em>) – 这个操作是否应该是异步操作。</li>
</ul>
<table>
<thead>
<tr>
<th>返回:</th>
<th>异步工作句柄，如果async_op设置为True。无，如果不是async_op或不是组的一部分</th>
</tr>
</thead>
<tbody></tbody>
</table>
<pre><code class="language-py">torch.distributed.reduce_multigpu(tensor_list, dst, op=ReduceOp.SUM, group=&lt;object object&gt;, async_op=False, dst_tensor=0)
</code></pre>
<p>减少所有计算机上多个GPU的张量数据。<code>tensor_list</code>中的每个张量应位于单独的GPU上。</p>
<p>只有级别为'dst<code>的进程中的'tensor_list [dst_tensor]</code>的GPU才会收到最终结果。</p>
<p>目前仅支持nccl后端张量应该只是GPU张量。</p>
<p>参数：</p>
<ul>
<li><strong>tensor_list</strong> (<em>List</em>_[<em><a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></em>]_) – 输入和输出集体的GPU张量。该功能就地运行，您还需要确保调用此函数的所有分布式进程的<code>len（tensor_list）</code>是相同的。</li>
<li><strong>dst</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – 目的地排名。</li>
<li><strong>op</strong> (<em>optional</em>) – 来自<code>torch.distributed.ReduceOp</code>枚举的值之一。指定一个逐元素减少的操作。</li>
<li><strong>group</strong> (<em>ProcessGroup</em>_,_ <em>optional</em>) – The process group to work on</li>
<li><strong>async_op</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>,</em> <em>optional</em>) – 这个操作是否应该是异步操作。</li>
<li><strong>dst_tensor</strong> (<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>,</em> <em>optional</em>) – 目标张量在<code>tensor_list</code>中排名。</li>
</ul>
<table>
<thead>
<tr>
<th>返回:</th>
<th>异步工作句柄，如果async_op设置为True。没有，否则</th>
</tr>
</thead>
<tbody></tbody>
</table>
<pre><code class="language-py">torch.distributed.all_gather_multigpu(output_tensor_lists, input_tensor_list, group=&lt;object object&gt;, async_op=False)
</code></pre>
<p>从列表中收集整个组的张量。<code>tensor_list</code>中的每个张量应位于单独的GPU上。</p>
<p>目前仅支持nccl后端张量应该只是GPU张量。</p>
<p>参数：</p>
<ul>
<li><strong>output_tensor_lists</strong> (<em>List</em>_[<strong>List</strong>[<em><a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></em>]__]_) – 输出列表。它应该在每个GPU上包含正确大小的张量，以用于集合的输出。例如 <code>output_tensor_lists [i]</code>包含驻留在<code>input_tensor_list [i]</code>的GPU上的all_gather结果。请注意，<code>output_tensor_lists [i]</code>的每个元素都具有<code>world_size * len（input_tensor_list）</code>的大小，因为该函数全部收集组中每个GPU的结果。要解释<code>output_tensor_list [i]</code>的每个元素，请注意等级k的<code>input_tensor_list [j]</code>将出现在<code>output_tensor_list [i] [rank * world_size + j]中。还要注意</code>len（output_tensor_lists）<code>，并且</code>output_tensor_lists<code>中的每个元素的大小（每个元素都是一个列表，因此</code>len（output_tensor_lists [i]）`）对于调用此函数的所有分布式进程都需要相同。</li>
<li><strong>input_tensor_list</strong> (<em>List</em>_[<em><a href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a></em>]_) – 从当前进程广播的张量（在不同的GPU上）的列表。请注意，调用此函数的所有分布式进程的<code>len（input_tensor_list）</code>必须相同。</li>
<li><strong>group</strong> (<em>ProcessGroup</em>_,_ <em>optional</em>) – 要处理的进程组。</li>
<li><strong>async_op</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>,</em> <em>optional</em>) – 这个操作是否应该是异步操作</li>
</ul>
<table>
<thead>
<tr>
<th>返回:</th>
<th>异步工作句柄，如果async_op设置为True。无，如果不是async_op或不是组的一部分</th>
</tr>
</thead>
<tbody></tbody>
</table>
<h2 id="启动实用程序">启动实用程序</h2>
<p><code>torch.distributed</code>包还在<code>torch.distributed.launch</code>中提供了一个启动实用程序。此帮助实用程序可用于为每个节点启动多个进程以进行分布式训练。该实用程序还支持python2和python3。</p>
<p><code>torch.distributed.launch</code>是一个模块，它在每个训练节点上产生多个分布式训练过程。</p>
<p>该实用程序可用于单节点分布式训练，其中将生成每个节点的一个或多个进程。该实用程序可用于CPU训练或GPU训练。如果该实用程序用于GPU训练，则每个分布式进程将在单个GPU上运行。这可以实现良好改进的单节点训练性能。它还可以用于多节点分布式训练，通过在每个节点上产生多个进程来获得良好改进的多节点分布式训练性能。这对于具有多个具有直接GPU支持的Infiniband接口的系统尤其有利，因为所有这些接口都可用于聚合通信带宽。</p>
<p>在单节点分布式训练或多节点分布式训练的两种情况下，该实用程序将为每个节点启动给定数量的进程（<code>--nproc_per_node</code>）。如果用于GPU训练，此数字需要小于或等于当前系统上的GPU数量（'nproc_per_node`），并且每个进程将在单个GPU上运行，从_GPU 0到GPU（nproc_per_node - 1）_。</p>
<p><strong>如何使用这个模块：</strong></p>
<ol>
<li>单节点多进程分布式训练</li>
</ol>
<pre><code class="language-py">&gt;&gt;&gt; python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE
 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other
 arguments of your training script)

</code></pre>
<ol>
<li>多节点多进程分布式训练:(例如两个节点）</li>
</ol>
<p>节点1：<em>（IP：192.168.1.1，并且有一个空闲端口：1234）</em></p>
<pre><code class="language-py">&gt;&gt;&gt; python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE
 --nnodes=2 --node_rank=0 --master_addr="192.168.1.1"
 --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3
 and all other arguments of your training script)

</code></pre>
<p>节点2：</p>
<pre><code class="language-py">&gt;&gt;&gt; python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE
 --nnodes=2 --node_rank=1 --master_addr="192.168.1.1"
 --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3
 and all other arguments of your training script)

</code></pre>
<p>1.查找此模块提供的可选参数：</p>
<pre><code class="language-py">&gt;&gt;&gt; python -m torch.distributed.launch --help

</code></pre>
<p><strong>重要告示：</strong></p>
<p>1. 这种实用和多进程分布式（单节点或多节点）GPU训练目前仅使用NCCL分布式后端实现最佳性能。因此，NCCL后端是用于GPU训练的推荐后端。</p>
<p>2. 在您的训练程序中，您必须解析命令行参数：<code>--local_rank = LOCAL_PROCESS_RANK</code>，这将由此模块提供。如果您的训练计划使用GPU，则应确保您的代码仅在LOCAL_PROCESS_RANK的GPU设备上运行。这可以通过以下方式完成：</p>
<p>解析local_rank参数</p>
<pre><code class="language-py">&gt;&gt;&gt; import argparse
&gt;&gt;&gt; parser = argparse.ArgumentParser()
&gt;&gt;&gt; parser.add_argument("--local_rank", type=int)
&gt;&gt;&gt; args = parser.parse_args()

</code></pre>
<p>使用其中一个将您的设备设置为本地排名</p>
<pre><code class="language-py">&gt;&gt;&gt; torch.cuda.set_device(arg.local_rank)  # before your code runs

</code></pre>
<p>或者</p>
<pre><code class="language-py">&gt;&gt;&gt; with torch.cuda.device(arg.local_rank):
&gt;&gt;&gt;    # your code to run

</code></pre>
<p>3. 在您的训练计划中，您应该在开始时调用以下函数来启动分布式后端。您需要确保init_method使用<code>env：//</code>，这是该模块唯一支持的<code>init_method</code>。</p>
<pre><code class="language-py">torch.distributed.init_process_group(backend='YOUR BACKEND',
                                     init_method='env://')

</code></pre>
<p>4. 在您的训练计划中，您可以使用常规分布式功能或使用 <a href="nn.html#torch.nn.parallel.DistributedDataParallel" title="torch.nn.parallel.DistributedDataParallel"><code>torch.nn.parallel.DistributedDataParallel()</code></a> 模块。如果您的训练计划使用GPU进行训练，并且您希望使用 <a href="nn.html#torch.nn.parallel.DistributedDataParallel" title="torch.nn.parallel.DistributedDataParallel"><code>torch.nn.parallel.DistributedDataParallel()</code></a> 模块。这里是如何配置它。</p>
<pre><code class="language-py">model = torch.nn.parallel.DistributedDataParallel(model,
                                                  device_ids=[arg.local_rank],
                                                  output_device=arg.local_rank)

</code></pre>
<p>请确保将<code>device_ids</code>参数设置为您的代码将在其上运行的唯一GPU设备ID。这通常是流程的本地排名。换句话说，<code>device_ids</code>需要是<code>[args.local_rank]</code>，<code>output_device</code>需要是'args.local_rank`才能使用这个实用程序。</p>
<p>警告</p>
<p><code>local_rank</code>不是全局唯一的：它只对机器上的每个进程唯一。因此，不要使用它来决定是否应该，例如，写入网络文件系统，参考 <a href="https://github.com/pytorch/pytorch/issues/12042">https://github.com/pytorch/pytorch/issues/12042</a> 例如，如果您没有正确执行此操作，事情可能会出错。</p>
<h2 id="spawn实用程序">Spawn实用程序</h2>
<p>在 <a href="multiprocessing.html#torch.multiprocessing.spawn" title="torch.multiprocessing.spawn"><code>torch.multiprocessing.spawn()</code></a> 里面，torch.multiprocessing包还提供了一个<code>spawn</code>函数. 此辅助函数可用于生成多个进程。它通过传递您要运行的函数并生成N个进程来运行它。这也可以用于多进程分布式训练。</p>
<p>有关如何使用它的参考，请参阅 <a href="https://github.com/pytorch/examples/tree/master/imagenet">PyToch example - ImageNet implementation</a></p>
<p>请注意，此函数需要Python 3.4或更高版本。</p>
</div>
<hr class="uk-article-divider">
<div class="uk-block uk-block-muted uk-padding-top-remove uk-padding-bottom-remove uk-margin-large-top  book-recommend-wrap">
<div class="uk-margin-top uk-margin-bottom uk-margin-left uk-margin-right">
<div class="uk-margin uk-text-muted "><i class="uk-icon-outdent uk-icon-justify uk-margin-small-right"></i>书籍推荐</div>
<div class="books">
<ul class="uk-book-list">
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/96/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/96/index.html">零基础学Python</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/59.html">qiwsir</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">80页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月29日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 1635个">1635</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/68/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/68/index.html">Python 资源大全中文版</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/19.html">伯乐在线</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">1页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月6日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 10237个">10237</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/172/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/172/index.html">Seaborn 0.9 中文文档</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/18.html">ApacheCN</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">76页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月26日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 32个">32</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/182/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/amazonwebservices_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/182/index.html">亚马逊(aws)web服务实用指南</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/102.html">open-guides</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="amazonwebservices">amazonwebservices</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">314页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月26日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 24000个">24000</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/163/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/git_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/163/index.html">Git的奇技淫巧</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/92.html">jackfrued</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="git">git</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">77页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月26日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 28个">28</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/194/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/postgresql_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/194/index.html">postgresql教程</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/110.html">hfpp2012</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="postgresql">postgresql</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">17页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2021年10月24日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 9个">9</span>
</div>
</div>
</div>
</li>
<hr>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
<nav class="tm-navbar uk-navbar uk-navbar-attached reader-nav">
<div class="uk-float-left uk-margin-small-top">
<a href="javascript:;" title="目录菜单" class="show-menu  uk-icon-hover  uk-icon-align-justify uk-margin-right"></a>
<div data-uk-dropdown="{mode:'click',pos:'bottom-left'}" class="font-setting-wrap">
<a class="uk-icon-hover uk-icon-font uk-margin-right" aria-label="字体设置" href="javascript:;"></a>
<div class="uk-dropdown dropdown-menu">
<div class="dropdown-caret"><span class="caret-outer"></span><span class="caret-inner"></span></div>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-reduce">小字</button>
<button class="uk-button-link button size-2 font-enlarge">大字</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-1 ">宋体</button>
<button class="uk-button-link button size-2 font-2 ">黑体</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-3 color-theme-sun "><i class="uk-icon-sun-o"></i>白天</button>
<button class="uk-button-link button size-3 color-theme-eye "><i class="uk-icon-eye"></i>护眼</button>
<button class="uk-button-link button size-3 color-theme-moon "><i class="uk-icon-moon-o"></i>夜晚</button></div>
</div>
</div>
<a class="logo uk-margin-right" href="../../../../" title="返回首页"><img class="" src="../../../../static/components/images/icon_32.png" /></a>
</div>
<div class="uk-navbar-flip  uk-hidden-small">
<div id="share-box"></div>
</div>
</nav>
<div id="menu-id" class="uk-offcanvas reader-offcanvas">
<div class="uk-offcanvas-bar">
<ul class="book-menu-bar uk-nav uk-nav-offcanvas" data-uk-nav>
<li>
<a href="../../../../book/169/index.html" data-book-page-rel-url="index.html" data-book-page-id="0" title="封面">封面</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/readme.html" data-book-page-rel-url="readme.html" data-book-page-id="0" title="简介">简介</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_getting_started.html" title="起步" data-book-page-rel-url="docs/1.0/tut_getting_started.html" data-book-page-id="11555">起步</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/deep_learning_60min_blitz.html" title="PyTorch 深度学习: 60 分钟极速入门" data-book-page-rel-url="docs/1.0/deep_learning_60min_blitz.html" data-book-page-id="11556">PyTorch 深度学习: 60 分钟极速入门</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/blitz_tensor_tutorial.html" title="什么是 PyTorch？" data-book-page-rel-url="docs/1.0/blitz_tensor_tutorial.html" data-book-page-id="11557">什么是 PyTorch？</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/blitz_autograd_tutorial.html" title="Autograd：自动求导" data-book-page-rel-url="docs/1.0/blitz_autograd_tutorial.html" data-book-page-id="11558">Autograd：自动求导</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/blitz_neural_networks_tutorial.html" title="神经网络" data-book-page-rel-url="docs/1.0/blitz_neural_networks_tutorial.html" data-book-page-id="11559">神经网络</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/blitz_cifar10_tutorial.html" title="训练分类器" data-book-page-rel-url="docs/1.0/blitz_cifar10_tutorial.html" data-book-page-id="11560">训练分类器</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/blitz_data_parallel_tutorial.html" title="可选：数据并行处理" data-book-page-rel-url="docs/1.0/blitz_data_parallel_tutorial.html" data-book-page-id="11561">可选：数据并行处理</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/data_loading_tutorial.html" title="数据加载和处理教程" data-book-page-rel-url="docs/1.0/data_loading_tutorial.html" data-book-page-id="11562">数据加载和处理教程</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/pytorch_with_examples.html" title="用例子学习 PyTorch" data-book-page-rel-url="docs/1.0/pytorch_with_examples.html" data-book-page-id="11563">用例子学习 PyTorch</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/transfer_learning_tutorial.html" title="迁移学习教程" data-book-page-rel-url="docs/1.0/transfer_learning_tutorial.html" data-book-page-id="11564">迁移学习教程</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/deploy_seq2seq_hybrid_frontend_tutorial.html" title="混合前端的 seq2seq 模型部署" data-book-page-rel-url="docs/1.0/deploy_seq2seq_hybrid_frontend_tutorial.html" data-book-page-id="11565">混合前端的 seq2seq 模型部署</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/saving_loading_models.html" title="Saving and Loading Models" data-book-page-rel-url="docs/1.0/saving_loading_models.html" data-book-page-id="11566">Saving and Loading Models</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nn_tutorial.html" title="What is `torch.nn` _really_?" data-book-page-rel-url="docs/1.0/nn_tutorial.html" data-book-page-id="11567">What is `torch.nn` _really_?</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_image.html" title="图像" data-book-page-rel-url="docs/1.0/tut_image.html" data-book-page-id="11568">图像</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/finetuning_torchvision_models_tutorial.html" title="Torchvision 模型微调" data-book-page-rel-url="docs/1.0/finetuning_torchvision_models_tutorial.html" data-book-page-id="11569">Torchvision 模型微调</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/spatial_transformer_tutorial.html" title="空间变换器网络教程" data-book-page-rel-url="docs/1.0/spatial_transformer_tutorial.html" data-book-page-id="11570">空间变换器网络教程</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/neural_style_tutorial.html" title="使用 PyTorch 进行图像风格转换" data-book-page-rel-url="docs/1.0/neural_style_tutorial.html" data-book-page-id="11571">使用 PyTorch 进行图像风格转换</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/fgsm_tutorial.html" title="对抗性示例生成" data-book-page-rel-url="docs/1.0/fgsm_tutorial.html" data-book-page-id="11572">对抗性示例生成</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/super_resolution_with_caffe2.html" title="使用 ONNX 将模型从 PyTorch 传输到 Caffe2 和移动端" data-book-page-rel-url="docs/1.0/super_resolution_with_caffe2.html" data-book-page-id="11573">使用 ONNX 将模型从 PyTorch 传输到 Caffe2 和移动端</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_text.html" title="文本" data-book-page-rel-url="docs/1.0/tut_text.html" data-book-page-id="11574">文本</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/chatbot_tutorial.html" title="聊天机器人教程" data-book-page-rel-url="docs/1.0/chatbot_tutorial.html" data-book-page-id="11575">聊天机器人教程</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/char_rnn_generation_tutorial.html" title="使用字符级别特征的 RNN 网络生成姓氏" data-book-page-rel-url="docs/1.0/char_rnn_generation_tutorial.html" data-book-page-id="11576">使用字符级别特征的 RNN 网络生成姓氏</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/char_rnn_classification_tutorial.html" title="使用字符级别特征的 RNN 网络进行姓氏分类" data-book-page-rel-url="docs/1.0/char_rnn_classification_tutorial.html" data-book-page-id="11577">使用字符级别特征的 RNN 网络进行姓氏分类</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/deep_learning_nlp_tutorial.html" title="Deep Learning for NLP with Pytorch" data-book-page-rel-url="docs/1.0/deep_learning_nlp_tutorial.html" data-book-page-id="11578">Deep Learning for NLP with Pytorch</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nlp_pytorch_tutorial.html" title="PyTorch 介绍" data-book-page-rel-url="docs/1.0/nlp_pytorch_tutorial.html" data-book-page-id="11579">PyTorch 介绍</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nlp_deep_learning_tutorial.html" title="使用 PyTorch 进行深度学习" data-book-page-rel-url="docs/1.0/nlp_deep_learning_tutorial.html" data-book-page-id="11580">使用 PyTorch 进行深度学习</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nlp_word_embeddings_tutorial.html" title="Word Embeddings: Encoding Lexical Semantics" data-book-page-rel-url="docs/1.0/nlp_word_embeddings_tutorial.html" data-book-page-id="11581">Word Embeddings: Encoding Lexical Semantics</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nlp_sequence_models_tutorial.html" title="序列模型和 LSTM 网络" data-book-page-rel-url="docs/1.0/nlp_sequence_models_tutorial.html" data-book-page-id="11582">序列模型和 LSTM 网络</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nlp_advanced_tutorial.html" title="Advanced: Making Dynamic Decisions and the Bi-LSTM CRF" data-book-page-rel-url="docs/1.0/nlp_advanced_tutorial.html" data-book-page-id="11583">Advanced: Making Dynamic Decisions and the Bi-LSTM CRF</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/seq2seq_translation_tutorial.html" title="基于注意力机制的 seq2seq 神经网络翻译" data-book-page-rel-url="docs/1.0/seq2seq_translation_tutorial.html" data-book-page-id="11584">基于注意力机制的 seq2seq 神经网络翻译</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_generative.html" title="生成" data-book-page-rel-url="docs/1.0/tut_generative.html" data-book-page-id="11585">生成</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/dcgan_faces_tutorial.html" title="DCGAN Tutorial" data-book-page-rel-url="docs/1.0/dcgan_faces_tutorial.html" data-book-page-id="11586">DCGAN Tutorial</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_reinforcement_learning.html" title="强化学习" data-book-page-rel-url="docs/1.0/tut_reinforcement_learning.html" data-book-page-id="11587">强化学习</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/reinforcement_q_learning.html" title="Reinforcement Learning (DQN) Tutorial" data-book-page-rel-url="docs/1.0/reinforcement_q_learning.html" data-book-page-id="11588">Reinforcement Learning (DQN) Tutorial</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_extending_pytorch.html" title="扩展 PyTorch" data-book-page-rel-url="docs/1.0/tut_extending_pytorch.html" data-book-page-id="11589">扩展 PyTorch</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/numpy_extensions_tutorial.html" title="用 numpy 和 scipy 创建扩展" data-book-page-rel-url="docs/1.0/numpy_extensions_tutorial.html" data-book-page-id="11590">用 numpy 和 scipy 创建扩展</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/cpp_extension.html" title="Custom C++   and CUDA Extensions" data-book-page-rel-url="docs/1.0/cpp_extension.html" data-book-page-id="11591">Custom C++ and CUDA Extensions</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/torch_script_custom_ops.html" title="Extending TorchScript with Custom C++   Operators" data-book-page-rel-url="docs/1.0/torch_script_custom_ops.html" data-book-page-id="11592">Extending TorchScript with Custom C++ Operators</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_production_usage.html" title="生产性使用" data-book-page-rel-url="docs/1.0/tut_production_usage.html" data-book-page-id="11593">生产性使用</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/dist_tuto.html" title="Writing Distributed Applications with PyTorch" data-book-page-rel-url="docs/1.0/dist_tuto.html" data-book-page-id="11594">Writing Distributed Applications with PyTorch</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/aws_distributed_training_tutorial.html" title="使用 Amazon AWS 进行分布式训练" data-book-page-rel-url="docs/1.0/aws_distributed_training_tutorial.html" data-book-page-id="11595">使用 Amazon AWS 进行分布式训练</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/ONNXLive.html" title="ONNX 现场演示教程" data-book-page-rel-url="docs/1.0/ONNXLive.html" data-book-page-id="11596">ONNX 现场演示教程</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/cpp_export.html" title="在 C++ 中加载 PYTORCH 模型" data-book-page-rel-url="docs/1.0/cpp_export.html" data-book-page-id="11597">在 C++ 中加载 PYTORCH 模型</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_other_language.html" title="其它语言中的 PyTorch" data-book-page-rel-url="docs/1.0/tut_other_language.html" data-book-page-id="11598">其它语言中的 PyTorch</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/cpp_frontend.html" title="使用 PyTorch C++ 前端" data-book-page-rel-url="docs/1.0/cpp_frontend.html" data-book-page-id="11599">使用 PyTorch C++ 前端</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/docs_notes.html" title="注解" data-book-page-rel-url="docs/1.0/docs_notes.html" data-book-page-id="11600">注解</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_autograd.html" title="自动求导机制" data-book-page-rel-url="docs/1.0/notes_autograd.html" data-book-page-id="11601">自动求导机制</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_broadcasting.html" title="广播语义" data-book-page-rel-url="docs/1.0/notes_broadcasting.html" data-book-page-id="11602">广播语义</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_cuda.html" title="CUDA 语义" data-book-page-rel-url="docs/1.0/notes_cuda.html" data-book-page-id="11603">CUDA 语义</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_extending.html" title="Extending PyTorch" data-book-page-rel-url="docs/1.0/notes_extending.html" data-book-page-id="11604">Extending PyTorch</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_faq.html" title="Frequently Asked Questions" data-book-page-rel-url="docs/1.0/notes_faq.html" data-book-page-id="11605">Frequently Asked Questions</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_multiprocessing.html" title="Multiprocessing best practices" data-book-page-rel-url="docs/1.0/notes_multiprocessing.html" data-book-page-id="11606">Multiprocessing best practices</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_randomness.html" title="Reproducibility" data-book-page-rel-url="docs/1.0/notes_randomness.html" data-book-page-id="11607">Reproducibility</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_serialization.html" title="Serialization semantics" data-book-page-rel-url="docs/1.0/notes_serialization.html" data-book-page-id="11608">Serialization semantics</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_windows.html" title="Windows FAQ" data-book-page-rel-url="docs/1.0/notes_windows.html" data-book-page-id="11609">Windows FAQ</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/docs_package_ref.html" title="包参考" data-book-page-rel-url="docs/1.0/docs_package_ref.html" data-book-page-id="11610">包参考</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/torch.html" title="torch" data-book-page-rel-url="docs/1.0/torch.html" data-book-page-id="11611">torch</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tensors.html" title="torch.Tensor" data-book-page-rel-url="docs/1.0/tensors.html" data-book-page-id="11612">torch.Tensor</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tensor_attributes.html" title="Tensor Attributes" data-book-page-rel-url="docs/1.0/tensor_attributes.html" data-book-page-id="11613">Tensor Attributes</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/type_info.html" title="数据类型信息" data-book-page-rel-url="docs/1.0/type_info.html" data-book-page-id="11614">数据类型信息</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/sparse.html" title="torch.sparse" data-book-page-rel-url="docs/1.0/sparse.html" data-book-page-id="11615">torch.sparse</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/cuda.html" title="torch.cuda" data-book-page-rel-url="docs/1.0/cuda.html" data-book-page-id="11616">torch.cuda</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/storage.html" title="torch.Storage" data-book-page-rel-url="docs/1.0/storage.html" data-book-page-id="11617">torch.Storage</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nn.html" title="torch.nn" data-book-page-rel-url="docs/1.0/nn.html" data-book-page-id="11618">torch.nn</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nn_functional.html" title="torch.nn.functional" data-book-page-rel-url="docs/1.0/nn_functional.html" data-book-page-id="11619">torch.nn.functional</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nn_init.html" title="torch.nn.init" data-book-page-rel-url="docs/1.0/nn_init.html" data-book-page-id="11620">torch.nn.init</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/optim.html" title="torch.optim" data-book-page-rel-url="docs/1.0/optim.html" data-book-page-id="11621">torch.optim</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/autograd.html" title="Automatic differentiation package - torch.autograd" data-book-page-rel-url="docs/1.0/autograd.html" data-book-page-id="11622">Automatic differentiation package - torch.autograd</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/distributed.html" title="Distributed communication package - torch.distributed" data-book-page-rel-url="docs/1.0/distributed.html" data-book-page-id="11623">Distributed communication package - torch.distributed</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/distributions.html" title="Probability distributions - torch.distributions" data-book-page-rel-url="docs/1.0/distributions.html" data-book-page-id="11624">Probability distributions - torch.distributions</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/jit.html" title="Torch Script" data-book-page-rel-url="docs/1.0/jit.html" data-book-page-id="11625">Torch Script</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/multiprocessing.html" title="多进程包 - torch.multiprocessing" data-book-page-rel-url="docs/1.0/multiprocessing.html" data-book-page-id="11626">多进程包 - torch.multiprocessing</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/bottleneck.html" title="torch.utils.bottleneck" data-book-page-rel-url="docs/1.0/bottleneck.html" data-book-page-id="11627">torch.utils.bottleneck</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/checkpoint.html" title="torch.utils.checkpoint" data-book-page-rel-url="docs/1.0/checkpoint.html" data-book-page-id="11628">torch.utils.checkpoint</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/docs_cpp_extension.html" title="torch.utils.cpp_extension" data-book-page-rel-url="docs/1.0/docs_cpp_extension.html" data-book-page-id="11629">torch.utils.cpp_extension</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/data.html" title="torch.utils.data" data-book-page-rel-url="docs/1.0/data.html" data-book-page-id="11630">torch.utils.data</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/dlpack.html" title="torch.utils.dlpack" data-book-page-rel-url="docs/1.0/dlpack.html" data-book-page-id="11631">torch.utils.dlpack</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/hub.html" title="torch.hub" data-book-page-rel-url="docs/1.0/hub.html" data-book-page-id="11632">torch.hub</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/model_zoo.html" title="torch.utils.model_zoo" data-book-page-rel-url="docs/1.0/model_zoo.html" data-book-page-id="11633">torch.utils.model_zoo</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/onnx.html" title="torch.onnx" data-book-page-rel-url="docs/1.0/onnx.html" data-book-page-id="11634">torch.onnx</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/distributed_deprecated.html" title="Distributed communication package (deprecated) - torch.distributed.deprecated" data-book-page-rel-url="docs/1.0/distributed_deprecated.html" data-book-page-id="11635">Distributed communication package (deprecated) - torch.distributed.deprecated</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/docs_torchvision_ref.html" title="torchvision 参考" data-book-page-rel-url="docs/1.0/docs_torchvision_ref.html" data-book-page-id="11636">torchvision 参考</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/torchvision_datasets.html" title="torchvision.datasets" data-book-page-rel-url="docs/1.0/torchvision_datasets.html" data-book-page-id="11637">torchvision.datasets</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/torchvision_models.html" title="torchvision.models" data-book-page-rel-url="docs/1.0/torchvision_models.html" data-book-page-id="11638">torchvision.models</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/torchvision_transforms.html" title="torchvision.transforms" data-book-page-rel-url="docs/1.0/torchvision_transforms.html" data-book-page-id="11639">torchvision.transforms</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/torchvision_utils.html" title="torchvision.utils" data-book-page-rel-url="docs/1.0/torchvision_utils.html" data-book-page-id="11640">torchvision.utils</a>
</li>
</ul>
</li>
</ul>
</div>
</div>
<script src="https://cdn.staticfile.net/jquery/1.12.4/jquery.min.js"></script>
<script type="text/javascript" src="../../../../static/components/uikit-2.27.5/js/uikit.reader.js"></script>
<script type="text/javascript" src="../../../../static/components/social-share/social-share.min.js"></script>
<script>(function(){var bp =document.createElement('script');var curProtocol =window.location.protocol.split(':')[0];if (curProtocol ==='https') {bp.src ='https://zz.bdstatic.com/linksubmit/push.js';}
else {bp.src ='http://push.zhanzhang.baidu.com/push.js';}
var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(bp,s);})();</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
<script src="https://cdn.staticfile.net/highlight.js/9.12.0/highlight.min.js"></script>
<script src="https://cdn.staticfile.net/jquery.pjax/2.0.1/jquery.pjax.min.js"></script>
<script src="https://cdn.staticfile.net/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="https://cdn.staticfile.net/uikit/2.27.5/js/components/lightbox.min.js"></script>
<link rel="dns-prefetch" href="../../../..//cdn.mathjax.org" />
<script type="text/x-mathjax-config">
 function initMathJax() {
    var mathId = $("book-content-section")[0];
    MathJax.Hub.Config({
        tex2jax: {skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code','a']},
        showProcessingMessages: false,
        messageStyle: "none"
    });
    MathJax.Hub.Queue(["Typeset",MathJax.Hub,mathId]);
 };
initMathJax();
</script>
<script src='https://cdn.staticfile.net/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML' async></script>
<style>
	.MathJax_Display{display:inline!important;}
</style>
<script type="text/javascript" src="../../../../static/components/js/reader.js"></script>
<script type="text/javascript">var bookId =169;var bookPageId =11623;var bookPageRelUrl ='docs/1.0/distributed.html';</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
</body>
</html>