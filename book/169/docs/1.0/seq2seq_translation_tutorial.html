
<!DOCTYPE html>
<html lang="zh-CN">
<head>
<title>基于注意力机制的 seq2seq 神经网络翻译-PyTorch 1.0 中文文档 & 教程</title>
<meta content='基于注意力机制的 seq2seq 神经网络翻译,PyTorch 1.0 中文文档 & 教程' name='keywords'>
<meta content='基于注意力机制的 seq2seq 神经网络翻译,PyTorch 1.0 中文文档 & 教程' name='description'>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta http-equiv="Content-Language" content="zh-CN" />
<meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1, maximum-scale=1, user-scalable=no"../../../../>
<meta name="applicable-device" content="pc,mobile">
<link rel="shortcut icon" href="../../../../favicon.ico" type="image/x-icon" />
<meta name="renderer" content="webkit">
<link rel="stylesheet" href="../../../../static/components/uikit-2.27.5/css/uikit.custom.css">
<link rel="stylesheet" href="../../../../static/components/social-share/social-share.min.css">
<link rel="stylesheet" href="../../../../static/components/highlight/styles/custom.css">
<link rel="stylesheet" href="../../../../static/components/css/base.css">
<link rel="stylesheet" href="../../../../static/components/css/reader.css">
<link rel="stylesheet" href="../../../../static/components/css/markdown.css">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5313208362165053" crossorigin="anonymous"></script>
</head>
<body>
<div class=" book-main-wrap uk-container uk-container-center uk-margin-top ">
<div class="uk-grid">
<div class="uk-width-1-1 reader-wrap ">
<div class=" bottom-nav uk-clearfix ">
<div class="uk-align-left ">
<a href="../../../../book/169/docs/1.0/nlp_advanced_tutorial.html">
<i class="nav-icon-left uk-icon-small  uk-icon-caret-left"></i>
<span class="">Advanced: M..</span>
</a>
</div>
<div class="uk-align-right ">
<a href="../../../../book/169/docs/1.0/tut_generative.html">
<span class="">生成</span>
<i class="nav-icon-right uk-icon-small  uk-icon-caret-right"></i>
</a>
</div>
</div>
<div class="uk-text-center">
<h2 class="book-page-title uk-container-center">
<a href="../../../../book/169/index.html">PyTorch 1.0 中文文档 & 教程</a>
<a target="_blank" rel="nofollow" href="https://github.com/apachecn/pytorch-doc-zh" class="uk-icon-button uk-icon-github" title="github项目地址"></a>
</h2>
</div>
<script type="text/javascript" src="../../../../static/components/js/app_intro.js"></script>
<ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-5313208362165053" data-ad-slot="1328047120"></ins>
<script>(adsbygoogle =window.adsbygoogle ||[]).push({});</script>
<hr class="uk-article-divider">
<div class="book-content-section  md-content-section  uk-margin-bottom">
<h1 id="基于注意力机制的-seq2seq-神经网络翻译">基于注意力机制的 seq2seq 神经网络翻译</h1>
<blockquote>
<p>译者：<a href="https://github.com/mengfu188">mengfu188</a></p>
<p>校对者：<a href="https://github.com/Zhiyu-Chen">Zhiyu-Chen</a></p>
</blockquote>
<p><strong>作者</strong>: <a href="https://github.com/spro/practical-pytorch">Sean Robertson</a></p>
<p>在这个项目中，我们将教一个把把法语翻译成英语的神经网络。</p>
<pre><code class="language-py">[KEY: &gt; input, = target, &lt; output]

&gt; il est en train de peindre un tableau .
= he is painting a picture .
&lt; he is painting a picture .

&gt; pourquoi ne pas essayer ce vin delicieux ?
= why not try that delicious wine ?
&lt; why not try that delicious wine ?

&gt; elle n est pas poete mais romanciere .
= she is not a poet but a novelist .
&lt; she not not a poet but a novelist .

&gt; vous etes trop maigre .
= you re too skinny .
&lt; you re all alone .

</code></pre>
<p>… 取得了不同程度的成功</p>
<p>这是通过<a href="https://arxiv.org/abs/1409.3215">seq2seq网络</a>来进行实现的，在这个网络中使用两个递归的神经网络（编码器网络和解码器网络）一起工作使得一段序列变成令一段序列。 编码器网络将输入序列变成一个向量，解码器网络将该向量展开为新的序列。</p>
<p><a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/b01274082109b1019682274a0d4ca4d8.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/b01274082109b1019682274a0d4ca4d8.jpg" alt=""></a></p>
<p>我们将使用<a href="https://arxiv.org/abs/1409.0473">注意力机制</a>改进这个模型，它可以让解码器学会集中在输入序列的特定范围中。</p>
<p><strong>推荐阅读：</strong></p>
<p>我假设你至少已经了解Python，安装了PyTorch，并且了解什么是张量：</p>
<ul>
<li><a href="https://pytorch.org/">https://pytorch.org/</a> PyTorch安装说明</li>
<li><a href="../beginner/deep_learning_60min_blitz.html">PyTorch 深度学习: 60 分钟极速入门教程</a> 开始使用PyTorch</li>
<li><a href="../beginner/pytorch_with_examples.html">跟着例子学习PyTorch</a> 更加广泛而深入的了解PyTorch</li>
<li><a href="../beginner/former_torchies_tutorial.html">PyTorch for Former Torch Users</a> 如果你是Lua Torch用户</li>
</ul>
<p>这些内容有利于了解seq2seq网络和工作机制：</p>
<ul>
<li><a href="https://arxiv.org/abs/1406.1078">用RNN编码器 - 解码器来学习用于统计机器翻译的短语表示</a></li>
<li><a href="https://arxiv.org/abs/1409.3215">用神经网络进行seq2seq学习</a></li>
<li><a href="https://arxiv.org/abs/1409.0473">通过共同学习对齐和翻译的神经机器翻译</a></li>
<li><a href="https://arxiv.org/abs/1506.05869">神经会话模型</a></li>
</ul>
<p>你还可以找到以前类似于编码器和解码器的教程，如<a href="char_rnn_classification_tutorial.html">用字符集RNN分类名称</a> 和 [用字符集RNN生成名称]](char_rnn_generation_tutorial.html)，学习这些概念比较有帮助。</p>
<p>更多内容请阅读以下论文：</p>
<ul>
<li><a href="https://arxiv.org/abs/1406.1078">用RNN编码器 - 解码器来学习用于统计机器翻译的短语表示</a></li>
<li><a href="https://arxiv.org/abs/1409.3215">用神经网络进行seq2seq学习</a></li>
<li><a href="https://arxiv.org/abs/1409.0473">通过共同学习对齐和翻译的神经机器翻译</a></li>
<li><a href="https://arxiv.org/abs/1506.05869">神经会话模型</a></li>
</ul>
<p><strong>需要如下</strong></p>
<pre><code class="language-py">from __future__ import unicode_literals, print_function, division
from io import open
import unicodedata
import string
import re
import random

import torch
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

</code></pre>
<h2 id="加载数据文件">加载数据文件</h2>
<p>这个项目的数据是一组数以千计的英语到法语的翻译用例</p>
<p><a href="https://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages">这个问题在 Open Data Stack Exchange 上</a> 点我打开翻译网址 <a href="https://tatoeba.org/">https://tatoeba.org/</a> 这个网站的下载地址 <a href="https://tatoeba.org/eng/downloads">https://tatoeba.org/eng/downloads</a> - 更棒的是，有人将这些语言切分成单个文件: <a href="https://www.manythings.org/anki/">https://www.manythings.org/anki/</a></p>
<p>由于翻译文件太大而不能放到repo中，请继续往下前下载数据到 <code>data/eng-fra.txt</code> 。该文件是一个使用制表符分割的翻译列表:</p>
<pre><code class="language-py">I am cold.    J'ai froid.

</code></pre>
<p>注意</p>
<p>从 <a href="https://download.pytorch.org/tutorial/data.zip">这里</a> 下载数据和解压到相关的路径.</p>
<p>与character-level RNN教程中使用的字符编码类似,我们将用语言中的每个单词 作为独热向量,或者除了单个单词之外(在单词的索引处)的大的零向量. 相较于可能 存在于一种语言中仅有十个字符相比,多数都是有大量的字,因此编码向量很大. 然而,我们会欺骗性的做一些数据修剪,保证每种语言只使用几千字.</p>
<p><a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/7fa129004e942671707f8f2d4fb80a20.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/7fa129004e942671707f8f2d4fb80a20.jpg" alt=""></a></p>
<p>我们需要每个单词对应唯一的索引作为稍后的网络输入和目标.为了追踪这些索引我们使用一个帮助类 <code>Lang</code> 类中有 词 → 索引 (<code>word2index</code>) 和 索引 → 词(<code>index2word</code>) 的字典, 以及每个词<code>word2count</code> 用来替换稀疏词汇.</p>
<pre><code class="language-py">SOS_token = 0
EOS_token = 1

class Lang:
    def __init__(self, name):
        self.name = name
        self.word2index = {}
        self.word2count = {}
        self.index2word = {0: "SOS", 1: "EOS"}
        self.n_words = 2  # Count SOS and EOS

    def addSentence(self, sentence):
        for word in sentence.split(' '):
            self.addWord(word)

    def addWord(self, word):
        if word not in self.word2index:
            self.word2index[word] = self.n_words
            self.word2count[word] = 1
            self.index2word[self.n_words] = word
            self.n_words += 1
        else:
            self.word2count[word] += 1

</code></pre>
<p>这些文件全部采用Unicode编码，为了简化起见，我们将Unicode字符转换成ASCII编码、所有内容小写、并修剪大部分标点符号。</p>
<pre><code class="language-py"># 感谢您将Unicode字符转换成ASCII
# https://stackoverflow.com/a/518232/2809427
def unicodeToAscii(s):
    return ''.join(
        c for c in unicodedata.normalize('NFD', s)
        if unicodedata.category(c) != 'Mn'
    )

# 小写，修剪和删除非字符字符

def normalizeString(s):
    s = unicodeToAscii(s.lower().strip())
    s = re.sub(r"([.!?])", r" \1", s)
    s = re.sub(r"[^a-zA-Z.!?]+", r" ", s)
    return s

</code></pre>
<p>我们将按行分开并将每一行分成两列来读取文件。这些文件都是英语 -&gt; 其他语言，所以如果我们想从其他语言翻译 -&gt; 英语德华，添加<code>reverse</code>标志来翻转词语对。</p>
<pre><code class="language-py">def readLangs(lang1, lang2, reverse=False):
    print("Reading lines...")

    # Read the file and split into lines
    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\
        read().strip().split('\n')

    # Split every line into pairs and normalize
    pairs = [[normalizeString(s) for s in l.split('\t')] for l in lines]

    # Reverse pairs, make Lang instances
    if reverse:
        pairs = [list(reversed(p)) for p in pairs]
        input_lang = Lang(lang2)
        output_lang = Lang(lang1)
    else:
        input_lang = Lang(lang1)
        output_lang = Lang(lang2)

    return input_lang, output_lang, pairs

</code></pre>
<p>Since there are a <em>lot</em> of example sentences and we want to train something quickly, we’ll trim the data set to only relatively short and simple sentences. Here the maximum length is 10 words (that includes ending punctuation) and we’re filtering to sentences that translate to the form “I am” or “He is” etc. (accounting for apostrophes replaced earlier).</p>
<p>由于有很多例句，我们希望快速的训练一些东西，就将数据集裁剪为相对剪短的句子。这里最大的长度是10个单词（包括标点符号），我们过滤翻译成“I am”或“he is”等形式的句子。（考虑到先前齐欢乐撇号）。</p>
<pre><code class="language-py">MAX_LENGTH = 10

eng_prefixes = (
    "i am ", "i m ",
    "he is", "he s ",
    "she is", "she s ",
    "you are", "you re ",
    "we are", "we re ",
    "they are", "they re "
)

def filterPair(p):
    return len(p[0].split(' ')) &lt; MAX_LENGTH and \
        len(p[1].split(' ')) &lt; MAX_LENGTH and \
        p[1].startswith(eng_prefixes)

def filterPairs(pairs):
    return [pair for pair in pairs if filterPair(pair)]

</code></pre>
<p>完整的数据准备过程：</p>
<ul>
<li>读取文本文件，拆分成行，将行拆分成对</li>
<li>规范文本，按长度和内容过滤</li>
<li>从句子中成对列出单词列表</li>
</ul>
<pre><code class="language-py">def prepareData(lang1, lang2, reverse=False):
    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)
    print("Read %s sentence pairs" % len(pairs))
    pairs = filterPairs(pairs)
    print("Trimmed to %s sentence pairs" % len(pairs))
    print("Counting words...")
    for pair in pairs:
        input_lang.addSentence(pair[0])
        output_lang.addSentence(pair[1])
    print("Counted words:")
    print(input_lang.name, input_lang.n_words)
    print(output_lang.name, output_lang.n_words)
    return input_lang, output_lang, pairs

input_lang, output_lang, pairs = prepareData('eng', 'fra', True)
print(random.choice(pairs))

</code></pre>
<p>输出:</p>
<pre><code class="language-py">Reading lines...
Read 135842 sentence pairs
Trimmed to 10599 sentence pairs
Counting words...
Counted words:
fra 4345
eng 2803
['ils ne sont pas encore chez eux .', 'they re not home yet .']

</code></pre>
<h2 id="seq2seq模型">Seq2Seq模型</h2>
<p>递归神经网络（RNN）是一种对序列进行操作并利用自己的输出作为后序步骤的输入的网络</p>
<p><a href="https://arxiv.org/abs/1409.3215">序列到序列网络</a>, 也叫做 seq2seq 网络, 又或者是 <a href="https://arxiv.org/pdf/1406.1078v3.pdf">编码器解码器网络</a>, 是一个由两个称为编码器解码器的RNN组成的模型。编码器读取输入序列并输出一个矢量，解码器读取该矢量并产生输出序列。</p>
<p><a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/b01274082109b1019682274a0d4ca4d8.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/b01274082109b1019682274a0d4ca4d8.jpg" alt=""></a></p>
<p>与每个输入对应一个输出的单个RNN的序列预测不同，seq2seq模型将我们从序列长度和顺序中解放出来，这使得它更适合两种语言的转换。</p>
<p>考虑这句话“Je ne suis pas le chat noir” → “I am not the black cat”.大多数在输入序列中的单词在输出序列中有直接的翻译，但是在顺序中略有不同，例如: “chat noir” 和 “black cat”.由于 “ne/pas”结构, 其中另一个单词在输入的句子中. 直接从输入词的序列中直接生成正确的翻译是很困难的.</p>
<p>使用seq2seq模型，编码器会创建一个向量，在理想的情况下，将输入序列的“含义”转为单个向量 - 某个N维句子空间中的一个点。</p>
<h3 id="编码器">编码器</h3>
<p>The encoder of a seq2seq network is a RNN that outputs some value for every word from the input sentence. For every input word the encoder outputs a vector and a hidden state, and uses the hidden state for the next input word.</p>
<p>在seq2seq网络中，编码器是一个会为输入句子中的每个单词输出一些值的RNN网络。对每个输入的单词编码器会输出一个向量和一个隐藏状态，并将该状态作为下一个输入。</p>
<p><a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/9b7e299515676cf41cd2c0fd6ab1295d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/9b7e299515676cf41cd2c0fd6ab1295d.jpg" alt=""></a></p>
<pre><code class="language-py">class EncoderRNN(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(EncoderRNN, self).__init__()
        self.hidden_size = hidden_size

        self.embedding = nn.Embedding(input_size, hidden_size)
        self.gru = nn.GRU(hidden_size, hidden_size)

    def forward(self, input, hidden):
        embedded = self.embedding(input).view(1, 1, -1)
        output = embedded
        output, hidden = self.gru(output, hidden)
        return output, hidden

    def initHidden(self):
        return torch.zeros(1, 1, self.hidden_size, device=device)

</code></pre>
<h3 id="解码器">解码器</h3>
<p>解码器是一个接受编码器输出向量并输出一系列单词以创建翻译的RNN。</p>
<h4 id="简单的编码器">简单的编码器</h4>
<p>在最简单的seq2seq解码器中，我们只使用编码器的最后输出。这最后一个输出有时称为上下文向量因为它从整个序列中编码上下文。该上下文向量用作解码器的初始隐藏状态。</p>
<p>在解码的每一步,解码器都被赋予一个输入指令和隐藏状态. 初始输入指令字符串开始的<code>&lt;SOS&gt;</code>指令,第一个隐藏状态是上下文向量(编码器的最后隐藏状态).</p>
<p><a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/34b376e0c7299810f7349ab99c2c5497.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/34b376e0c7299810f7349ab99c2c5497.jpg" alt=""></a></p>
<pre><code class="language-py">class DecoderRNN(nn.Module):
    def __init__(self, hidden_size, output_size):
        super(DecoderRNN, self).__init__()
        self.hidden_size = hidden_size

        self.embedding = nn.Embedding(output_size, hidden_size)
        self.gru = nn.GRU(hidden_size, hidden_size)
        self.out = nn.Linear(hidden_size, output_size)
        self.softmax = nn.LogSoftmax(dim=1)

    def forward(self, input, hidden):
        output = self.embedding(input).view(1, 1, -1)
        output = F.relu(output)
        output, hidden = self.gru(output, hidden)
        output = self.softmax(self.out(output[0]))
        return output, hidden

    def initHidden(self):
        return torch.zeros(1, 1, self.hidden_size, device=device)

</code></pre>
<p>我们鼓励你训练和观察这个模型的结果,但为了节省空间,我们将直接进正题引入注意力机制.</p>
<h4 id="注意力解码器">注意力解码器</h4>
<p>如果仅在编码器和解码器之间传递上下文向量,则该单个向量承担编码整个句子的负担.</p>
<p>注意力允许解码器网络针对解码器自身输出的每一步”聚焦”编码器输出的不同部分. 首先我们计算一组注意力权重. 这些将被乘以编码器输出矢量获得加权的组合. 结果(在代码中为<code>attn_applied</code>) 应该包含关于输入序列的特定部分的信息, 从而帮助解码器选择正确的输出单词.</p>
<p><a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/3313f4800c7d01049e2a2ef2079e5905.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/3313f4800c7d01049e2a2ef2079e5905.jpg" alt=""></a></p>
<p>注意权值的计算是用另一个前馈层<code>attn</code>进行的, 将解码器的输入和隐藏层状态作为输入. 由于训练数据中有各种大小的句子,为了实际创建和训练此层, 我们必须选择最大长度的句子(输入长度,用于编码器输出),以适用于此层. 最大长度的句子将使用所有注意力权重,而较短的句子只使用前几个.</p>
<p><a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/32ec68a6e0d29efae32b0f50db877598.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/32ec68a6e0d29efae32b0f50db877598.jpg" alt=""></a></p>
<pre><code class="language-py">class AttnDecoderRNN(nn.Module):
    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):
        super(AttnDecoderRNN, self).__init__()
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.dropout_p = dropout_p
        self.max_length = max_length

        self.embedding = nn.Embedding(self.output_size, self.hidden_size)
        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)
        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)
        self.dropout = nn.Dropout(self.dropout_p)
        self.gru = nn.GRU(self.hidden_size, self.hidden_size)
        self.out = nn.Linear(self.hidden_size, self.output_size)

    def forward(self, input, hidden, encoder_outputs):
        embedded = self.embedding(input).view(1, 1, -1)
        embedded = self.dropout(embedded)

        attn_weights = F.softmax(
            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)
        attn_applied = torch.bmm(attn_weights.unsqueeze(0),
                                 encoder_outputs.unsqueeze(0))

        output = torch.cat((embedded[0], attn_applied[0]), 1)
        output = self.attn_combine(output).unsqueeze(0)

        output = F.relu(output)
        output, hidden = self.gru(output, hidden)

        output = F.log_softmax(self.out(output[0]), dim=1)
        return output, hidden, attn_weights

    def initHidden(self):
        return torch.zeros(1, 1, self.hidden_size, device=device)

</code></pre>
<p>注意</p>
<p>还有其他形式的注意力通过使用相对位置方法来解决长度限制. 阅读关于 “local attention” 在 <a href="https://arxiv.org/abs/1508.04025">基于注意力的神经机器翻译的有效途径</a>.</p>
<h2 id="训练">训练</h2>
<h3 id="准备训练数据">准备训练数据</h3>
<p>为了训练,对于每一对我们将需要输入的张量(输入句子中的词的索引)和 目标张量(目标语句中的词的索引). 在创建这些向量时,我们会将EOS标记添加到两个序列中.</p>
<pre><code class="language-py">def indexesFromSentence(lang, sentence):
    return [lang.word2index[word] for word in sentence.split(' ')]

def tensorFromSentence(lang, sentence):
    indexes = indexesFromSentence(lang, sentence)
    indexes.append(EOS_token)
    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)

def tensorsFromPair(pair):
    input_tensor = tensorFromSentence(input_lang, pair[0])
    target_tensor = tensorFromSentence(output_lang, pair[1])
    return (input_tensor, target_tensor)

</code></pre>
<h3 id="训练模型">训练模型</h3>
<p>为了训练我们通过编码器运行输入句子,并跟踪每个输出和最新的隐藏状态. 然后解码器被赋予<code>&lt;SOS&gt;</code> 标志作为其第一个输入, 并将编码器的最后一个隐藏状态作为其第一个隐藏状态.</p>
<p>“Teacher forcing” 是将实际目标输出用作每个下一个输入的概念,而不是将解码器的 猜测用作下一个输入.使用教师强迫会使其更快地收敛,但是 <a href="http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf">当训练好的网络被利用时,它可能表现出不稳定性.</a>.</p>
<p>你可以观察到教师的输出-用连贯的语法阅读，却远离正确的翻译-直观地说，它已经学会了表示输出语法，并且一旦老师告诉它前几个单词，它就可以“拾起”意义，但它一开始就没有学会如何从翻译中创造句子。</p>
<p>由于PyTorch的autograd给我们的自由,我们可以随意选择使用老师强制或不使用简单的if语句. 打开<code>teacher_forcing_ratio</code>来使用它.</p>
<pre><code class="language-py">teacher_forcing_ratio = 0.5

def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):
    encoder_hidden = encoder.initHidden()

    encoder_optimizer.zero_grad()
    decoder_optimizer.zero_grad()

    input_length = input_tensor.size(0)
    target_length = target_tensor.size(0)

    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)

    loss = 0

    for ei in range(input_length):
        encoder_output, encoder_hidden = encoder(
            input_tensor[ei], encoder_hidden)
        encoder_outputs[ei] = encoder_output[0, 0]

    decoder_input = torch.tensor([[SOS_token]], device=device)

    decoder_hidden = encoder_hidden

    use_teacher_forcing = True if random.random() &lt; teacher_forcing_ratio else False

    if use_teacher_forcing:
        # Teacher forcing: Feed the target as the next input
        for di in range(target_length):
            decoder_output, decoder_hidden, decoder_attention = decoder(
                decoder_input, decoder_hidden, encoder_outputs)
            loss += criterion(decoder_output, target_tensor[di])
            decoder_input = target_tensor[di]  # Teacher forcing

    else:
        # Without teacher forcing: use its own predictions as the next input
        for di in range(target_length):
            decoder_output, decoder_hidden, decoder_attention = decoder(
                decoder_input, decoder_hidden, encoder_outputs)
            topv, topi = decoder_output.topk(1)
            decoder_input = topi.squeeze().detach()  # detach from history as input

            loss += criterion(decoder_output, target_tensor[di])
            if decoder_input.item() == EOS_token:
                break

    loss.backward()

    encoder_optimizer.step()
    decoder_optimizer.step()

    return loss.item() / target_length

</code></pre>
<p>这是一个帮助函数，用于在给定当前时间和进度%的情况下打印经过的时间和估计的剩余时间。</p>
<pre><code class="language-py">import time
import math

def asMinutes(s):
    m = math.floor(s / 60)
    s -= m * 60
    return '%dm %ds' % (m, s)

def timeSince(since, percent):
    now = time.time()
    s = now - since
    es = s / (percent)
    rs = es - s
    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))

</code></pre>
<p>整个训练过程如下所示:</p>
<ul>
<li>启动计时器</li>
<li>初始化优化器和准则</li>
<li>创建一组训练队</li>
<li>为进行绘图启动空损失数组</li>
</ul>
<p>之后我们多次调用<code>train</code>函数，偶尔打印进度 (样本的百分比，到目前为止的时间，狙击的时间) 和平均损失</p>
<pre><code class="language-py">def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):
    start = time.time()
    plot_losses = []
    print_loss_total = 0  # Reset every print_every
    plot_loss_total = 0  # Reset every plot_every

    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)
    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)
    training_pairs = [tensorsFromPair(random.choice(pairs))
                      for i in range(n_iters)]
    criterion = nn.NLLLoss()

    for iter in range(1, n_iters + 1):
        training_pair = training_pairs[iter - 1]
        input_tensor = training_pair[0]
        target_tensor = training_pair[1]

        loss = train(input_tensor, target_tensor, encoder,
                     decoder, encoder_optimizer, decoder_optimizer, criterion)
        print_loss_total += loss
        plot_loss_total += loss

        if iter % print_every == 0:
            print_loss_avg = print_loss_total / print_every
            print_loss_total = 0
            print('%s (%d  %d%%) %.4f' % (timeSince(start, iter / n_iters),
                                         iter, iter / n_iters * 100, print_loss_avg))

        if iter % plot_every == 0:
            plot_loss_avg = plot_loss_total / plot_every
            plot_losses.append(plot_loss_avg)
            plot_loss_total = 0

    showPlot(plot_losses)

</code></pre>
<h3 id="绘制结果">绘制结果</h3>
<p>使用matplotlib完成绘图，使用<code>plot_losses</code>保存训练时的数组。</p>
<pre><code class="language-py">import matplotlib.pyplot as plt
plt.switch_backend('agg')
import matplotlib.ticker as ticker
import numpy as np

def showPlot(points):
    plt.figure()
    fig, ax = plt.subplots()
    # this locator puts ticks at regular intervals
    loc = ticker.MultipleLocator(base=0.2)
    ax.yaxis.set_major_locator(loc)
    plt.plot(points)

</code></pre>
<h2 id="评估">评估</h2>
<p>评估与训练大部分相同,但没有目标,因此我们只是将解码器的每一步预测反馈给它自身. 每当它预测到一个单词时,我们就会将它添加到输出字符串中,并且如果它预测到我们在那里停止的EOS指令. 我们还存储解码器的注意力输出以供稍后显示.</p>
<pre><code class="language-py">def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):
    with torch.no_grad():
        input_tensor = tensorFromSentence(input_lang, sentence)
        input_length = input_tensor.size()[0]
        encoder_hidden = encoder.initHidden()

        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)

        for ei in range(input_length):
            encoder_output, encoder_hidden = encoder(input_tensor[ei],
                                                     encoder_hidden)
            encoder_outputs[ei] += encoder_output[0, 0]

        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS

        decoder_hidden = encoder_hidden

        decoded_words = []
        decoder_attentions = torch.zeros(max_length, max_length)

        for di in range(max_length):
            decoder_output, decoder_hidden, decoder_attention = decoder(
                decoder_input, decoder_hidden, encoder_outputs)
            decoder_attentions[di] = decoder_attention.data
            topv, topi = decoder_output.data.topk(1)
            if topi.item() == EOS_token:
                decoded_words.append('&lt;EOS&gt;')
                break
            else:
                decoded_words.append(output_lang.index2word[topi.item()])

            decoder_input = topi.squeeze().detach()

        return decoded_words, decoder_attentions[:di + 1]

</code></pre>
<p>我们可以从训练集中对随机句子进行评估，并打印出输入、目标和输出，从而做出一些主观的质量判断：</p>
<pre><code class="language-py">def evaluateRandomly(encoder, decoder, n=10):
    for i in range(n):
        pair = random.choice(pairs)
        print('&gt;', pair[0])
        print('=', pair[1])
        output_words, attentions = evaluate(encoder, decoder, pair[0])
        output_sentence = ' '.join(output_words)
        print('&lt;', output_sentence)
        print('')

</code></pre>
<h2 id="训练和评估">训练和评估</h2>
<p>有了所有这些帮助函数(它看起来像是额外的工作，但它使运行多个实验更容易)，我们实际上可以初始化一个网络并开始训练。</p>
<p>请记住输入句子被严重过滤, 对于这个小数据集,我们可以使用包含256个隐藏节点 和单个GRU层的相对较小的网络.在MacBook CPU上约40分钟后,我们会得到一些合理的结果.</p>
<p>注</p>
<p>如果你运行这个笔记本，你可以训练，中断内核，评估，并在以后继续训练。 注释编码器和解码器初始化的行并再次运行 <code>trainIters</code> .</p>
<pre><code class="language-py">hidden_size = 256
encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)
attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)

trainIters(encoder1, attn_decoder1, 75000, print_every=5000)

</code></pre>
<ul>
<li><a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/5015200eb4481feb8a71a658b384ec39.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/5015200eb4481feb8a71a658b384ec39.jpg" alt="https://pytorch.org/tutorials/_images/sphx_glr_seq2seq_translation_tutorial_001.png"></a></li>
<li><a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/89adff7333b116436cf785388029ba1a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/89adff7333b116436cf785388029ba1a.jpg" alt="https://pytorch.org/tutorials/_images/sphx_glr_seq2seq_translation_tutorial_002.png"></a></li>
</ul>
<p>输出:</p>
<pre><code class="language-py">1m 47s (- 25m 8s) (5000 6%) 2.8641
3m 30s (- 22m 45s) (10000 13%) 2.2666
5m 15s (- 21m 1s) (15000 20%) 1.9537
7m 0s (- 19m 17s) (20000 26%) 1.7170
8m 46s (- 17m 32s) (25000 33%) 1.5182
10m 31s (- 15m 46s) (30000 40%) 1.3280
12m 15s (- 14m 0s) (35000 46%) 1.2137
14m 1s (- 12m 16s) (40000 53%) 1.0843
15m 48s (- 10m 32s) (45000 60%) 0.9847
17m 34s (- 8m 47s) (50000 66%) 0.8515
19m 20s (- 7m 2s) (55000 73%) 0.7940
21m 6s (- 5m 16s) (60000 80%) 0.7189
22m 53s (- 3m 31s) (65000 86%) 0.6490
24m 41s (- 1m 45s) (70000 93%) 0.5954
26m 26s (- 0m 0s) (75000 100%) 0.5257

</code></pre>
<pre><code class="language-py">evaluateRandomly(encoder1, attn_decoder1)

</code></pre>
<p>输出:</p>
<pre><code class="language-py">&gt; nous sommes contents que tu sois la .
= we re glad you re here .
&lt; we re glad you re here . &lt;EOS&gt;

&gt; il est dependant a l heroine .
= he is a heroin addict .
&lt; he is in heroin heroin . &lt;EOS&gt;

&gt; nous sommes les meilleurs .
= we are the best .
&lt; we are the best . &lt;EOS&gt;

&gt; tu es puissant .
= you re powerful .
&lt; you re powerful . &lt;EOS&gt;

&gt; j ai peur des chauves souris .
= i m afraid of bats .
&lt; i m afraid of bats . &lt;EOS&gt;

&gt; tu es enseignant n est ce pas ?
= you re a teacher right ?
&lt; you re a teacher aren t you ? &lt;EOS&gt;

&gt; je suis pret a tout faire pour toi .
= i am ready to do anything for you .
&lt; i am ready to do anything for you . &lt;EOS&gt;

&gt; c est desormais un homme .
= he s a man now .
&lt; he is in an man . &lt;EOS&gt;

&gt; elle est une mere tres avisee .
= she s a very wise mother .
&lt; she s a very wise mother . &lt;EOS&gt;

&gt; je suis completement vanne .
= i m completely exhausted .
&lt; i m completely exhausted . &lt;EOS&gt;

</code></pre>
<h3 id="可视化注意力">可视化注意力</h3>
<p>注意力机制的一个有用的特性是其高度可解释的输出。由于它用于加权输入序列的特定编码器输出，因此我们可以想象，在每个时间步骤中，查看网络最集中的位置。</p>
<p>你可以简单地运行<code>plt.matshow(attentions)</code>来查看显示为矩阵的注意力输出，列为输入步骤，行位输出步骤。</p>
<pre><code class="language-py">output_words, attentions = evaluate(
    encoder1, attn_decoder1, "je suis trop froid .")
plt.matshow(attentions.numpy())

</code></pre>
<p><a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/5412faceb18bc6fa2823be3ae1bdfd8d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/5412faceb18bc6fa2823be3ae1bdfd8d.jpg" alt="https://pytorch.org/tutorials/_images/sphx_glr_seq2seq_translation_tutorial_003.png"></a></p>
<p>为了获得更好的观看体验,我们将额外添加轴和标签:</p>
<pre><code class="language-py">def showAttention(input_sentence, output_words, attentions):
    # Set up figure with colorbar
    fig = plt.figure()
    ax = fig.add_subplot(111)
    cax = ax.matshow(attentions.numpy(), cmap='bone')
    fig.colorbar(cax)

    # Set up axes
    ax.set_xticklabels([''] + input_sentence.split(' ') +
                       ['&lt;EOS&gt;'], rotation=90)
    ax.set_yticklabels([''] + output_words)

    # Show label at every tick
    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))
    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))

    plt.show()

def evaluateAndShowAttention(input_sentence):
    output_words, attentions = evaluate(
        encoder1, attn_decoder1, input_sentence)
    print('input =', input_sentence)
    print('output =', ' '.join(output_words))
    showAttention(input_sentence, output_words, attentions)

evaluateAndShowAttention("elle a cinq ans de moins que moi .")

evaluateAndShowAttention("elle est trop petit .")

evaluateAndShowAttention("je ne crains pas de mourir .")

evaluateAndShowAttention("c est un jeune directeur plein de talent .")

</code></pre>
<ul>
<li><a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/6e09db671ada03c61bdd1f32b6a7624b.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/6e09db671ada03c61bdd1f32b6a7624b.jpg" alt="https://pytorch.org/tutorials/_images/sphx_glr_seq2seq_translation_tutorial_004.png"></a></li>
<li><a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/08bcfed65e8ab03ac7f380e20d421434.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/08bcfed65e8ab03ac7f380e20d421434.jpg" alt="https://pytorch.org/tutorials/_images/sphx_glr_seq2seq_translation_tutorial_005.png"></a></li>
<li><a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/b1ba956974f3e844b0b0cea490cc1148.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/b1ba956974f3e844b0b0cea490cc1148.jpg" alt="https://pytorch.org/tutorials/_images/sphx_glr_seq2seq_translation_tutorial_006.png"></a></li>
<li><a href="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/1226e2e8b7633b50c2c45ce699d8f71f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/pytorch-doc-zh/docs/1.0/img/1226e2e8b7633b50c2c45ce699d8f71f.jpg" alt="https://pytorch.org/tutorials/_images/sphx_glr_seq2seq_translation_tutorial_007.png"></a></li>
</ul>
<p>输出:</p>
<pre><code class="language-py">input = elle a cinq ans de moins que moi .
output = she s five years younger than me . &lt;EOS&gt;
input = elle est trop petit .
output = she s too slow . &lt;EOS&gt;
input = je ne crains pas de mourir .
output = i m not scared to die . &lt;EOS&gt;
input = c est un jeune directeur plein de talent .
output = he s a talented young player . &lt;EOS&gt;

</code></pre>
<h2 id="练习题">练习题</h2>
<ul>
<li>尝试使用不同的数据集
<ul>
<li>另一种语言对</li>
<li>人 → 及其 (例如 IOT 命令)</li>
<li>聊天 → 响应</li>
<li>问题 → 回答</li>
</ul> </li>
<li>将嵌入替换为预先训练过的单词嵌入，例如word2vec或者GloVe</li>
<li>尝试用更多的层次，更多的隐藏单位，更多的句子。比较训练时间和结果。</li>
<li>如果使用一个翻译文件，其中成对有两个相同的短语(<code>I am test \t I am test</code>)，您可以将其用作自动编码器。试试这个：
<ul>
<li>训练为自动编码器</li>
<li>只保存编码器网络</li>
<li>训练一种新的翻译解码器</li>
</ul> </li>
</ul>
</div>
<hr class="uk-article-divider">
<div class="uk-block uk-block-muted uk-padding-top-remove uk-padding-bottom-remove uk-margin-large-top  book-recommend-wrap">
<div class="uk-margin-top uk-margin-bottom uk-margin-left uk-margin-right">
<div class="uk-margin uk-text-muted "><i class="uk-icon-outdent uk-icon-justify uk-margin-small-right"></i>书籍推荐</div>
<div class="books">
<ul class="uk-book-list">
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/161/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/161/index.html">关于python的面试题</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/92.html">jackfrued</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">271页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月26日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 33个">33</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/165/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/165/index.html">Python学习知识库</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/94.html">coco369</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">85页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月26日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 190个">190</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/179/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/179/index.html">30秒学会常用Python代码</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/99.html">kriadmin</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">1页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月26日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 2716个">2716</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/131/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/visualstudio_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/131/index.html">Office 365 开发入门指南</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/70.html">chenxizhang</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="visualstudio">visualstudio</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">51页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年8月3日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 98个">98</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/118/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/influxdb_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/118/index.html">Influxdb 简明手册 教程 入门 文档</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/62.html">tzivanmoe</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="influxdb">influxdb</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">18页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年7月1日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 0个">0</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/98/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/visualstudio_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/98/index.html">Microsoft Visual Studio Code 中文手册</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/60.html">likebeta</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="visualstudio">visualstudio</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">66页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月29日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 0个">0</span>
</div>
</div>
</div>
</li>
<hr>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
<nav class="tm-navbar uk-navbar uk-navbar-attached reader-nav">
<div class="uk-float-left uk-margin-small-top">
<a href="javascript:;" title="目录菜单" class="show-menu  uk-icon-hover  uk-icon-align-justify uk-margin-right"></a>
<div data-uk-dropdown="{mode:'click',pos:'bottom-left'}" class="font-setting-wrap">
<a class="uk-icon-hover uk-icon-font uk-margin-right" aria-label="字体设置" href="javascript:;"></a>
<div class="uk-dropdown dropdown-menu">
<div class="dropdown-caret"><span class="caret-outer"></span><span class="caret-inner"></span></div>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-reduce">小字</button>
<button class="uk-button-link button size-2 font-enlarge">大字</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-1 ">宋体</button>
<button class="uk-button-link button size-2 font-2 ">黑体</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-3 color-theme-sun "><i class="uk-icon-sun-o"></i>白天</button>
<button class="uk-button-link button size-3 color-theme-eye "><i class="uk-icon-eye"></i>护眼</button>
<button class="uk-button-link button size-3 color-theme-moon "><i class="uk-icon-moon-o"></i>夜晚</button></div>
</div>
</div>
<a class="logo uk-margin-right" href="../../../../" title="返回首页"><img class="" src="../../../../static/components/images/icon_32.png" /></a>
</div>
<div class="uk-navbar-flip  uk-hidden-small">
<div id="share-box"></div>
</div>
</nav>
<div id="menu-id" class="uk-offcanvas reader-offcanvas">
<div class="uk-offcanvas-bar">
<ul class="book-menu-bar uk-nav uk-nav-offcanvas" data-uk-nav>
<li>
<a href="../../../../book/169/index.html" data-book-page-rel-url="index.html" data-book-page-id="0" title="封面">封面</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/readme.html" data-book-page-rel-url="readme.html" data-book-page-id="0" title="简介">简介</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_getting_started.html" title="起步" data-book-page-rel-url="docs/1.0/tut_getting_started.html" data-book-page-id="11555">起步</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/deep_learning_60min_blitz.html" title="PyTorch 深度学习: 60 分钟极速入门" data-book-page-rel-url="docs/1.0/deep_learning_60min_blitz.html" data-book-page-id="11556">PyTorch 深度学习: 60 分钟极速入门</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/blitz_tensor_tutorial.html" title="什么是 PyTorch？" data-book-page-rel-url="docs/1.0/blitz_tensor_tutorial.html" data-book-page-id="11557">什么是 PyTorch？</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/blitz_autograd_tutorial.html" title="Autograd：自动求导" data-book-page-rel-url="docs/1.0/blitz_autograd_tutorial.html" data-book-page-id="11558">Autograd：自动求导</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/blitz_neural_networks_tutorial.html" title="神经网络" data-book-page-rel-url="docs/1.0/blitz_neural_networks_tutorial.html" data-book-page-id="11559">神经网络</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/blitz_cifar10_tutorial.html" title="训练分类器" data-book-page-rel-url="docs/1.0/blitz_cifar10_tutorial.html" data-book-page-id="11560">训练分类器</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/blitz_data_parallel_tutorial.html" title="可选：数据并行处理" data-book-page-rel-url="docs/1.0/blitz_data_parallel_tutorial.html" data-book-page-id="11561">可选：数据并行处理</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/data_loading_tutorial.html" title="数据加载和处理教程" data-book-page-rel-url="docs/1.0/data_loading_tutorial.html" data-book-page-id="11562">数据加载和处理教程</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/pytorch_with_examples.html" title="用例子学习 PyTorch" data-book-page-rel-url="docs/1.0/pytorch_with_examples.html" data-book-page-id="11563">用例子学习 PyTorch</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/transfer_learning_tutorial.html" title="迁移学习教程" data-book-page-rel-url="docs/1.0/transfer_learning_tutorial.html" data-book-page-id="11564">迁移学习教程</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/deploy_seq2seq_hybrid_frontend_tutorial.html" title="混合前端的 seq2seq 模型部署" data-book-page-rel-url="docs/1.0/deploy_seq2seq_hybrid_frontend_tutorial.html" data-book-page-id="11565">混合前端的 seq2seq 模型部署</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/saving_loading_models.html" title="Saving and Loading Models" data-book-page-rel-url="docs/1.0/saving_loading_models.html" data-book-page-id="11566">Saving and Loading Models</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nn_tutorial.html" title="What is `torch.nn` _really_?" data-book-page-rel-url="docs/1.0/nn_tutorial.html" data-book-page-id="11567">What is `torch.nn` _really_?</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_image.html" title="图像" data-book-page-rel-url="docs/1.0/tut_image.html" data-book-page-id="11568">图像</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/finetuning_torchvision_models_tutorial.html" title="Torchvision 模型微调" data-book-page-rel-url="docs/1.0/finetuning_torchvision_models_tutorial.html" data-book-page-id="11569">Torchvision 模型微调</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/spatial_transformer_tutorial.html" title="空间变换器网络教程" data-book-page-rel-url="docs/1.0/spatial_transformer_tutorial.html" data-book-page-id="11570">空间变换器网络教程</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/neural_style_tutorial.html" title="使用 PyTorch 进行图像风格转换" data-book-page-rel-url="docs/1.0/neural_style_tutorial.html" data-book-page-id="11571">使用 PyTorch 进行图像风格转换</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/fgsm_tutorial.html" title="对抗性示例生成" data-book-page-rel-url="docs/1.0/fgsm_tutorial.html" data-book-page-id="11572">对抗性示例生成</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/super_resolution_with_caffe2.html" title="使用 ONNX 将模型从 PyTorch 传输到 Caffe2 和移动端" data-book-page-rel-url="docs/1.0/super_resolution_with_caffe2.html" data-book-page-id="11573">使用 ONNX 将模型从 PyTorch 传输到 Caffe2 和移动端</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_text.html" title="文本" data-book-page-rel-url="docs/1.0/tut_text.html" data-book-page-id="11574">文本</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/chatbot_tutorial.html" title="聊天机器人教程" data-book-page-rel-url="docs/1.0/chatbot_tutorial.html" data-book-page-id="11575">聊天机器人教程</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/char_rnn_generation_tutorial.html" title="使用字符级别特征的 RNN 网络生成姓氏" data-book-page-rel-url="docs/1.0/char_rnn_generation_tutorial.html" data-book-page-id="11576">使用字符级别特征的 RNN 网络生成姓氏</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/char_rnn_classification_tutorial.html" title="使用字符级别特征的 RNN 网络进行姓氏分类" data-book-page-rel-url="docs/1.0/char_rnn_classification_tutorial.html" data-book-page-id="11577">使用字符级别特征的 RNN 网络进行姓氏分类</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/deep_learning_nlp_tutorial.html" title="Deep Learning for NLP with Pytorch" data-book-page-rel-url="docs/1.0/deep_learning_nlp_tutorial.html" data-book-page-id="11578">Deep Learning for NLP with Pytorch</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nlp_pytorch_tutorial.html" title="PyTorch 介绍" data-book-page-rel-url="docs/1.0/nlp_pytorch_tutorial.html" data-book-page-id="11579">PyTorch 介绍</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nlp_deep_learning_tutorial.html" title="使用 PyTorch 进行深度学习" data-book-page-rel-url="docs/1.0/nlp_deep_learning_tutorial.html" data-book-page-id="11580">使用 PyTorch 进行深度学习</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nlp_word_embeddings_tutorial.html" title="Word Embeddings: Encoding Lexical Semantics" data-book-page-rel-url="docs/1.0/nlp_word_embeddings_tutorial.html" data-book-page-id="11581">Word Embeddings: Encoding Lexical Semantics</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nlp_sequence_models_tutorial.html" title="序列模型和 LSTM 网络" data-book-page-rel-url="docs/1.0/nlp_sequence_models_tutorial.html" data-book-page-id="11582">序列模型和 LSTM 网络</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nlp_advanced_tutorial.html" title="Advanced: Making Dynamic Decisions and the Bi-LSTM CRF" data-book-page-rel-url="docs/1.0/nlp_advanced_tutorial.html" data-book-page-id="11583">Advanced: Making Dynamic Decisions and the Bi-LSTM CRF</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/seq2seq_translation_tutorial.html" title="基于注意力机制的 seq2seq 神经网络翻译" data-book-page-rel-url="docs/1.0/seq2seq_translation_tutorial.html" data-book-page-id="11584">基于注意力机制的 seq2seq 神经网络翻译</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_generative.html" title="生成" data-book-page-rel-url="docs/1.0/tut_generative.html" data-book-page-id="11585">生成</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/dcgan_faces_tutorial.html" title="DCGAN Tutorial" data-book-page-rel-url="docs/1.0/dcgan_faces_tutorial.html" data-book-page-id="11586">DCGAN Tutorial</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_reinforcement_learning.html" title="强化学习" data-book-page-rel-url="docs/1.0/tut_reinforcement_learning.html" data-book-page-id="11587">强化学习</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/reinforcement_q_learning.html" title="Reinforcement Learning (DQN) Tutorial" data-book-page-rel-url="docs/1.0/reinforcement_q_learning.html" data-book-page-id="11588">Reinforcement Learning (DQN) Tutorial</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_extending_pytorch.html" title="扩展 PyTorch" data-book-page-rel-url="docs/1.0/tut_extending_pytorch.html" data-book-page-id="11589">扩展 PyTorch</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/numpy_extensions_tutorial.html" title="用 numpy 和 scipy 创建扩展" data-book-page-rel-url="docs/1.0/numpy_extensions_tutorial.html" data-book-page-id="11590">用 numpy 和 scipy 创建扩展</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/cpp_extension.html" title="Custom C++   and CUDA Extensions" data-book-page-rel-url="docs/1.0/cpp_extension.html" data-book-page-id="11591">Custom C++ and CUDA Extensions</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/torch_script_custom_ops.html" title="Extending TorchScript with Custom C++   Operators" data-book-page-rel-url="docs/1.0/torch_script_custom_ops.html" data-book-page-id="11592">Extending TorchScript with Custom C++ Operators</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_production_usage.html" title="生产性使用" data-book-page-rel-url="docs/1.0/tut_production_usage.html" data-book-page-id="11593">生产性使用</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/dist_tuto.html" title="Writing Distributed Applications with PyTorch" data-book-page-rel-url="docs/1.0/dist_tuto.html" data-book-page-id="11594">Writing Distributed Applications with PyTorch</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/aws_distributed_training_tutorial.html" title="使用 Amazon AWS 进行分布式训练" data-book-page-rel-url="docs/1.0/aws_distributed_training_tutorial.html" data-book-page-id="11595">使用 Amazon AWS 进行分布式训练</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/ONNXLive.html" title="ONNX 现场演示教程" data-book-page-rel-url="docs/1.0/ONNXLive.html" data-book-page-id="11596">ONNX 现场演示教程</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/cpp_export.html" title="在 C++ 中加载 PYTORCH 模型" data-book-page-rel-url="docs/1.0/cpp_export.html" data-book-page-id="11597">在 C++ 中加载 PYTORCH 模型</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tut_other_language.html" title="其它语言中的 PyTorch" data-book-page-rel-url="docs/1.0/tut_other_language.html" data-book-page-id="11598">其它语言中的 PyTorch</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/cpp_frontend.html" title="使用 PyTorch C++ 前端" data-book-page-rel-url="docs/1.0/cpp_frontend.html" data-book-page-id="11599">使用 PyTorch C++ 前端</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/docs_notes.html" title="注解" data-book-page-rel-url="docs/1.0/docs_notes.html" data-book-page-id="11600">注解</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_autograd.html" title="自动求导机制" data-book-page-rel-url="docs/1.0/notes_autograd.html" data-book-page-id="11601">自动求导机制</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_broadcasting.html" title="广播语义" data-book-page-rel-url="docs/1.0/notes_broadcasting.html" data-book-page-id="11602">广播语义</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_cuda.html" title="CUDA 语义" data-book-page-rel-url="docs/1.0/notes_cuda.html" data-book-page-id="11603">CUDA 语义</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_extending.html" title="Extending PyTorch" data-book-page-rel-url="docs/1.0/notes_extending.html" data-book-page-id="11604">Extending PyTorch</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_faq.html" title="Frequently Asked Questions" data-book-page-rel-url="docs/1.0/notes_faq.html" data-book-page-id="11605">Frequently Asked Questions</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_multiprocessing.html" title="Multiprocessing best practices" data-book-page-rel-url="docs/1.0/notes_multiprocessing.html" data-book-page-id="11606">Multiprocessing best practices</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_randomness.html" title="Reproducibility" data-book-page-rel-url="docs/1.0/notes_randomness.html" data-book-page-id="11607">Reproducibility</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_serialization.html" title="Serialization semantics" data-book-page-rel-url="docs/1.0/notes_serialization.html" data-book-page-id="11608">Serialization semantics</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/notes_windows.html" title="Windows FAQ" data-book-page-rel-url="docs/1.0/notes_windows.html" data-book-page-id="11609">Windows FAQ</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/docs_package_ref.html" title="包参考" data-book-page-rel-url="docs/1.0/docs_package_ref.html" data-book-page-id="11610">包参考</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/torch.html" title="torch" data-book-page-rel-url="docs/1.0/torch.html" data-book-page-id="11611">torch</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tensors.html" title="torch.Tensor" data-book-page-rel-url="docs/1.0/tensors.html" data-book-page-id="11612">torch.Tensor</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/tensor_attributes.html" title="Tensor Attributes" data-book-page-rel-url="docs/1.0/tensor_attributes.html" data-book-page-id="11613">Tensor Attributes</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/type_info.html" title="数据类型信息" data-book-page-rel-url="docs/1.0/type_info.html" data-book-page-id="11614">数据类型信息</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/sparse.html" title="torch.sparse" data-book-page-rel-url="docs/1.0/sparse.html" data-book-page-id="11615">torch.sparse</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/cuda.html" title="torch.cuda" data-book-page-rel-url="docs/1.0/cuda.html" data-book-page-id="11616">torch.cuda</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/storage.html" title="torch.Storage" data-book-page-rel-url="docs/1.0/storage.html" data-book-page-id="11617">torch.Storage</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nn.html" title="torch.nn" data-book-page-rel-url="docs/1.0/nn.html" data-book-page-id="11618">torch.nn</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nn_functional.html" title="torch.nn.functional" data-book-page-rel-url="docs/1.0/nn_functional.html" data-book-page-id="11619">torch.nn.functional</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/nn_init.html" title="torch.nn.init" data-book-page-rel-url="docs/1.0/nn_init.html" data-book-page-id="11620">torch.nn.init</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/optim.html" title="torch.optim" data-book-page-rel-url="docs/1.0/optim.html" data-book-page-id="11621">torch.optim</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/autograd.html" title="Automatic differentiation package - torch.autograd" data-book-page-rel-url="docs/1.0/autograd.html" data-book-page-id="11622">Automatic differentiation package - torch.autograd</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/distributed.html" title="Distributed communication package - torch.distributed" data-book-page-rel-url="docs/1.0/distributed.html" data-book-page-id="11623">Distributed communication package - torch.distributed</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/distributions.html" title="Probability distributions - torch.distributions" data-book-page-rel-url="docs/1.0/distributions.html" data-book-page-id="11624">Probability distributions - torch.distributions</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/jit.html" title="Torch Script" data-book-page-rel-url="docs/1.0/jit.html" data-book-page-id="11625">Torch Script</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/multiprocessing.html" title="多进程包 - torch.multiprocessing" data-book-page-rel-url="docs/1.0/multiprocessing.html" data-book-page-id="11626">多进程包 - torch.multiprocessing</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/bottleneck.html" title="torch.utils.bottleneck" data-book-page-rel-url="docs/1.0/bottleneck.html" data-book-page-id="11627">torch.utils.bottleneck</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/checkpoint.html" title="torch.utils.checkpoint" data-book-page-rel-url="docs/1.0/checkpoint.html" data-book-page-id="11628">torch.utils.checkpoint</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/docs_cpp_extension.html" title="torch.utils.cpp_extension" data-book-page-rel-url="docs/1.0/docs_cpp_extension.html" data-book-page-id="11629">torch.utils.cpp_extension</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/data.html" title="torch.utils.data" data-book-page-rel-url="docs/1.0/data.html" data-book-page-id="11630">torch.utils.data</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/dlpack.html" title="torch.utils.dlpack" data-book-page-rel-url="docs/1.0/dlpack.html" data-book-page-id="11631">torch.utils.dlpack</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/hub.html" title="torch.hub" data-book-page-rel-url="docs/1.0/hub.html" data-book-page-id="11632">torch.hub</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/model_zoo.html" title="torch.utils.model_zoo" data-book-page-rel-url="docs/1.0/model_zoo.html" data-book-page-id="11633">torch.utils.model_zoo</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/onnx.html" title="torch.onnx" data-book-page-rel-url="docs/1.0/onnx.html" data-book-page-id="11634">torch.onnx</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/distributed_deprecated.html" title="Distributed communication package (deprecated) - torch.distributed.deprecated" data-book-page-rel-url="docs/1.0/distributed_deprecated.html" data-book-page-id="11635">Distributed communication package (deprecated) - torch.distributed.deprecated</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/docs_torchvision_ref.html" title="torchvision 参考" data-book-page-rel-url="docs/1.0/docs_torchvision_ref.html" data-book-page-id="11636">torchvision 参考</a>
<ul>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/torchvision_datasets.html" title="torchvision.datasets" data-book-page-rel-url="docs/1.0/torchvision_datasets.html" data-book-page-id="11637">torchvision.datasets</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/torchvision_models.html" title="torchvision.models" data-book-page-rel-url="docs/1.0/torchvision_models.html" data-book-page-id="11638">torchvision.models</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/torchvision_transforms.html" title="torchvision.transforms" data-book-page-rel-url="docs/1.0/torchvision_transforms.html" data-book-page-id="11639">torchvision.transforms</a>
</li>
<li>
<a class="pjax" href="../../../../book/169/docs/1.0/torchvision_utils.html" title="torchvision.utils" data-book-page-rel-url="docs/1.0/torchvision_utils.html" data-book-page-id="11640">torchvision.utils</a>
</li>
</ul>
</li>
</ul>
</div>
</div>
<script src="https://cdn.staticfile.net/jquery/1.12.4/jquery.min.js"></script>
<script type="text/javascript" src="../../../../static/components/uikit-2.27.5/js/uikit.reader.js"></script>
<script type="text/javascript" src="../../../../static/components/social-share/social-share.min.js"></script>
<script>(function(){var bp =document.createElement('script');var curProtocol =window.location.protocol.split(':')[0];if (curProtocol ==='https') {bp.src ='https://zz.bdstatic.com/linksubmit/push.js';}
else {bp.src ='http://push.zhanzhang.baidu.com/push.js';}
var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(bp,s);})();</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
<script src="https://cdn.staticfile.net/highlight.js/9.12.0/highlight.min.js"></script>
<script src="https://cdn.staticfile.net/jquery.pjax/2.0.1/jquery.pjax.min.js"></script>
<script src="https://cdn.staticfile.net/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="https://cdn.staticfile.net/uikit/2.27.5/js/components/lightbox.min.js"></script>
<link rel="dns-prefetch" href="../../../..//cdn.mathjax.org" />
<script type="text/x-mathjax-config">
 function initMathJax() {
    var mathId = $("book-content-section")[0];
    MathJax.Hub.Config({
        tex2jax: {skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code','a']},
        showProcessingMessages: false,
        messageStyle: "none"
    });
    MathJax.Hub.Queue(["Typeset",MathJax.Hub,mathId]);
 };
initMathJax();
</script>
<script src='https://cdn.staticfile.net/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML' async></script>
<style>
	.MathJax_Display{display:inline!important;}
</style>
<script type="text/javascript" src="../../../../static/components/js/reader.js"></script>
<script type="text/javascript">var bookId =169;var bookPageId =11584;var bookPageRelUrl ='docs/1.0/seq2seq_translation_tutorial.html';</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
</body>
</html>