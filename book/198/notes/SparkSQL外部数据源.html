
<!DOCTYPE html>
<html lang="zh-CN">
<head>
<title>Spark SQL 外部数据源-大数据入门指南</title>
<meta content='Spark SQL 外部数据源,大数据入门指南' name='keywords'>
<meta content='Spark SQL 外部数据源,大数据入门指南' name='description'>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta http-equiv="Content-Language" content="zh-CN" />
<meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1, maximum-scale=1, user-scalable=no"../../../>
<meta name="applicable-device" content="pc,mobile">
<link rel="shortcut icon" href="../../../favicon.ico" type="image/x-icon" />
<meta name="renderer" content="webkit">
<link rel="stylesheet" href="../../../static/components/uikit-2.27.5/css/uikit.custom.css">
<link rel="stylesheet" href="../../../static/components/social-share/social-share.min.css">
<link rel="stylesheet" href="../../../static/components/highlight/styles/custom.css">
<link rel="stylesheet" href="../../../static/components/css/base.css">
<link rel="stylesheet" href="../../../static/components/css/reader.css">
<link rel="stylesheet" href="../../../static/components/css/markdown.css">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5313208362165053" crossorigin="anonymous"></script>
</head>
<body>
<div class=" book-main-wrap uk-container uk-container-center uk-margin-top ">
<div class="uk-grid">
<div class="uk-width-1-1 reader-wrap ">
<div class=" bottom-nav uk-clearfix ">
<div class="uk-align-left ">
<a href="../../../book/198/notes/Spark_Structured_API的基本使用.html">
<i class="nav-icon-left uk-icon-small  uk-icon-caret-left"></i>
<span class="">Structured ..</span>
</a>
</div>
<div class="uk-align-right ">
<a href="../../../book/198/notes/SparkSQL常用聚合函数.html">
<span class="">Spark SQL 常..</span>
<i class="nav-icon-right uk-icon-small  uk-icon-caret-right"></i>
</a>
</div>
</div>
<div class="uk-text-center">
<h2 class="book-page-title uk-container-center">
<a href="../../../book/198/index.html">大数据入门指南</a>
<a target="_blank" rel="nofollow" href="https://github.com/heibaiying/BigData-Notes" class="uk-icon-button uk-icon-github" title="github项目地址"></a>
</h2>
</div>
<script type="text/javascript" src="../../../static/components/js/app_intro.js"></script>
<ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-5313208362165053" data-ad-slot="1328047120"></ins>
<script>(adsbygoogle =window.adsbygoogle ||[]).push({});</script>
<hr class="uk-article-divider">
<div class="book-content-section  md-content-section  uk-margin-bottom">
<h1 id="spark-sql-外部数据源">Spark SQL 外部数据源</h1>
<nav>
<a href="#一简介">一、简介</a>
<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#11-多数据源支持">1.1 多数据源支持</a>
<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#12-读数据格式">1.2 读数据格式</a>
<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#13-写数据格式">1.3 写数据格式</a>
<br>
<a href="#二CSV">二、CSV</a>
<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#21-读取CSV文件">2.1 读取CSV文件</a>
<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#22-写入CSV文件">2.2 写入CSV文件</a>
<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#23-可选配置">2.3 可选配置</a>
<br>
<a href="#三JSON">三、JSON</a>
<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#31-读取JSON文件">3.1 读取JSON文件</a>
<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#32-写入JSON文件">3.2 写入JSON文件</a>
<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#33-可选配置">3.3 可选配置</a>
<br>
<a href="#四Parquet">四、Parquet</a>
<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#41-读取Parquet文件">4.1 读取Parquet文件</a>
<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#22-写入Parquet文件">2.2 写入Parquet文件</a>
<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#23-可选配置">2.3 可选配置</a>
<br>
<a href="#五ORC">五、ORC </a>
<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#51-读取ORC文件">5.1 读取ORC文件 </a>
<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#42-写入ORC文件">4.2 写入ORC文件</a>
<br>
<a href="#六SQL-Databases">六、SQL Databases </a>
<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#61-读取数据">6.1 读取数据</a>
<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#62-写入数据">6.2 写入数据</a>
<br>
<a href="#七Text">七、Text </a>
<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#71-读取Text数据">7.1 读取Text数据</a>
<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#72-写入Text数据">7.2 写入Text数据</a>
<br>
<a href="#八数据读写高级特性">八、数据读写高级特性</a>
<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#81-并行读">8.1 并行读</a>
<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#82-并行写">8.2 并行写</a>
<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#83-分区写入">8.3 分区写入</a>
<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#83-分桶写入">8.3 分桶写入</a>
<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#85-文件大小管理">8.5 文件大小管理</a>
<br>
<a href="#九可选配置附录">九、可选配置附录</a>
<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#91-CSV读写可选配置">9.1 CSV读写可选配置</a>
<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#92-JSON读写可选配置">9.2 JSON读写可选配置</a>
<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="#93-数据库读写可选配置">9.3 数据库读写可选配置</a>
<br>
</nav>
<h2 id="一简介">一、简介</h2>
<h3 id="11-多数据源支持">1.1 多数据源支持</h3>
<p>Spark 支持以下六个核心数据源，同时 Spark 社区还提供了多达上百种数据源的读取方式，能够满足绝大部分使用场景。</p>
<ul>
<li>CSV</li>
<li>JSON</li>
<li>Parquet</li>
<li>ORC</li>
<li>JDBC/ODBC connections</li>
<li>Plain-text files</li>
</ul>
<blockquote>
<p>注：以下所有测试文件均可从本仓库的<a href="https://github.com/heibaiying/BigData-Notes/tree/master/resources">resources</a> 目录进行下载</p>
</blockquote>
<h3 id="12-读数据格式">1.2 读数据格式</h3>
<p>所有读取 API 遵循以下调用格式：</p>
<pre><code class="language-scala">// 格式
DataFrameReader.format(...).option("key", "value").schema(...).load()

// 示例
spark.read.format("csv")
.option("mode", "FAILFAST")          // 读取模式
.option("inferSchema", "true")       // 是否自动推断 schema
.option("path", "path/to/file(s)")   // 文件路径
.schema(someSchema)                  // 使用预定义的 schema      
.load()
</code></pre>
<p>读取模式有以下三种可选项：</p>
<table>
<thead>
<tr>
<th>读模式</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>permissive</code></td>
<td>当遇到损坏的记录时，将其所有字段设置为 null，并将所有损坏的记录放在名为 _corruption t_record 的字符串列中</td>
</tr>
<tr>
<td><code>dropMalformed</code></td>
<td>删除格式不正确的行</td>
</tr>
<tr>
<td><code>failFast</code></td>
<td>遇到格式不正确的数据时立即失败</td>
</tr>
</tbody>
</table>
<h3 id="13-写数据格式">1.3 写数据格式</h3>
<pre><code class="language-scala">// 格式
DataFrameWriter.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save()

//示例
dataframe.write.format("csv")
.option("mode", "OVERWRITE")         //写模式
.option("dateFormat", "yyyy-MM-dd")  //日期格式
.option("path", "path/to/file(s)")
.save()
</code></pre>
<p>写数据模式有以下四种可选项：</p>
<table>
<thead>
<tr>
<th align="left">Scala/Java</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><code>SaveMode.ErrorIfExists</code></td>
<td align="left">如果给定的路径已经存在文件，则抛出异常，这是写数据默认的模式</td>
</tr>
<tr>
<td align="left"><code>SaveMode.Append</code></td>
<td align="left">数据以追加的方式写入</td>
</tr>
<tr>
<td align="left"><code>SaveMode.Overwrite</code></td>
<td align="left">数据以覆盖的方式写入</td>
</tr>
<tr>
<td align="left"><code>SaveMode.Ignore</code></td>
<td align="left">如果给定的路径已经存在文件，则不做任何操作</td>
</tr>
</tbody>
</table>
<br>
<h2 id="二csv">二、CSV</h2>
<p>CSV 是一种常见的文本文件格式，其中每一行表示一条记录，记录中的每个字段用逗号分隔。</p>
<h3 id="21-读取csv文件">2.1 读取CSV文件</h3>
<p>自动推断类型读取读取示例：</p>
<pre><code class="language-scala">spark.read.format("csv")
.option("header", "false")        // 文件中的第一行是否为列的名称
.option("mode", "FAILFAST")      // 是否快速失败
.option("inferSchema", "true")   // 是否自动推断 schema
.load("../../../usr/file/csv/dept.csv")
.show()
</code></pre>
<p>使用预定义类型：</p>
<pre><code class="language-scala">import org.apache.spark.sql.types.{StructField, StructType, StringType,LongType}
//预定义数据格式
val myManualSchema = new StructType(Array(
    StructField("deptno", LongType, nullable = false),
    StructField("dname", StringType,nullable = true),
    StructField("loc", StringType,nullable = true)
))
spark.read.format("csv")
.option("mode", "FAILFAST")
.schema(myManualSchema)
.load("../../../usr/file/csv/dept.csv")
.show()
</code></pre>
<h3 id="22-写入csv文件">2.2 写入CSV文件</h3>
<pre><code class="language-scala">df.write.format("csv").mode("overwrite").save("../../../tmp/csv/dept2")
</code></pre>
<p>也可以指定具体的分隔符：</p>
<pre><code class="language-scala">df.write.format("csv").mode("overwrite").option("sep", "\t").save("../../../tmp/csv/dept2")
</code></pre>
<h3 id="23-可选配置">2.3 可选配置</h3>
<p>为节省主文篇幅，所有读写配置项见文末 9.1 小节。</p>
<br>
<h2 id="三json">三、JSON</h2>
<h3 id="31-读取json文件">3.1 读取JSON文件</h3>
<pre><code class="language-json">spark.read.format("json").option("mode", "FAILFAST").load("../../../usr/file/json/dept.json").show(5)
</code></pre>
<p>需要注意的是：默认不支持一条数据记录跨越多行 (如下)，可以通过配置 <code>multiLine</code> 为 <code>true</code> 来进行更改，其默认值为 <code>false</code>。</p>
<pre><code class="language-json">// 默认支持单行
{"DEPTNO": 10,"DNAME": "ACCOUNTING","LOC": "NEW YORK"}

//默认不支持多行
{
  "DEPTNO": 10,
  "DNAME": "ACCOUNTING",
  "LOC": "NEW YORK"
}
</code></pre>
<h3 id="32-写入json文件">3.2 写入JSON文件</h3>
<pre><code class="language-scala">df.write.format("json").mode("overwrite").save("../../../tmp/spark/json/dept")
</code></pre>
<h3 id="33-可选配置">3.3 可选配置</h3>
<p>为节省主文篇幅，所有读写配置项见文末 9.2 小节。</p>
<br>
<h2 id="四parquet">四、Parquet</h2>
<p>Parquet 是一个开源的面向列的数据存储，它提供了多种存储优化，允许读取单独的列非整个文件，这不仅节省了存储空间而且提升了读取效率，它是 Spark 是默认的文件格式。</p>
<h3 id="41-读取parquet文件">4.1 读取Parquet文件</h3>
<pre><code class="language-scala">spark.read.format("parquet").load("../../../usr/file/parquet/dept.parquet").show(5)
</code></pre>
<h3 id="22-写入parquet文件">2.2 写入Parquet文件</h3>
<pre><code class="language-scala">df.write.format("parquet").mode("overwrite").save("../../../tmp/spark/parquet/dept")
</code></pre>
<h3 id="23-可选配置-1">2.3 可选配置</h3>
<p>Parquet 文件有着自己的存储规则，因此其可选配置项比较少，常用的有如下两个：</p>
<table>
<thead>
<tr>
<th>读写操作</th>
<th>配置项</th>
<th>可选值</th>
<th>默认值</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Write</td>
<td>compression or codec</td>
<td>None,<br>uncompressed,<br>bzip2,<br>deflate, gzip,<br>lz4, or snappy</td>
<td>None</td>
<td>压缩文件格式</td>
</tr>
<tr>
<td>Read</td>
<td>mergeSchema</td>
<td>true, false</td>
<td>取决于配置项 <code>spark.sql.parquet.mergeSchema</code></td>
<td>当为真时，Parquet 数据源将所有数据文件收集的 Schema 合并在一起，否则将从摘要文件中选择 Schema，如果没有可用的摘要文件，则从随机数据文件中选择 Schema。</td>
</tr>
</tbody>
</table>
<blockquote>
<p>更多可选配置可以参阅官方文档：https://spark.apache.org/docs/latest/sql-data-sources-parquet.html</p>
</blockquote>
<br>
<h2 id="五orc">五、ORC</h2>
<p>ORC 是一种自描述的、类型感知的列文件格式，它针对大型数据的读写进行了优化，也是大数据中常用的文件格式。</p>
<h3 id="51-读取orc文件">5.1 读取ORC文件</h3>
<pre><code class="language-scala">spark.read.format("orc").load("../../../usr/file/orc/dept.orc").show(5)
</code></pre>
<h3 id="42-写入orc文件">4.2 写入ORC文件</h3>
<pre><code class="language-scala">csvFile.write.format("orc").mode("overwrite").save("../../../tmp/spark/orc/dept")
</code></pre>
<br>
<h2 id="六sql-databases">六、SQL Databases</h2>
<p>Spark 同样支持与传统的关系型数据库进行数据读写。但是 Spark 程序默认是没有提供数据库驱动的，所以在使用前需要将对应的数据库驱动上传到安装目录下的 <code>jars</code> 目录中。下面示例使用的是 Mysql 数据库，使用前需要将对应的 <code>mysql-connector-java-x.x.x.jar</code> 上传到 <code>jars</code> 目录下。</p>
<h3 id="61-读取数据">6.1 读取数据</h3>
<p>读取全表数据示例如下，这里的 <code>help_keyword</code> 是 mysql 内置的字典表，只有 <code>help_keyword_id</code> 和 <code>name</code> 两个字段。</p>
<pre><code class="language-scala">spark.read
.format("jdbc")
.option("driver", "com.mysql.jdbc.Driver")            //驱动
.option("url", "jdbc:mysql://127.0.0.1:3306/mysql")   //数据库地址
.option("dbtable", "help_keyword")                    //表名
.option("user", "root").option("password","root").load().show(10)
</code></pre>
<p>从查询结果读取数据：</p>
<pre><code class="language-scala">val pushDownQuery = """(SELECT * FROM help_keyword WHERE help_keyword_id &lt;20) AS help_keywords"""
spark.read.format("jdbc")
.option("url", "jdbc:mysql://127.0.0.1:3306/mysql")
.option("driver", "com.mysql.jdbc.Driver")
.option("user", "root").option("password", "root")
.option("dbtable", pushDownQuery)
.load().show()

//输出
+---------------+-----------+
|help_keyword_id|       name|
+---------------+-----------+
|              0|         &lt;&gt;|
|              1|     ACTION|
|              2|        ADD|
|              3|AES_DECRYPT|
|              4|AES_ENCRYPT|
|              5|      AFTER|
|              6|    AGAINST|
|              7|  AGGREGATE|
|              8|  ALGORITHM|
|              9|        ALL|
|             10|      ALTER|
|             11|    ANALYSE|
|             12|    ANALYZE|
|             13|        AND|
|             14|    ARCHIVE|
|             15|       AREA|
|             16|         AS|
|             17|   ASBINARY|
|             18|        ASC|
|             19|     ASTEXT|
+---------------+-----------+
</code></pre>
<p>也可以使用如下的写法进行数据的过滤：</p>
<pre><code class="language-scala">val props = new java.util.Properties
props.setProperty("driver", "com.mysql.jdbc.Driver")
props.setProperty("user", "root")
props.setProperty("password", "root")
val predicates = Array("help_keyword_id &lt; 10  OR name = 'WHEN'")   //指定数据过滤条件
spark.read.jdbc("jdbc:mysql://127.0.0.1:3306/mysql", "help_keyword", predicates, props).show() 

//输出：
+---------------+-----------+
|help_keyword_id|       name|
+---------------+-----------+
|              0|         &lt;&gt;|
|              1|     ACTION|
|              2|        ADD|
|              3|AES_DECRYPT|
|              4|AES_ENCRYPT|
|              5|      AFTER|
|              6|    AGAINST|
|              7|  AGGREGATE|
|              8|  ALGORITHM|
|              9|        ALL|
|            604|       WHEN|
+---------------+-----------+
</code></pre>
<p>可以使用 <code>numPartitions</code> 指定读取数据的并行度：</p>
<pre><code class="language-scala">option("numPartitions", 10)
</code></pre>
<p>在这里，除了可以指定分区外，还可以设置上界和下界，任何小于下界的值都会被分配在第一个分区中，任何大于上界的值都会被分配在最后一个分区中。</p>
<pre><code class="language-scala">val colName = "help_keyword_id"   //用于判断上下界的列
val lowerBound = 300L    //下界
val upperBound = 500L    //上界
val numPartitions = 10   //分区综述
val jdbcDf = spark.read.jdbc("jdbc:mysql://127.0.0.1:3306/mysql","help_keyword",
                             colName,lowerBound,upperBound,numPartitions,props)
</code></pre>
<p>想要验证分区内容，可以使用 <code>mapPartitionsWithIndex</code> 这个算子，代码如下：</p>
<pre><code class="language-scala">jdbcDf.rdd.mapPartitionsWithIndex((index, iterator) =&gt; {
    val buffer = new ListBuffer[String]
    while (iterator.hasNext) {
        buffer.append(index + "分区:" + iterator.next())
    }
    buffer.toIterator
}).foreach(println)
</code></pre>
<p>执行结果如下：<code>help_keyword</code> 这张表只有 600 条左右的数据，本来数据应该均匀分布在 10 个分区，但是 0 分区里面却有 319 条数据，这是因为设置了下限，所有小于 300 的数据都会被限制在第一个分区，即 0 分区。同理所有大于 500 的数据被分配在 9 分区，即最后一个分区。</p>
<div align="center">
<a href="https://gitee.com/heibaiying/BigData-Notes/raw/master/pictures/spark-mysql-分区上下限.png" data-uk-lightbox><img src="https://gitee.com/heibaiying/BigData-Notes/raw/master/pictures/spark-mysql-分区上下限.png"></a>
</div>
<h3 id="62-写入数据">6.2 写入数据</h3>
<pre><code class="language-scala">val df = spark.read.format("json").load("../../../usr/file/json/emp.json")
df.write
.format("jdbc")
.option("url", "jdbc:mysql://127.0.0.1:3306/mysql")
.option("user", "root").option("password", "root")
.option("dbtable", "emp")
.save()
</code></pre>
<br>
<h2 id="七text">七、Text</h2>
<p>Text 文件在读写性能方面并没有任何优势，且不能表达明确的数据结构，所以其使用的比较少，读写操作如下：</p>
<h3 id="71-读取text数据">7.1 读取Text数据</h3>
<pre><code class="language-scala">spark.read.textFile("../../../usr/file/txt/dept.txt").show()
</code></pre>
<h3 id="72-写入text数据">7.2 写入Text数据</h3>
<pre><code class="language-scala">df.write.text("../../../tmp/spark/txt/dept")
</code></pre>
<br>
<h2 id="八数据读写高级特性">八、数据读写高级特性</h2>
<h3 id="81-并行读">8.1 并行读</h3>
<p>多个 Executors 不能同时读取同一个文件，但它们可以同时读取不同的文件。这意味着当您从一个包含多个文件的文件夹中读取数据时，这些文件中的每一个都将成为 DataFrame 中的一个分区，并由可用的 Executors 并行读取。</p>
<h3 id="82-并行写">8.2 并行写</h3>
<p>写入的文件或数据的数量取决于写入数据时 DataFrame 拥有的分区数量。默认情况下，每个数据分区写一个文件。</p>
<h3 id="83-分区写入">8.3 分区写入</h3>
<p>分区和分桶这两个概念和 Hive 中分区表和分桶表是一致的。都是将数据按照一定规则进行拆分存储。需要注意的是 <code>partitionBy</code> 指定的分区和 RDD 中分区不是一个概念：这里的<strong>分区表现为输出目录的子目录</strong>，数据分别存储在对应的子目录中。</p>
<pre><code class="language-scala">val df = spark.read.format("json").load("../../../usr/file/json/emp.json")
df.write.mode("overwrite").partitionBy("deptno").save("../../../tmp/spark/partitions")
</code></pre>
<p>输出结果如下：可以看到输出被按照部门编号分为三个子目录，子目录中才是对应的输出文件。</p>
<div align="center">
<a href="https://gitee.com/heibaiying/BigData-Notes/raw/master/pictures/spark-分区.png" data-uk-lightbox><img src="https://gitee.com/heibaiying/BigData-Notes/raw/master/pictures/spark-分区.png"></a>
</div>
<h3 id="83-分桶写入">8.3 分桶写入</h3>
<p>分桶写入就是将数据按照指定的列和桶数进行散列，目前分桶写入只支持保存为表，实际上这就是 Hive 的分桶表。</p>
<pre><code class="language-scala">val numberBuckets = 10
val columnToBucketBy = "empno"
df.write.format("parquet").mode("overwrite")
.bucketBy(numberBuckets, columnToBucketBy).saveAsTable("bucketedFiles")
</code></pre>
<h3 id="85-文件大小管理">8.5 文件大小管理</h3>
<p>如果写入产生小文件数量过多，这时会产生大量的元数据开销。Spark 和 HDFS 一样，都不能很好的处理这个问题，这被称为“small file problem”。同时数据文件也不能过大，否则在查询时会有不必要的性能开销，因此要把文件大小控制在一个合理的范围内。</p>
<p>在上文我们已经介绍过可以通过分区数量来控制生成文件的数量，从而间接控制文件大小。Spark 2.2 引入了一种新的方法，以更自动化的方式控制文件大小，这就是 <code>maxRecordsPerFile</code> 参数，它允许你通过控制写入文件的记录数来控制文件大小。</p>
<pre><code class="language-scala"> // Spark 将确保文件最多包含 5000 条记录
df.write.option(“maxRecordsPerFile”, 5000)
</code></pre>
<br>
<h2 id="九可选配置附录">九、可选配置附录</h2>
<h3 id="91-csv读写可选配置">9.1 CSV读写可选配置</h3>
<table>
<thead>
<tr>
<th>读\写操作</th>
<th>配置项</th>
<th>可选值</th>
<th>默认值</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Both</td>
<td>seq</td>
<td>任意字符</td>
<td><code>,</code>(逗号)</td>
<td>分隔符</td>
</tr>
<tr>
<td>Both</td>
<td>header</td>
<td>true, false</td>
<td>false</td>
<td>文件中的第一行是否为列的名称。</td>
</tr>
<tr>
<td>Read</td>
<td>escape</td>
<td>任意字符</td>
<td>\</td>
<td>转义字符</td>
</tr>
<tr>
<td>Read</td>
<td>inferSchema</td>
<td>true, false</td>
<td>false</td>
<td>是否自动推断列类型</td>
</tr>
<tr>
<td>Read</td>
<td>ignoreLeadingWhiteSpace</td>
<td>true, false</td>
<td>false</td>
<td>是否跳过值前面的空格</td>
</tr>
<tr>
<td>Both</td>
<td>ignoreTrailingWhiteSpace</td>
<td>true, false</td>
<td>false</td>
<td>是否跳过值后面的空格</td>
</tr>
<tr>
<td>Both</td>
<td>nullValue</td>
<td>任意字符</td>
<td>“”</td>
<td>声明文件中哪个字符表示空值</td>
</tr>
<tr>
<td>Both</td>
<td>nanValue</td>
<td>任意字符</td>
<td>NaN</td>
<td>声明哪个值表示 NaN 或者缺省值</td>
</tr>
<tr>
<td>Both</td>
<td>positiveInf</td>
<td>任意字符</td>
<td>Inf</td>
<td>正无穷</td>
</tr>
<tr>
<td>Both</td>
<td>negativeInf</td>
<td>任意字符</td>
<td>-Inf</td>
<td>负无穷</td>
</tr>
<tr>
<td>Both</td>
<td>compression or codec</td>
<td>None,<br>uncompressed,<br>bzip2, deflate,<br>gzip, lz4, or<br>snappy</td>
<td>none</td>
<td>文件压缩格式</td>
</tr>
<tr>
<td>Both</td>
<td>dateFormat</td>
<td>任何能转换为 Java 的 <br>SimpleDataFormat 的字符串</td>
<td>yyyy-MM-dd</td>
<td>日期格式</td>
</tr>
<tr>
<td>Both</td>
<td>timestampFormat</td>
<td>任何能转换为 Java 的 <br>SimpleDataFormat 的字符串</td>
<td>yyyy-MMdd’T’HH:mm:ss.SSSZZ</td>
<td>时间戳格式</td>
</tr>
<tr>
<td>Read</td>
<td>maxColumns</td>
<td>任意整数</td>
<td>20480</td>
<td>声明文件中的最大列数</td>
</tr>
<tr>
<td>Read</td>
<td>maxCharsPerColumn</td>
<td>任意整数</td>
<td>1000000</td>
<td>声明一个列中的最大字符数。</td>
</tr>
<tr>
<td>Read</td>
<td>escapeQuotes</td>
<td>true, false</td>
<td>true</td>
<td>是否应该转义行中的引号。</td>
</tr>
<tr>
<td>Read</td>
<td>maxMalformedLogPerPartition</td>
<td>任意整数</td>
<td>10</td>
<td>声明每个分区中最多允许多少条格式错误的数据，超过这个值后格式错误的数据将不会被读取</td>
</tr>
<tr>
<td>Write</td>
<td>quoteAll</td>
<td>true, false</td>
<td>false</td>
<td>指定是否应该将所有值都括在引号中，而不只是转义具有引号字符的值。</td>
</tr>
<tr>
<td>Read</td>
<td>multiLine</td>
<td>true, false</td>
<td>false</td>
<td>是否允许每条完整记录跨域多行</td>
</tr>
</tbody>
</table>
<h3 id="92-json读写可选配置">9.2 JSON读写可选配置</h3>
<table>
<thead>
<tr>
<th>读\写操作</th>
<th>配置项</th>
<th>可选值</th>
<th>默认值</th>
</tr>
</thead>
<tbody>
<tr>
<td>Both</td>
<td>compression or codec</td>
<td>None,<br>uncompressed,<br>bzip2, deflate,<br>gzip, lz4, or<br>snappy</td>
<td>none</td>
</tr>
<tr>
<td>Both</td>
<td>dateFormat</td>
<td>任何能转换为 Java 的 SimpleDataFormat 的字符串</td>
<td>yyyy-MM-dd</td>
</tr>
<tr>
<td>Both</td>
<td>timestampFormat</td>
<td>任何能转换为 Java 的 SimpleDataFormat 的字符串</td>
<td>yyyy-MMdd’T’HH:mm:ss.SSSZZ</td>
</tr>
<tr>
<td>Read</td>
<td>primitiveAsString</td>
<td>true, false</td>
<td>false</td>
</tr>
<tr>
<td>Read</td>
<td>allowComments</td>
<td>true, false</td>
<td>false</td>
</tr>
<tr>
<td>Read</td>
<td>allowUnquotedFieldNames</td>
<td>true, false</td>
<td>false</td>
</tr>
<tr>
<td>Read</td>
<td>allowSingleQuotes</td>
<td>true, false</td>
<td>true</td>
</tr>
<tr>
<td>Read</td>
<td>allowNumericLeadingZeros</td>
<td>true, false</td>
<td>false</td>
</tr>
<tr>
<td>Read</td>
<td>allowBackslashEscapingAnyCharacter</td>
<td>true, false</td>
<td>false</td>
</tr>
<tr>
<td>Read</td>
<td>columnNameOfCorruptRecord</td>
<td>true, false</td>
<td>Value of spark.sql.column&amp;NameOf</td>
</tr>
<tr>
<td>Read</td>
<td>multiLine</td>
<td>true, false</td>
<td>false</td>
</tr>
</tbody>
</table>
<h3 id="93-数据库读写可选配置">9.3 数据库读写可选配置</h3>
<table>
<thead>
<tr>
<th>属性名称</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>url</td>
<td>数据库地址</td>
</tr>
<tr>
<td>dbtable</td>
<td>表名称</td>
</tr>
<tr>
<td>driver</td>
<td>数据库驱动</td>
</tr>
<tr>
<td>partitionColumn,<br>lowerBound, upperBoun</td>
<td>分区总数，上界，下界</td>
</tr>
<tr>
<td>numPartitions</td>
<td>可用于表读写并行性的最大分区数。如果要写的分区数量超过这个限制，那么可以调用 coalesce(numpartition) 重置分区数。</td>
</tr>
<tr>
<td>fetchsize</td>
<td>每次往返要获取多少行数据。此选项仅适用于读取数据。</td>
</tr>
<tr>
<td>batchsize</td>
<td>每次往返插入多少行数据，这个选项只适用于写入数据。默认值是 1000。</td>
</tr>
<tr>
<td>isolationLevel</td>
<td>事务隔离级别：可以是 NONE，READ_COMMITTED, READ_UNCOMMITTED，REPEATABLE_READ 或 SERIALIZABLE，即标准事务隔离级别。<br>默认值是 READ_UNCOMMITTED。这个选项只适用于数据读取。</td>
</tr>
<tr>
<td>createTableOptions</td>
<td>写入数据时自定义创建表的相关配置</td>
</tr>
<tr>
<td>createTableColumnTypes</td>
<td>写入数据时自定义创建列的列类型</td>
</tr>
</tbody>
</table>
<blockquote>
<p>数据库读写更多配置可以参阅官方文档：https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html</p>
</blockquote>
<h2 id="参考资料">参考资料</h2>
<ol>
<li>Matei Zaharia, Bill Chambers . Spark: The Definitive Guide[M] . 2018-02</li>
<li>https://spark.apache.org/docs/latest/sql-data-sources.html</li>
</ol>
<div align="center">
<a href="https://gitee.com/heibaiying/BigData-Notes/raw/master/pictures/weixin-desc.png" data-uk-lightbox><img src="https://gitee.com/heibaiying/BigData-Notes/raw/master/pictures/weixin-desc.png"></a>
</div>
</div>
<hr class="uk-article-divider">
<div class="uk-block uk-block-muted uk-padding-top-remove uk-padding-bottom-remove uk-margin-large-top  book-recommend-wrap">
<div class="uk-margin-top uk-margin-bottom uk-margin-left uk-margin-right">
<div class="uk-margin uk-text-muted "><i class="uk-icon-outdent uk-icon-justify uk-margin-small-right"></i>书籍推荐</div>
<div class="books">
<ul class="uk-book-list">
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/108/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/storm_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/108/index.html">Apache Storm 官方文档中文版</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/62.html">tzivanmoe</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="storm">storm</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">74页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年7月1日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 3个">3</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/196/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/code_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/196/index.html">全栈开发指南2021</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/112.html">frank-lam</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="code">code</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">51页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2021年10月24日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 个"></span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/65/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/java_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/65/index.html">更先进的Java - Java 8指南</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/41.html">winterbe</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="java">java</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">1页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月6日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 9341个">9341</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/108/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/storm_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/108/index.html">Apache Storm 官方文档中文版</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/62.html">tzivanmoe</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="storm">storm</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">74页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年7月1日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 3个">3</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/81/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/springboot_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/81/index.html">《SpringBoot参考指南》中文翻译 基于1.5.7 RELEASE</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/52.html">wangjingjing</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="springboot">springboot</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">100页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月24日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 1个">1</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/95/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/spring_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/95/index.html">Spring Framework 5 中文文档</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/58.html">lfvepclr</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="spring">spring</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">134页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月29日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 25个">25</span>
</div>
</div>
</div>
</li>
<hr>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
<nav class="tm-navbar uk-navbar uk-navbar-attached reader-nav">
<div class="uk-float-left uk-margin-small-top">
<a href="javascript:;" title="目录菜单" class="show-menu  uk-icon-hover  uk-icon-align-justify uk-margin-right"></a>
<div data-uk-dropdown="{mode:'click',pos:'bottom-left'}" class="font-setting-wrap">
<a class="uk-icon-hover uk-icon-font uk-margin-right" aria-label="字体设置" href="javascript:;"></a>
<div class="uk-dropdown dropdown-menu">
<div class="dropdown-caret"><span class="caret-outer"></span><span class="caret-inner"></span></div>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-reduce">小字</button>
<button class="uk-button-link button size-2 font-enlarge">大字</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-1 ">宋体</button>
<button class="uk-button-link button size-2 font-2 ">黑体</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-3 color-theme-sun "><i class="uk-icon-sun-o"></i>白天</button>
<button class="uk-button-link button size-3 color-theme-eye "><i class="uk-icon-eye"></i>护眼</button>
<button class="uk-button-link button size-3 color-theme-moon "><i class="uk-icon-moon-o"></i>夜晚</button></div>
</div>
</div>
<a class="logo uk-margin-right" href="../../../" title="返回首页"><img class="" src="../../../static/components/images/icon_32.png" /></a>
</div>
<div class="uk-navbar-flip  uk-hidden-small">
<div id="share-box"></div>
</div>
</nav>
<div id="menu-id" class="uk-offcanvas reader-offcanvas">
<div class="uk-offcanvas-bar">
<ul class="book-menu-bar uk-nav uk-nav-offcanvas" data-uk-nav>
<li>
<a href="../../../book/198/index.html" data-book-page-rel-url="index.html" data-book-page-id="0" title="封面">封面</a>
</li>
<li>
<a class="pjax" href="../../../book/198/readme.html" data-book-page-rel-url="readme.html" data-book-page-id="0" title="简介">简介</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/大数据学习路线.html" title="大数据学习路线" data-book-page-rel-url="notes/大数据学习路线.html" data-book-page-id="13354">大数据学习路线</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/大数据技术栈思维导图.html" title="大数据技术栈思维导图" data-book-page-rel-url="notes/大数据技术栈思维导图.html" data-book-page-id="13355">大数据技术栈思维导图</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/大数据常用软件安装指南.html" title="大数据常用软件安装指南" data-book-page-rel-url="notes/大数据常用软件安装指南.html" data-book-page-id="13356">大数据常用软件安装指南</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Hadoop-HDFS.html" title="分布式文件存储系统 —— HDFS" data-book-page-rel-url="notes/Hadoop-HDFS.html" data-book-page-id="13357">分布式文件存储系统 —— HDFS</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Hadoop-MapReduce.html" title="分布式计算框架 —— MapReduce" data-book-page-rel-url="notes/Hadoop-MapReduce.html" data-book-page-id="13358">分布式计算框架 —— MapReduce</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Hadoop-YARN.html" title="集群资源管理器 —— YARN" data-book-page-rel-url="notes/Hadoop-YARN.html" data-book-page-id="13359">集群资源管理器 —— YARN</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/installation/Hadoop单机环境搭建.html" title="Hadoop 单机伪集群环境搭建" data-book-page-rel-url="notes/installation/Hadoop单机环境搭建.html" data-book-page-id="13360">Hadoop 单机伪集群环境搭建</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/installation/Hadoop集群环境搭建.html" title="Hadoop 集群环境搭建" data-book-page-rel-url="notes/installation/Hadoop集群环境搭建.html" data-book-page-id="13361">Hadoop 集群环境搭建</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/HDFS常用Shell命令.html" title="HDFS 常用 Shell 命令" data-book-page-rel-url="notes/HDFS常用Shell命令.html" data-book-page-id="13362">HDFS 常用 Shell 命令</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/HDFS-Java-API.html" title="HDFS Java API 的使用" data-book-page-rel-url="notes/HDFS-Java-API.html" data-book-page-id="13363">HDFS Java API 的使用</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/installation/基于Zookeeper搭建Hadoop高可用集群.html" title="基于 Zookeeper 搭建 Hadoop 高可用集群" data-book-page-rel-url="notes/installation/基于Zookeeper搭建Hadoop高可用集群.html" data-book-page-id="13364">基于 Zookeeper 搭建 Hadoop 高可用集群</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Hive简介及核心概念.html" title="Hive 简介及核心概念" data-book-page-rel-url="notes/Hive简介及核心概念.html" data-book-page-id="13365">Hive 简介及核心概念</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/installation/Linux环境下Hive的安装部署.html" title="Linux 环境下 Hive 的安装部署" data-book-page-rel-url="notes/installation/Linux环境下Hive的安装部署.html" data-book-page-id="13366">Linux 环境下 Hive 的安装部署</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/HiveCLI和Beeline命令行的基本使用.html" title="Hive CLI 和 Beeline 命令行的基本使用" data-book-page-rel-url="notes/HiveCLI和Beeline命令行的基本使用.html" data-book-page-id="13367">Hive CLI 和 Beeline 命令行的基本使用</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Hive常用DDL操作.html" title="Hive 常用 DDL 操作" data-book-page-rel-url="notes/Hive常用DDL操作.html" data-book-page-id="13368">Hive 常用 DDL 操作</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Hive分区表和分桶表.html" title="Hive 分区表和分桶表" data-book-page-rel-url="notes/Hive分区表和分桶表.html" data-book-page-id="13369">Hive 分区表和分桶表</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Hive视图和索引.html" title="Hive 视图和索引" data-book-page-rel-url="notes/Hive视图和索引.html" data-book-page-id="13370">Hive 视图和索引</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Hive常用DML操作.html" title="Hive 常用 DML 操作" data-book-page-rel-url="notes/Hive常用DML操作.html" data-book-page-id="13371">Hive 常用 DML 操作</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Hive数据查询详解.html" title="Hive 数据查询详解" data-book-page-rel-url="notes/Hive数据查询详解.html" data-book-page-id="13372">Hive 数据查询详解</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Spark简介.html" title="Spark 简介" data-book-page-rel-url="notes/Spark简介.html" data-book-page-id="13373">Spark 简介</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/installation/Spark开发环境搭建.html" title="Spark 开发环境搭建" data-book-page-rel-url="notes/installation/Spark开发环境搭建.html" data-book-page-id="13374">Spark 开发环境搭建</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Spark_RDD.html" title="弹性式数据集 RDD" data-book-page-rel-url="notes/Spark_RDD.html" data-book-page-id="13375">弹性式数据集 RDD</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Spark_Transformation和Action算子.html" title="RDD 常用算子详解" data-book-page-rel-url="notes/Spark_Transformation和Action算子.html" data-book-page-id="13376">RDD 常用算子详解</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Spark部署模式与作业提交.html" title="Spark 运行模式与作业提交" data-book-page-rel-url="notes/Spark部署模式与作业提交.html" data-book-page-id="13377">Spark 运行模式与作业提交</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Spark累加器与广播变量.html" title="Spark 累加器与广播变量" data-book-page-rel-url="notes/Spark累加器与广播变量.html" data-book-page-id="13378">Spark 累加器与广播变量</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/installation/Spark集群环境搭建.html" title="基于 Zookeeper 搭建 Spark 高可用集群" data-book-page-rel-url="notes/installation/Spark集群环境搭建.html" data-book-page-id="13379">基于 Zookeeper 搭建 Spark 高可用集群</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/SparkSQL_Dataset和DataFrame简介.html" title="DateFrame 和 DataSet" data-book-page-rel-url="notes/SparkSQL_Dataset和DataFrame简介.html" data-book-page-id="13380">DateFrame 和 DataSet</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Spark_Structured_API的基本使用.html" title="Structured API 的基本使用" data-book-page-rel-url="notes/Spark_Structured_API的基本使用.html" data-book-page-id="13381">Structured API 的基本使用</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/SparkSQL外部数据源.html" title="Spark SQL 外部数据源" data-book-page-rel-url="notes/SparkSQL外部数据源.html" data-book-page-id="13382">Spark SQL 外部数据源</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/SparkSQL常用聚合函数.html" title="Spark SQL 常用聚合函数" data-book-page-rel-url="notes/SparkSQL常用聚合函数.html" data-book-page-id="13383">Spark SQL 常用聚合函数</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/SparkSQL联结操作.html" title="Spark SQL JOIN 操作" data-book-page-rel-url="notes/SparkSQL联结操作.html" data-book-page-id="13384">Spark SQL JOIN 操作</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Spark_Streaming与流处理.html" title="Spark Streaming 简介" data-book-page-rel-url="notes/Spark_Streaming与流处理.html" data-book-page-id="13385">Spark Streaming 简介</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Spark_Streaming基本操作.html" title="Spark Streaming 基本操作" data-book-page-rel-url="notes/Spark_Streaming基本操作.html" data-book-page-id="13386">Spark Streaming 基本操作</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Spark_Streaming整合Flume.html" title="Spark Streaming 整合 Flume" data-book-page-rel-url="notes/Spark_Streaming整合Flume.html" data-book-page-id="13387">Spark Streaming 整合 Flume</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Spark_Streaming整合Kafka.html" title="Spark Streaming 整合 Kafka" data-book-page-rel-url="notes/Spark_Streaming整合Kafka.html" data-book-page-id="13388">Spark Streaming 整合 Kafka</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Storm和流处理简介.html" title="Storm 和流处理简介" data-book-page-rel-url="notes/Storm和流处理简介.html" data-book-page-id="13389">Storm 和流处理简介</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Storm核心概念详解.html" title="Storm 核心概念详解" data-book-page-rel-url="notes/Storm核心概念详解.html" data-book-page-id="13390">Storm 核心概念详解</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/installation/Storm单机环境搭建.html" title="Storm 单机环境搭建" data-book-page-rel-url="notes/installation/Storm单机环境搭建.html" data-book-page-id="13391">Storm 单机环境搭建</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/installation/Storm集群环境搭建.html" title="Storm 集群环境搭建" data-book-page-rel-url="notes/installation/Storm集群环境搭建.html" data-book-page-id="13392">Storm 集群环境搭建</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Storm编程模型详解.html" title="Storm 编程模型详解" data-book-page-rel-url="notes/Storm编程模型详解.html" data-book-page-id="13393">Storm 编程模型详解</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Storm三种打包方式对比分析.html" title="Storm 项目三种打包方式对比分析" data-book-page-rel-url="notes/Storm三种打包方式对比分析.html" data-book-page-id="13394">Storm 项目三种打包方式对比分析</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Storm集成Redis详解.html" title="Storm 集成 Redis 详解" data-book-page-rel-url="notes/Storm集成Redis详解.html" data-book-page-id="13395">Storm 集成 Redis 详解</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Storm集成HBase和HDFS.html" title="Storm 集成 HDFS/HBase" data-book-page-rel-url="notes/Storm集成HBase和HDFS.html" data-book-page-id="13396">Storm 集成 HDFS/HBase</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Storm集成Kakfa.html" title="Storm 集成 Kafka" data-book-page-rel-url="notes/Storm集成Kakfa.html" data-book-page-id="13397">Storm 集成 Kafka</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Flink核心概念综述.html" title="Flink 核心概念综述" data-book-page-rel-url="notes/Flink核心概念综述.html" data-book-page-id="13398">Flink 核心概念综述</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Flink开发环境搭建.html" title="Flink 开发环境搭建" data-book-page-rel-url="notes/Flink开发环境搭建.html" data-book-page-id="13399">Flink 开发环境搭建</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Flink_Data_Source.html" title="Flink Data Source" data-book-page-rel-url="notes/Flink_Data_Source.html" data-book-page-id="13400">Flink Data Source</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Flink_Data_Transformation.html" title="Flink Data Transformation" data-book-page-rel-url="notes/Flink_Data_Transformation.html" data-book-page-id="13401">Flink Data Transformation</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Flink_Data_Sink.html" title="Flink Data Sink" data-book-page-rel-url="notes/Flink_Data_Sink.html" data-book-page-id="13402">Flink Data Sink</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Flink_Windows.html" title="Flink 窗口模型" data-book-page-rel-url="notes/Flink_Windows.html" data-book-page-id="13403">Flink 窗口模型</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Flink状态管理与检查点机制.html" title="Flink 状态管理与检查点机制" data-book-page-rel-url="notes/Flink状态管理与检查点机制.html" data-book-page-id="13404">Flink 状态管理与检查点机制</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/installation/Flink_Standalone_Cluster.html" title="Flink Standalone 集群部署" data-book-page-rel-url="notes/installation/Flink_Standalone_Cluster.html" data-book-page-id="13405">Flink Standalone 集群部署</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Hbase简介.html" title="Hbase 简介" data-book-page-rel-url="notes/Hbase简介.html" data-book-page-id="13406">Hbase 简介</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Hbase系统架构及数据结构.html" title="HBase 系统架构及数据结构" data-book-page-rel-url="notes/Hbase系统架构及数据结构.html" data-book-page-id="13407">HBase 系统架构及数据结构</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/installation/HBase单机环境搭建.html" title="HBase 基本环境搭建 (Standalone /pseudo-distributed mode)" data-book-page-rel-url="notes/installation/HBase单机环境搭建.html" data-book-page-id="13408">HBase 基本环境搭建 (Standalone /pseudo-distributed mode)</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/installation/HBase集群环境搭建.html" title="HBase 集群环境搭建" data-book-page-rel-url="notes/installation/HBase集群环境搭建.html" data-book-page-id="13409">HBase 集群环境搭建</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Hbase_Shell.html" title="HBase 常用 Shell 命令" data-book-page-rel-url="notes/Hbase_Shell.html" data-book-page-id="13410">HBase 常用 Shell 命令</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Hbase_Java_API.html" title="HBase Java API" data-book-page-rel-url="notes/Hbase_Java_API.html" data-book-page-id="13411">HBase Java API</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Hbase过滤器详解.html" title="HBase 过滤器详解" data-book-page-rel-url="notes/Hbase过滤器详解.html" data-book-page-id="13412">HBase 过滤器详解</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Hbase协处理器详解.html" title="HBase 协处理器详解" data-book-page-rel-url="notes/Hbase协处理器详解.html" data-book-page-id="13413">HBase 协处理器详解</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Hbase容灾与备份.html" title="HBase 容灾与备份" data-book-page-rel-url="notes/Hbase容灾与备份.html" data-book-page-id="13414">HBase 容灾与备份</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Hbase的SQL中间层_Phoenix.html" title="HBase的 SQL 中间层 —— Phoenix" data-book-page-rel-url="notes/Hbase的SQL中间层_Phoenix.html" data-book-page-id="13415">HBase的 SQL 中间层 —— Phoenix</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Spring+Mybtais+Phoenix整合.html" title="Spring/Spring Boot 整合 Mybatis + Phoenix" data-book-page-rel-url="notes/Spring+Mybtais+Phoenix整合.html" data-book-page-id="13416">Spring/Spring Boot 整合 Mybatis + Phoenix</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Kafka简介.html" title="Kafka 简介" data-book-page-rel-url="notes/Kafka简介.html" data-book-page-id="13417">Kafka 简介</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/installation/基于Zookeeper搭建Kafka高可用集群.html" title="基于 Zookeeper 搭建 Kafka 高可用集群" data-book-page-rel-url="notes/installation/基于Zookeeper搭建Kafka高可用集群.html" data-book-page-id="13418">基于 Zookeeper 搭建 Kafka 高可用集群</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Kafka生产者详解.html" title="Kafka 生产者详解" data-book-page-rel-url="notes/Kafka生产者详解.html" data-book-page-id="13419">Kafka 生产者详解</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Kafka消费者详解.html" title="Kafka 消费者详解" data-book-page-rel-url="notes/Kafka消费者详解.html" data-book-page-id="13420">Kafka 消费者详解</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Kafka深入理解分区副本机制.html" title="深入理解 Kafka 副本机制" data-book-page-rel-url="notes/Kafka深入理解分区副本机制.html" data-book-page-id="13421">深入理解 Kafka 副本机制</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Zookeeper简介及核心概念.html" title="Zookeeper 简介及核心概念" data-book-page-rel-url="notes/Zookeeper简介及核心概念.html" data-book-page-id="13422">Zookeeper 简介及核心概念</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/installation/Zookeeper单机环境和集群环境搭建.html" title="Zookeeper 单机环境和集群环境搭建" data-book-page-rel-url="notes/installation/Zookeeper单机环境和集群环境搭建.html" data-book-page-id="13423">Zookeeper 单机环境和集群环境搭建</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Zookeeper常用Shell命令.html" title="Zookeeper 常用 Shell 命令" data-book-page-rel-url="notes/Zookeeper常用Shell命令.html" data-book-page-id="13424">Zookeeper 常用 Shell 命令</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Zookeeper_Java客户端Curator.html" title="Zookeeper Java 客户端 —— Apache Curator" data-book-page-rel-url="notes/Zookeeper_Java客户端Curator.html" data-book-page-id="13425">Zookeeper Java 客户端 —— Apache Curator</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Zookeeper_ACL权限控制.html" title="Zookeeper  ACL 权限控制" data-book-page-rel-url="notes/Zookeeper_ACL权限控制.html" data-book-page-id="13426">Zookeeper ACL 权限控制</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Flume简介及基本使用.html" title="Flume 简介及基本使用" data-book-page-rel-url="notes/Flume简介及基本使用.html" data-book-page-id="13427">Flume 简介及基本使用</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/installation/Linux下Flume的安装.html" title="Linux 环境下 Flume 的安装部署" data-book-page-rel-url="notes/installation/Linux下Flume的安装.html" data-book-page-id="13428">Linux 环境下 Flume 的安装部署</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Flume整合Kafka.html" title="Flume 整合 Kafka" data-book-page-rel-url="notes/Flume整合Kafka.html" data-book-page-id="13429">Flume 整合 Kafka</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Sqoop简介与安装.html" title="Sqoop 简介与安装" data-book-page-rel-url="notes/Sqoop简介与安装.html" data-book-page-id="13430">Sqoop 简介与安装</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Sqoop基本使用.html" title="Sqoop 的基本使用" data-book-page-rel-url="notes/Sqoop基本使用.html" data-book-page-id="13431">Sqoop 的基本使用</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Azkaban简介.html" title="Azkaban 简介" data-book-page-rel-url="notes/Azkaban简介.html" data-book-page-id="13432">Azkaban 简介</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/installation/Azkaban_3.x_编译及部署.html" title="Azkaban3.x 编译及部署" data-book-page-rel-url="notes/installation/Azkaban_3.x_编译及部署.html" data-book-page-id="13433">Azkaban3.x 编译及部署</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Azkaban_Flow_1.0_的使用.html" title="Azkaban Flow 1.0 的使用" data-book-page-rel-url="notes/Azkaban_Flow_1.0_的使用.html" data-book-page-id="13434">Azkaban Flow 1.0 的使用</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Azkaban_Flow_2.0_的使用.html" title="Azkaban Flow 2.0 的使用" data-book-page-rel-url="notes/Azkaban_Flow_2.0_的使用.html" data-book-page-id="13435">Azkaban Flow 2.0 的使用</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Scala简介及开发环境配置.html" title="Scala 简介及开发环境配置" data-book-page-rel-url="notes/Scala简介及开发环境配置.html" data-book-page-id="13436">Scala 简介及开发环境配置</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Scala基本数据类型和运算符.html" title="基本数据类型和运算符" data-book-page-rel-url="notes/Scala基本数据类型和运算符.html" data-book-page-id="13437">基本数据类型和运算符</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Scala流程控制语句.html" title="流程控制语句" data-book-page-rel-url="notes/Scala流程控制语句.html" data-book-page-id="13438">流程控制语句</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Scala数组.html" title="数组 —— Array" data-book-page-rel-url="notes/Scala数组.html" data-book-page-id="13439">数组 —— Array</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Scala集合类型.html" title="集合类型综述" data-book-page-rel-url="notes/Scala集合类型.html" data-book-page-id="13440">集合类型综述</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Scala列表和集.html" title="常用集合类型之 —— List & Set" data-book-page-rel-url="notes/Scala列表和集.html" data-book-page-id="13441">常用集合类型之 —— List & Set</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Scala映射和元组.html" title="常用集合类型之 —— Map & Tuple" data-book-page-rel-url="notes/Scala映射和元组.html" data-book-page-id="13442">常用集合类型之 —— Map & Tuple</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Scala类和对象.html" title="类和对象" data-book-page-rel-url="notes/Scala类和对象.html" data-book-page-id="13443">类和对象</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Scala继承和特质.html" title="继承和特质" data-book-page-rel-url="notes/Scala继承和特质.html" data-book-page-id="13444">继承和特质</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Scala函数和闭包.html" title="函数 & 闭包 & 柯里化" data-book-page-rel-url="notes/Scala函数和闭包.html" data-book-page-id="13445">函数 & 闭包 & 柯里化</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Scala模式匹配.html" title="模式匹配" data-book-page-rel-url="notes/Scala模式匹配.html" data-book-page-id="13446">模式匹配</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Scala类型参数.html" title="类型参数" data-book-page-rel-url="notes/Scala类型参数.html" data-book-page-id="13447">类型参数</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/Scala隐式转换和隐式参数.html" title="隐式转换和隐式参数" data-book-page-rel-url="notes/Scala隐式转换和隐式参数.html" data-book-page-id="13448">隐式转换和隐式参数</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/大数据应用常用打包方式.html" title="大数据应用常用打包方式" data-book-page-rel-url="notes/大数据应用常用打包方式.html" data-book-page-id="13449">大数据应用常用打包方式</a>
</li>
<li>
<a class="pjax" href="../../../book/198/notes/资料分享与工具推荐.html" title="资料分享与开发工具推荐" data-book-page-rel-url="notes/资料分享与工具推荐.html" data-book-page-id="13450">资料分享与开发工具推荐</a>
</li>
</ul>
</div>
</div>
<script src="https://cdn.staticfile.net/jquery/1.12.4/jquery.min.js"></script>
<script type="text/javascript" src="../../../static/components/uikit-2.27.5/js/uikit.reader.js"></script>
<script type="text/javascript" src="../../../static/components/social-share/social-share.min.js"></script>
<script>(function(){var bp =document.createElement('script');var curProtocol =window.location.protocol.split(':')[0];if (curProtocol ==='https') {bp.src ='https://zz.bdstatic.com/linksubmit/push.js';}
else {bp.src ='http://push.zhanzhang.baidu.com/push.js';}
var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(bp,s);})();</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
<script src="https://cdn.staticfile.net/highlight.js/9.12.0/highlight.min.js"></script>
<script src="https://cdn.staticfile.net/jquery.pjax/2.0.1/jquery.pjax.min.js"></script>
<script src="https://cdn.staticfile.net/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="https://cdn.staticfile.net/uikit/2.27.5/js/components/lightbox.min.js"></script>
<link rel="dns-prefetch" href="../../..//cdn.mathjax.org" />
<script type="text/x-mathjax-config">
 function initMathJax() {
    var mathId = $("book-content-section")[0];
    MathJax.Hub.Config({
        tex2jax: {skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code','a']},
        showProcessingMessages: false,
        messageStyle: "none"
    });
    MathJax.Hub.Queue(["Typeset",MathJax.Hub,mathId]);
 };
initMathJax();
</script>
<script src='https://cdn.staticfile.net/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML' async></script>
<style>
	.MathJax_Display{display:inline!important;}
</style>
<script type="text/javascript" src="../../../static/components/js/reader.js"></script>
<script type="text/javascript">var bookId =198;var bookPageId =13382;var bookPageRelUrl ='notes/SparkSQL外部数据源.html';</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
</body>
</html>