
<!DOCTYPE html>
<html lang="zh-CN">
<head>
<title>3.3. 模型评估: 量化预测的质量-scikit-learn (sklearn) 官方文档中文版</title>
<meta content='3.3. 模型评估: 量化预测的质量,scikit-learn (sklearn) 官方文档中文版' name='keywords'>
<meta content='3.3. 模型评估: 量化预测的质量,scikit-learn (sklearn) 官方文档中文版' name='description'>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta http-equiv="Content-Language" content="zh-CN" />
<meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1, maximum-scale=1, user-scalable=no"../../../>
<meta name="applicable-device" content="pc,mobile">
<link rel="shortcut icon" href="../../../favicon.ico" type="image/x-icon" />
<meta name="renderer" content="webkit">
<link rel="stylesheet" href="../../../static/components/uikit-2.27.5/css/uikit.custom.css">
<link rel="stylesheet" href="../../../static/components/social-share/social-share.min.css">
<link rel="stylesheet" href="../../../static/components/highlight/styles/custom.css">
<link rel="stylesheet" href="../../../static/components/css/base.css">
<link rel="stylesheet" href="../../../static/components/css/reader.css">
<link rel="stylesheet" href="../../../static/components/css/markdown.css">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5313208362165053" crossorigin="anonymous"></script>
</head>
<body>
<div class=" book-main-wrap uk-container uk-container-center uk-margin-top ">
<div class="uk-grid">
<div class="uk-width-1-1 reader-wrap ">
<div class=" bottom-nav uk-clearfix ">
<div class="uk-align-left ">
<a href="../../../book/170/docs/35.html">
<i class="nav-icon-left uk-icon-small  uk-icon-caret-left"></i>
<span class="">3.2. 调整估计器的..</span>
</a>
</div>
<div class="uk-align-right ">
<a href="../../../book/170/docs/54.html">
<span class="">3.4. 模型持久化</span>
<i class="nav-icon-right uk-icon-small  uk-icon-caret-right"></i>
</a>
</div>
</div>
<div class="uk-text-center">
<h2 class="book-page-title uk-container-center">
<a href="../../../book/170/index.html">scikit-learn (sklearn) 官方文档中文版</a>
<a target="_blank" rel="nofollow" href="https://github.com/apachecn/scikit-learn-doc-zh" class="uk-icon-button uk-icon-github" title="github项目地址"></a>
</h2>
</div>
<script type="text/javascript" src="../../../static/components/js/app_intro.js"></script>
<ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-5313208362165053" data-ad-slot="1328047120"></ins>
<script>(adsbygoogle =window.adsbygoogle ||[]).push({});</script>
<hr class="uk-article-divider">
<div class="book-content-section  md-content-section  uk-margin-bottom">
<h1 id="33-模型评估-量化预测的质量">3.3. 模型评估: 量化预测的质量</h1>
<p>校验者: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@飓风</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@小瑶</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@FAME</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@v</a> 翻译者: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@小瑶</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@片刻</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@那伊抹微笑</a></p>
<p>有 3 种不同的 API 用于评估模型预测的质量:</p>
<ul>
<li><strong>Estimator score method（估计器得分的方法）</strong>: Estimators（估计器）有一个 <code>score（得分）</code> 方法，为其解决的问题提供了默认的 evaluation criterion （评估标准）。 在这个页面上没有相关讨论，但是在每个 estimator （估计器）的文档中会有相关的讨论。</li>
<li><strong>Scoring parameter（评分参数）</strong>: Model-evaluation tools （模型评估工具）使用 <a href="cross_validation.html#cross-validation">cross-validation</a> (如 <a href="generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score" title="sklearn.model_selection.cross_val_score"><code>model_selection.cross_val_score</code></a> 和 <a href="generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV" title="sklearn.model_selection.GridSearchCV"><code>model_selection.GridSearchCV</code></a>) 依靠 internal <em>scoring</em> strategy （内部 <em>scoring（得分）</em> 策略）。这在 <a href="#scoring-parameter">scoring 参数: 定义模型评估规则</a> 部分讨论。</li>
<li><strong>Metric functions（指标函数）</strong>: <code>metrics</code> 模块实现了针对特定目的评估预测误差的函数。这些指标在以下部分部分详细介绍 <a href="#classification-metrics">分类指标</a>, <a href="#multilabel-ranking-metrics">多标签排名指标</a>, <a href="#regression-metrics">回归指标</a> 和 <a href="#clustering-metrics">聚类指标</a> 。</li>
</ul>
<p>最后， <a href="#dummy-estimators">虚拟估计</a> 用于获取随机预测的这些指标的基准值。</p>
<p>See also</p>
<p>对于 “pairwise（成对）” metrics（指标），<em>samples（样本）</em> 之间而不是 estimators （估计量）或者 predictions（预测值），请参阅 <a href="metrics.html#metrics">成对的矩阵, 类别和核函数</a> 部分。</p>
<h2 id="331-scoring-参数-定义模型评估规则">3.3.1. <code>scoring</code> 参数: 定义模型评估规则</h2>
<p>Model selection （模型选择）和 evaluation （评估）使用工具，例如 <a href="generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV" title="sklearn.model_selection.GridSearchCV"><code>model_selection.GridSearchCV</code></a> 和 <a href="generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score" title="sklearn.model_selection.cross_val_score"><code>model_selection.cross_val_score</code></a> ，采用 <code>scoring</code> 参数来控制它们对 estimators evaluated （评估的估计量）应用的指标。</p>
<h3 id="3311-常见场景-预定义值">3.3.1.1. 常见场景: 预定义值</h3>
<p>对于最常见的用例, 您可以使用 <code>scoring</code> 参数指定一个 scorer object （记分对象）; 下表显示了所有可能的值。 所有 scorer objects （记分对象）遵循惯例 <strong>higher return values are better than lower return values（较高的返回值优于较低的返回值）</strong> 。因此，测量模型和数据之间距离的 metrics （度量），如 <a href="generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error" title="sklearn.metrics.mean_squared_error"><code>metrics.mean_squared_error</code></a> 可用作返回 metric （指数）的 negated value （否定值）的 neg_mean_squared_error 。</p>
<table>
<thead>
<tr>
<th>Scoring（得分）</th>
<th>Function（函数）</th>
<th>Comment（注解）</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Classification（分类）</strong></td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr>
<td>‘accuracy’</td>
<td><a href="generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score" title="sklearn.metrics.accuracy_score"><code>metrics.accuracy_score</code></a></td>
<td>&nbsp;</td>
</tr>
<tr>
<td>‘average_precision’</td>
<td><a href="generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score" title="sklearn.metrics.average_precision_score"><code>metrics.average_precision_score</code></a></td>
<td>&nbsp;</td>
</tr>
<tr>
<td>‘f1’</td>
<td><a href="generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score"><code>metrics.f1_score</code></a></td>
<td>for binary targets（用于二进制目标）</td>
</tr>
<tr>
<td>‘f1_micro’</td>
<td><a href="generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score"><code>metrics.f1_score</code></a></td>
<td>micro-averaged（微平均）</td>
</tr>
<tr>
<td>‘f1_macro’</td>
<td><a href="generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score"><code>metrics.f1_score</code></a></td>
<td>macro-averaged（微平均）</td>
</tr>
<tr>
<td>‘f1_weighted’</td>
<td><a href="generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score"><code>metrics.f1_score</code></a></td>
<td>weighted average（加权平均）</td>
</tr>
<tr>
<td>‘f1_samples’</td>
<td><a href="generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score"><code>metrics.f1_score</code></a></td>
<td>by multilabel sample（通过 multilabel 样本）</td>
</tr>
<tr>
<td>‘neg_log_loss’</td>
<td><a href="generated/sklearn.metrics.log_loss.html#sklearn.metrics.log_loss" title="sklearn.metrics.log_loss"><code>metrics.log_loss</code></a></td>
<td>requires <code>predict_proba</code> support（需要 <code>predict_proba</code> 支持）</td>
</tr>
<tr>
<td>‘precision’ etc.</td>
<td><a href="generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score" title="sklearn.metrics.precision_score"><code>metrics.precision_score</code></a></td>
<td>suffixes apply as with ‘f1’（后缀适用于 ‘f1’）</td>
</tr>
<tr>
<td>‘recall’ etc.</td>
<td><a href="generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score" title="sklearn.metrics.recall_score"><code>metrics.recall_score</code></a></td>
<td>suffixes apply as with ‘f1’（后缀适用于 ‘f1’）</td>
</tr>
<tr>
<td>‘roc_auc’</td>
<td><a href="generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score" title="sklearn.metrics.roc_auc_score"><code>metrics.roc_auc_score</code></a></td>
<td>&nbsp;</td>
</tr>
<tr>
<td><strong>Clustering（聚类）</strong></td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr>
<td>‘adjusted_mutual_info_score’</td>
<td><a href="generated/sklearn.metrics.adjusted_mutual_info_score.html#sklearn.metrics.adjusted_mutual_info_score" title="sklearn.metrics.adjusted_mutual_info_score"><code>metrics.adjusted_mutual_info_score</code></a></td>
<td>&nbsp;</td>
</tr>
<tr>
<td>‘adjusted_rand_score’</td>
<td><a href="generated/sklearn.metrics.adjusted_rand_score.html#sklearn.metrics.adjusted_rand_score" title="sklearn.metrics.adjusted_rand_score"><code>metrics.adjusted_rand_score</code></a></td>
<td>&nbsp;</td>
</tr>
<tr>
<td>‘completeness_score’</td>
<td><a href="generated/sklearn.metrics.completeness_score.html#sklearn.metrics.completeness_score" title="sklearn.metrics.completeness_score"><code>metrics.completeness_score</code></a></td>
<td>&nbsp;</td>
</tr>
<tr>
<td>‘fowlkes_mallows_score’</td>
<td><a href="generated/sklearn.metrics.fowlkes_mallows_score.html#sklearn.metrics.fowlkes_mallows_score" title="sklearn.metrics.fowlkes_mallows_score"><code>metrics.fowlkes_mallows_score</code></a></td>
<td>&nbsp;</td>
</tr>
<tr>
<td>‘homogeneity_score’</td>
<td><a href="generated/sklearn.metrics.homogeneity_score.html#sklearn.metrics.homogeneity_score" title="sklearn.metrics.homogeneity_score"><code>metrics.homogeneity_score</code></a></td>
<td>&nbsp;</td>
</tr>
<tr>
<td>‘mutual_info_score’</td>
<td><a href="generated/sklearn.metrics.mutual_info_score.html#sklearn.metrics.mutual_info_score" title="sklearn.metrics.mutual_info_score"><code>metrics.mutual_info_score</code></a></td>
<td>&nbsp;</td>
</tr>
<tr>
<td>‘normalized_mutual_info_score’</td>
<td><a href="generated/sklearn.metrics.normalized_mutual_info_score.html#sklearn.metrics.normalized_mutual_info_score" title="sklearn.metrics.normalized_mutual_info_score"><code>metrics.normalized_mutual_info_score</code></a></td>
<td>&nbsp;</td>
</tr>
<tr>
<td>‘v_measure_score’</td>
<td><a href="generated/sklearn.metrics.v_measure_score.html#sklearn.metrics.v_measure_score" title="sklearn.metrics.v_measure_score"><code>metrics.v_measure_score</code></a></td>
<td>&nbsp;</td>
</tr>
<tr>
<td><strong>Regression（回归）</strong></td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr>
<td>‘explained_variance’</td>
<td><a href="generated/sklearn.metrics.explained_variance_score.html#sklearn.metrics.explained_variance_score" title="sklearn.metrics.explained_variance_score"><code>metrics.explained_variance_score</code></a></td>
<td>&nbsp;</td>
</tr>
<tr>
<td>‘neg_mean_absolute_error’</td>
<td><a href="generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error" title="sklearn.metrics.mean_absolute_error"><code>metrics.mean_absolute_error</code></a></td>
<td>&nbsp;</td>
</tr>
<tr>
<td>‘neg_mean_squared_error’</td>
<td><a href="generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error" title="sklearn.metrics.mean_squared_error"><code>metrics.mean_squared_error</code></a></td>
<td>&nbsp;</td>
</tr>
<tr>
<td>‘neg_mean_squared_log_error’</td>
<td><a href="generated/sklearn.metrics.mean_squared_log_error.html#sklearn.metrics.mean_squared_log_error" title="sklearn.metrics.mean_squared_log_error"><code>metrics.mean_squared_log_error</code></a></td>
<td>&nbsp;</td>
</tr>
<tr>
<td>‘neg_median_absolute_error’</td>
<td><a href="generated/sklearn.metrics.median_absolute_error.html#sklearn.metrics.median_absolute_error" title="sklearn.metrics.median_absolute_error"><code>metrics.median_absolute_error</code></a></td>
<td>&nbsp;</td>
</tr>
<tr>
<td>‘r2’</td>
<td><a href="generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score" title="sklearn.metrics.r2_score"><code>metrics.r2_score</code></a></td>
<td>&nbsp;</td>
</tr>
</tbody>
</table>
<p>使用案例:</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn import svm, datasets
&gt;&gt;&gt; from sklearn.model_selection import cross_val_score
&gt;&gt;&gt; iris = datasets.load_iris()
&gt;&gt;&gt; X, y = iris.data, iris.target
&gt;&gt;&gt; clf = svm.SVC(probability=True, random_state=0)
&gt;&gt;&gt; cross_val_score(clf, X, y, scoring='neg_log_loss') 
array([-0.07..., -0.16..., -0.06...])
&gt;&gt;&gt; model = svm.SVC()
&gt;&gt;&gt; cross_val_score(model, X, y, scoring='wrong_choice')
Traceback (most recent call last):
ValueError: 'wrong_choice' is not a valid scoring value. Valid options are ['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', 'completeness_score', 'explained_variance', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', 'mutual_info_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'normalized_mutual_info_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'v_measure_score']

</code></pre>
<p>Note</p>
<p>ValueError exception 列出的值对应于以下部分描述的 functions measuring prediction accuracy （测量预测精度的函数）。 这些函数的 scorer objects （记分对象）存储在 dictionary <code>sklearn.metrics.SCORERS</code> 中。</p>
<h3 id="3312-根据-metric-函数定义您的评分策略">3.3.1.2. 根据 metric 函数定义您的评分策略</h3>
<p>模块 <a href="classes.html#module-sklearn.metrics" title="sklearn.metrics"><code>sklearn.metrics</code></a> 还公开了一组 measuring a prediction error （测量预测误差）的简单函数，给出了基础真实的数据和预测:</p>
<ul>
<li>函数以 <code>_score</code> 结尾返回一个值来最大化，越高越好。</li>
<li>函数 <code>_error</code> 或 <code>_loss</code> 结尾返回一个值来 minimize （最小化），越低越好。当使用 <a href="generated/sklearn.metrics.make_scorer.html#sklearn.metrics.make_scorer" title="sklearn.metrics.make_scorer"><code>make_scorer</code></a> 转换成 scorer object （记分对象）时，将 <code>greater_is_better</code> 参数设置为 False（默认为 True; 请参阅下面的参数说明）。</li>
</ul>
<p>可用于各种机器学习任务的 Metrics （指标）在下面详细介绍。</p>
<p>许多 metrics （指标）没有被用作 <code>scoring（得分）</code> 值的名称，有时是因为它们需要额外的参数，例如 <a href="generated/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score" title="sklearn.metrics.fbeta_score"><code>fbeta_score</code></a> 。在这种情况下，您需要生成一个适当的 scoring object （评分对象）。生成 callable object for scoring （可评估对象进行评分）的最简单方法是使用 <a href="generated/sklearn.metrics.make_scorer.html#sklearn.metrics.make_scorer" title="sklearn.metrics.make_scorer"><code>make_scorer</code></a> 。该函数将 metrics （指数）转换为可用于可调用的 model evaluation （模型评估）。</p>
<p>一个典型的用例是从库中包含一个非默认值参数的 existing metric function （现有指数函数），例如 <a href="generated/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score" title="sklearn.metrics.fbeta_score"><code>fbeta_score</code></a> 函数的 <code>beta</code> 参数:</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn.metrics import fbeta_score, make_scorer
&gt;&gt;&gt; ftwo_scorer = make_scorer(fbeta_score, beta=2)
&gt;&gt;&gt; from sklearn.model_selection import GridSearchCV
&gt;&gt;&gt; from sklearn.svm import LinearSVC
&gt;&gt;&gt; grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]}, scoring=ftwo_scorer)

</code></pre>
<p>第二个用例是使用 <a href="generated/sklearn.metrics.make_scorer.html#sklearn.metrics.make_scorer" title="sklearn.metrics.make_scorer"><code>make_scorer</code></a> 从简单的 python 函数构建一个完全 custom scorer object （自定义的记分对象），可以使用几个参数 :</p>
<ul>
<li>你要使用的 python 函数（在下面的例子中是 <code>my_custom_loss_func</code>）</li>
<li>python 函数是否返回一个分数 (<code>greater_is_better=True</code>, 默认值) 或者一个 loss （损失） (<code>greater_is_better=False</code>)。 如果是一个 loss （损失），scorer object （记分对象）的 python 函数的输出被 negated （否定），符合 cross validation convention （交叉验证约定），scorers 为更好的模型返回更高的值。</li>
<li>仅用于 classification metrics （分类指数）: 您提供的 python 函数是否需要连续的 continuous decision certainties （判断确定性）（<code>needs_threshold=True</code>）。默认值为 False 。</li>
<li>任何其他参数，如 <code>beta</code> 或者 <code>labels</code> 在 函数 <a href="generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score"><code>f1_score</code></a> 。</li>
</ul>
<p>以下是建立 custom scorers （自定义记分对象）的示例，并使用 <code>greater_is_better</code> 参数:</p>
<pre><code class="language-py">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; def my_custom_loss_func(ground_truth, predictions):
...     diff = np.abs(ground_truth - predictions).max()
...     return np.log(1 + diff)
...
&gt;&gt;&gt; # loss_func will negate the return value of my_custom_loss_func,
&gt;&gt;&gt; #  which will be np.log(2), 0.693, given the values for ground_truth
&gt;&gt;&gt; #  and predictions defined below.
&gt;&gt;&gt; loss  = make_scorer(my_custom_loss_func, greater_is_better=False)
&gt;&gt;&gt; score = make_scorer(my_custom_loss_func, greater_is_better=True)
&gt;&gt;&gt; ground_truth = [[1], [1]]
&gt;&gt;&gt; predictions  = [0, 1]
&gt;&gt;&gt; from sklearn.dummy import DummyClassifier
&gt;&gt;&gt; clf = DummyClassifier(strategy='most_frequent', random_state=0)
&gt;&gt;&gt; clf = clf.fit(ground_truth, predictions)
&gt;&gt;&gt; loss(clf,ground_truth, predictions) 
-0.69...
&gt;&gt;&gt; score(clf,ground_truth, predictions) 
0.69...

</code></pre>
<h3 id="3313-实现自己的记分对象">3.3.1.3. 实现自己的记分对象</h3>
<p>您可以通过从头开始构建自己的 scoring object （记分对象），而不使用 <a href="generated/sklearn.metrics.make_scorer.html#sklearn.metrics.make_scorer" title="sklearn.metrics.make_scorer"><code>make_scorer</code></a> factory 来生成更加灵活的 model scorers （模型记分对象）。 对于被叫做 scorer 来说，它需要符合以下两个规则所指定的协议:</p>
<ul>
<li>可以使用参数 <code>(estimator, X, y)</code> 来调用它，其中 <code>estimator</code> 是要被评估的模型，<code>X</code> 是验证数据， <code>y</code> 是 <code>X</code> (在有监督情况下) 或 <code>None</code> (在无监督情况下) 已经被标注的真实数据目标。</li>
<li>它返回一个浮点数，用于对 <code>X</code> 进行量化 <code>estimator</code> 的预测质量，参考 <code>y</code> 。 再次，按照惯例，更高的数字更好，所以如果你的 scorer 返回 loss ，那么这个值应该被 negated 。</li>
</ul>
<h3 id="3314-使用多个指数评估">3.3.1.4. 使用多个指数评估</h3>
<p>Scikit-learn 还允许在 <code>GridSearchCV</code>, <code>RandomizedSearchCV</code> 和 <code>cross_validate</code> 中评估 multiple metric （多个指数）。</p>
<p>为 <code>scoring</code> 参数指定多个评分指标有两种方法:</p>
<ul>
<li> <pre><code class="language-py">As an iterable of string metrics（作为 string metrics 的迭代）::
</code></pre> <pre><code class="language-py">&amp;gt;&amp;gt;&amp;gt; scoring = ['accuracy', 'precision']

</code></pre> </li>
<li> <pre><code class="language-py">As a dict mapping the scorer name to the scoring function（作为 dict ，将 scorer 名称映射到 scoring 函数）::
</code></pre> <pre><code class="language-py">&amp;gt;&amp;gt;&amp;gt; from sklearn.metrics import accuracy_score
&amp;gt;&amp;gt;&amp;gt; from sklearn.metrics import make_scorer
&amp;gt;&amp;gt;&amp;gt; scoring = {'accuracy': make_scorer(accuracy_score),
...            'prec': 'precision'}

</code></pre> </li>
</ul>
<p>请注意， dict 值可以是 scorer functions （记分函数）或者 predefined metric strings （预定义 metric 字符串）之一。</p>
<p>目前，只有那些返回 single score （单一分数）的 scorer functions （记分函数）才能在 dict 内传递。不允许返回多个值的 Scorer functions （Scorer 函数），并且需要一个 wrapper 才能返回 single metric（单个指标）:</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn.model_selection import cross_validate
&gt;&gt;&gt; from sklearn.metrics import confusion_matrix
&gt;&gt;&gt; # A sample toy binary classification dataset
&gt;&gt;&gt; X, y = datasets.make_classification(n_classes=2, random_state=0)
&gt;&gt;&gt; svm = LinearSVC(random_state=0)
&gt;&gt;&gt; def tp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 0]
&gt;&gt;&gt; def tn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 0]
&gt;&gt;&gt; def fp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 0]
&gt;&gt;&gt; def fn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 1]
&gt;&gt;&gt; scoring = {'tp' : make_scorer(tp), 'tn' : make_scorer(tn),
...            'fp' : make_scorer(fp), 'fn' : make_scorer(fn)}
&gt;&gt;&gt; cv_results = cross_validate(svm.fit(X, y), X, y, scoring=scoring)
&gt;&gt;&gt; # Getting the test set true positive scores
&gt;&gt;&gt; print(cv_results['test_tp'])          
[12 13 15]
&gt;&gt;&gt; # Getting the test set false negative scores
&gt;&gt;&gt; print(cv_results['test_fn'])          
[5 4 1]

</code></pre>
<h2 id="332-分类指标">3.3.2. 分类指标</h2>
<p><a href="classes.html#module-sklearn.metrics" title="sklearn.metrics"><code>sklearn.metrics</code></a> 模块实现了几个 loss, score, 和 utility 函数来衡量 classification （分类）性能。 某些 metrics （指标）可能需要 positive class （正类），confidence values（置信度值）或 binary decisions values （二进制决策值）的概率估计。 大多数的实现允许每个样本通过 <code>sample_weight</code> 参数为 overall score （总分）提供 weighted contribution （加权贡献）。</p>
<p>其中一些仅限于二分类案例:</p>
<p>| <a href="generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve" title="sklearn.metrics.precision_recall_curve"><code>precision_recall_curve</code></a>(y_true,&nbsp;probas_pred) | Compute precision-recall pairs for different probability thresholds | | <a href="generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve" title="sklearn.metrics.roc_curve"><code>roc_curve</code></a>(y_true,&nbsp;y_score[,&nbsp;pos_label,&nbsp;…]) | Compute Receiver operating characteristic (ROC) |</p>
<p>其他也可以在多分类案例中运行:</p>
<p>| <a href="generated/sklearn.metrics.cohen_kappa_score.html#sklearn.metrics.cohen_kappa_score" title="sklearn.metrics.cohen_kappa_score"><code>cohen_kappa_score</code></a>(y1,&nbsp;y2[,&nbsp;labels,&nbsp;weights,&nbsp;…]) | Cohen’s kappa: a statistic that measures inter-annotator agreement. | | <a href="generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix" title="sklearn.metrics.confusion_matrix"><code>confusion_matrix</code></a>(y_true,&nbsp;y_pred[,&nbsp;labels,&nbsp;…]) | Compute confusion matrix to evaluate the accuracy of a classification | | <a href="generated/sklearn.metrics.hinge_loss.html#sklearn.metrics.hinge_loss" title="sklearn.metrics.hinge_loss"><code>hinge_loss</code></a>(y_true,&nbsp;pred_decision[,&nbsp;labels,&nbsp;…]) | Average hinge loss (non-regularized) | | <a href="generated/sklearn.metrics.matthews_corrcoef.html#sklearn.metrics.matthews_corrcoef" title="sklearn.metrics.matthews_corrcoef"><code>matthews_corrcoef</code></a>(y_true,&nbsp;y_pred[,&nbsp;…]) | Compute the Matthews correlation coefficient (MCC) |</p>
<p>有些还可以在 multilabel case （多重案例）中工作:</p>
<p>| <a href="generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score" title="sklearn.metrics.accuracy_score"><code>accuracy_score</code></a>(y_true,&nbsp;y_pred[,&nbsp;normalize,&nbsp;…]) | Accuracy classification score. | | <a href="generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report" title="sklearn.metrics.classification_report"><code>classification_report</code></a>(y_true,&nbsp;y_pred[,&nbsp;…]) | Build a text report showing the main classification metrics | | <a href="generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score"><code>f1_score</code></a>(y_true,&nbsp;y_pred[,&nbsp;labels,&nbsp;…]) | Compute the F1 score, also known as balanced F-score or F-measure | | <a href="generated/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score" title="sklearn.metrics.fbeta_score"><code>fbeta_score</code></a>(y_true,&nbsp;y_pred,&nbsp;beta[,&nbsp;labels,&nbsp;…]) | Compute the F-beta score | | <a href="generated/sklearn.metrics.hamming_loss.html#sklearn.metrics.hamming_loss" title="sklearn.metrics.hamming_loss"><code>hamming_loss</code></a>(y_true,&nbsp;y_pred[,&nbsp;labels,&nbsp;…]) | Compute the average Hamming loss. | | <a href="generated/sklearn.metrics.jaccard_similarity_score.html#sklearn.metrics.jaccard_similarity_score" title="sklearn.metrics.jaccard_similarity_score"><code>jaccard_similarity_score</code></a>(y_true,&nbsp;y_pred[,&nbsp;…]) | Jaccard similarity coefficient score | | <a href="generated/sklearn.metrics.log_loss.html#sklearn.metrics.log_loss" title="sklearn.metrics.log_loss"><code>log_loss</code></a>(y_true,&nbsp;y_pred[,&nbsp;eps,&nbsp;normalize,&nbsp;…]) | Log loss, aka logistic loss or cross-entropy loss. | | <a href="generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support" title="sklearn.metrics.precision_recall_fscore_support"><code>precision_recall_fscore_support</code></a>(y_true,&nbsp;y_pred) | Compute precision, recall, F-measure and support for each class | | <a href="generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score" title="sklearn.metrics.precision_score"><code>precision_score</code></a>(y_true,&nbsp;y_pred[,&nbsp;labels,&nbsp;…]) | Compute the precision | | <a href="generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score" title="sklearn.metrics.recall_score"><code>recall_score</code></a>(y_true,&nbsp;y_pred[,&nbsp;labels,&nbsp;…]) | Compute the recall | | <a href="generated/sklearn.metrics.zero_one_loss.html#sklearn.metrics.zero_one_loss" title="sklearn.metrics.zero_one_loss"><code>zero_one_loss</code></a>(y_true,&nbsp;y_pred[,&nbsp;normalize,&nbsp;…]) | Zero-one classification loss. |</p>
<p>一些通常用于 ranking:</p>
<p>| <a href="generated/sklearn.metrics.dcg_score.html#sklearn.metrics.dcg_score" title="sklearn.metrics.dcg_score"><code>dcg_score</code></a>(y_true,&nbsp;y_score[,&nbsp;k]) | Discounted cumulative gain (DCG) at rank K. | | <a href="generated/sklearn.metrics.ndcg_score.html#sklearn.metrics.ndcg_score" title="sklearn.metrics.ndcg_score"><code>ndcg_score</code></a>(y_true,&nbsp;y_score[,&nbsp;k]) | Normalized discounted cumulative gain (NDCG) at rank K. |</p>
<p>有些工作与 binary 和 multilabel （但不是多类）的问题:</p>
<p>| <a href="generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score" title="sklearn.metrics.average_precision_score"><code>average_precision_score</code></a>(y_true,&nbsp;y_score[,&nbsp;…]) | Compute average precision (AP) from prediction scores | | <a href="generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score" title="sklearn.metrics.roc_auc_score"><code>roc_auc_score</code></a>(y_true,&nbsp;y_score[,&nbsp;average,&nbsp;…]) | Compute Area Under the Curve (AUC) from prediction scores |</p>
<p>在以下小节中，我们将介绍每个这些功能，前面是一些关于通用 API 和 metric 定义的注释。</p>
<h3 id="3321-从二分到多分类和-multilabel">3.3.2.1. 从二分到多分类和 multilabel</h3>
<p>一些 metrics 基本上是为 binary classification tasks （二分类任务）定义的 (例如 <a href="generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score"><code>f1_score</code></a>, <a href="generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score" title="sklearn.metrics.roc_auc_score"><code>roc_auc_score</code></a>) 。在这些情况下，默认情况下仅评估 positive label （正标签），假设默认情况下，positive label （正类）标记为 <code>1</code> （尽管可以通过 <code>pos_label</code> 参数进行配置）。</p>
<p>将 binary metric （二分指标）扩展为 multiclass （多类）或 multilabel （多标签）问题时，数据将被视为二分问题的集合，每个类都有一个。 然后可以使用多种方法在整个类中 average binary metric calculations （平均二分指标计算），每种类在某些情况下可能会有用。 如果可用，您应该使用 <code>average</code> 参数来选择它们。</p>
<ul>
<li><code>"macro（宏）"</code> 简单地计算 binary metrics （二分指标）的平均值，赋予每个类别相同的权重。在不常见的类别重要的问题上，macro-averaging （宏观平均）可能是突出表现的一种手段。另一方面，所有类别同样重要的假设通常是不真实的，因此 macro-averaging （宏观平均）将过度强调不频繁类的典型的低性能。</li>
<li><code>"weighted（加权）"</code> 通过计算其在真实数据样本中的存在来对每个类的 score 进行加权的 binary metrics （二分指标）的平均值来计算类不平衡。</li>
<li><code>"micro（微）"</code> 给每个 sample-class pair （样本类对）对 overall metric （总体指数）（sample-class 权重的结果除外） 等同的贡献。除了对每个类别的 metric 进行求和之外，这个总和构成每个类别度量的 dividends （除数）和 divisors （除数）计算一个整体商。 在 multilabel settings （多标签设置）中，Micro-averaging 可能是优先选择的，包括要忽略 majority class （多数类）的 multiclass classification （多类分类）。</li>
<li><code>"samples（样本）"</code> 仅适用于 multilabel problems （多标签问题）。它 does not calculate a per-class measure （不计算每个类别的 measure），而是计算 evaluation data （评估数据）中的每个样本的 true and predicted classes （真实和预测类别）的 metric （指标），并返回 (<code>sample_weight</code>-weighted) 加权平均。</li>
<li>选择 <code>average=None</code> 将返回一个 array 与每个类的 score 。</li>
</ul>
<p>虽然将 multiclass data （多类数据）提供给 metric ，如 binary targets （二分类目标），作为 array of class labels （类标签的数组），multilabel data （多标签数据）被指定为 indicator matrix（指示符矩阵），其中 cell <code>[i, j]</code> 具有值 1，如果样本 <code>i</code> 具有标号 <code>j</code> ，否则为值 0 。</p>
<h3 id="3322-精确度得分">3.3.2.2. 精确度得分</h3>
<p><a href="generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score" title="sklearn.metrics.accuracy_score"><code>accuracy_score</code></a> 函数计算 <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>, 正确预测的分数（默认）或计数 (normalize=False)。</p>
<p>在 multilabel classification （多标签分类）中，函数返回 subset accuracy（子集精度）。如果样本的 entire set of predicted labels （整套预测标签）与真正的标签组合匹配，则子集精度为 1.0; 否则为 0.0 。</p>
<p>如果 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/07794b8fa83c7e18c5d1fb175fd7d7bd.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/07794b8fa83c7e18c5d1fb175fd7d7bd.jpg" alt="\hat{y}_i"></a> 是第 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" alt="i"></a> 个样本的预测值，<a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4a22ca544916918b2358e5fc7c71b8e6.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4a22ca544916918b2358e5fc7c71b8e6.jpg" alt="y_i"></a> 是相应的真实值，则 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/bafb2b9486fa2f91dcc020843770eef6.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/bafb2b9486fa2f91dcc020843770eef6.jpg" alt="n_\text{samples}"></a> 上的正确预测的分数被定义为</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2a332823ff053f404ac53657eb86b1a0.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2a332823ff053f404ac53657eb86b1a0.jpg" alt="\texttt{accuracy}(y, \hat{y}) = \frac{1}{n_\text{samples}} \sum_{i=0}^{n_\text{samples}-1} 1(\hat{y}_i = y_i)"></a></p>
<p>其中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1e4e584cd8a99da7f18a5581de1f7be3.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1e4e584cd8a99da7f18a5581de1f7be3.jpg" alt="1(x)"></a> 是 <a href="https://en.wikipedia.org/wiki/Indicator_function">indicator function（指示函数）</a>.</p>
<pre><code class="language-py">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.metrics import accuracy_score
&gt;&gt;&gt; y_pred = [0, 2, 1, 3]
&gt;&gt;&gt; y_true = [0, 1, 2, 3]
&gt;&gt;&gt; accuracy_score(y_true, y_pred)
0.5
&gt;&gt;&gt; accuracy_score(y_true, y_pred, normalize=False)
2

</code></pre>
<p>In the multilabel case with binary label indicators（在具有二分标签指示符的多标签情况下）:</p>
<pre><code class="language-py">&gt;&gt;&gt; accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))
0.5

</code></pre>
<p>示例:</p>
<ul>
<li>参阅 <a href="../auto_examples/feature_selection/plot_permutation_test_for_classification.html#sphx-glr-auto-examples-feature-selection-plot-permutation-test-for-classification-py">Test with permutations the significance of a classification score</a> 例如使用数据集排列的 accuracy score （精度分数）。</li>
</ul>
<h3 id="3323-cohens-kappa">3.3.2.3. Cohen’s kappa</h3>
<p>函数 <a href="generated/sklearn.metrics.cohen_kappa_score.html#sklearn.metrics.cohen_kappa_score" title="sklearn.metrics.cohen_kappa_score"><code>cohen_kappa_score</code></a> 计算 <a href="https://en.wikipedia.org/wiki/Cohen%27s_kappa">Cohen’s kappa</a> statistic（统计）。 这个 measure （措施）旨在比较不同人工标注者的标签，而不是 classifier （分类器）与 ground truth （真实数据）。</p>
<p>kappa score （参阅 docstring ）是 -1 和 1 之间的数字。 .8 以上的 scores 通常被认为是很好的 agreement （协议）; 0 或者 更低表示没有 agreement （实际上是 random labels （随机标签））。</p>
<p>Kappa scores 可以计算 binary or multiclass （二分或者多分类）问题，但不能用于 multilabel problems （多标签问题）（除了手动计算 per-label score （每个标签分数）），而不是两个以上的 annotators （注释器）。</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn.metrics import cohen_kappa_score
&gt;&gt;&gt; y_true = [2, 0, 2, 2, 0, 1]
&gt;&gt;&gt; y_pred = [0, 0, 2, 2, 0, 2]
&gt;&gt;&gt; cohen_kappa_score(y_true, y_pred)
0.4285714285714286

</code></pre>
<h3 id="3324-混淆矩阵">3.3.2.4. 混淆矩阵</h3>
<p><a href="generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix" title="sklearn.metrics.confusion_matrix"><code>confusion_matrix</code></a> 函数通过计算 <a href="https://en.wikipedia.org/wiki/Confusion_matrix">confusion matrix（混淆矩阵）</a> 来 evaluates classification accuracy （评估分类的准确性）。</p>
<p>根据定义，confusion matrix （混淆矩阵）中的 entry（条目） <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c63f13d52ac4f65cde6e5dfd9e941562.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c63f13d52ac4f65cde6e5dfd9e941562.jpg" alt="i, j"></a>，是实际上在 group <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" alt="i"></a> 中的 observations （观察数），但预测在 group <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7b215f2882ce8aaa33a97e43ad626314.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7b215f2882ce8aaa33a97e43ad626314.jpg" alt="j"></a> 中。这里是一个示例:</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn.metrics import confusion_matrix
&gt;&gt;&gt; y_true = [2, 0, 2, 2, 0, 1]
&gt;&gt;&gt; y_pred = [0, 0, 2, 2, 0, 2]
&gt;&gt;&gt; confusion_matrix(y_true, y_pred)
array([[2, 0, 0],
 [0, 0, 1],
 [1, 0, 2]])

</code></pre>
<p>这是一个这样的 confusion matrix （混淆矩阵）的可视化表示 （这个数字来自于 <a href="../auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py">Confusion matrix</a>）:</p>
<p><a href="../auto_examples/model_selection/plot_confusion_matrix.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/362af37df82b07d11576fc5e45db7828.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/362af37df82b07d11576fc5e45db7828.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_confusion_matrix_0011.png"></a></a></p>
<p>对于 binary problems （二分类问题），我们可以得到 true negatives（真 negatives）, false positives（假 positives）, false negatives（假 negatives） 和 true positives（真 positives） 的数量如下:</p>
<pre><code class="language-py">&gt;&gt;&gt; y_true = [0, 0, 0, 1, 1, 1, 1, 1]
&gt;&gt;&gt; y_pred = [0, 1, 0, 1, 0, 1, 0, 1]
&gt;&gt;&gt; tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
&gt;&gt;&gt; tn, fp, fn, tp
(2, 1, 2, 3)

</code></pre>
<p>示例:</p>
<ul>
<li>参阅 <a href="../auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py">Confusion matrix</a> 例如使用 confusion matrix （混淆矩阵）来评估 classifier （分类器）的输出质量。</li>
<li>参阅 <a href="../auto_examples/classification/plot_digits_classification.html#sphx-glr-auto-examples-classification-plot-digits-classification-py">Recognizing hand-written digits</a> 例如使用 confusion matrix （混淆矩阵）来分类手写数字。</li>
<li>参阅 <a href="../auto_examples/text/document_classification_20newsgroups.html#sphx-glr-auto-examples-text-document-classification-20newsgroups-py">Classification of text documents using sparse features</a> 例如使用 confusion matrix （混淆矩阵）对文本文档进行分类。</li>
</ul>
<h3 id="3325-分类报告">3.3.2.5. 分类报告</h3>
<p><a href="generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report" title="sklearn.metrics.classification_report"><code>classification_report</code></a> 函数构建一个显示 main classification metrics （主分类指标）的文本报告。这是一个小例子，其中包含自定义的 <code>target_names</code> 和 inferred labels （推断标签）:</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn.metrics import classification_report
&gt;&gt;&gt; y_true = [0, 1, 2, 2, 0]
&gt;&gt;&gt; y_pred = [0, 0, 2, 1, 0]
&gt;&gt;&gt; target_names = ['class 0', 'class 1', 'class 2']
&gt;&gt;&gt; print(classification_report(y_true, y_pred, target_names=target_names))
 precision    recall  f1-score   support

 class 0       0.67      1.00      0.80         2
 class 1       0.00      0.00      0.00         1
 class 2       1.00      0.50      0.67         2

avg / total       0.67      0.60      0.59         5

</code></pre>
<p>示例:</p>
<ul>
<li>参阅 <a href="../auto_examples/classification/plot_digits_classification.html#sphx-glr-auto-examples-classification-plot-digits-classification-py">Recognizing hand-written digits</a> 作为手写数字的分类报告的使用示例。</li>
<li>参阅 <a href="../auto_examples/text/document_classification_20newsgroups.html#sphx-glr-auto-examples-text-document-classification-20newsgroups-py">Classification of text documents using sparse features</a> 作为文本文档的分类报告使用的示例。</li>
<li>参阅 <a href="../auto_examples/model_selection/plot_grid_search_digits.html#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py">Parameter estimation using grid search with cross-validation</a> 例如使用 grid search with nested cross-validation （嵌套交叉验证进行网格搜索）的分类报告。</li>
</ul>
<h3 id="3326-汉明损失">3.3.2.6. 汉明损失</h3>
<p><a href="generated/sklearn.metrics.hamming_loss.html#sklearn.metrics.hamming_loss" title="sklearn.metrics.hamming_loss"><code>hamming_loss</code></a> 计算两组样本之间的 average Hamming loss （平均汉明损失）或者 <a href="https://en.wikipedia.org/wiki/Hamming_distance">Hamming distance（汉明距离）</a> 。</p>
<p>如果 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d12d5f9823ac608127ac67df8cecff9d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d12d5f9823ac608127ac67df8cecff9d.jpg" alt="\hat{y}_j"></a> 是给定样本的第 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7b215f2882ce8aaa33a97e43ad626314.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7b215f2882ce8aaa33a97e43ad626314.jpg" alt="j"></a> 个标签的预测值，则 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8610705cf45aa68b12197abd65653479.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8610705cf45aa68b12197abd65653479.jpg" alt="y_j"></a> 是相应的真实值，而 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ed51906ca8fbc868248006c841aefa2b.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ed51906ca8fbc868248006c841aefa2b.jpg" alt="n_\text{labels}"></a> 是 classes or labels （类或者标签）的数量，则两个样本之间的 Hamming loss （汉明损失） <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/866891e7bebe66615daa04976af79f99.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/866891e7bebe66615daa04976af79f99.jpg" alt="L_{Hamming}"></a> 定义为:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/94a5c73d8f351280b6313519455a11c7.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/94a5c73d8f351280b6313519455a11c7.jpg" alt="L_{Hamming}(y, \hat{y}) = \frac{1}{n_\text{labels}} \sum_{j=0}^{n_\text{labels} - 1} 1(\hat{y}_j \not= y_j)"></a></p>
<p>其中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1e4e584cd8a99da7f18a5581de1f7be3.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1e4e584cd8a99da7f18a5581de1f7be3.jpg" alt="1(x)"></a> 是 <a href="https://en.wikipedia.org/wiki/Indicator_function">indicator function（指标函数）</a>.</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn.metrics import hamming_loss
&gt;&gt;&gt; y_pred = [1, 2, 3, 4]
&gt;&gt;&gt; y_true = [2, 2, 3, 4]
&gt;&gt;&gt; hamming_loss(y_true, y_pred)
0.25

</code></pre>
<p>在具有 binary label indicators （二分标签指示符）的 multilabel （多标签）情况下:</p>
<pre><code class="language-py">&gt;&gt;&gt; hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))
0.75

</code></pre>
<p>Note</p>
<p>在 multiclass classification （多类分类）中， Hamming loss （汉明损失）对应于 <code>y_true</code> 和 <code>y_pred</code> 之间的 Hamming distance（汉明距离），它类似于 <a href="#zero-one-loss">零一损失</a> 函数。然而， zero-one loss penalizes （0-1损失惩罚）不严格匹配真实集合的预测集，Hamming loss （汉明损失）惩罚 individual labels （独立标签）。因此，Hamming loss（汉明损失）高于 zero-one loss（0-1 损失），总是在 0 和 1 之间，包括 0 和 1;预测真正的标签的正确的 subset or superset （子集或超集）将给出 0 和 1 之间的 Hamming loss（汉明损失）。</p>
<h3 id="3327-jaccard-相似系数-score">3.3.2.7. Jaccard 相似系数 score</h3>
<p><a href="generated/sklearn.metrics.jaccard_similarity_score.html#sklearn.metrics.jaccard_similarity_score" title="sklearn.metrics.jaccard_similarity_score"><code>jaccard_similarity_score</code></a> 函数计算 pairs of label sets （标签组对）之间的 <a href="https://en.wikipedia.org/wiki/Jaccard_index">Jaccard similarity coefficients</a> 也称作 Jaccard index 的平均值（默认）或总和。</p>
<p>将第 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" alt="i"></a> 个样本的 Jaccard similarity coefficient 与 被标注过的真实数据的标签集 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4a22ca544916918b2358e5fc7c71b8e6.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4a22ca544916918b2358e5fc7c71b8e6.jpg" alt="y_i"></a> 和 predicted label set （预测标签集）:math:&lt;cite&gt;hat{y}_i&lt;/cite&gt; 定义为</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6b1c74edd599db63c339ead392e8e54a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6b1c74edd599db63c339ead392e8e54a.jpg" alt="J(y_i, \hat{y}_i) = \frac{|y_i \cap \hat{y}_i|}{|y_i \cup \hat{y}_i|}."></a></p>
<p>在 binary and multiclass classification （二分和多类分类）中，Jaccard similarity coefficient score 等于 classification accuracy（分类精度）。</p>
<pre><code class="language-py">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.metrics import jaccard_similarity_score
&gt;&gt;&gt; y_pred = [0, 2, 1, 3]
&gt;&gt;&gt; y_true = [0, 1, 2, 3]
&gt;&gt;&gt; jaccard_similarity_score(y_true, y_pred)
0.5
&gt;&gt;&gt; jaccard_similarity_score(y_true, y_pred, normalize=False)
2

</code></pre>
<p>在具有 binary label indicators （二分标签指示符）的 multilabel （多标签）情况下:</p>
<pre><code class="language-py">&gt;&gt;&gt; jaccard_similarity_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))
0.75

</code></pre>
<h3 id="3328-精准召回和-f-measures">3.3.2.8. 精准，召回和 F-measures</h3>
<p>直观地来理解，<a href="https://en.wikipedia.org/wiki/Precision_and_recall#Precision">precision</a> 是 the ability of the classifier not to label as positive a sample that is negative （classifier （分类器）的标签不能被标记为正的样本为负的能力），并且 <a href="https://en.wikipedia.org/wiki/Precision_and_recall#Recall">recall</a> 是 classifier （分类器）查找所有 positive samples （正样本）的能力。</p>
<p><a href="https://en.wikipedia.org/wiki/F1_score">F-measure</a> (<a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5bd6037aeb804486a6f7cc0415ace8fc.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5bd6037aeb804486a6f7cc0415ace8fc.jpg" alt="F_\beta"></a> 和 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a51f44dfa9b1942326c669c5ffe3f9f6.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a51f44dfa9b1942326c669c5ffe3f9f6.jpg" alt="F_1"></a> measures) 可以解释为 precision （精度）和 recall （召回）的 weighted harmonic mean （加权调和平均值）。 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5bd6037aeb804486a6f7cc0415ace8fc.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5bd6037aeb804486a6f7cc0415ace8fc.jpg" alt="F_\beta"></a> measure 值达到其最佳值 1 ，其最差分数为 0 。与 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7ca5945518e2f3eff72bd67b029e919a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7ca5945518e2f3eff72bd67b029e919a.jpg" alt="\beta = 1"></a>, <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5bd6037aeb804486a6f7cc0415ace8fc.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5bd6037aeb804486a6f7cc0415ace8fc.jpg" alt="F_\beta"></a> 和 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a51f44dfa9b1942326c669c5ffe3f9f6.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a51f44dfa9b1942326c669c5ffe3f9f6.jpg" alt="F_1"></a> 是等价的， recall （召回）和 precision （精度）同样重要。</p>
<p><a href="generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve" title="sklearn.metrics.precision_recall_curve"><code>precision_recall_curve</code></a> 通过改变 decision threshold （决策阈值）从 ground truth label （被标记的真实数据标签） 和 score given by the classifier （分类器给出的分数）计算 precision-recall curve （精确召回曲线）。</p>
<p><a href="generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score" title="sklearn.metrics.average_precision_score"><code>average_precision_score</code></a> 函数根据 prediction scores （预测分数）计算出 average precision (AP)（平均精度）。该分数对应于 precision-recall curve （精确召回曲线）下的面积。该值在 0 和 1 之间，并且越高越好。通过 random predictions （随机预测）， AP 是 fraction of positive samples （正样本的分数）。</p>
<p>几个函数可以让您 analyze the precision （分析精度），recall（召回） 和 F-measures 得分:</p>
<p>| <a href="generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score" title="sklearn.metrics.average_precision_score"><code>average_precision_score</code></a>(y_true,&nbsp;y_score[,&nbsp;…]) | Compute average precision (AP) from prediction scores | | <a href="generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score"><code>f1_score</code></a>(y_true,&nbsp;y_pred[,&nbsp;labels,&nbsp;…]) | Compute the F1 score, also known as balanced F-score or F-measure | | <a href="generated/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score" title="sklearn.metrics.fbeta_score"><code>fbeta_score</code></a>(y_true,&nbsp;y_pred,&nbsp;beta[,&nbsp;labels,&nbsp;…]) | Compute the F-beta score | | <a href="generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve" title="sklearn.metrics.precision_recall_curve"><code>precision_recall_curve</code></a>(y_true,&nbsp;probas_pred) | Compute precision-recall pairs for different probability thresholds | | <a href="generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support" title="sklearn.metrics.precision_recall_fscore_support"><code>precision_recall_fscore_support</code></a>(y_true,&nbsp;y_pred) | Compute precision, recall, F-measure and support for each class | | <a href="generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score" title="sklearn.metrics.precision_score"><code>precision_score</code></a>(y_true,&nbsp;y_pred[,&nbsp;labels,&nbsp;…]) | Compute the precision | | <a href="generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score" title="sklearn.metrics.recall_score"><code>recall_score</code></a>(y_true,&nbsp;y_pred[,&nbsp;labels,&nbsp;…]) | Compute the recall |</p>
<p>请注意，<a href="generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve" title="sklearn.metrics.precision_recall_curve"><code>precision_recall_curve</code></a> 函数仅限于 binary case （二分情况）。 <a href="generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score" title="sklearn.metrics.average_precision_score"><code>average_precision_score</code></a> 函数只适用于 binary classification and multilabel indicator format （二分类和多标签指示器格式）。</p>
<p>示例:</p>
<ul>
<li>参阅 <a href="../auto_examples/text/document_classification_20newsgroups.html#sphx-glr-auto-examples-text-document-classification-20newsgroups-py">Classification of text documents using sparse features</a> 例如 <a href="generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score"><code>f1_score</code></a> 用于分类文本文档的用法。</li>
<li>参阅 <a href="../auto_examples/model_selection/plot_grid_search_digits.html#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py">Parameter estimation using grid search with cross-validation</a> 例如 <a href="generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score" title="sklearn.metrics.precision_score"><code>precision_score</code></a> 和 <a href="generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score" title="sklearn.metrics.recall_score"><code>recall_score</code></a> 用于 using grid search with nested cross-validation （使用嵌套交叉验证的网格搜索）来估计参数。</li>
<li>参阅 <a href="../auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py">Precision-Recall</a> 例如 <a href="generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve" title="sklearn.metrics.precision_recall_curve"><code>precision_recall_curve</code></a> 用于 evaluate classifier output quality（评估分类器输出质量）。</li>
</ul>
<h4 id="33281-二分类">3.3.2.8.1. 二分类</h4>
<p>在二分类任务中，术语 ‘’positive（正）’’ 和 ‘’negative（负）’’ 是指 classifier’s prediction （分类器的预测），术语 ‘’true（真）’’ 和 ‘’false（假）’’ 是指该预测是否对应于 external judgment （外部判断）（有时被称为 ‘’observation（观测值）’‘）。给出这些定义，我们可以指定下表:</p>
<p>| &nbsp; | Actual class (observation) | | Predicted class (expectation) | tp (true positive) Correct result | fp (false positive) Unexpected result | | fn (false negative) Missing result | tn (true negative) Correct absence of result |</p>
<p>在这种情况下，我们可以定义 precision（精度）, recall（召回） 和 F-measure 的概念:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3771db7af1e3b7bf33e15ec20d278f39.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3771db7af1e3b7bf33e15ec20d278f39.jpg" alt="\text{precision} = \frac{tp}{tp + fp},"></a></p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/407341c3d4d055b857bb3229003b9daf.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/407341c3d4d055b857bb3229003b9daf.jpg" alt="\text{recall} = \frac{tp}{tp + fn},"></a></p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b3edbb24837112f795a22e3574457416.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b3edbb24837112f795a22e3574457416.jpg" alt="F_\beta = (1 + \beta2) \frac{\text{precision} \times \text{recall}}{\beta2 \text{precision} + \text{recall}}."></a></p>
<p>以下是 binary classification （二分类）中的一些小例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn import metrics
&gt;&gt;&gt; y_pred = [0, 1, 0, 0]
&gt;&gt;&gt; y_true = [0, 1, 0, 1]
&gt;&gt;&gt; metrics.precision_score(y_true, y_pred)
1.0
&gt;&gt;&gt; metrics.recall_score(y_true, y_pred)
0.5
&gt;&gt;&gt; metrics.f1_score(y_true, y_pred)  
0.66...
&gt;&gt;&gt; metrics.fbeta_score(y_true, y_pred, beta=0.5)  
0.83...
&gt;&gt;&gt; metrics.fbeta_score(y_true, y_pred, beta=1)  
0.66...
&gt;&gt;&gt; metrics.fbeta_score(y_true, y_pred, beta=2) 
0.55...
&gt;&gt;&gt; metrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5)  
(array([ 0.66...,  1\.        ]), array([ 1\. ,  0.5]), array([ 0.71...,  0.83...]), array([2, 2]...))

&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.metrics import precision_recall_curve
&gt;&gt;&gt; from sklearn.metrics import average_precision_score
&gt;&gt;&gt; y_true = np.array([0, 0, 1, 1])
&gt;&gt;&gt; y_scores = np.array([0.1, 0.4, 0.35, 0.8])
&gt;&gt;&gt; precision, recall, threshold = precision_recall_curve(y_true, y_scores)
&gt;&gt;&gt; precision  
array([ 0.66...,  0.5       ,  1\.        ,  1\.        ])
&gt;&gt;&gt; recall
array([ 1\. ,  0.5,  0.5,  0\. ])
&gt;&gt;&gt; threshold
array([ 0.35,  0.4 ,  0.8 ])
&gt;&gt;&gt; average_precision_score(y_true, y_scores)  
0.83...

</code></pre>
<h4 id="33282-多类和多标签分类">3.3.2.8.2. 多类和多标签分类</h4>
<p>在 multiclass and multilabel classification task（多类和多标签分类任务）中，precision（精度）, recall（召回）, and F-measures 的概念可以独立地应用于每个标签。 有以下几种方法 combine results across labels （将结果跨越标签组合），由 <code>average</code> 参数指定为 <a href="generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score" title="sklearn.metrics.average_precision_score"><code>average_precision_score</code></a> （仅用于 multilabel）， <a href="generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score"><code>f1_score</code></a>, <a href="generated/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score" title="sklearn.metrics.fbeta_score"><code>fbeta_score</code></a>, <a href="generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support" title="sklearn.metrics.precision_recall_fscore_support"><code>precision_recall_fscore_support</code></a>, <a href="generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score" title="sklearn.metrics.precision_score"><code>precision_score</code></a> 和 <a href="generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score" title="sklearn.metrics.recall_score"><code>recall_score</code></a> 函数，如上 <a href="#average">above</a> 所述。请注意，对于在包含所有标签的多类设置中进行 “micro”-averaging （”微”平均），将产生相等的 precision（精度）， recall（召回）和 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8ef970a809e9beef56eb7a78d0133978.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8ef970a809e9beef56eb7a78d0133978.jpg" alt="F"></a> ，而 “weighted（加权）” averaging（平均）可能会产生 precision（精度）和 recall（召回）之间的 F-score 。</p>
<p>为了使这一点更加明确，请考虑以下 notation （符号）:</p>
<ul>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0775c03fc710a24df297dedcec515aaf.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0775c03fc710a24df297dedcec515aaf.jpg" alt="y"></a> <em>predicted（预测）</em> <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/943add7649d85f7ef63a83356dd6f234.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/943add7649d85f7ef63a83356dd6f234.jpg" alt="(sample, label)"></a> 对</li>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/047826f1c2e6f2687b304cb5217be8d8.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/047826f1c2e6f2687b304cb5217be8d8.jpg" alt="\hat{y}"></a> <em>true（真）</em> <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/943add7649d85f7ef63a83356dd6f234.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/943add7649d85f7ef63a83356dd6f234.jpg" alt="(sample, label)"></a> 对</li>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/639e82f3829a0ad677110cc33a028c98.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/639e82f3829a0ad677110cc33a028c98.jpg" alt="L"></a> labels 集合</li>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/12ecd862769bee1e71c75c134b6423bb.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/12ecd862769bee1e71c75c134b6423bb.jpg" alt="S"></a> samples 集合</li>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/67f18f488d2173299bc076b212f6aee9.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/67f18f488d2173299bc076b212f6aee9.jpg" alt="y_s"></a> <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0775c03fc710a24df297dedcec515aaf.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0775c03fc710a24df297dedcec515aaf.jpg" alt="y"></a> 的子集与样本 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0faa297883831c0432cf4d72960eeb6c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0faa297883831c0432cf4d72960eeb6c.jpg" alt="s"></a>, 即 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0a7b173908e1ba21b1132121dd409ded.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0a7b173908e1ba21b1132121dd409ded.jpg" alt="y_s := \left\{(s', l) \in y | s' = s\right\}"></a></li>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/42f93b2b294f585223e6c663f86504d0.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/42f93b2b294f585223e6c663f86504d0.jpg" alt="y_l"></a> <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0775c03fc710a24df297dedcec515aaf.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0775c03fc710a24df297dedcec515aaf.jpg" alt="y"></a> 的子集与 label <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/eb604628a01ce7d6db62d61eba6e2e2f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/eb604628a01ce7d6db62d61eba6e2e2f.jpg" alt="l"></a></li>
<li>类似的, <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/572f614c4b9bc376ebbf6ca259b6558e.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/572f614c4b9bc376ebbf6ca259b6558e.jpg" alt="\hat{y}_s"></a> 和 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/280a00b263d3144cd3a9c424ed44ee51.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/280a00b263d3144cd3a9c424ed44ee51.jpg" alt="\hat{y}_l"></a> 是 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/047826f1c2e6f2687b304cb5217be8d8.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/047826f1c2e6f2687b304cb5217be8d8.jpg" alt="\hat{y}"></a> 的子集</li>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7e76cbbbf685f7ec4bec704a9b5ea007.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7e76cbbbf685f7ec4bec704a9b5ea007.jpg" alt="P(A, B) := \frac{\left| A \cap B \right|}{\left|A\right|}"></a></li>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/99e96cb74c925ba51098fe6167e22c44.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/99e96cb74c925ba51098fe6167e22c44.jpg" alt="R(A, B) := \frac{\left| A \cap B \right|}{\left|B\right|}"></a> (Conventions （公约）在处理 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9a2b4cb70f12a0e7419ffde362e1bed4.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9a2b4cb70f12a0e7419ffde362e1bed4.jpg" alt="B = \emptyset"></a> 有所不同; 这个实现使用 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4f11727a275459ce82826a9e02800c28.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4f11727a275459ce82826a9e02800c28.jpg" alt="R(A, B):=0"></a>, 与 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4bfe956324cef23278c5192b0fb8029b.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4bfe956324cef23278c5192b0fb8029b.jpg" alt="P"></a> 类似.)</li>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3109bc087a626380237668dfcc4ecd96.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3109bc087a626380237668dfcc4ecd96.jpg" alt="F_\beta(A, B) := \left(1 + \beta2\right) \frac{P(A, B) \times R(A, B)}{\beta2 P(A, B) + R(A, B)}"></a></li>
</ul>
<p>然后将 metrics （指标）定义为:</p>
<table>
<thead>
<tr>
<th><code>average</code></th>
<th>Precision</th>
<th>Recall</th>
<th>F_beta</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>"micro"</code></td>
<td><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/fc080793a40b71dc553fe8966ad7516a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/fc080793a40b71dc553fe8966ad7516a.jpg" alt="P(y, \hat{y})"></a></td>
<td><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9afe9de54aeed2a857e99cf6444ff0e5.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9afe9de54aeed2a857e99cf6444ff0e5.jpg" alt="R(y, \hat{y})"></a></td>
<td><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6e87bd4511cfd9af64076cc1cf8f1bbc.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6e87bd4511cfd9af64076cc1cf8f1bbc.jpg" alt="F_\beta(y, \hat{y})"></a></td>
</tr>
<tr>
<td><code>"samples"</code></td>
<td><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c471a6ca04f68a6d888d4c8ad95ba189.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c471a6ca04f68a6d888d4c8ad95ba189.jpg" alt="\frac{1}{\left|S\right|} \sum_{s \in S} P(y_s, \hat{y}_s)"></a></td>
<td><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/97f4093251d6c6f6f7d0902a86a08dbe.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/97f4093251d6c6f6f7d0902a86a08dbe.jpg" alt="\frac{1}{\left|S\right|} \sum_{s \in S} R(y_s, \hat{y}_s)"></a></td>
<td><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b36270d22513e9645235b5ad4c3cd7dd.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b36270d22513e9645235b5ad4c3cd7dd.jpg" alt="\frac{1}{\left|S\right|} \sum_{s \in S} F_\beta(y_s, \hat{y}_s)"></a></td>
</tr>
<tr>
<td><code>"macro"</code></td>
<td><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/47738e3e36a9bddb5bc708e8fc666204.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/47738e3e36a9bddb5bc708e8fc666204.jpg" alt="\frac{1}{\left|L\right|} \sum_{l \in L} P(y_l, \hat{y}_l)"></a></td>
<td><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/87fef5b961bf8a90d58faa5e4084a081.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/87fef5b961bf8a90d58faa5e4084a081.jpg" alt="\frac{1}{\left|L\right|} \sum_{l \in L} R(y_l, \hat{y}_l)"></a></td>
<td><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/412f5988c3736daa68f47a0dc9fba659.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/412f5988c3736daa68f47a0dc9fba659.jpg" alt="\frac{1}{\left|L\right|} \sum_{l \in L} F_\beta(y_l, \hat{y}_l)"></a></td>
</tr>
<tr>
<td><code>"weighted"</code></td>
<td><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c3de98827a889725d91141a5780692b5.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c3de98827a889725d91141a5780692b5.jpg" alt="\frac{1}{\sum_{l \in L} \left|\hat{y}l\right|} \sum{l \in L} \left|\hat{y}_l\right| P(y_l, \hat{y}_l)"></a></td>
<td><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0e7d39317aed470ee92522354b5fbe04.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0e7d39317aed470ee92522354b5fbe04.jpg" alt="\frac{1}{\sum_{l \in L} \left|\hat{y}l\right|} \sum{l \in L} \left|\hat{y}_l\right| R(y_l, \hat{y}_l)"></a></td>
<td><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f43793e26fc93870e33eb062060e309a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f43793e26fc93870e33eb062060e309a.jpg" alt="\frac{1}{\sum_{l \in L} \left|\hat{y}l\right|} \sum{l \in L} \left|\hat{y}l\right| F\beta(y_l, \hat{y}_l)"></a></td>
</tr>
<tr>
<td><code>None</code></td>
<td><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1fa419ff78b610bf4a5b3b71df728cec.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1fa419ff78b610bf4a5b3b71df728cec.jpg" alt="\langle P(y_l, \hat{y}_l) | l \in L \rangle"></a></td>
<td><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/cddb30ab60430b100271b055376e8363.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/cddb30ab60430b100271b055376e8363.jpg" alt="\langle R(y_l, \hat{y}_l) | l \in L \rangle"></a></td>
<td><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a0c2e3fe9c6a7a8416435260aa55dc4a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a0c2e3fe9c6a7a8416435260aa55dc4a.jpg" alt="\langle F_\beta(y_l, \hat{y}_l) | l \in L \rangle"></a></td>
</tr>
</tbody>
</table>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn import metrics
&gt;&gt;&gt; y_true = [0, 1, 2, 0, 1, 2]
&gt;&gt;&gt; y_pred = [0, 2, 1, 0, 0, 1]
&gt;&gt;&gt; metrics.precision_score(y_true, y_pred, average='macro')  
0.22...
&gt;&gt;&gt; metrics.recall_score(y_true, y_pred, average='micro')
... 
0.33...
&gt;&gt;&gt; metrics.f1_score(y_true, y_pred, average='weighted')  
0.26...
&gt;&gt;&gt; metrics.fbeta_score(y_true, y_pred, average='macro', beta=0.5)  
0.23...
&gt;&gt;&gt; metrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5, average=None)
... 
(array([ 0.66...,  0\.        ,  0\.        ]), array([ 1.,  0.,  0.]), array([ 0.71...,  0\.        ,  0\.        ]), array([2, 2, 2]...))

</code></pre>
<p>For multiclass classification with a “negative class”, it is possible to exclude some labels:</p>
<pre><code class="language-py">&gt;&gt;&gt; metrics.recall_score(y_true, y_pred, labels=[1, 2], average='micro')
... # excluding 0, no labels were correctly recalled
0.0

</code></pre>
<p>Similarly, labels not present in the data sample may be accounted for in macro-averaging.</p>
<pre><code class="language-py">&gt;&gt;&gt; metrics.precision_score(y_true, y_pred, labels=[0, 1, 2, 3], average='macro')
... 
0.166...

</code></pre>
<h3 id="3329-hinge-loss">3.3.2.9. Hinge loss</h3>
<p><a href="generated/sklearn.metrics.hinge_loss.html#sklearn.metrics.hinge_loss" title="sklearn.metrics.hinge_loss"><code>hinge_loss</code></a> 函数使用 <a href="https://en.wikipedia.org/wiki/Hinge_loss">hinge loss</a> 计算模型和数据之间的 average distance （平均距离），这是一种只考虑 prediction errors （预测误差）的 one-sided metric （单向指标）。（Hinge loss 用于最大边界分类器，如支持向量机）</p>
<p>如果标签用 +1 和 -1 编码，则 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0775c03fc710a24df297dedcec515aaf.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0775c03fc710a24df297dedcec515aaf.jpg" alt="y"></a>: 是真实值，并且 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8a58e8df6a985a3273e39bac7dd72b1f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8a58e8df6a985a3273e39bac7dd72b1f.jpg" alt="w"></a> 是由 <code>decision_function</code> 输出的 predicted decisions （预测决策），则 hinge loss 定义为:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3d59995ed97bdad674b6afd6fbd928ec.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3d59995ed97bdad674b6afd6fbd928ec.jpg" alt="L_\text{Hinge}(y, w) = \max\left\{1 - wy, 0\right\} = \left|1 - wy\right|_+"></a></p>
<p>如果有两个以上的标签， <a href="generated/sklearn.metrics.hinge_loss.html#sklearn.metrics.hinge_loss" title="sklearn.metrics.hinge_loss"><code>hinge_loss</code></a> 由于 Crammer &amp; Singer 而使用了 multiclass variant （多类型变体）。 <a href="http://jmlr.csail.mit.edu/papers/volume2/crammer01a/crammer01a.pdf">Here</a> 是描述它的论文。</p>
<p>如果 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/decc30cee202697370eb9e21062c54b7.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/decc30cee202697370eb9e21062c54b7.jpg" alt="y_w"></a> 是真实标签的 predicted decision （预测决策），并且 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/014b479ec81146a77562d251269a0f2e.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/014b479ec81146a77562d251269a0f2e.jpg" alt="y_t"></a> 是所有其他标签的预测决策的最大值，其中预测决策由 decision function （决策函数）输出，则 multiclass hinge loss 定义如下:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6569ca3d831148970ddb4c7dfc3f2572.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6569ca3d831148970ddb4c7dfc3f2572.jpg" alt="L_\text{Hinge}(y_w, y_t) = \max\left\{1 + y_t - y_w, 0\right\}"></a></p>
<p>这里是一个小例子，演示了在 binary class （二类）问题中使用了具有 svm classifier （svm 的分类器）的 <a href="generated/sklearn.metrics.hinge_loss.html#sklearn.metrics.hinge_loss" title="sklearn.metrics.hinge_loss"><code>hinge_loss</code></a> 函数:</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn import svm
&gt;&gt;&gt; from sklearn.metrics import hinge_loss
&gt;&gt;&gt; X = [[0], [1]]
&gt;&gt;&gt; y = [-1, 1]
&gt;&gt;&gt; est = svm.LinearSVC(random_state=0)
&gt;&gt;&gt; est.fit(X, y)
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
 intercept_scaling=1, loss='squared_hinge', max_iter=1000,
 multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
 verbose=0)
&gt;&gt;&gt; pred_decision = est.decision_function([[-2], [3], [0.5]])
&gt;&gt;&gt; pred_decision  
array([-2.18...,  2.36...,  0.09...])
&gt;&gt;&gt; hinge_loss([-1, 1, 1], pred_decision)  
0.3...

</code></pre>
<p>这里是一个示例，演示了在 multiclass problem （多类问题）中使用了具有 svm 分类器的 <a href="generated/sklearn.metrics.hinge_loss.html#sklearn.metrics.hinge_loss" title="sklearn.metrics.hinge_loss"><code>hinge_loss</code></a> 函数:</p>
<pre><code class="language-py">&gt;&gt;&gt; X = np.array([[0], [1], [2], [3]])
&gt;&gt;&gt; Y = np.array([0, 1, 2, 3])
&gt;&gt;&gt; labels = np.array([0, 1, 2, 3])
&gt;&gt;&gt; est = svm.LinearSVC()
&gt;&gt;&gt; est.fit(X, Y)
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
 intercept_scaling=1, loss='squared_hinge', max_iter=1000,
 multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
 verbose=0)
&gt;&gt;&gt; pred_decision = est.decision_function([[-1], [2], [3]])
&gt;&gt;&gt; y_true = [0, 2, 3]
&gt;&gt;&gt; hinge_loss(y_true, pred_decision, labels)  
0.56...

</code></pre>
<h3 id="33210-log-损失">3.3.2.10. Log 损失</h3>
<p>Log loss，又被称为 logistic regression loss（logistic 回归损失）或者 cross-entropy loss（交叉熵损失） 定义在 probability estimates （概率估计）。它通常用于 (multinomial) logistic regression （（多项式）logistic 回归）和 neural networks （神经网络）以及 expectation-maximization （期望最大化）的一些变体中，并且可用于评估分类器的 probability outputs （概率输出）（<code>predict_proba</code>）而不是其 discrete predictions （离散预测）。</p>
<p>对于具有真实标签 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8480724cd60359c7a8ceda7bee5590bd.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8480724cd60359c7a8ceda7bee5590bd.jpg" alt="y \in \{0,1\}"></a> 的 binary classification （二分类）和 probability estimate （概率估计） <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/685006d43de154949bfb11efd87df4f1.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/685006d43de154949bfb11efd87df4f1.jpg" alt="p = \operatorname{Pr}(y = 1)"></a>, 每个样本的 log loss 是给定的分类器的 negative log-likelihood 真正的标签:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7d1cd85c2f165c7bde33eccf4be29e75.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7d1cd85c2f165c7bde33eccf4be29e75.jpg" alt="L_{\log}(y, p) = -\log \operatorname{Pr}(y|p) = -(y \log (p) + (1 - y) \log (1 - p))"></a></p>
<p>这扩展到 multiclass case （多类案例）如下。 让一组样本的真实标签被编码为 1-of-K binary indicator matrix <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/966bd0680e0e71a4df98abab98818724.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/966bd0680e0e71a4df98abab98818724.jpg" alt="Y"></a>, 即 如果样本 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" alt="i"></a> 具有取自一组 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e279b8169ddd6581c5606c868ba52fae.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e279b8169ddd6581c5606c868ba52fae.jpg" alt="K"></a> 个标签的标签 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> ，则 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7947f721109ac76f1366b72715d3e7e3.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7947f721109ac76f1366b72715d3e7e3.jpg" alt="y_{i,k} = 1"></a> 。令 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4bfe956324cef23278c5192b0fb8029b.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4bfe956324cef23278c5192b0fb8029b.jpg" alt="P"></a> 为 matrix of probability estimates （概率估计矩阵）， <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/418f3dbcb32031257a948ec23d05e53e.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/418f3dbcb32031257a948ec23d05e53e.jpg" alt="p_{i,k} = \operatorname{Pr}(t_{i,k} = 1)"></a> 。那么整套的 log loss 就是</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5fc2f399717cfe5187dc09896972a850.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5fc2f399717cfe5187dc09896972a850.jpg" alt="L_{\log}(Y, P) = -\log \operatorname{Pr}(Y|P) = - \frac{1}{N} \sum_{i=0}{N-1} \sum_{k=0}{K-1} y_{i,k} \log p_{i,k}"></a></p>
<p>为了看这这里如何 generalizes （推广）上面给出的 binary log loss （二分 log loss），请注意，在 binary case （二分情况下），<a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0bc78b13595e61ff422e00bb2686c7e8.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0bc78b13595e61ff422e00bb2686c7e8.jpg" alt="p_{i,0} = 1 - p_{i,1}"></a> 和 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c82c4d24e15330860a4ca71a31ddd553.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c82c4d24e15330860a4ca71a31ddd553.jpg" alt="y_{i,0} = 1 - y_{i,1}"></a> ，因此扩展 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9f2872d8b84b398d8dd6408fd880d3b4.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9f2872d8b84b398d8dd6408fd880d3b4.jpg" alt="y_{i,k} \in \{0,1\}"></a> 的 inner sum （内部和），给出 binary log loss （二分 log loss）。</p>
<p><a href="generated/sklearn.metrics.log_loss.html#sklearn.metrics.log_loss" title="sklearn.metrics.log_loss"><code>log_loss</code></a> 函数计算出一个 a list of ground-truth labels （已标注的真实数据的标签的列表）和一个 probability matrix （概率矩阵） 的 log loss，由 estimator （估计器）的 <code>predict_proba</code> 方法返回。</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn.metrics import log_loss
&gt;&gt;&gt; y_true = [0, 0, 1, 1]
&gt;&gt;&gt; y_pred = [[.9, .1], [.8, .2], [.3, .7], [.01, .99]]
&gt;&gt;&gt; log_loss(y_true, y_pred)    
0.1738...

</code></pre>
<p><code>y_pred</code> 中的第一个 <code>[.9, .1]</code> 表示第一个样本具有标签 0 的 90% 概率。log loss 是非负数。</p>
<h3 id="33211-马修斯相关系数">3.3.2.11. 马修斯相关系数</h3>
<p><a href="generated/sklearn.metrics.matthews_corrcoef.html#sklearn.metrics.matthews_corrcoef" title="sklearn.metrics.matthews_corrcoef"><code>matthews_corrcoef</code></a> 函数用于计算 binary classes （二分类）的 <a href="https://en.wikipedia.org/wiki/Matthews_correlation_coefficient">Matthew’s correlation coefficient (MCC)</a> 引用自 Wikipedia:</p>
<blockquote>
<p>“Matthews correlation coefficient（马修斯相关系数）用于机器学习，作为 binary (two-class) classifications （二分类）分类质量的度量。它考虑到 true and false positives and negatives （真和假的 positives 和 negatives），通常被认为是可以使用的 balanced measure（平衡措施），即使 classes are of very different sizes （类别大小不同）。MCC 本质上是 -1 和 +1 之间的相关系数值。系数 +1 表示完美预测，0 表示平均随机预测， -1 表示反向预测。statistic （统计量）也称为 phi coefficient （phi）系数。”</p>
</blockquote>
<p>在 binary (two-class) （二分类）情况下，<a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0fd5cc61b1ca3db3b190fbfad2a38813.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0fd5cc61b1ca3db3b190fbfad2a38813.jpg" alt="tp"></a>, <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/107556ec49c074270575d6b99f3d2029.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/107556ec49c074270575d6b99f3d2029.jpg" alt="tn"></a>, <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ad8371d06b000849fa4e2fbd6b386c7d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ad8371d06b000849fa4e2fbd6b386c7d.jpg" alt="fp"></a> 和 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/00a1e1837f700cef7352acfafd328607.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/00a1e1837f700cef7352acfafd328607.jpg" alt="fn"></a> 分别是 true positives, true negatives, false positives 和 false negatives 的数量，MCC 定义为</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b2b4edffe444aa936ad59a769317f692.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b2b4edffe444aa936ad59a769317f692.jpg" alt="MCC = \frac{tp \times tn - fp \times fn}{\sqrt{(tp + fp)(tp + fn)(tn + fp)(tn + fn)}}."></a></p>
<p>在 multiclass case （多类的情况）下， Matthews correlation coefficient（马修斯相关系数） 可以根据 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e279b8169ddd6581c5606c868ba52fae.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e279b8169ddd6581c5606c868ba52fae.jpg" alt="K"></a> classes （类）的 <a href="generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix" title="sklearn.metrics.confusion_matrix"><code>confusion_matrix</code></a> <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4b6d782a67ac392e97215c46b7590bf7.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4b6d782a67ac392e97215c46b7590bf7.jpg" alt="C"></a> 定义 <a href="http://rk.kvl.dk/introduction/index.html">defined</a> 。为了简化定义，考虑以下中间变量:</p>
<ul>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/341145ae3a937e5fa152262d13dc6fcf.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/341145ae3a937e5fa152262d13dc6fcf.jpg" alt="t_k=\sum_{i}^{K} C_{ik}"></a> 真正发生了 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> 类的次数,</li>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5d197c73e8e0bc6ba78f74b2a205886e.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5d197c73e8e0bc6ba78f74b2a205886e.jpg" alt="p_k=\sum_{i}^{K} C_{ki}"></a> <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> 类被预测的次数,</li>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c5396cf8ee04b897a37bc7cd54383eaa.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c5396cf8ee04b897a37bc7cd54383eaa.jpg" alt="c=\sum_{k}^{K} C_{kk}"></a> 正确预测的样本总数,</li>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/094a3a73abc84f5a6c1e0b72e15152d7.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/094a3a73abc84f5a6c1e0b72e15152d7.jpg" alt="s=\sum_{i}{K} \sum_{j}{K} C_{ij}"></a> 样本总数.</li>
</ul>
<p>然后 multiclass MCC 定义为:</p>
<pre><code class="language-py">
![MCC = \frac{
    c \times s - \sum_{k}^{K} p_k \times t_k
}{\sqrt{
    (s^2 - \sum_{k}^{K} p_k^2) \times
    (s^2 - \sum_{k}^{K} t_k^2)
}}](img/e73c79ca71fe87074008fd5f464d686d.jpg)

</code></pre>
<p>当有两个以上的标签时， MCC 的值将不再在 -1 和 +1 之间。相反，根据已经标注的真实数据的数量和分布情况，最小值将介于 -1 和 0 之间。最大值始终为 +1 。</p>
<p>这是一个小例子，说明了使用 <a href="generated/sklearn.metrics.matthews_corrcoef.html#sklearn.metrics.matthews_corrcoef" title="sklearn.metrics.matthews_corrcoef"><code>matthews_corrcoef</code></a> 函数:</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn.metrics import matthews_corrcoef
&gt;&gt;&gt; y_true = [+1, +1, +1, -1]
&gt;&gt;&gt; y_pred = [+1, -1, +1, +1]
&gt;&gt;&gt; matthews_corrcoef(y_true, y_pred)  
-0.33...

</code></pre>
<h3 id="33212-receiver-operating-characteristic-roc">3.3.2.12. Receiver operating characteristic (ROC)</h3>
<p>函数 <a href="generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve" title="sklearn.metrics.roc_curve"><code>roc_curve</code></a> 计算 <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">receiver operating characteristic curve, or ROC curve</a>. 引用 Wikipedia :</p>
<blockquote>
<p>“A receiver operating characteristic (ROC), 或者简单的 ROC 曲线，是一个图形图，说明了 binary classifier （二分分类器）系统的性能，因为 discrimination threshold （鉴别阈值）是变化的。它是通过在不同的阈值设置下，从 true positives out of the positives (TPR = true positive 比例) 与 false positives out of the negatives (FPR = false positive 比例) 绘制 true positive 的比例来创建的。 TPR 也称为 sensitivity（灵敏度），FPR 是减去 specificity（特异性） 或 true negative 比例。”</p>
</blockquote>
<p>该函数需要真正的 binar value （二分值）和 target scores（目标分数），这可以是 positive class 的 probability estimates （概率估计），confidence values（置信度值）或 binary decisions（二分决策）。 这是一个如何使用 <a href="generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve" title="sklearn.metrics.roc_curve"><code>roc_curve</code></a> 函数的小例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.metrics import roc_curve
&gt;&gt;&gt; y = np.array([1, 1, 2, 2])
&gt;&gt;&gt; scores = np.array([0.1, 0.4, 0.35, 0.8])
&gt;&gt;&gt; fpr, tpr, thresholds = roc_curve(y, scores, pos_label=2)
&gt;&gt;&gt; fpr
array([ 0\. ,  0.5,  0.5,  1\. ])
&gt;&gt;&gt; tpr
array([ 0.5,  0.5,  1\. ,  1\. ])
&gt;&gt;&gt; thresholds
array([ 0.8 ,  0.4 ,  0.35,  0.1 ])

</code></pre>
<p>该图显示了这样的 ROC 曲线的示例:</p>
<p><a href="../auto_examples/model_selection/plot_roc.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b38a0de3f98aa9b8837354765bc9e3f6.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b38a0de3f98aa9b8837354765bc9e3f6.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_roc_0011.png"></a></a></p>
<p><a href="generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score" title="sklearn.metrics.roc_auc_score"><code>roc_auc_score</code></a> 函数计算 receiver operating characteristic (ROC) 曲线下的面积，也由 AUC 和 AUROC 表示。通过计算 roc 曲线下的面积，曲线信息总结为一个数字。 有关更多的信息，请参阅 <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve">Wikipedia article on AUC</a> .</p>
<pre><code class="language-py">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.metrics import roc_auc_score
&gt;&gt;&gt; y_true = np.array([0, 0, 1, 1])
&gt;&gt;&gt; y_scores = np.array([0.1, 0.4, 0.35, 0.8])
&gt;&gt;&gt; roc_auc_score(y_true, y_scores)
0.75

</code></pre>
<p>在 multi-label classification （多标签分类）中， <a href="generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score" title="sklearn.metrics.roc_auc_score"><code>roc_auc_score</code></a> 函数通过在标签上进行平均来扩展 <a href="#average">above</a> .</p>
<p>与诸如 subset accuracy （子集精确度），Hamming loss（汉明损失）或 F1 score 的 metrics（指标）相比， ROC 不需要优化每个标签的阈值。<a href="generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score" title="sklearn.metrics.roc_auc_score"><code>roc_auc_score</code></a> 函数也可以用于 multi-class classification （多类分类），如果预测的输出被 binarized （二分化）。</p>
<p><a href="../auto_examples/model_selection/plot_roc.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/aa5e9ace265afd9e1f881564a1923a17.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/aa5e9ace265afd9e1f881564a1923a17.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_roc_0021.png"></a></a></p>
<p>示例:</p>
<ul>
<li>参阅 <a href="../auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py">Receiver Operating Characteristic (ROC)</a> 例如使用 ROC 来评估分类器输出的质量。</li>
<li>参阅 <a href="../auto_examples/model_selection/plot_roc_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py">Receiver Operating Characteristic (ROC) with cross validation</a> 例如使用 ROC 来评估分类器输出质量，使用 cross-validation （交叉验证）。</li>
<li>参阅 <a href="../auto_examples/applications/plot_species_distribution_modeling.html#sphx-glr-auto-examples-applications-plot-species-distribution-modeling-py">Species distribution modeling</a> 例如使用 ROC 来 model species distribution 模拟物种分布。</li>
</ul>
<h3 id="33213-零一损失">3.3.2.13. 零一损失</h3>
<p><a href="generated/sklearn.metrics.zero_one_loss.html#sklearn.metrics.zero_one_loss" title="sklearn.metrics.zero_one_loss"><code>zero_one_loss</code></a> 函数通过 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b3348b4a0dc8e97bcefc5c7489b006db.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b3348b4a0dc8e97bcefc5c7489b006db.jpg" alt="n_{\text{samples}}"></a> 计算 0-1 classification loss (<a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9f4178a540cbe2b9f093702d71bafbe5.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9f4178a540cbe2b9f093702d71bafbe5.jpg" alt="L_{0-1}"></a>) 的 sum （和）或 average （平均值）。默认情况下，函数在样本上 normalizes （标准化）。要获得 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9f4178a540cbe2b9f093702d71bafbe5.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9f4178a540cbe2b9f093702d71bafbe5.jpg" alt="L_{0-1}"></a> 的总和，将 <code>normalize</code> 设置为 <code>False</code>。</p>
<p>在 multilabel classification （多标签分类）中，如果零标签与标签严格匹配，则 <a href="generated/sklearn.metrics.zero_one_loss.html#sklearn.metrics.zero_one_loss" title="sklearn.metrics.zero_one_loss"><code>zero_one_loss</code></a> 将一个子集作为一个子集，如果有任何错误，则为零。默认情况下，函数返回不完全预测子集的百分比。为了得到这样的子集的计数，将 <code>normalize</code> 设置为 <code>False</code> 。</p>
<p>如果 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/07794b8fa83c7e18c5d1fb175fd7d7bd.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/07794b8fa83c7e18c5d1fb175fd7d7bd.jpg" alt="\hat{y}_i"></a> 是第 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" alt="i"></a> 个样本的预测值，<a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4a22ca544916918b2358e5fc7c71b8e6.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4a22ca544916918b2358e5fc7c71b8e6.jpg" alt="y_i"></a> 是相应的真实值，则 0-1 loss <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9f4178a540cbe2b9f093702d71bafbe5.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9f4178a540cbe2b9f093702d71bafbe5.jpg" alt="L_{0-1}"></a> 定义为:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7eb576473ec1de4500c33294115b0719.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7eb576473ec1de4500c33294115b0719.jpg" alt="L_{0-1}(y_i, \hat{y}_i) = 1(\hat{y}_i \not= y_i)"></a></p>
<p>其中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1e4e584cd8a99da7f18a5581de1f7be3.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1e4e584cd8a99da7f18a5581de1f7be3.jpg" alt="1(x)"></a> 是 <a href="https://en.wikipedia.org/wiki/Indicator_function">indicator function</a>.</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn.metrics import zero_one_loss
&gt;&gt;&gt; y_pred = [1, 2, 3, 4]
&gt;&gt;&gt; y_true = [2, 2, 3, 4]
&gt;&gt;&gt; zero_one_loss(y_true, y_pred)
0.25
&gt;&gt;&gt; zero_one_loss(y_true, y_pred, normalize=False)
1

</code></pre>
<p>在具有 binary label indicators （二分标签指示符）的 multilabel （多标签）情况下，第一个标签集 [0,1] 有错误:</p>
<pre><code class="language-py">&gt;&gt;&gt; zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))
0.5

&gt;&gt;&gt; zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)),  normalize=False)
1

</code></pre>
<p>示例:</p>
<ul>
<li>参阅 <a href="../auto_examples/feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py">Recursive feature elimination with cross-validation</a> 例如 zero one loss 使用以通过 cross-validation （交叉验证）执行递归特征消除。</li>
</ul>
<h3 id="33214-brier-分数损失">3.3.2.14. Brier 分数损失</h3>
<p><a href="generated/sklearn.metrics.brier_score_loss.html#sklearn.metrics.brier_score_loss" title="sklearn.metrics.brier_score_loss"><code>brier_score_loss</code></a> 函数计算二进制类的 <a href="https://en.wikipedia.org/wiki/Brier_score">Brier 分数</a> 。引用维基百科：</p>
<blockquote>
<p>“Brier 分数是一个特有的分数函数，用于衡量概率预测的准确性。它适用于预测必须将概率分配给一组相互排斥的离散结果的任务。”</p>
</blockquote>
<p>该函数返回的是 实际结果与可能结果 的预测概率之间均方差的得分。 实际结果必须为1或0（真或假），而实际结果的预测概率可以是0到1之间的值。</p>
<p>Brier 分数损失也在0到1之间，分数越低（均方差越小），预测越准确。它可以被认为是对一组概率预测的 “校准” 的度量。</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4c0a0003e110c44c538fbf113c159a3a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4c0a0003e110c44c538fbf113c159a3a.jpg" alt="BS = \frac{1}{N} \sum_{t=1}{N}(f_t - o_t)2"></a></p>
<p>其中: <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" alt="N"></a> 是预测的总数， <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9b5fb6c6e0f320a3e8e0ba606d601c98.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9b5fb6c6e0f320a3e8e0ba606d601c98.jpg" alt="f_t"></a> 是实际结果 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/09eb9862841b1c17d77e2e4830df3770.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/09eb9862841b1c17d77e2e4830df3770.jpg" alt="o_t"></a> 的预测概率。</p>
<p>这是一个使用这个函数的小例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.metrics import brier_score_loss
&gt;&gt;&gt; y_true = np.array([0, 1, 1, 0])
&gt;&gt;&gt; y_true_categorical = np.array(["spam", "ham", "ham", "spam"])
&gt;&gt;&gt; y_prob = np.array([0.1, 0.9, 0.8, 0.4])
&gt;&gt;&gt; y_pred = np.array([0, 1, 1, 0])
&gt;&gt;&gt; brier_score_loss(y_true, y_prob)
0.055
&gt;&gt;&gt; brier_score_loss(y_true, 1-y_prob, pos_label=0)
0.055
&gt;&gt;&gt; brier_score_loss(y_true_categorical, y_prob, pos_label="ham")
0.055
&gt;&gt;&gt; brier_score_loss(y_true, y_prob &gt; 0.5)
0.0

</code></pre>
<p>示例:</p>
<ul>
<li>请参阅分类器的概率校准 <a href="../auto_examples/calibration/plot_calibration.html#sphx-glr-auto-examples-calibration-plot-calibration-py">Probability calibration of classifiers</a> ，通过 Brier 分数损失使用示例 来执行分类器的概率校准。</li>
</ul>
<p>参考文献:</p>
<ul>
<li>
<ol>
<li>Brier, <a href="http://docs.lib.noaa.gov/rescue/mwr/078/mwr-078-01-0001.pdf">以概率表示的预测验证</a> , 月度天气评估78.1（1950）</li>
</ol> </li>
</ul>
<h2 id="333-多标签排名指标">3.3.3. 多标签排名指标</h2>
<p>在多分类学习中，每个样本可以具有与其相关联的任何数量的真实标签。目标是给予高分，更好地评价真实标签。</p>
<h3 id="3331-覆盖误差">3.3.3.1. 覆盖误差</h3>
<p><a href="generated/sklearn.metrics.coverage_error.html#sklearn.metrics.coverage_error" title="sklearn.metrics.coverage_error"><code>coverage_error</code></a> 函数计算必须包含在最终预测中的标签的平均数，以便预测所有真正的标签。 如果您想知道有多少 top 评分标签，您必须通过平均来预测，而不会丢失任何真正的标签，这很有用。 因此，此指标的最佳价值是真正标签的平均数量。</p>
<p>Note</p>
<p>我们的实现的分数比 Tsoumakas 等人在2010年的分数大1。 这扩展了它来处理一个具有0个真实标签实例的退化情况。</p>
<p>正式地，给定真实标签 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2b0d9f09a2b8a107ace9ce7aa234481e.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2b0d9f09a2b8a107ace9ce7aa234481e.jpg" alt="y \in \left\{0, 1\right\}^{n_\text{samples} \times n_\text{labels}}"></a> 的二进制指示矩阵和与每个标签 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d325b0db5d92ebf952f4b6d810fa43bd.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d325b0db5d92ebf952f4b6d810fa43bd.jpg" alt="\hat{f} \in \mathbb{R}^{n_\text{samples} \times n_\text{labels}}"></a> 相关联的分数，覆盖范围被定义为</p>
<pre><code class="language-py">
![coverage(y, \hat{f}) = \frac{1}{n_{\text{samples}}}
  \sum_{i=0}^{n_{\text{samples}} - 1} \max_{j:y_{ij} = 1} \text{rank}_{ij}](img/fb8da9a6dd6e45015b629002d748d9b1.jpg)

</code></pre>
<p>与 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5eea9f6c78020e75b9cc37d038d297ab.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5eea9f6c78020e75b9cc37d038d297ab.jpg" alt="\text{rank}{ij} = \left|\left\{k: \hat{f}{ik} \geq \hat{f}_{ij} \right\}\right|"></a> 。给定等级定义，通过给出将被分配给所有绑定值的最大等级， <code>y_scores</code> 中的关系会被破坏。</p>
<p>这是一个使用这个函数的小例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.metrics import coverage_error
&gt;&gt;&gt; y_true = np.array([[1, 0, 0], [0, 0, 1]])
&gt;&gt;&gt; y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])
&gt;&gt;&gt; coverage_error(y_true, y_score)
2.5

</code></pre>
<h3 id="3332-标签排名平均精度">3.3.3.2. 标签排名平均精度</h3>
<p><a href="generated/sklearn.metrics.label_ranking_average_precision_score.html#sklearn.metrics.label_ranking_average_precision_score" title="sklearn.metrics.label_ranking_average_precision_score"><code>label_ranking_average_precision_score</code></a> 函数实现标签排名平均精度（LRAP）。 该度量值与 <a href="generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score" title="sklearn.metrics.average_precision_score"><code>average_precision_score</code></a> 函数相关联，但是基于标签排名的概念，而不是精确度和召回。</p>
<p>标签排名平均精度（LRAP）是分配给每个样本的每个真实标签的平均值，真实对总标签与较低分数的比率。 如果能够为每个样本相关标签提供更好的排名，这个指标就会产生更好的分数。 获得的得分总是严格大于0，最佳值为1。如果每个样本只有一个相关标签，则标签排名平均精度等于 <a href="https://en.wikipedia.org/wiki/Mean_reciprocal_rank">平均倒数等级</a> 。</p>
<p>正式地，给定真实标签 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e310c621bd78988800b952eb7542cd88.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e310c621bd78988800b952eb7542cd88.jpg" alt="y \in \mathcal{R}^{n_\text{samples} \times n_\text{labels}}"></a> 的二进制指示矩阵和与每个标签 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9255ba83a88cb73b04d1ca968f9c2b4e.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9255ba83a88cb73b04d1ca968f9c2b4e.jpg" alt="\hat{f} \in \mathcal{R}^{n_\text{samples} \times n_\text{labels}}"></a> 相关联的得分，平均精度被定义为</p>
<pre><code class="language-py">
![LRAP(y, \hat{f}) = \frac{1}{n_{\text{samples}}}
  \sum_{i=0}^{n_{\text{samples}} - 1} \frac{1}{|y_i|}
  \sum_{j:y_{ij} = 1} \frac{|\mathcal{L}_{ij}|}{\text{rank}_{ij}}](img/bec3afcb1362068f9caf79c5c58ea816.jpg)

</code></pre>
<p>与 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f6ce0899ba52f1169500b726ee9c8a92.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f6ce0899ba52f1169500b726ee9c8a92.jpg" alt="\mathcal{L}{ij} = \left\{k: y{ik} = 1, \hat{f}{ik} \geq \hat{f}{ij} \right\}"></a>， <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5eea9f6c78020e75b9cc37d038d297ab.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5eea9f6c78020e75b9cc37d038d297ab.jpg" alt="\text{rank}{ij} = \left|\left\{k: \hat{f}{ik} \geq \hat{f}_{ij} \right\}\right|"></a> 和 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f7dd5b16c1d8c3e278e9a1fa7f49dcd2.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f7dd5b16c1d8c3e278e9a1fa7f49dcd2.jpg" alt="|\cdot|"></a> 是集合的 l0 范数或基数。</p>
<p>这是一个使用这个函数的小例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.metrics import label_ranking_average_precision_score
&gt;&gt;&gt; y_true = np.array([[1, 0, 0], [0, 0, 1]])
&gt;&gt;&gt; y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])
&gt;&gt;&gt; label_ranking_average_precision_score(y_true, y_score) 
0.416...

</code></pre>
<h3 id="3333-排序损失">3.3.3.3. 排序损失</h3>
<p><a href="generated/sklearn.metrics.label_ranking_loss.html#sklearn.metrics.label_ranking_loss" title="sklearn.metrics.label_ranking_loss"><code>label_ranking_loss</code></a> 函数计算在样本上平均排序错误的标签对数量的排序损失，即真实标签的分数低于假标签，由虚假和真实标签的倒数加权。最低可实现的排名损失为零。</p>
<p>正式地，给定真相标签 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2b0d9f09a2b8a107ace9ce7aa234481e.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2b0d9f09a2b8a107ace9ce7aa234481e.jpg" alt="y \in \left\{0, 1\right\}^{n_\text{samples} \times n_\text{labels}}"></a> 的二进制指示矩阵和与每个标签 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d325b0db5d92ebf952f4b6d810fa43bd.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d325b0db5d92ebf952f4b6d810fa43bd.jpg" alt="\hat{f} \in \mathbb{R}^{n_\text{samples} \times n_\text{labels}}"></a> 相关联的得分，排序损失被定义为</p>
<pre><code class="language-py">
![\text{ranking\_loss}(y, \hat{f}) =  \frac{1}{n_{\text{samples}}}
  \sum_{i=0}^{n_{\text{samples}} - 1} \frac{1}{|y_i|(n_\text{labels} - |y_i|)}
  \left|\left\{(k, l): \hat{f}_{ik} &amp;lt; \hat{f}_{il}, y_{ik} = 1, y_{il} = 0&nbsp;\right\}\right|](img/eeb2bac86ebedef3d8d2dcbf5b8c735b.jpg)

</code></pre>
<p>其中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f7dd5b16c1d8c3e278e9a1fa7f49dcd2.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f7dd5b16c1d8c3e278e9a1fa7f49dcd2.jpg" alt="|\cdot|"></a> 是 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/abee3460000f8532d0df4e1b1d1928e8.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/abee3460000f8532d0df4e1b1d1928e8.jpg" alt="\ell_0"></a> 范数或集合的基数。</p>
<p>这是一个使用这个函数的小例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.metrics import label_ranking_loss
&gt;&gt;&gt; y_true = np.array([[1, 0, 0], [0, 0, 1]])
&gt;&gt;&gt; y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])
&gt;&gt;&gt; label_ranking_loss(y_true, y_score) 
0.75...
&gt;&gt;&gt; # With the following prediction, we have perfect and minimal loss
&gt;&gt;&gt; y_score = np.array([[1.0, 0.1, 0.2], [0.1, 0.2, 0.9]])
&gt;&gt;&gt; label_ranking_loss(y_true, y_score)
0.0

</code></pre>
<p>参考文献:</p>
<ul>
<li>Tsoumakas, G., Katakis, I., &amp; Vlahavas, I. (2010). 挖掘多标签数据。在数据挖掘和知识发现手册（第667-685页）。美国 Springer.</li>
</ul>
<h2 id="334-回归指标">3.3.4. 回归指标</h2>
<p>该 <a href="classes.html#module-sklearn.metrics" title="sklearn.metrics"><code>sklearn.metrics</code></a> 模块实现了一些 loss, score 以及 utility 函数以测量 regression（回归）的性能. 其中一些已经被加强以处理多个输出的场景: <a href="generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error" title="sklearn.metrics.mean_squared_error"><code>mean_squared_error</code></a>, <a href="generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error" title="sklearn.metrics.mean_absolute_error"><code>mean_absolute_error</code></a>, <a href="generated/sklearn.metrics.explained_variance_score.html#sklearn.metrics.explained_variance_score" title="sklearn.metrics.explained_variance_score"><code>explained_variance_score</code></a> 和 <a href="generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score" title="sklearn.metrics.r2_score"><code>r2_score</code></a>.</p>
<p>这些函数有 <code>multioutput</code> 这样一个 keyword（关键的）参数, 它指定每一个目标的 score（得分）或 loss（损失）的平均值的方式. 默认是 <code>'uniform_average'</code>, 其指定了输出时一致的权重均值. 如果一个 <code>ndarray</code> 的 shape <code>(n_outputs,)</code> 被传递, 则其中的 entries（条目）将被解释为权重，并返回相应的加权平均值. 如果 <code>multioutput</code> 指定了 <code>'raw_values'</code> , 则所有未改变的部分 score（得分）或 loss（损失）将以 <code>(n_outputs,)</code> 形式的数组返回.</p>
<p>该 <a href="generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score" title="sklearn.metrics.r2_score"><code>r2_score</code></a> 和 <a href="generated/sklearn.metrics.explained_variance_score.html#sklearn.metrics.explained_variance_score" title="sklearn.metrics.explained_variance_score"><code>explained_variance_score</code></a> 函数接受一个额外的值 <code>'variance_weighted'</code> 用于 <code>multioutput</code> 参数. 该选项通过相应目标变量的方差使得每个单独的 score 进行加权. 该设置量化了全局捕获的未缩放方差. 如果目标变量的大小不一样, 则该 score 更好地解释了较高的方差变量. <code>multioutput='variance_weighted'</code> 是 <a href="generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score" title="sklearn.metrics.r2_score"><code>r2_score</code></a> 的默认值以向后兼容. 以后该值会被改成 <code>uniform_average</code>.</p>
<h3 id="3341-解释方差得分">3.3.4.1. 解释方差得分</h3>
<p>该 <a href="generated/sklearn.metrics.explained_variance_score.html#sklearn.metrics.explained_variance_score" title="sklearn.metrics.explained_variance_score"><code>explained_variance_score</code></a> 函数计算了 <a href="https://en.wikipedia.org/wiki/Explained_variation">explained variance regression score（解释的方差回归得分）</a>.</p>
<p>如果 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/047826f1c2e6f2687b304cb5217be8d8.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/047826f1c2e6f2687b304cb5217be8d8.jpg" alt="\hat{y}"></a> 是预估的目标输出, <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0775c03fc710a24df297dedcec515aaf.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0775c03fc710a24df297dedcec515aaf.jpg" alt="y"></a> 是相应（正确的）目标输出, 并且 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/94435503540f66cab82015a35139213d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/94435503540f66cab82015a35139213d.jpg" alt="Var"></a> is <a href="https://en.wikipedia.org/wiki/Variance">方差</a>, 标准差的平方, 那么解释的方差预估如下:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/507fd1a87cb6a0196c0203a0af5e9bbb.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/507fd1a87cb6a0196c0203a0af5e9bbb.jpg" alt="\texttt{explained\_{}variance}(y, \hat{y}) = 1 - \frac{Var\{ y - \hat{y}\}}{Var\{y\}}"></a></p>
<p>最好的得分是 1.0, 值越低越差.</p>
<p>下面是一下有关 <a href="generated/sklearn.metrics.explained_variance_score.html#sklearn.metrics.explained_variance_score" title="sklearn.metrics.explained_variance_score"><code>explained_variance_score</code></a> 函数使用的一些例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn.metrics import explained_variance_score
&gt;&gt;&gt; y_true = [3, -0.5, 2, 7]
&gt;&gt;&gt; y_pred = [2.5, 0.0, 2, 8]
&gt;&gt;&gt; explained_variance_score(y_true, y_pred)  
0.957...
&gt;&gt;&gt; y_true = [[0.5, 1], [-1, 1], [7, -6]]
&gt;&gt;&gt; y_pred = [[0, 2], [-1, 2], [8, -5]]
&gt;&gt;&gt; explained_variance_score(y_true, y_pred, multioutput='raw_values')
... 
array([ 0.967...,  1\.        ])
&gt;&gt;&gt; explained_variance_score(y_true, y_pred, multioutput=[0.3, 0.7])
... 
0.990...

</code></pre>
<h3 id="3342-平均绝对误差">3.3.4.2. 平均绝对误差</h3>
<p>该 <a href="generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error" title="sklearn.metrics.mean_absolute_error"><code>mean_absolute_error</code></a> 函数计算了 <a href="https://en.wikipedia.org/wiki/Mean_absolute_error">平均绝对误差</a>, 一个对应绝对误差损失预期值或者 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/bd3f0762bab34671ec8bfb8ace2cc129.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/bd3f0762bab34671ec8bfb8ace2cc129.jpg" alt="l1"></a>-norm 损失的风险度量.</p>
<p>如果 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/07794b8fa83c7e18c5d1fb175fd7d7bd.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/07794b8fa83c7e18c5d1fb175fd7d7bd.jpg" alt="\hat{y}_i"></a> 是 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" alt="i"></a>-th 样本的预测值, 并且 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4a22ca544916918b2358e5fc7c71b8e6.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4a22ca544916918b2358e5fc7c71b8e6.jpg" alt="y_i"></a> 是对应的真实值, 则平均绝对误差 (MAE) 预估的 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b3348b4a0dc8e97bcefc5c7489b006db.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b3348b4a0dc8e97bcefc5c7489b006db.jpg" alt="n_{\text{samples}}"></a> 定义如下</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7d379816608eb84009d45f0e26772256.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7d379816608eb84009d45f0e26772256.jpg" alt="\text{MAE}(y, \hat{y}) = \frac{1}{n_{\text{samples}}} \sum_{i=0}^{n_{\text{samples}}-1} \left| y_i - \hat{y}_i \right|."></a></p>
<p>下面是一个有关 <a href="generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error" title="sklearn.metrics.mean_absolute_error"><code>mean_absolute_error</code></a> 函数用法的小例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn.metrics import mean_absolute_error
&gt;&gt;&gt; y_true = [3, -0.5, 2, 7]
&gt;&gt;&gt; y_pred = [2.5, 0.0, 2, 8]
&gt;&gt;&gt; mean_absolute_error(y_true, y_pred)
0.5
&gt;&gt;&gt; y_true = [[0.5, 1], [-1, 1], [7, -6]]
&gt;&gt;&gt; y_pred = [[0, 2], [-1, 2], [8, -5]]
&gt;&gt;&gt; mean_absolute_error(y_true, y_pred)
0.75
&gt;&gt;&gt; mean_absolute_error(y_true, y_pred, multioutput='raw_values')
array([ 0.5,  1\. ])
&gt;&gt;&gt; mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])
... 
0.849...

</code></pre>
<h3 id="3343-均方误差">3.3.4.3. 均方误差</h3>
<p>该 <a href="generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error" title="sklearn.metrics.mean_squared_error"><code>mean_squared_error</code></a> 函数计算了 <a href="https://en.wikipedia.org/wiki/Mean_squared_error">均方误差</a>, 一个对应于平方（二次）误差或损失的预期值的风险度量.</p>
<p>如果 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/07794b8fa83c7e18c5d1fb175fd7d7bd.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/07794b8fa83c7e18c5d1fb175fd7d7bd.jpg" alt="\hat{y}_i"></a> 是 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" alt="i"></a>-th 样本的预测值, 并且 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4a22ca544916918b2358e5fc7c71b8e6.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4a22ca544916918b2358e5fc7c71b8e6.jpg" alt="y_i"></a> 是对应的真实值, 则均方误差（MSE）预估的 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b3348b4a0dc8e97bcefc5c7489b006db.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b3348b4a0dc8e97bcefc5c7489b006db.jpg" alt="n_{\text{samples}}"></a> 定义如下</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ba3c1f82d7a66df41015761326619e26.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ba3c1f82d7a66df41015761326619e26.jpg" alt="\text{MSE}(y, \hat{y}) = \frac{1}{n_\text{samples}} \sum_{i=0}{n_\text{samples} - 1} (y_i - \hat{y}_i)2."></a></p>
<p>下面是一个有关 <a href="generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error" title="sklearn.metrics.mean_squared_error"><code>mean_squared_error</code></a> 函数用法的小例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn.metrics import mean_squared_error
&gt;&gt;&gt; y_true = [3, -0.5, 2, 7]
&gt;&gt;&gt; y_pred = [2.5, 0.0, 2, 8]
&gt;&gt;&gt; mean_squared_error(y_true, y_pred)
0.375
&gt;&gt;&gt; y_true = [[0.5, 1], [-1, 1], [7, -6]]
&gt;&gt;&gt; y_pred = [[0, 2], [-1, 2], [8, -5]]
&gt;&gt;&gt; mean_squared_error(y_true, y_pred)  
0.7083...

</code></pre>
<p>Examples:</p>
<ul>
<li>点击 <a href="../auto_examples/ensemble/plot_gradient_boosting_regression.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regression-py">Gradient Boosting regression</a> 查看均方误差用于梯度上升（gradient boosting）回归的使用例子。</li>
</ul>
<h3 id="3344-均方误差对数">3.3.4.4. 均方误差对数</h3>
<p>该 <a href="generated/sklearn.metrics.mean_squared_log_error.html#sklearn.metrics.mean_squared_log_error" title="sklearn.metrics.mean_squared_log_error"><code>mean_squared_log_error</code></a> 函数计算了一个对应平方对数（二次）误差或损失的预估值风险度量.</p>
<p>如果 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/07794b8fa83c7e18c5d1fb175fd7d7bd.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/07794b8fa83c7e18c5d1fb175fd7d7bd.jpg" alt="\hat{y}_i"></a> 是 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" alt="i"></a>-th 样本的预测值, 并且 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4a22ca544916918b2358e5fc7c71b8e6.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4a22ca544916918b2358e5fc7c71b8e6.jpg" alt="y_i"></a> 是对应的真实值, 则均方误差对数（MSLE）预估的 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b3348b4a0dc8e97bcefc5c7489b006db.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b3348b4a0dc8e97bcefc5c7489b006db.jpg" alt="n_{\text{samples}}"></a> 定义如下</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/843ad36e77423c1d0d068bf1b0af24fb.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/843ad36e77423c1d0d068bf1b0af24fb.jpg" alt="\text{MSLE}(y, \hat{y}) = \frac{1}{n_\text{samples}} \sum_{i=0}{n_\text{samples} - 1} (\log_e (1 + y_i) - \log_e (1 + \hat{y}_i) )2."></a></p>
<p>其中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/001d34ad977d110ce0931112c362d07e.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/001d34ad977d110ce0931112c362d07e.jpg" alt="\log_e (x)"></a> 表示 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5c82dbae35dc43d2f556f9f284d9d184.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5c82dbae35dc43d2f556f9f284d9d184.jpg" alt="x"></a> 的自然对数. 当目标具有指数增长的趋势时, 该指标最适合使用, 例如人口数量, 跨年度商品的平均销售额等. 请注意, 该指标会对低于预测的估计值进行估计.</p>
<p>下面是一个有关 <a href="generated/sklearn.metrics.mean_squared_log_error.html#sklearn.metrics.mean_squared_log_error" title="sklearn.metrics.mean_squared_log_error"><code>mean_squared_log_error</code></a> 函数用法的小例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn.metrics import mean_squared_log_error
&gt;&gt;&gt; y_true = [3, 5, 2.5, 7]
&gt;&gt;&gt; y_pred = [2.5, 5, 4, 8]
&gt;&gt;&gt; mean_squared_log_error(y_true, y_pred)  
0.039...
&gt;&gt;&gt; y_true = [[0.5, 1], [1, 2], [7, 6]]
&gt;&gt;&gt; y_pred = [[0.5, 2], [1, 2.5], [8, 8]]
&gt;&gt;&gt; mean_squared_log_error(y_true, y_pred)  
0.044...

</code></pre>
<h3 id="3345-中位绝对误差">3.3.4.5. 中位绝对误差</h3>
<p>该 <a href="generated/sklearn.metrics.median_absolute_error.html#sklearn.metrics.median_absolute_error" title="sklearn.metrics.median_absolute_error"><code>median_absolute_error</code></a> 函数尤其有趣, 因为它的离群值很强. 通过取目标和预测之间的所有绝对差值的中值来计算损失.</p>
<p>如果 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/07794b8fa83c7e18c5d1fb175fd7d7bd.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/07794b8fa83c7e18c5d1fb175fd7d7bd.jpg" alt="\hat{y}_i"></a> 是 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" alt="i"></a>-th 样本的预测值, 并且 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4a22ca544916918b2358e5fc7c71b8e6.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4a22ca544916918b2358e5fc7c71b8e6.jpg" alt="y_i"></a> 是对应的真实值, 则中位绝对误差（MedAE）预估的 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b3348b4a0dc8e97bcefc5c7489b006db.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b3348b4a0dc8e97bcefc5c7489b006db.jpg" alt="n_{\text{samples}}"></a> 定义如下</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/60fef7a79e647e4e8dc02f0b0dc25772.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/60fef7a79e647e4e8dc02f0b0dc25772.jpg" alt="\text{MedAE}(y, \hat{y}) = \text{median}(\mid y_1 - \hat{y}_1 \mid, \ldots, \mid y_n - \hat{y}_n \mid)."></a></p>
<p>该 <a href="generated/sklearn.metrics.median_absolute_error.html#sklearn.metrics.median_absolute_error" title="sklearn.metrics.median_absolute_error"><code>median_absolute_error</code></a> 函数不支持多输出.</p>
<p>下面是一个有关 <a href="generated/sklearn.metrics.median_absolute_error.html#sklearn.metrics.median_absolute_error" title="sklearn.metrics.median_absolute_error"><code>median_absolute_error</code></a> 函数用法的小例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn.metrics import median_absolute_error
&gt;&gt;&gt; y_true = [3, -0.5, 2, 7]
&gt;&gt;&gt; y_pred = [2.5, 0.0, 2, 8]
&gt;&gt;&gt; median_absolute_error(y_true, y_pred)
0.5

</code></pre>
<h3 id="3346-r-score-可决系数">3.3.4.6. R² score, 可决系数</h3>
<p>该 <a href="generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score" title="sklearn.metrics.r2_score"><code>r2_score</code></a> 函数计算了 computes R², 即 <a href="https://en.wikipedia.org/wiki/Coefficient_of_determination">可决系数</a>. 它提供了将来样本如何可能被模型预测的估量. 最佳分数为 1.0, 可以为负数（因为模型可能会更糟）. 总是预测 y 的预期值，不考虑输入特征的常数模型将得到 R^2 得分为 0.0.</p>
<p>如果 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/07794b8fa83c7e18c5d1fb175fd7d7bd.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/07794b8fa83c7e18c5d1fb175fd7d7bd.jpg" alt="\hat{y}_i"></a> 是 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" alt="i"></a>-th 样本的预测值, 并且 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4a22ca544916918b2358e5fc7c71b8e6.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4a22ca544916918b2358e5fc7c71b8e6.jpg" alt="y_i"></a> 是对应的真实值, 则 R² 得分预估的 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b3348b4a0dc8e97bcefc5c7489b006db.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b3348b4a0dc8e97bcefc5c7489b006db.jpg" alt="n_{\text{samples}}"></a> 定义如下</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/520d26d427ec8afe74b5538d779f5f49.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/520d26d427ec8afe74b5538d779f5f49.jpg" alt="R2(y, \hat{y}) = 1 - \frac{\sum_{i=0}{n_{\text{samples}} - 1} (y_i - \hat{y}i)^2}{\sum{i=0}{n_\text{samples} - 1} (y_i - \bar{y})2}"></a></p>
<p>其中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7a95e7e32309847d96c207051da29ea9.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7a95e7e32309847d96c207051da29ea9.jpg" alt="\bar{y} =  \frac{1}{n_{\text{samples}}} \sum_{i=0}^{n_{\text{samples}} - 1} y_i"></a>.</p>
<p>下面是一个有关 <a href="generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score" title="sklearn.metrics.r2_score"><code>r2_score</code></a> 函数用法的小例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn.metrics import r2_score
&gt;&gt;&gt; y_true = [3, -0.5, 2, 7]
&gt;&gt;&gt; y_pred = [2.5, 0.0, 2, 8]
&gt;&gt;&gt; r2_score(y_true, y_pred)  
0.948...
&gt;&gt;&gt; y_true = [[0.5, 1], [-1, 1], [7, -6]]
&gt;&gt;&gt; y_pred = [[0, 2], [-1, 2], [8, -5]]
&gt;&gt;&gt; r2_score(y_true, y_pred, multioutput='variance_weighted')
... 
0.938...
&gt;&gt;&gt; y_true = [[0.5, 1], [-1, 1], [7, -6]]
&gt;&gt;&gt; y_pred = [[0, 2], [-1, 2], [8, -5]]
&gt;&gt;&gt; r2_score(y_true, y_pred, multioutput='uniform_average')
... 
0.936...
&gt;&gt;&gt; r2_score(y_true, y_pred, multioutput='raw_values')
... 
array([ 0.965...,  0.908...])
&gt;&gt;&gt; r2_score(y_true, y_pred, multioutput=[0.3, 0.7])
... 
0.925...

</code></pre>
<p>示例:</p>
<ul>
<li>点击 <a href="../auto_examples/linear_model/plot_lasso_and_elasticnet.html#sphx-glr-auto-examples-linear-model-plot-lasso-and-elasticnet-py">Lasso and Elastic Net for Sparse Signals</a> 查看关于R²用于评估在Lasso and Elastic Net on sparse signals上的使用.</li>
</ul>
<h2 id="335-聚类指标">3.3.5. 聚类指标</h2>
<p>该 <a href="classes.html#module-sklearn.metrics" title="sklearn.metrics"><code>sklearn.metrics</code></a> 模块实现了一些 loss, score 和 utility 函数. 更多信息请参阅 <a href="clustering.html#clustering-evaluation">聚类性能度量</a> 部分, 例如聚类, 以及用于二分聚类的 <a href="biclustering.html#biclustering-evaluation">Biclustering 评测</a>.</p>
<h2 id="336-虚拟估计">3.3.6. 虚拟估计</h2>
<p>在进行监督学习的过程中，简单的 sanity check（理性检查）包括将人的估计与简单的经验法则进行比较. <a href="generated/sklearn.dummy.DummyClassifier.html#sklearn.dummy.DummyClassifier" title="sklearn.dummy.DummyClassifier"><code>DummyClassifier</code></a> 实现了几种简单的分类策略:</p>
<ul>
<li> <p><code>stratified</code> 通过在训练集类分布方面来生成随机预测.</p> </li>
<li> <p><code>most_frequent</code> 总是预测训练集中最常见的标签.</p> </li>
<li> <p><code>prior</code> always predicts the class that maximizes the class prior (like <code>most_frequent</code>) and ``predict_proba` returns the class prior.</p> </li>
<li> <p><code>uniform</code> 随机产生预测.</p> </li>
<li> <pre><code class="language-py">constant 总是预测用户提供的常量标签.
</code></pre> <p>A major motivation of this method is F1-scoring, when the positive class is in the minority. 这种方法的主要动机是 F1-scoring, 当 positive class（正类）较少时.</p> </li>
</ul>
<p>请注意, 这些所有的策略, <code>predict</code> 方法彻底的忽略了输入数据!</p>
<p>为了说明 <a href="generated/sklearn.dummy.DummyClassifier.html#sklearn.dummy.DummyClassifier" title="sklearn.dummy.DummyClassifier"><code>DummyClassifier</code></a>, 首先让我们创建一个 imbalanced dataset:</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn.datasets import load_iris
&gt;&gt;&gt; from sklearn.model_selection import train_test_split
&gt;&gt;&gt; iris = load_iris()
&gt;&gt;&gt; X, y = iris.data, iris.target
&gt;&gt;&gt; y[y != 1] = -1
&gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

</code></pre>
<p>接下来, 让我们比较一下 <code>SVC</code> 和 <code>most_frequent</code> 的准确性.</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn.dummy import DummyClassifier
&gt;&gt;&gt; from sklearn.svm import SVC
&gt;&gt;&gt; clf = SVC(kernel='linear', C=1).fit(X_train, y_train)
&gt;&gt;&gt; clf.score(X_test, y_test) 
0.63...
&gt;&gt;&gt; clf = DummyClassifier(strategy='most_frequent',random_state=0)
&gt;&gt;&gt; clf.fit(X_train, y_train)
DummyClassifier(constant=None, random_state=0, strategy='most_frequent')
&gt;&gt;&gt; clf.score(X_test, y_test)  
0.57...

</code></pre>
<p>我们看到 <code>SVC</code> 没有比一个 dummy classifier（虚拟分类器）好很多. 现在, 让我们来更改一下 kernel:</p>
<pre><code class="language-py">&gt;&gt;&gt; clf = SVC(kernel='rbf', C=1).fit(X_train, y_train)
&gt;&gt;&gt; clf.score(X_test, y_test)  
0.97...

</code></pre>
<p>我们看到准确率提升到将近 100%. 建议采用交叉验证策略, 以更好地估计精度, 如果不是太耗 CPU 的话. 更多信息请参阅 <a href="cross_validation.html#cross-validation">交叉验证：评估估算器的表现</a> 部分. 此外，如果要优化参数空间，强烈建议您使用适当的方法; 更多详情请参阅 <a href="grid_search.html#grid-search">调整估计器的超参数</a> 部分.</p>
<p>通常来说，当分类器的准确度太接近随机情况时，这可能意味着出现了一些问题: 特征没有帮助, 超参数没有正确调整, class 不平衡造成分类器有问题等…</p>
<p><a href="generated/sklearn.dummy.DummyRegressor.html#sklearn.dummy.DummyRegressor" title="sklearn.dummy.DummyRegressor"><code>DummyRegressor</code></a> 还实现了四个简单的经验法则来进行回归:</p>
<ul>
<li><code>mean</code> 总是预测训练目标的平均值.</li>
<li><code>median</code> 总是预测训练目标的中位数.</li>
<li><code>quantile</code> 总是预测用户提供的训练目标的 quantile（分位数）.</li>
<li><code>constant</code> 总是预测由用户提供的常数值.</li>
</ul>
<p>在以上所有的策略中, <code>predict</code> 方法完全忽略了输入数据.</p>
</div>
<hr class="uk-article-divider">
<div class="uk-block uk-block-muted uk-padding-top-remove uk-padding-bottom-remove uk-margin-large-top  book-recommend-wrap">
<div class="uk-margin-top uk-margin-bottom uk-margin-left uk-margin-right">
<div class="uk-margin uk-text-muted "><i class="uk-icon-outdent uk-icon-justify uk-margin-small-right"></i>书籍推荐</div>
<div class="books">
<ul class="uk-book-list">
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/33/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/33/index.html">Scapy 中文文档</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/15.html">wizardforcel</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">10页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年5月3日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 17个">17</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/154/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/154/index.html">Python 学习总结</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/86.html">itroger</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">11页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月12日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 0个">0</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/161/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/161/index.html">关于python的面试题</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/92.html">jackfrued</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">271页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月26日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 33个">33</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/159/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/code_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/159/index.html">im-service 简介</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/91.html">yu000hong</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="code">code</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">37页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月26日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 0个">0</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/69/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/android_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/69/index.html">Android 资源大全中文版</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/19.html">伯乐在线</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="android">android</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">1页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月6日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 1046个">1046</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/12/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/swift_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/12/index.html">Swift 官方教程</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/8.html">numbbbbb</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="swift">swift</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">51页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年5月3日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 18022个">18022</span>
</div>
</div>
</div>
</li>
<hr>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
<nav class="tm-navbar uk-navbar uk-navbar-attached reader-nav">
<div class="uk-float-left uk-margin-small-top">
<a href="javascript:;" title="目录菜单" class="show-menu  uk-icon-hover  uk-icon-align-justify uk-margin-right"></a>
<div data-uk-dropdown="{mode:'click',pos:'bottom-left'}" class="font-setting-wrap">
<a class="uk-icon-hover uk-icon-font uk-margin-right" aria-label="字体设置" href="javascript:;"></a>
<div class="uk-dropdown dropdown-menu">
<div class="dropdown-caret"><span class="caret-outer"></span><span class="caret-inner"></span></div>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-reduce">小字</button>
<button class="uk-button-link button size-2 font-enlarge">大字</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-1 ">宋体</button>
<button class="uk-button-link button size-2 font-2 ">黑体</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-3 color-theme-sun "><i class="uk-icon-sun-o"></i>白天</button>
<button class="uk-button-link button size-3 color-theme-eye "><i class="uk-icon-eye"></i>护眼</button>
<button class="uk-button-link button size-3 color-theme-moon "><i class="uk-icon-moon-o"></i>夜晚</button></div>
</div>
</div>
<a class="logo uk-margin-right" href="../../../" title="返回首页"><img class="" src="../../../static/components/images/icon_32.png" /></a>
</div>
<div class="uk-navbar-flip  uk-hidden-small">
<div id="share-box"></div>
</div>
</nav>
<div id="menu-id" class="uk-offcanvas reader-offcanvas">
<div class="uk-offcanvas-bar">
<ul class="book-menu-bar uk-nav uk-nav-offcanvas" data-uk-nav>
<li>
<a href="../../../book/170/index.html" data-book-page-rel-url="index.html" data-book-page-id="0" title="封面">封面</a>
</li>
<li>
<a class="pjax" href="../../../book/170/readme.html" data-book-page-rel-url="readme.html" data-book-page-id="0" title="简介">简介</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/79.html" title="安装 scikit-learn" data-book-page-rel-url="docs/79.html" data-book-page-id="11491">安装 scikit-learn</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/1.html" title="1. 监督学习" data-book-page-rel-url="docs/1.html" data-book-page-id="11492">1. 监督学习</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/2.html" title="1.1. 广义线性模型" data-book-page-rel-url="docs/2.html" data-book-page-id="11493">1.1. 广义线性模型</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/3.html" title="1.2. 线性和二次判别分析" data-book-page-rel-url="docs/3.html" data-book-page-id="11494">1.2. 线性和二次判别分析</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/4.html" title="1.3. 内核岭回归" data-book-page-rel-url="docs/4.html" data-book-page-id="11495">1.3. 内核岭回归</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/5.html" title="1.4. 支持向量机" data-book-page-rel-url="docs/5.html" data-book-page-id="11496">1.4. 支持向量机</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/6.html" title="1.5. 随机梯度下降" data-book-page-rel-url="docs/6.html" data-book-page-id="11497">1.5. 随机梯度下降</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/7.html" title="1.6. 最近邻" data-book-page-rel-url="docs/7.html" data-book-page-id="11498">1.6. 最近邻</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/8.html" title="1.7. 高斯过程" data-book-page-rel-url="docs/8.html" data-book-page-id="11499">1.7. 高斯过程</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/9.html" title="1.8. 交叉分解" data-book-page-rel-url="docs/9.html" data-book-page-id="11500">1.8. 交叉分解</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/10.html" title="1.9. 朴素贝叶斯" data-book-page-rel-url="docs/10.html" data-book-page-id="11501">1.9. 朴素贝叶斯</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/11.html" title="1.10. 决策树" data-book-page-rel-url="docs/11.html" data-book-page-id="11502">1.10. 决策树</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/12.html" title="1.11. 集成方法" data-book-page-rel-url="docs/12.html" data-book-page-id="11503">1.11. 集成方法</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/13.html" title="1.12. 多类和多标签算法" data-book-page-rel-url="docs/13.html" data-book-page-id="11504">1.12. 多类和多标签算法</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/14.html" title="1.13. 特征选择" data-book-page-rel-url="docs/14.html" data-book-page-id="11505">1.13. 特征选择</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/15.html" title="1.14. 半监督学习" data-book-page-rel-url="docs/15.html" data-book-page-id="11506">1.14. 半监督学习</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/16.html" title="1.15. 等式回归" data-book-page-rel-url="docs/16.html" data-book-page-id="11507">1.15. 等式回归</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/17.html" title="1.16. 概率校准" data-book-page-rel-url="docs/17.html" data-book-page-id="11508">1.16. 概率校准</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/18.html" title="1.17. 神经网络模型（有监督）" data-book-page-rel-url="docs/18.html" data-book-page-id="11509">1.17. 神经网络模型（有监督）</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/19.html" title="2. 无监督学习" data-book-page-rel-url="docs/19.html" data-book-page-id="11510">2. 无监督学习</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/20.html" title="2.1. 高斯混合模型" data-book-page-rel-url="docs/20.html" data-book-page-id="11511">2.1. 高斯混合模型</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/21.html" title="2.2. 流形学习" data-book-page-rel-url="docs/21.html" data-book-page-id="11512">2.2. 流形学习</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/22.html" title="2.3. 聚类" data-book-page-rel-url="docs/22.html" data-book-page-id="11513">2.3. 聚类</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/23.html" title="2.4. 双聚类" data-book-page-rel-url="docs/23.html" data-book-page-id="11514">2.4. 双聚类</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/24.html" title="2.5. 分解成分中的信号（矩阵分解问题）" data-book-page-rel-url="docs/24.html" data-book-page-id="11515">2.5. 分解成分中的信号（矩阵分解问题）</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/25.html" title="2.6. 协方差估计" data-book-page-rel-url="docs/25.html" data-book-page-id="11516">2.6. 协方差估计</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/26.html" title="2.7. 经验协方差" data-book-page-rel-url="docs/26.html" data-book-page-id="11517">2.7. 经验协方差</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/27.html" title="2.8. 收敛协方差" data-book-page-rel-url="docs/27.html" data-book-page-id="11518">2.8. 收敛协方差</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/28.html" title="2.9. 稀疏逆协方差" data-book-page-rel-url="docs/28.html" data-book-page-id="11519">2.9. 稀疏逆协方差</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/29.html" title="2.10. Robust 协方差估计" data-book-page-rel-url="docs/29.html" data-book-page-id="11520">2.10. Robust 协方差估计</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/30.html" title="2.11. 新奇和异常值检测" data-book-page-rel-url="docs/30.html" data-book-page-id="11521">2.11. 新奇和异常值检测</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/31.html" title="2.12. 密度估计" data-book-page-rel-url="docs/31.html" data-book-page-id="11522">2.12. 密度估计</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/32.html" title="2.13. 神经网络模型（无监督）" data-book-page-rel-url="docs/32.html" data-book-page-id="11523">2.13. 神经网络模型（无监督）</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/33.html" title="3. 模型选择和评估" data-book-page-rel-url="docs/33.html" data-book-page-id="11524">3. 模型选择和评估</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/34.html" title="3.1. 交叉验证：评估估算器的表现" data-book-page-rel-url="docs/34.html" data-book-page-id="11525">3.1. 交叉验证：评估估算器的表现</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/35.html" title="3.2. 调整估计器的超参数" data-book-page-rel-url="docs/35.html" data-book-page-id="11526">3.2. 调整估计器的超参数</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/53.html" title="3.3. 模型评估: 量化预测的质量" data-book-page-rel-url="docs/53.html" data-book-page-id="11527">3.3. 模型评估: 量化预测的质量</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/54.html" title="3.4. 模型持久化" data-book-page-rel-url="docs/54.html" data-book-page-id="11528">3.4. 模型持久化</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/55.html" title="3.5. 验证曲线: 绘制分数以评估模型" data-book-page-rel-url="docs/55.html" data-book-page-id="11529">3.5. 验证曲线: 绘制分数以评估模型</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/56.html" title="4. 数据集转换" data-book-page-rel-url="docs/56.html" data-book-page-id="11530">4. 数据集转换</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/57.html" title="4.1. Pipeline（管道）和 FeatureUnion（特征联合）: 合并的评估器" data-book-page-rel-url="docs/57.html" data-book-page-id="11531">4.1. Pipeline（管道）和 FeatureUnion（特征联合）: 合并的评估器</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/58.html" title="4.2. 特征提取" data-book-page-rel-url="docs/58.html" data-book-page-id="11532">4.2. 特征提取</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/59.html" title="4.3. 预处理数据" data-book-page-rel-url="docs/59.html" data-book-page-id="11533">4.3. 预处理数据</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/60.html" title="4.4. 无监督降维" data-book-page-rel-url="docs/60.html" data-book-page-id="11534">4.4. 无监督降维</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/61.html" title="4.5. 随机投影" data-book-page-rel-url="docs/61.html" data-book-page-id="11535">4.5. 随机投影</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/62.html" title="4.6. 内核近似" data-book-page-rel-url="docs/62.html" data-book-page-id="11536">4.6. 内核近似</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/63.html" title="4.7. 成对的矩阵, 类别和核函数" data-book-page-rel-url="docs/63.html" data-book-page-id="11537">4.7. 成对的矩阵, 类别和核函数</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/64.html" title="4.8. 预测目标 (`y`.html) 的转换" data-book-page-rel-url="docs/64.html" data-book-page-id="11538">4.8. 预测目标 (`y`.html) 的转换</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/65.html" title="5. 数据集加载工具" data-book-page-rel-url="docs/65.html" data-book-page-id="11539">5. 数据集加载工具</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/66.html" title="6. 大规模计算的策略: 更大量的数据" data-book-page-rel-url="docs/66.html" data-book-page-id="11540">6. 大规模计算的策略: 更大量的数据</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/67.html" title="7. 计算性能" data-book-page-rel-url="docs/67.html" data-book-page-id="11541">7. 计算性能</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/68.html" title="使用 scikit-learn 介绍机器学习" data-book-page-rel-url="docs/68.html" data-book-page-id="11542">使用 scikit-learn 介绍机器学习</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/69.html" title="关于科学数据处理的统计学习教程" data-book-page-rel-url="docs/69.html" data-book-page-id="11543">关于科学数据处理的统计学习教程</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/70.html" title="机器学习: scikit-learn 中的设置以及预估对象" data-book-page-rel-url="docs/70.html" data-book-page-id="11544">机器学习: scikit-learn 中的设置以及预估对象</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/71.html" title="监督学习：从高维观察预测输出变量" data-book-page-rel-url="docs/71.html" data-book-page-id="11545">监督学习：从高维观察预测输出变量</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/72.html" title="模型选择：选择估计量及其参数" data-book-page-rel-url="docs/72.html" data-book-page-id="11546">模型选择：选择估计量及其参数</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/73.html" title="无监督学习: 寻求数据表示" data-book-page-rel-url="docs/73.html" data-book-page-id="11547">无监督学习: 寻求数据表示</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/74.html" title="把它们放在一起" data-book-page-rel-url="docs/74.html" data-book-page-id="11548">把它们放在一起</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/75.html" title="寻求帮助" data-book-page-rel-url="docs/75.html" data-book-page-id="11549">寻求帮助</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/76.html" title="处理文本数据" data-book-page-rel-url="docs/76.html" data-book-page-id="11550">处理文本数据</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/77.html" title="选择正确的评估器(estimator.html)" data-book-page-rel-url="docs/77.html" data-book-page-id="11551">选择正确的评估器(estimator.html)</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/78.html" title="外部资源，视频和谈话" data-book-page-rel-url="docs/78.html" data-book-page-id="11552">外部资源，视频和谈话</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/80.html" title="常见问题" data-book-page-rel-url="docs/80.html" data-book-page-id="11553">常见问题</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/81.html" title="时光轴" data-book-page-rel-url="docs/81.html" data-book-page-id="11554">时光轴</a>
</li>
</ul>
</div>
</div>
<script src="https://cdn.staticfile.net/jquery/1.12.4/jquery.min.js"></script>
<script type="text/javascript" src="../../../static/components/uikit-2.27.5/js/uikit.reader.js"></script>
<script type="text/javascript" src="../../../static/components/social-share/social-share.min.js"></script>
<script>(function(){var bp =document.createElement('script');var curProtocol =window.location.protocol.split(':')[0];if (curProtocol ==='https') {bp.src ='https://zz.bdstatic.com/linksubmit/push.js';}
else {bp.src ='http://push.zhanzhang.baidu.com/push.js';}
var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(bp,s);})();</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
<script src="https://cdn.staticfile.net/highlight.js/9.12.0/highlight.min.js"></script>
<script src="https://cdn.staticfile.net/jquery.pjax/2.0.1/jquery.pjax.min.js"></script>
<script src="https://cdn.staticfile.net/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="https://cdn.staticfile.net/uikit/2.27.5/js/components/lightbox.min.js"></script>
<link rel="dns-prefetch" href="../../..//cdn.mathjax.org" />
<script type="text/x-mathjax-config">
 function initMathJax() {
    var mathId = $("book-content-section")[0];
    MathJax.Hub.Config({
        tex2jax: {skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code','a']},
        showProcessingMessages: false,
        messageStyle: "none"
    });
    MathJax.Hub.Queue(["Typeset",MathJax.Hub,mathId]);
 };
initMathJax();
</script>
<script src='https://cdn.staticfile.net/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML' async></script>
<style>
	.MathJax_Display{display:inline!important;}
</style>
<script type="text/javascript" src="../../../static/components/js/reader.js"></script>
<script type="text/javascript">var bookId =170;var bookPageId =11527;var bookPageRelUrl ='docs/53.html';</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
</body>
</html>