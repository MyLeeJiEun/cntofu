
<!DOCTYPE html>
<html lang="zh-CN">
<head>
<title>1.5. 随机梯度下降-scikit-learn (sklearn) 官方文档中文版</title>
<meta content='1.5. 随机梯度下降,scikit-learn (sklearn) 官方文档中文版' name='keywords'>
<meta content='1.5. 随机梯度下降,scikit-learn (sklearn) 官方文档中文版' name='description'>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta http-equiv="Content-Language" content="zh-CN" />
<meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1, maximum-scale=1, user-scalable=no"../../../>
<meta name="applicable-device" content="pc,mobile">
<link rel="shortcut icon" href="../../../favicon.ico" type="image/x-icon" />
<meta name="renderer" content="webkit">
<link rel="stylesheet" href="../../../static/components/uikit-2.27.5/css/uikit.custom.css">
<link rel="stylesheet" href="../../../static/components/social-share/social-share.min.css">
<link rel="stylesheet" href="../../../static/components/highlight/styles/custom.css">
<link rel="stylesheet" href="../../../static/components/css/base.css">
<link rel="stylesheet" href="../../../static/components/css/reader.css">
<link rel="stylesheet" href="../../../static/components/css/markdown.css">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5313208362165053" crossorigin="anonymous"></script>
</head>
<body>
<div class=" book-main-wrap uk-container uk-container-center uk-margin-top ">
<div class="uk-grid">
<div class="uk-width-1-1 reader-wrap ">
<div class=" bottom-nav uk-clearfix ">
<div class="uk-align-left ">
<a href="../../../book/170/docs/5.html">
<i class="nav-icon-left uk-icon-small  uk-icon-caret-left"></i>
<span class="">1.4. 支持向量机</span>
</a>
</div>
<div class="uk-align-right ">
<a href="../../../book/170/docs/7.html">
<span class="">1.6. 最近邻</span>
<i class="nav-icon-right uk-icon-small  uk-icon-caret-right"></i>
</a>
</div>
</div>
<div class="uk-text-center">
<h2 class="book-page-title uk-container-center">
<a href="../../../book/170/index.html">scikit-learn (sklearn) 官方文档中文版</a>
<a target="_blank" rel="nofollow" href="https://github.com/apachecn/scikit-learn-doc-zh" class="uk-icon-button uk-icon-github" title="github项目地址"></a>
</h2>
</div>
<script type="text/javascript" src="../../../static/components/js/app_intro.js"></script>
<ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-5313208362165053" data-ad-slot="1328047120"></ins>
<script>(adsbygoogle =window.adsbygoogle ||[]).push({});</script>
<hr class="uk-article-divider">
<div class="book-content-section  md-content-section  uk-margin-bottom">
<h1 id="15-随机梯度下降">1.5. 随机梯度下降</h1>
<p>校验者: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@A</a> 翻译者: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@L</a> 校验者: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/HelloSilicat">@HelloSilicat</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@A</a> 翻译者: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@L</a></p>
<p><strong>随机梯度下降(SGD)</strong> 是一种简单但又非常高效的方法，主要用于凸损失函数下线性分类器的判别式学习，例如(线性) <a href="https://en.wikipedia.org/wiki/Support_vector_machine">支持向量机</a> 和 <a href="https://en.wikipedia.org/wiki/Logistic_regression">Logistic 回归</a> 。 尽管 SGD 在机器学习社区已经存在了很长时间, 但是最近在 large-scale learning （大规模学习）方面 SGD 获得了相当大的关注。</p>
<p>SGD 已成功应用于在文本分类和自然语言处理中经常遇到的大规模和稀疏的机器学习问题。对于稀疏数据，本模块的分类器可以轻易的处理超过 105 的训练样本和超过 105 的特征。</p>
<p>Stochastic Gradient Descent （随机梯度下降法）的优势:</p>
<blockquote>
<ul>
<li>高效。</li>
<li>易于实现 (有大量优化代码的机会)。</li>
</ul>
</blockquote>
<p>Stochastic Gradient Descent （随机梯度下降法）的劣势:</p>
<blockquote>
<ul>
<li>SGD 需要一些超参数，例如 regularization （正则化）参数和 number of iterations （迭代次数）。</li>
<li>SGD 对 feature scaling （特征缩放）敏感。</li>
</ul>
</blockquote>
<h2 id="151-分类">1.5.1. 分类</h2>
<p>Warning</p>
<p>在拟合模型前，确保你重新排列了（打乱）)你的训练数据，或者在每次迭代后用 <code>shuffle=True</code> 来打乱。</p>
<p><a href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code>SGDClassifier</code></a> 类实现了一个简单的随机梯度下降学习例程, 支持不同的 loss functions（损失函数）和 penalties for classification（分类处罚）。</p>
<p><a href="../auto_examples/linear_model/plot_sgd_separating_hyperplane.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b3206aa7b52a9c0918727730873d1363.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b3206aa7b52a9c0918727730873d1363.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_sgd_separating_hyperplane_0011.png"></a></a></p>
<p>作为另一个 classifier （分类器）, 拟合 SGD 我们需要两个 array （数组）：保存训练样本的 size 为 [n_samples, n_features] 的数组 X 以及保存训练样本目标值（类标签）的 size 为 [n_samples] 的数组 Y</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn.linear_model import SGDClassifier
&gt;&gt;&gt; X = [[0., 0.], [1., 1.]]
&gt;&gt;&gt; y = [0, 1]
&gt;&gt;&gt; clf = SGDClassifier(loss="hinge", penalty="l2")
&gt;&gt;&gt; clf.fit(X, y)
SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,
 eta0=0.0, fit_intercept=True, l1_ratio=0.15,
 learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,
 n_jobs=1, penalty='l2', power_t=0.5, random_state=None,
 shuffle=True, tol=None, verbose=0, warm_start=False)

</code></pre>
<p>拟合后，我们可以用该模型来预测新值:</p>
<pre><code class="language-py">&gt;&gt;&gt; clf.predict([[2., 2.]])
array([1])

</code></pre>
<p>SGD 通过训练数据来拟合一个线性模型。成员 <code>coef_</code> 保存模型参数:</p>
<pre><code class="language-py">&gt;&gt;&gt; clf.coef_                                         
array([[ 9.9...,  9.9...]])

</code></pre>
<p>成员 <code>intercept_</code> 保存 intercept（截距） （又称作 offset（偏移）或 bias（偏差））:</p>
<pre><code class="language-py">&gt;&gt;&gt; clf.intercept_                                    
array([-9.9...])

</code></pre>
<p>模型是否使用 intercept（截距）, 即 a biased hyperplane(一个偏置的超平面), 是由参数 <code>fit_intercept</code> 控制的。</p>
<p>使用 <a href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier.decision_function" title="sklearn.linear_model.SGDClassifier.decision_function"><code>SGDClassifier.decision_function</code></a> 来获得到此超平面的 signed distance (符号距离)</p>
<pre><code class="language-py">&gt;&gt;&gt; clf.decision_function([[2., 2.]])                 
array([ 29.6...])

</code></pre>
<p>具体的 loss function（损失函数） 可以通过 <code>loss</code> 参数来设置。 <a href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code>SGDClassifier</code></a> 支持以下的 loss functions（损失函数）：</p>
<blockquote>
<ul>
<li><code>loss="hinge"</code>: (soft-margin) linear Support Vector Machine （（软-间隔）线性支持向量机），</li>
<li><code>loss="modified_huber"</code>: smoothed hinge loss （平滑的 hinge 损失），</li>
<li><code>loss="log"</code>: logistic regression （logistic 回归），</li>
<li>and all regression losses below（以及所有的回归损失）。</li>
</ul>
</blockquote>
<p>前两个 loss functions（损失函数）是懒惰的，如果一个例子违反了 margin constraint（边界约束），它们仅更新模型的参数, 这使得训练非常有效率,即使使用了 L2 penalty（惩罚）我们仍然可能得到稀疏的模型结果。</p>
<p>使用 <code>loss="log"</code> 或者 <code>loss="modified_huber"</code> 来启用 <code>predict_proba</code> 方法, 其给出每个样本 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5c82dbae35dc43d2f556f9f284d9d184.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5c82dbae35dc43d2f556f9f284d9d184.jpg" alt="x"></a> 的概率估计 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3cca81fd08a4732dc7061cd246b323ed.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3cca81fd08a4732dc7061cd246b323ed.jpg" alt="P(y|x)"></a> 的一个向量：</p>
<pre><code class="language-py">&gt;&gt;&gt; clf = SGDClassifier(loss="log").fit(X, y)
&gt;&gt;&gt; clf.predict_proba([[1., 1.]])                      
array([[ 0.00...,  0.99...]])

</code></pre>
<p>具体的惩罚方法可以通过 <code>penalty</code> 参数来设定。 SGD 支持以下 penalties（惩罚）:</p>
<blockquote>
<ul>
<li><code>penalty="l2"</code>: L2 norm penalty on <code>coef_</code>.</li>
<li><code>penalty="l1"</code>: L1 norm penalty on <code>coef_</code>.</li>
<li><code>penalty="elasticnet"</code>: Convex combination of L2 and L1（L2 型和 L1 型的凸组合）; <code>(1 - l1_ratio) * L2 + l1_ratio * L1</code>.</li>
</ul>
</blockquote>
<p>默认设置为 <code>penalty="l2"</code> 。 L1 penalty （惩罚）导致稀疏解，使得大多数系数为零。 Elastic Net（弹性网）解决了在特征高相关时 L1 penalty（惩罚）的一些不足。参数 <code>l1_ratio</code> 控制了 L1 和 L2 penalty（惩罚）的 convex combination （凸组合）。</p>
<p><a href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code>SGDClassifier</code></a> 通过利用 “one versus all” （OVA）方法来组合多个二分类器，从而实现多分类。对于每一个 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e279b8169ddd6581c5606c868ba52fae.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e279b8169ddd6581c5606c868ba52fae.jpg" alt="K"></a> 类, 可以训练一个二分类器来区分自身和其他 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7ce09555ac9e490df7f81ef7eb0e58e8.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7ce09555ac9e490df7f81ef7eb0e58e8.jpg" alt="K-1"></a> 个类。在测试阶段，我们计算每个分类器的 confidence score（置信度分数）（也就是与超平面的距离），并选择置信度最高的分类。下图阐释了基于 iris（鸢尾花）数据集上的 OVA 方法。虚线表示三个 OVA 分类器; 不同背景色代表由三个分类器产生的决策面。</p>
<p><a href="../auto_examples/linear_model/plot_sgd_iris.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7ec998f799acf05b040856bc6b37657f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7ec998f799acf05b040856bc6b37657f.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_sgd_iris_0011.png"></a></a></p>
<p>在 multi-class classification （多类分类）的情况下， <code>coef_</code> 是 <code>shape=[n_classes, n_features]</code> 的一个二维数组， <code>intercept_</code> 是 <code>shape=[n_classes]</code> 的一个一维数组。 <code>coef_</code> 的第 i 行保存了第 i 类的 OVA 分类器的权重向量；类以升序索引 （参照属性 <code>classes_</code> ）。 注意，原则上，由于它们允许创建一个概率模型，所以 <code>loss="log"</code> 和 <code>loss="modified_huber"</code> 更适合于 one-vs-all 分类。</p>
<p><a href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code>SGDClassifier</code></a> 通过拟合参数 <code>class_weight</code> 和 <code>sample_weight</code> 来支持 weighted classes （加权类）和 weighted instances（加权实例）。更多信息请参照下面的示例和 <a href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier.fit" title="sklearn.linear_model.SGDClassifier.fit"><code>SGDClassifier.fit</code></a> 的文档。</p>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/linear_model/plot_sgd_separating_hyperplane.html#sphx-glr-auto-examples-linear-model-plot-sgd-separating-hyperplane-py">SGD: Maximum margin separating hyperplane</a>,</li>
<li><a href="../auto_examples/linear_model/plot_sgd_iris.html#sphx-glr-auto-examples-linear-model-plot-sgd-iris-py">Plot multi-class SGD on the iris dataset</a></li>
<li><a href="../auto_examples/linear_model/plot_sgd_weighted_samples.html#sphx-glr-auto-examples-linear-model-plot-sgd-weighted-samples-py">SGD: Weighted samples</a></li>
<li><a href="../auto_examples/linear_model/plot_sgd_comparison.html#sphx-glr-auto-examples-linear-model-plot-sgd-comparison-py">Comparing various online solvers</a></li>
<li><a href="../auto_examples/svm/plot_separating_hyperplane_unbalanced.html#sphx-glr-auto-examples-svm-plot-separating-hyperplane-unbalanced-py">SVM: Separating hyperplane for unbalanced classes</a> (参见 &lt;cite&gt;Note&lt;/cite&gt;)</li>
</ul>
<p><a href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code>SGDClassifier</code></a> 支持 averaged SGD (ASGD)。Averaging（均值化）可以通过设置 <code>average=True</code> 来启用。AGSD 工作原理是在普通 SGD 的基础上，对每个样本的每次迭代后的系数取均值。当使用 ASGD 时，学习速率可以更大甚至是恒定，在一些数据集上能够加速训练过程。</p>
<p>对于带 logistic loss（logistic 损失）的分类，在 <a href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code>LogisticRegression</code></a> 中提供了另一个采取 averaging strategy（平均策略）的 SGD 变体，其使用了随机平均梯度 (SAG) 算法。</p>
<h2 id="152-回归">1.5.2. 回归</h2>
<p><a href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code>SGDRegressor</code></a> 类实现了一个简单的随机梯度下降学习例程，它支持用不同的损失函数和惩罚来拟合线性回归模型。 <a href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code>SGDRegressor</code></a> 非常适用于有大量训练样本（&gt;10.000)的回归问题，对于其他问题，我们推荐使用 <a href="generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code>Ridge</code></a> ，<a href="generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso" title="sklearn.linear_model.Lasso"><code>Lasso</code></a> ，或 <a href="generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet" title="sklearn.linear_model.ElasticNet"><code>ElasticNet</code></a> 。</p>
<p>具体的损失函数可以通过 <code>loss</code> 参数设置。 <a href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code>SGDRegressor</code></a> 支持以下的损失函数:</p>
<blockquote>
<ul>
<li><code>loss="squared_loss"</code>: Ordinary least squares（普通最小二乘法）,</li>
<li><code>loss="huber"</code>: Huber loss for robust regression（Huber回归）,</li>
<li><code>loss="epsilon_insensitive"</code>: linear Support Vector Regression（线性支持向量回归）.</li>
</ul>
</blockquote>
<p>Huber 和 epsilon-insensitive 损失函数可用于 robust regression（鲁棒回归）。不敏感区域的宽度必须通过参数 <code>epsilon</code> 来设定。这个参数取决于目标变量的规模。</p>
<p><a href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code>SGDRegressor</code></a> 支持 ASGD（平均随机梯度下降） 作为 <a href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code>SGDClassifier</code></a>。 均值化可以通过设置 <code>average=True</code> 来启用。</p>
<p>对于利用了 squared loss（平方损失）和 l2 penalty（l2惩罚）的回归，在 <a href="generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code>Ridge</code></a> 中提供了另一个采取 averaging strategy（平均策略）的 SGD 变体，其使用了随机平均梯度 (SAG) 算法。</p>
<h2 id="153-稀疏数据的随机梯度下降">1.5.3. 稀疏数据的随机梯度下降</h2>
<p>Note</p>
<p>由于在截距部分收敛学习速率的差异，稀疏实现与密集实现相比产生的结果略有不同。</p>
<p>在 <a href="https://docs.scipy.org/doc/scipy/reference/sparse.html">scipy.sparse</a> 支持的格式中，任意矩阵都有对稀疏数据的内置支持方法。但是，为了获得最高的效率，请使用 <a href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html">scipy.sparse.csr_matrix</a> 中定义的 CSR 矩阵格式.</p>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/text/document_classification_20newsgroups.html#sphx-glr-auto-examples-text-document-classification-20newsgroups-py">Classification of text documents using sparse features</a></li>
</ul>
<h2 id="154-复杂度">1.5.4. 复杂度</h2>
<p>SGD 主要的优势在于它的高效性，对于不同规模的训练样本，处理复杂度基本上是线性的。假如 X 是 size 为 (n, p) 的矩阵，训练成本为 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d11a3c7c81eef9f2c8ece06f91411e9e.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d11a3c7c81eef9f2c8ece06f91411e9e.jpg" alt="O(k n \bar p)"></a>，其中 k 是迭代次数， <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/77e01a63c0620550a5f11a7613001120.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/77e01a63c0620550a5f11a7613001120.jpg" alt="\bar p"></a> 是每个样本非零特征的平均数。</p>
<p>但是，最近的理论结果表明，得到期望优化精度的运行时间并不会随着训练集规模扩大而增加。</p>
<h2 id="155-实用小贴士">1.5.5. 实用小贴士</h2>
<blockquote>
<ul>
<li> <p>随机梯度下降法对 feature scaling （特征缩放）很敏感，因此强烈建议您缩放您的数据。例如，将输入向量 X 上的每个特征缩放到 [0,1] 或 [- 1，+1]， 或将其标准化，使其均值为 0，方差为 1。请注意，必须将 <em>相同</em> 的缩放应用于对应的测试向量中，以获得有意义的结果。使用 <code>StandardScaler</code>: 很容易做到这一点：</p> <p>&gt; from sklearn.preprocessing import StandardScaler scaler = StandardScaler() scaler.fit(X_train) # Don’t cheat - fit only on training data X_train = scaler.transform(X_train) X_test = scaler.transform(X_test) # apply same transformation to test data</p> <p>假如你的 attributes （属性）有一个固有尺度（例如 word frequencies （词频）或 indicator features（指标特征））就不需要缩放。</p> </li>
<li> <p>最好使用 <code>GridSearchCV</code> 找到一个合理的 regularization term （正则化项） <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d8b3d5242d513369a44f8bf0c6112744.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d8b3d5242d513369a44f8bf0c6112744.jpg" alt="\alpha"></a> ， 它的范围通常在 <code>10.0**-np.arange(1,7)</code> 。</p> </li>
<li> <p>经验表明，SGD 在处理约 10^6 训练样本后基本收敛。因此，对于迭代次数第一个合理的猜想是 <code>n_iter = np.ceil(10**6 / n)</code>，其中 <code>n</code> 是训练集的大小。</p> </li>
<li> <p>假如将 SGD 应用于使用 PCA 做特征提取，我们发现通过某个常数 &lt;cite&gt;c&lt;/cite&gt; 来缩放特征值是明智的，比如使训练数据的 L2 norm 平均值为 1。</p> </li>
<li> <p>我们发现，当特征很多或 eta0 很大时， ASGD（平均随机梯度下降） 效果更好。</p> </li>
</ul>
</blockquote>
<p>参考文献:</p>
<ul>
<li><a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">“Efficient BackProp”</a> Y. LeCun, L. Bottou, G. Orr, K. Müller - In Neural Networks: Tricks of the Trade 1998.</li>
</ul>
<h2 id="156-数学描述">1.5.6. 数学描述</h2>
<p>给定一组训练样本 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5421b26a31de754ee8d186d038006fa3.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5421b26a31de754ee8d186d038006fa3.jpg" alt="(x_1, y_1), \ldots, (x_n, y_n)"></a> ，其中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2736dbaab8f81e4cb2d0e388f2b0c6b2.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2736dbaab8f81e4cb2d0e388f2b0c6b2.jpg" alt="x_i \in \mathbf{R}^m"></a> ， <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e24edaeb407b6a696ddb188697f0934d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e24edaeb407b6a696ddb188697f0934d.jpg" alt="y_i \in \{-1,1\}"></a>， 我们的目标是一个线性 scoring function（评价函数） <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/75fd7809a46f43dcd922f39ff8f91026.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/75fd7809a46f43dcd922f39ff8f91026.jpg" alt="f(x) = w^T x + b"></a> ，其中模型参数 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/99d4804dc3d2ef82e10d91de99d0142a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/99d4804dc3d2ef82e10d91de99d0142a.jpg" alt="w \in \mathbf{R}^m"></a> ，截距 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f40416aceb254b77100eb361321c1804.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f40416aceb254b77100eb361321c1804.jpg" alt="b \in \mathbf{R}"></a>。为了做预测， 我们只需要看 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/da2ce2d49bbab0c389600d1c82fccf9b.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/da2ce2d49bbab0c389600d1c82fccf9b.jpg" alt="f(x)"></a> 的符号。找到模型参数的一般选择是通过最小化由以下式子给出的正则化训练误差</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/580270908cf4e5ba3907b7267fcfbb44.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/580270908cf4e5ba3907b7267fcfbb44.jpg" alt="E(w,b) = \frac{1}{n}\sum_{i=1}^{n} L(y_i, f(x_i)) + \alpha R(w)"></a></p>
<p>其中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/639e82f3829a0ad677110cc33a028c98.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/639e82f3829a0ad677110cc33a028c98.jpg" alt="L"></a> 衡量模型(mis)拟合程度的损失函数，<a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0fccbdc535b0a4d8003725e8ad606561.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0fccbdc535b0a4d8003725e8ad606561.jpg" alt="R"></a> 是惩罚模型复杂度的正则化项（也叫作惩罚）; <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/11cde057716cf1a820780a60c8ffa8e4.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/11cde057716cf1a820780a60c8ffa8e4.jpg" alt="\alpha > 0"></a> 是一个非负超平面。</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/639e82f3829a0ad677110cc33a028c98.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/639e82f3829a0ad677110cc33a028c98.jpg" alt="L"></a> 的不同选择产生不同的分类器，例如</p>
<blockquote>
<ul>
<li>Hinge: (soft-margin) Support Vector Machines.</li>
<li>Hinge: (软-间隔) 支持向量机。</li>
<li>Log: Logistic Regression.</li>
<li>Log: Logistic 回归。</li>
<li>Least-Squares: Ridge Regression.</li>
<li>Least-Squares: 岭回归。</li>
<li>Epsilon-Insensitive: (soft-margin) Support Vector Regression.</li>
<li>Epsilon-Insensitive: (软-间隔) 支持向量回归。</li>
</ul>
</blockquote>
<p>所有上述损失函数可以看作是错误分类误差的上限（0 - 1损失），如下图所示。</p>
<p><a href="../auto_examples/linear_model/plot_sgd_loss_functions.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/42edb18b0951c4f7ab739e5c24bf9ba3.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/42edb18b0951c4f7ab739e5c24bf9ba3.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_sgd_loss_functions_0011.png"></a></a></p>
<p>比较流行的正则化项 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0fccbdc535b0a4d8003725e8ad606561.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0fccbdc535b0a4d8003725e8ad606561.jpg" alt="R"></a> 包括：</p>
<blockquote>
<ul>
<li>L2 norm: <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2e2461d59015f9759fa0612965e2425e.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2e2461d59015f9759fa0612965e2425e.jpg" alt="R(w) := \frac{1}{2} \sum_{i=1}{n} w_i2"></a>,</li>
<li>L1 norm: <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/61a540d6591602c8f513910fd2f33b40.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/61a540d6591602c8f513910fd2f33b40.jpg" alt="R(w) := \sum_{i=1}^{n} |w_i|"></a>, which leads to sparse solutions（）.</li>
<li>Elastic Net: <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f9d2fc91f381e1772999a738d3c8c32b.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f9d2fc91f381e1772999a738d3c8c32b.jpg" alt="R(w) := \frac{\rho}{2} \sum_{i=1}{n} w_i2 + (1-\rho) \sum_{i=1}^{n} |w_i|"></a>, a convex combination of L2 and L1, where <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b91e4507d9fd7068b02f689d697f8714.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b91e4507d9fd7068b02f689d697f8714.jpg" alt="\rho"></a> is given by <code>1 - l1_ratio</code>.</li>
</ul>
</blockquote>
<p>下图显示当 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/55f44df097de0ddde791d3084a69a1bf.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/55f44df097de0ddde791d3084a69a1bf.jpg" alt="R(w) = 1"></a> 时参数空间中不同正则项的轮廓。</p>
<p><a href="../auto_examples/linear_model/plot_sgd_penalties.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7c228fb3025521b5d9c2aef929547d1d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7c228fb3025521b5d9c2aef929547d1d.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_sgd_penalties_0011.png"></a></a></p>
<h3 id="1561-sgd">1.5.6.1. SGD</h3>
<p>随机梯度下降法是一种无约束优化问题的优化方法。与（批量）梯度下降法相反，SGD 通过一次只考虑单个训练样本来近似 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/343401666d8fc0aeeea395495b9dc570.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/343401666d8fc0aeeea395495b9dc570.jpg" alt="E(w,b)"></a> 真实的梯度。</p>
<p><a href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code>SGDClassifier</code></a> 类实现了一个 first-order SGD learning routine （一阶 SGD 学习程序）。 算法在训练样本上遍历，并且对每个样本根据由以下式子给出的更新规则来更新模型参数。</p>
<pre><code class="language-py">
![w \leftarrow w - \eta (\alpha \frac{\partial R(w)}{\partial w}
+ \frac{\partial L(w^T x_i + b, y_i)}{\partial w})](img/74f4ea0e25b673d30d56ab4269f03f3b.jpg)

</code></pre>
<p>其中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/fe1d79339349f9b6263e123094ffce7b.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/fe1d79339349f9b6263e123094ffce7b.jpg" alt="\eta"></a> 是在参数空间中控制步长的 learning rate （学习速率）。 intercept（截距） <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6ae91fb0f3221b92d2dd4e22204d8008.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6ae91fb0f3221b92d2dd4e22204d8008.jpg" alt="b"></a> 的更新类似但不需要正则化。</p>
<p>学习率 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/fe1d79339349f9b6263e123094ffce7b.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/fe1d79339349f9b6263e123094ffce7b.jpg" alt="\eta"></a> 可以恒定或者逐渐减小。对于分类来说， 默认的学习率设定方案 （<code>learning_rate='optimal'</code>）由下式给出。</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/938ee5c0c620fd2298ea16abe621e7bb.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/938ee5c0c620fd2298ea16abe621e7bb.jpg" alt="\eta^{(t)} = \frac {1}{\alpha  (t_0 + t)}"></a></p>
<p>其中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/12b2c1da1f9041738fa7153efc651372.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/12b2c1da1f9041738fa7153efc651372.jpg" alt="t"></a> 是时间步长（总共有 &lt;cite&gt;n_samples * n_iter&lt;/cite&gt; 时间步长）， <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/56c751b0714a570fdcef0caf63f81580.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/56c751b0714a570fdcef0caf63f81580.jpg" alt="t_0"></a> 是由 Léon Bottou 提出的启发式算法决定的，比如预期的初始更新可以设定为权重的期望大小（假设训练样本的范数近似1）。在 <code>BaseSGD</code> 中的 <code>_init_t</code> 中可以找到确切的定义。</p>
<p>对于回归来说，默认的学习率是反向缩放 (<code>learning_rate='invscaling'</code>)，由下式给出</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/114b22cba4861a82ce7df1eab3219a0d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/114b22cba4861a82ce7df1eab3219a0d.jpg" alt="\eta{(t)} = \frac{eta_0}{t{power\_t}}"></a></p>
<p>其中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0e8aa67015918fa2807e6ddf7192c32f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0e8aa67015918fa2807e6ddf7192c32f.jpg" alt="eta_0"></a> 和 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7ece088a96a66cb9675fde4610d67980.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7ece088a96a66cb9675fde4610d67980.jpg" alt="power\_t"></a> 是用户通过 <code>eta0</code> 和 <code>power_t</code> 分别选择的超参数。</p>
<p>使用固定的学习速率则设置 <code>learning_rate='constant'</code> ，或者设置 <code>eta0</code> 来指定学习速率。</p>
<p>模型参数可以通过成员 <code>coef_</code> 和 <code>intercept_</code> 来获得：</p>
<blockquote>
<ul>
<li>成员 <code>coef_</code> holds the weights（控制权重） <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8a58e8df6a985a3273e39bac7dd72b1f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8a58e8df6a985a3273e39bac7dd72b1f.jpg" alt="w"></a></li>
<li>成员 <code>intercept_</code> holds <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6ae91fb0f3221b92d2dd4e22204d8008.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6ae91fb0f3221b92d2dd4e22204d8008.jpg" alt="b"></a></li>
</ul>
</blockquote>
<p>参考文献：</p>
<ul>
<li><a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.58.7377">“Solving large scale linear prediction problems using stochastic gradient descent algorithms”</a> T. Zhang - In Proceedings of ICML ‘04.</li>
<li><a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.124.4696">“Regularization and variable selection via the elastic net”</a> H. Zou, T. Hastie - Journal of the Royal Statistical Society Series B, 67 (2), 301-320.</li>
<li><a href="http://arxiv.org/pdf/1107.2490v2.pdf">“Towards Optimal One Pass Large Scale Learning with Averaged Stochastic Gradient Descent”</a> Xu, Wei</li>
</ul>
<h2 id="157-实现细节">1.5.7. 实现细节</h2>
<p>SGD 的实现受到了 Léon Bottou <a href="http://leon.bottou.org/projects/sgd">Stochastic Gradient SVM</a> 的影响。类似于 SvmSGD，权重向量表达为一个标量和一个向量的内积，这样保证在使用L2正则项时可以高效更新权重。 在 sparse feature vectors （稀疏特征向量）的情况下， intercept （截距）是以更小的学习率（乘以 0.01）更新的，这导致了它的更新更加频繁。训练样本按顺序选取并且每处理一个样本就要降低学习速率。我们采用了 Shalev-Shwartz 等人2007年提出的的学习速率设定方案。 对于多类分类，我们使用了 “one versus all” 方法。 我们在 L1 正则化（和 Elastic Net ）中使用 Tsuruoka 等人2009年提出的 truncated gradient algorithm （截断梯度算法）。代码是用 Cython 编写的。</p>
<p>参考文献:</p>
<ul>
<li><a href="http://leon.bottou.org/projects/sgd">“Stochastic Gradient Descent”</a> L. Bottou - Website, 2010.</li>
<li><a href="http://leon.bottou.org/slides/largescale/lstut.pdf">“The Tradeoffs of Large Scale Machine Learning”</a> L. Bottou - Website, 2011.</li>
<li><a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.74.8513">“Pegasos: Primal estimated sub-gradient solver for svm”</a> S. Shalev-Shwartz, Y. Singer, N. Srebro - In Proceedings of ICML ‘07.</li>
<li><a href="http://www.aclweb.org/anthology/P/P09/P09-1054.pdf">“Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty”</a> Y. Tsuruoka, J. Tsujii, S. Ananiadou - In Proceedings of the AFNLP/ACL ‘09.</li>
</ul>
</div>
<hr class="uk-article-divider">
<div class="uk-block uk-block-muted uk-padding-top-remove uk-padding-bottom-remove uk-margin-large-top  book-recommend-wrap">
<div class="uk-margin-top uk-margin-bottom uk-margin-left uk-margin-right">
<div class="uk-margin uk-text-muted "><i class="uk-icon-outdent uk-icon-justify uk-margin-small-right"></i>书籍推荐</div>
<div class="books">
<ul class="uk-book-list">
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/74/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/74/index.html">Python进阶</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/46.html">东滨社</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">73页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月8日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 2664个">2664</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/97/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/97/index.html">Twisted与异步编程入门</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/60.html">likebeta</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">23页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月29日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 158个">158</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/169/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/169/index.html">PyTorch 1.0 中文文档 & 教程</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/18.html">ApacheCN</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">87页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月26日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 874个">874</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/71/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/java_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/71/index.html">使用 jMonkeyEngine 进行游戏开发</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/43.html">jmecn</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="java">java</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">23页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月8日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 8个">8</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/179/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/179/index.html">30秒学会常用Python代码</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/99.html">kriadmin</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">1页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月26日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 2716个">2716</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/22/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/cplusplus_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/22/index.html">计算与推断思维</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/17.html">Kivy Developers From China</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="cplusplus">cplusplus</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">19页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年5月3日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 243个">243</span>
</div>
</div>
</div>
</li>
<hr>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
<nav class="tm-navbar uk-navbar uk-navbar-attached reader-nav">
<div class="uk-float-left uk-margin-small-top">
<a href="javascript:;" title="目录菜单" class="show-menu  uk-icon-hover  uk-icon-align-justify uk-margin-right"></a>
<div data-uk-dropdown="{mode:'click',pos:'bottom-left'}" class="font-setting-wrap">
<a class="uk-icon-hover uk-icon-font uk-margin-right" aria-label="字体设置" href="javascript:;"></a>
<div class="uk-dropdown dropdown-menu">
<div class="dropdown-caret"><span class="caret-outer"></span><span class="caret-inner"></span></div>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-reduce">小字</button>
<button class="uk-button-link button size-2 font-enlarge">大字</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-1 ">宋体</button>
<button class="uk-button-link button size-2 font-2 ">黑体</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-3 color-theme-sun "><i class="uk-icon-sun-o"></i>白天</button>
<button class="uk-button-link button size-3 color-theme-eye "><i class="uk-icon-eye"></i>护眼</button>
<button class="uk-button-link button size-3 color-theme-moon "><i class="uk-icon-moon-o"></i>夜晚</button></div>
</div>
</div>
<a class="logo uk-margin-right" href="../../../" title="返回首页"><img class="" src="../../../static/components/images/icon_32.png" /></a>
</div>
<div class="uk-navbar-flip  uk-hidden-small">
<div id="share-box"></div>
</div>
</nav>
<div id="menu-id" class="uk-offcanvas reader-offcanvas">
<div class="uk-offcanvas-bar">
<ul class="book-menu-bar uk-nav uk-nav-offcanvas" data-uk-nav>
<li>
<a href="../../../book/170/index.html" data-book-page-rel-url="index.html" data-book-page-id="0" title="封面">封面</a>
</li>
<li>
<a class="pjax" href="../../../book/170/readme.html" data-book-page-rel-url="readme.html" data-book-page-id="0" title="简介">简介</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/79.html" title="安装 scikit-learn" data-book-page-rel-url="docs/79.html" data-book-page-id="11491">安装 scikit-learn</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/1.html" title="1. 监督学习" data-book-page-rel-url="docs/1.html" data-book-page-id="11492">1. 监督学习</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/2.html" title="1.1. 广义线性模型" data-book-page-rel-url="docs/2.html" data-book-page-id="11493">1.1. 广义线性模型</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/3.html" title="1.2. 线性和二次判别分析" data-book-page-rel-url="docs/3.html" data-book-page-id="11494">1.2. 线性和二次判别分析</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/4.html" title="1.3. 内核岭回归" data-book-page-rel-url="docs/4.html" data-book-page-id="11495">1.3. 内核岭回归</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/5.html" title="1.4. 支持向量机" data-book-page-rel-url="docs/5.html" data-book-page-id="11496">1.4. 支持向量机</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/6.html" title="1.5. 随机梯度下降" data-book-page-rel-url="docs/6.html" data-book-page-id="11497">1.5. 随机梯度下降</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/7.html" title="1.6. 最近邻" data-book-page-rel-url="docs/7.html" data-book-page-id="11498">1.6. 最近邻</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/8.html" title="1.7. 高斯过程" data-book-page-rel-url="docs/8.html" data-book-page-id="11499">1.7. 高斯过程</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/9.html" title="1.8. 交叉分解" data-book-page-rel-url="docs/9.html" data-book-page-id="11500">1.8. 交叉分解</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/10.html" title="1.9. 朴素贝叶斯" data-book-page-rel-url="docs/10.html" data-book-page-id="11501">1.9. 朴素贝叶斯</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/11.html" title="1.10. 决策树" data-book-page-rel-url="docs/11.html" data-book-page-id="11502">1.10. 决策树</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/12.html" title="1.11. 集成方法" data-book-page-rel-url="docs/12.html" data-book-page-id="11503">1.11. 集成方法</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/13.html" title="1.12. 多类和多标签算法" data-book-page-rel-url="docs/13.html" data-book-page-id="11504">1.12. 多类和多标签算法</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/14.html" title="1.13. 特征选择" data-book-page-rel-url="docs/14.html" data-book-page-id="11505">1.13. 特征选择</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/15.html" title="1.14. 半监督学习" data-book-page-rel-url="docs/15.html" data-book-page-id="11506">1.14. 半监督学习</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/16.html" title="1.15. 等式回归" data-book-page-rel-url="docs/16.html" data-book-page-id="11507">1.15. 等式回归</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/17.html" title="1.16. 概率校准" data-book-page-rel-url="docs/17.html" data-book-page-id="11508">1.16. 概率校准</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/18.html" title="1.17. 神经网络模型（有监督）" data-book-page-rel-url="docs/18.html" data-book-page-id="11509">1.17. 神经网络模型（有监督）</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/19.html" title="2. 无监督学习" data-book-page-rel-url="docs/19.html" data-book-page-id="11510">2. 无监督学习</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/20.html" title="2.1. 高斯混合模型" data-book-page-rel-url="docs/20.html" data-book-page-id="11511">2.1. 高斯混合模型</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/21.html" title="2.2. 流形学习" data-book-page-rel-url="docs/21.html" data-book-page-id="11512">2.2. 流形学习</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/22.html" title="2.3. 聚类" data-book-page-rel-url="docs/22.html" data-book-page-id="11513">2.3. 聚类</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/23.html" title="2.4. 双聚类" data-book-page-rel-url="docs/23.html" data-book-page-id="11514">2.4. 双聚类</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/24.html" title="2.5. 分解成分中的信号（矩阵分解问题）" data-book-page-rel-url="docs/24.html" data-book-page-id="11515">2.5. 分解成分中的信号（矩阵分解问题）</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/25.html" title="2.6. 协方差估计" data-book-page-rel-url="docs/25.html" data-book-page-id="11516">2.6. 协方差估计</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/26.html" title="2.7. 经验协方差" data-book-page-rel-url="docs/26.html" data-book-page-id="11517">2.7. 经验协方差</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/27.html" title="2.8. 收敛协方差" data-book-page-rel-url="docs/27.html" data-book-page-id="11518">2.8. 收敛协方差</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/28.html" title="2.9. 稀疏逆协方差" data-book-page-rel-url="docs/28.html" data-book-page-id="11519">2.9. 稀疏逆协方差</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/29.html" title="2.10. Robust 协方差估计" data-book-page-rel-url="docs/29.html" data-book-page-id="11520">2.10. Robust 协方差估计</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/30.html" title="2.11. 新奇和异常值检测" data-book-page-rel-url="docs/30.html" data-book-page-id="11521">2.11. 新奇和异常值检测</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/31.html" title="2.12. 密度估计" data-book-page-rel-url="docs/31.html" data-book-page-id="11522">2.12. 密度估计</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/32.html" title="2.13. 神经网络模型（无监督）" data-book-page-rel-url="docs/32.html" data-book-page-id="11523">2.13. 神经网络模型（无监督）</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/33.html" title="3. 模型选择和评估" data-book-page-rel-url="docs/33.html" data-book-page-id="11524">3. 模型选择和评估</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/34.html" title="3.1. 交叉验证：评估估算器的表现" data-book-page-rel-url="docs/34.html" data-book-page-id="11525">3.1. 交叉验证：评估估算器的表现</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/35.html" title="3.2. 调整估计器的超参数" data-book-page-rel-url="docs/35.html" data-book-page-id="11526">3.2. 调整估计器的超参数</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/53.html" title="3.3. 模型评估: 量化预测的质量" data-book-page-rel-url="docs/53.html" data-book-page-id="11527">3.3. 模型评估: 量化预测的质量</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/54.html" title="3.4. 模型持久化" data-book-page-rel-url="docs/54.html" data-book-page-id="11528">3.4. 模型持久化</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/55.html" title="3.5. 验证曲线: 绘制分数以评估模型" data-book-page-rel-url="docs/55.html" data-book-page-id="11529">3.5. 验证曲线: 绘制分数以评估模型</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/56.html" title="4. 数据集转换" data-book-page-rel-url="docs/56.html" data-book-page-id="11530">4. 数据集转换</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/57.html" title="4.1. Pipeline（管道）和 FeatureUnion（特征联合）: 合并的评估器" data-book-page-rel-url="docs/57.html" data-book-page-id="11531">4.1. Pipeline（管道）和 FeatureUnion（特征联合）: 合并的评估器</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/58.html" title="4.2. 特征提取" data-book-page-rel-url="docs/58.html" data-book-page-id="11532">4.2. 特征提取</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/59.html" title="4.3. 预处理数据" data-book-page-rel-url="docs/59.html" data-book-page-id="11533">4.3. 预处理数据</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/60.html" title="4.4. 无监督降维" data-book-page-rel-url="docs/60.html" data-book-page-id="11534">4.4. 无监督降维</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/61.html" title="4.5. 随机投影" data-book-page-rel-url="docs/61.html" data-book-page-id="11535">4.5. 随机投影</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/62.html" title="4.6. 内核近似" data-book-page-rel-url="docs/62.html" data-book-page-id="11536">4.6. 内核近似</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/63.html" title="4.7. 成对的矩阵, 类别和核函数" data-book-page-rel-url="docs/63.html" data-book-page-id="11537">4.7. 成对的矩阵, 类别和核函数</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/64.html" title="4.8. 预测目标 (`y`.html) 的转换" data-book-page-rel-url="docs/64.html" data-book-page-id="11538">4.8. 预测目标 (`y`.html) 的转换</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/65.html" title="5. 数据集加载工具" data-book-page-rel-url="docs/65.html" data-book-page-id="11539">5. 数据集加载工具</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/66.html" title="6. 大规模计算的策略: 更大量的数据" data-book-page-rel-url="docs/66.html" data-book-page-id="11540">6. 大规模计算的策略: 更大量的数据</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/67.html" title="7. 计算性能" data-book-page-rel-url="docs/67.html" data-book-page-id="11541">7. 计算性能</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/68.html" title="使用 scikit-learn 介绍机器学习" data-book-page-rel-url="docs/68.html" data-book-page-id="11542">使用 scikit-learn 介绍机器学习</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/69.html" title="关于科学数据处理的统计学习教程" data-book-page-rel-url="docs/69.html" data-book-page-id="11543">关于科学数据处理的统计学习教程</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/70.html" title="机器学习: scikit-learn 中的设置以及预估对象" data-book-page-rel-url="docs/70.html" data-book-page-id="11544">机器学习: scikit-learn 中的设置以及预估对象</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/71.html" title="监督学习：从高维观察预测输出变量" data-book-page-rel-url="docs/71.html" data-book-page-id="11545">监督学习：从高维观察预测输出变量</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/72.html" title="模型选择：选择估计量及其参数" data-book-page-rel-url="docs/72.html" data-book-page-id="11546">模型选择：选择估计量及其参数</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/73.html" title="无监督学习: 寻求数据表示" data-book-page-rel-url="docs/73.html" data-book-page-id="11547">无监督学习: 寻求数据表示</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/74.html" title="把它们放在一起" data-book-page-rel-url="docs/74.html" data-book-page-id="11548">把它们放在一起</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/75.html" title="寻求帮助" data-book-page-rel-url="docs/75.html" data-book-page-id="11549">寻求帮助</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/76.html" title="处理文本数据" data-book-page-rel-url="docs/76.html" data-book-page-id="11550">处理文本数据</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/77.html" title="选择正确的评估器(estimator.html)" data-book-page-rel-url="docs/77.html" data-book-page-id="11551">选择正确的评估器(estimator.html)</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/78.html" title="外部资源，视频和谈话" data-book-page-rel-url="docs/78.html" data-book-page-id="11552">外部资源，视频和谈话</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/80.html" title="常见问题" data-book-page-rel-url="docs/80.html" data-book-page-id="11553">常见问题</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/81.html" title="时光轴" data-book-page-rel-url="docs/81.html" data-book-page-id="11554">时光轴</a>
</li>
</ul>
</div>
</div>
<script src="https://cdn.staticfile.net/jquery/1.12.4/jquery.min.js"></script>
<script type="text/javascript" src="../../../static/components/uikit-2.27.5/js/uikit.reader.js"></script>
<script type="text/javascript" src="../../../static/components/social-share/social-share.min.js"></script>
<script>(function(){var bp =document.createElement('script');var curProtocol =window.location.protocol.split(':')[0];if (curProtocol ==='https') {bp.src ='https://zz.bdstatic.com/linksubmit/push.js';}
else {bp.src ='http://push.zhanzhang.baidu.com/push.js';}
var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(bp,s);})();</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
<script src="https://cdn.staticfile.net/highlight.js/9.12.0/highlight.min.js"></script>
<script src="https://cdn.staticfile.net/jquery.pjax/2.0.1/jquery.pjax.min.js"></script>
<script src="https://cdn.staticfile.net/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="https://cdn.staticfile.net/uikit/2.27.5/js/components/lightbox.min.js"></script>
<link rel="dns-prefetch" href="../../..//cdn.mathjax.org" />
<script type="text/x-mathjax-config">
 function initMathJax() {
    var mathId = $("book-content-section")[0];
    MathJax.Hub.Config({
        tex2jax: {skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code','a']},
        showProcessingMessages: false,
        messageStyle: "none"
    });
    MathJax.Hub.Queue(["Typeset",MathJax.Hub,mathId]);
 };
initMathJax();
</script>
<script src='https://cdn.staticfile.net/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML' async></script>
<style>
	.MathJax_Display{display:inline!important;}
</style>
<script type="text/javascript" src="../../../static/components/js/reader.js"></script>
<script type="text/javascript">var bookId =170;var bookPageId =11497;var bookPageRelUrl ='docs/6.html';</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
</body>
</html>