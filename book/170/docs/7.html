
<!DOCTYPE html>
<html lang="zh-CN">
<head>
<title>1.6. 最近邻-scikit-learn (sklearn) 官方文档中文版</title>
<meta content='1.6. 最近邻,scikit-learn (sklearn) 官方文档中文版' name='keywords'>
<meta content='1.6. 最近邻,scikit-learn (sklearn) 官方文档中文版' name='description'>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta http-equiv="Content-Language" content="zh-CN" />
<meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1, maximum-scale=1, user-scalable=no"../../../>
<meta name="applicable-device" content="pc,mobile">
<link rel="shortcut icon" href="../../../favicon.ico" type="image/x-icon" />
<meta name="renderer" content="webkit">
<link rel="stylesheet" href="../../../static/components/uikit-2.27.5/css/uikit.custom.css">
<link rel="stylesheet" href="../../../static/components/social-share/social-share.min.css">
<link rel="stylesheet" href="../../../static/components/highlight/styles/custom.css">
<link rel="stylesheet" href="../../../static/components/css/base.css">
<link rel="stylesheet" href="../../../static/components/css/reader.css">
<link rel="stylesheet" href="../../../static/components/css/markdown.css">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5313208362165053" crossorigin="anonymous"></script>
</head>
<body>
<div class=" book-main-wrap uk-container uk-container-center uk-margin-top ">
<div class="uk-grid">
<div class="uk-width-1-1 reader-wrap ">
<div class=" bottom-nav uk-clearfix ">
<div class="uk-align-left ">
<a href="../../../book/170/docs/6.html">
<i class="nav-icon-left uk-icon-small  uk-icon-caret-left"></i>
<span class="">1.5. 随机梯度下降</span>
</a>
</div>
<div class="uk-align-right ">
<a href="../../../book/170/docs/8.html">
<span class="">1.7. 高斯过程</span>
<i class="nav-icon-right uk-icon-small  uk-icon-caret-right"></i>
</a>
</div>
</div>
<div class="uk-text-center">
<h2 class="book-page-title uk-container-center">
<a href="../../../book/170/index.html">scikit-learn (sklearn) 官方文档中文版</a>
<a target="_blank" rel="nofollow" href="https://github.com/apachecn/scikit-learn-doc-zh" class="uk-icon-button uk-icon-github" title="github项目地址"></a>
</h2>
</div>
<script type="text/javascript" src="../../../static/components/js/app_intro.js"></script>
<ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-5313208362165053" data-ad-slot="1328047120"></ins>
<script>(adsbygoogle =window.adsbygoogle ||[]).push({});</script>
<hr class="uk-article-divider">
<div class="book-content-section  md-content-section  uk-margin-bottom">
<h1 id="16-最近邻">1.6. 最近邻</h1>
<p>校验者: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/DataMonk2017">@DataMonk2017</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/caopeirui"></a><a href="https://github.com/Veyron"><strong>@Veyron</strong></a> C &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/pan8664716">@舞空</a> 翻译者: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/wangyangting">@那伊抹微笑</a></p>
<p><a href="classes.html#module-sklearn.neighbors" title="sklearn.neighbors"><code>sklearn.neighbors</code></a> 提供了 neighbors-based (基于邻居的) 无监督学习以及监督学习方法的功能。 无监督的最近邻是许多其它学习方法的基础，尤其是 manifold learning (流行学习) 和 spectral clustering (谱聚类)。 neighbors-based (基于邻居的) 监督学习分为两种： <a href="#classification">classification</a> （分类）针对的是具有离散标签的数据，<a href="#regression">regression</a> （回归）针对的是具有连续标签的数据。</p>
<p>最近邻方法背后的原理是从训练样本中找到与新点在距离上最近的预定数量的几个点，然后从这些点中预测标签。 这些点的数量可以是用户自定义的常量（K-最近邻学习）， 也可以根据不同的点的局部密度（基于半径的最近邻学习）。距离通常可以通过任何度量来衡量： standard Euclidean distance（标准欧式距离）是最常见的选择。Neighbors-based（基于邻居的）方法被称为 <em>非泛化</em> 机器学习方法， 因为它们只是简单地”记住”了其所有的训练数据（可能转换为一个快速索引结构，如 <a href="#ball-tree">Ball Tree</a> 或 <a href="#kd-tree">KD Tree</a>）。</p>
<p>尽管它简单，但最近邻算法已经成功地适用于很多的分类和回归问题，例如手写数字或卫星图像的场景。 作为一个 non-parametric（非参数化）方法，它经常成功地应用于决策边界非常不规则的分类情景下。</p>
<p><a href="classes.html#module-sklearn.neighbors" title="sklearn.neighbors"><code>sklearn.neighbors</code></a> 可以处理 Numpy 数组或 &lt;cite&gt;scipy.sparse&lt;/cite&gt; 矩阵作为其输入。 对于密集矩阵，大多数可能的距离度量都是支持的。对于稀疏矩阵，支持搜索任意的 Minkowski 度量。</p>
<p>许多学习路径/方法都是依赖最近邻作为核心。 一个例子是 <a href="density.html#kernel-density">核密度估计</a> , 在 <a href="density.html#density-estimation">密度估计</a> 章节中有讨论。</p>
<h2 id="161-无监督最近邻">1.6.1. 无监督最近邻</h2>
<p><a href="generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors" title="sklearn.neighbors.NearestNeighbors"><code>NearestNeighbors</code></a> （最近邻）实现了 unsupervised nearest neighbors learning（无监督的最近邻学习）。 它为三种不同的最近邻算法提供统一的接口：<a href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code>BallTree</code></a>, <a href="generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code>KDTree</code></a>, 还有基于 <a href="classes.html#module-sklearn.metrics.pairwise" title="sklearn.metrics.pairwise"><code>sklearn.metrics.pairwise</code></a> 的 brute-force 算法。算法的选择可通过关键字 <code>'algorithm'</code> 来控制， 并必须是 <code>['auto', 'ball_tree', 'kd_tree', 'brute']</code> 其中的一个。当默认值设置为 <code>'auto'</code> 时，算法会尝试从训练数据中确定最佳方法。有关上述每个选项的优缺点，参见 <a href="#id11"><code>Nearest Neighbor Algorithms</code>_</a> 。</p>
<blockquote>
<p>Warning</p>
<p>关于最近邻算法，如果邻居 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/cd345cf1e9e01448cd544361983ab95a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/cd345cf1e9e01448cd544361983ab95a.jpg" alt="k+1"></a> 和邻居 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> 具有相同的距离，但具有不同的标签， 结果将取决于训练数据的顺序。</p>
</blockquote>
<h3 id="1611-找到最近邻">1.6.1.1. 找到最近邻</h3>
<p>为了完成找到两组数据集中最近邻点的简单任务, 可以使用 <a href="classes.html#module-sklearn.neighbors" title="sklearn.neighbors"><code>sklearn.neighbors</code></a> 中的无监督算法:</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn.neighbors import NearestNeighbors
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
&gt;&gt;&gt; nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)
&gt;&gt;&gt; distances, indices = nbrs.kneighbors(X)
&gt;&gt;&gt; indices                                           
array([[0, 1],
 [1, 0],
 [2, 1],
 [3, 4],
 [4, 3],
 [5, 4]]...)
&gt;&gt;&gt; distances
array([[ 0\.        ,  1\.        ],
 [ 0\.        ,  1\.        ],
 [ 0\.        ,  1.41421356],
 [ 0\.        ,  1\.        ],
 [ 0\.        ,  1\.        ],
 [ 0\.        ,  1.41421356]])

</code></pre>
<p>因为查询集匹配训练集，每个点的最近邻点是其自身，距离为0。</p>
<p>还可以有效地生成一个稀疏图来标识相连点之间的连接情况：</p>
<pre><code class="language-py">&gt;&gt;&gt; nbrs.kneighbors_graph(X).toarray()
array([[ 1.,  1.,  0.,  0.,  0.,  0.],
 [ 1.,  1.,  0.,  0.,  0.,  0.],
 [ 0.,  1.,  1.,  0.,  0.,  0.],
 [ 0.,  0.,  0.,  1.,  1.,  0.],
 [ 0.,  0.,  0.,  1.,  1.,  0.],
 [ 0.,  0.,  0.,  0.,  1.,  1.]])

</code></pre>
<p>我们的数据集是结构化的，因此按索引顺序的相邻点就在参数空间相邻，从而生成了近似 K-nearest neighbors（K-近邻）的块对角矩阵。 这种稀疏图在各种的利用点之间的空间关系进行无监督学习的情况下都很有用：特别地可参见 <a href="generated/sklearn.manifold.Isomap.html#sklearn.manifold.Isomap" title="sklearn.manifold.Isomap"><code>sklearn.manifold.Isomap</code></a>, <a href="generated/sklearn.manifold.LocallyLinearEmbedding.html#sklearn.manifold.LocallyLinearEmbedding" title="sklearn.manifold.LocallyLinearEmbedding"><code>sklearn.manifold.LocallyLinearEmbedding</code></a>, 和 <a href="generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering" title="sklearn.cluster.SpectralClustering"><code>sklearn.cluster.SpectralClustering</code></a>。</p>
<h3 id="1612-kdtree-和-balltree-类">1.6.1.2. KDTree 和 BallTree 类</h3>
<p>另外，我们可以使用 <a href="generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code>KDTree</code></a> 或 <a href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code>BallTree</code></a> 来找最近邻。 这是上文使用过的 <a href="generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors" title="sklearn.neighbors.NearestNeighbors"><code>NearestNeighbors</code></a> 类所包含的功能。 <a href="generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code>KDTree</code></a> 和 <a href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code>BallTree</code></a> 具有相同的接口； 我们将在这里展示使用 <a href="generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code>KDTree</code></a> 的例子：</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn.neighbors import KDTree
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
&gt;&gt;&gt; kdt = KDTree(X, leaf_size=30, metric='euclidean')
&gt;&gt;&gt; kdt.query(X, k=2, return_distance=False)          
array([[0, 1],
 [1, 0],
 [2, 1],
 [3, 4],
 [4, 3],
 [5, 4]]...)

</code></pre>
<p>对于近邻搜索中选项的更多信息，包括各种距离度量的说明和策略的说明等，请参阅 <a href="generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code>KDTree</code></a> 和 <a href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code>BallTree</code></a> 类文档。 关于可用度量距离的列表，请参阅 <a href="generated/sklearn.neighbors.DistanceMetric.html#sklearn.neighbors.DistanceMetric" title="sklearn.neighbors.DistanceMetric"><code>DistanceMetric</code></a> 类。</p>
<h2 id="162-最近邻分类">1.6.2. 最近邻分类</h2>
<p>最近邻分类属于 <em>基于实例的学习</em> 或 <em>非泛化学习</em> ：它不会去构造一个泛化的内部模型，而是简单地存储训练数据的实例。 分类是由每个点的最近邻的简单多数投票中计算得到的：一个查询点的数据类型是由它最近邻点中最具代表性的数据类型来决定的。</p>
<pre><code class="language-py">scikit-learn 实现了两种不同的最近邻分类器： 基于每个查询点的  个最近邻实现，
</code></pre>
<p>其中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> 是用户指定的整数值。<a href="generated/sklearn.neighbors.RadiusNeighborsClassifier.html#sklearn.neighbors.RadiusNeighborsClassifier" title="sklearn.neighbors.RadiusNeighborsClassifier"><code>RadiusNeighborsClassifier</code></a> 基于每个查询点的固定半径 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/451ef7ed1a14a6cdc38324c8a5c7c683.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/451ef7ed1a14a6cdc38324c8a5c7c683.jpg" alt="r"></a> 内的邻居数量实现， 其中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/451ef7ed1a14a6cdc38324c8a5c7c683.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/451ef7ed1a14a6cdc38324c8a5c7c683.jpg" alt="r"></a> 是用户指定的浮点数值。</p>
<pre><code class="language-py"> -邻居分类是  下的两种技术中比较常用的一种。 值的最佳选择是高度依赖数据的：
</code></pre>
<p>通常较大的 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> 是会抑制噪声的影响，但是使得分类界限不明显。</p>
<p>如果数据是不均匀采样的，那么 <a href="generated/sklearn.neighbors.RadiusNeighborsClassifier.html#sklearn.neighbors.RadiusNeighborsClassifier" title="sklearn.neighbors.RadiusNeighborsClassifier"><code>RadiusNeighborsClassifier</code></a> 中的基于半径的近邻分类可能是更好的选择。</p>
<pre><code class="language-py">用户指定一个固定半径 ，使得稀疏邻居中的点使用较少的最近邻来分类。
</code></pre>
<p>对于高维参数空间，这个方法会由于所谓的 “维度灾难” 而变得不那么有效。</p>
<p>基本的最近邻分类使用统一的权重：分配给查询点的值是从最近邻的简单多数投票中计算出来的。 在某些环境下，最好对邻居进行加权，使得更近邻更有利于拟合。可以通过 <code>weights</code> 关键字来实现。</p>
<p>默认值 <code>weights = 'uniform'</code> 为每个近邻分配统一的权重。而 <code>weights = 'distance'</code> 分配权重与查询点的距离成反比。 或者，用户可以自定义一个距离函数用来计算权重。</p>
<table>
<thead>
<tr>
<th>target:</th>
<th>../auto_examples/neighbors/plot_classification.html</th>
</tr>
</thead>
<tbody>
<tr>
<td>scale:</td>
<td>50</td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>target:</th>
<th>../auto_examples/neighbors/plot_classification.html</th>
</tr>
</thead>
<tbody>
<tr>
<td>scale:</td>
<td>50</td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
</tbody>
</table>
<p><strong><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1a91bab921cf39f58a522ed15f475235.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1a91bab921cf39f58a522ed15f475235.jpg" alt="classification_1"></a> <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ae484baf10384efcf4d993631f4641e7.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ae484baf10384efcf4d993631f4641e7.jpg" alt="classification_2"></a></strong></p>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py">Nearest Neighbors Classification</a>: 使用最近邻进行分类的示例。</li>
</ul>
<h2 id="163-最近邻回归">1.6.3. 最近邻回归</h2>
<p>最近邻回归是用在数据标签为连续变量，而不是离散变量的情况下。分配给查询点的标签是由它的最近邻标签的均值计算而来的。</p>
<p>scikit-learn 实现了两种不同的最近邻回归：<a href="generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor" title="sklearn.neighbors.KNeighborsRegressor"><code>KNeighborsRegressor</code></a> 基于每个查询点的 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> 个最近邻实现， 其中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> 是用户指定的整数值。<a href="generated/sklearn.neighbors.RadiusNeighborsRegressor.html#sklearn.neighbors.RadiusNeighborsRegressor" title="sklearn.neighbors.RadiusNeighborsRegressor"><code>RadiusNeighborsRegressor</code></a> 基于每个查询点的固定半径 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/451ef7ed1a14a6cdc38324c8a5c7c683.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/451ef7ed1a14a6cdc38324c8a5c7c683.jpg" alt="r"></a> 内的邻点数量实现， 其中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/451ef7ed1a14a6cdc38324c8a5c7c683.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/451ef7ed1a14a6cdc38324c8a5c7c683.jpg" alt="r"></a> 是用户指定的浮点数值。</p>
<p>基本的最近邻回归使用统一的权重：即，本地邻域内的每个邻点对查询点的分类贡献一致。 在某些环境下，对邻点加权可能是有利的，使得附近点对于回归所作出的贡献多于远处点。 这可以通过 <code>weights</code> 关键字来实现。默认值 <code>weights = 'uniform'</code> 为所有点分配同等权重。 而 <code>weights = 'distance'</code> 分配的权重与查询点距离呈反比。 或者，用户可以自定义一个距离函数用来计算权重。</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/207e92cfc624372bc9c72a160c02114f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/207e92cfc624372bc9c72a160c02114f.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_regression_0011.png"></a></p>
<p>| target: | ../auto_examples/neighbors/plot_regression.html :align: center</p>
<p>&gt; &lt;colgroup&gt;&lt;col class="field-name"&gt; &lt;col class="field-body"&gt;&lt;/colgroup&gt; &gt; | scale: | 75 | &gt; | --- | --- |</p>
<table>
<thead>
<tr>
<th>使用多输出的最近邻进行回归分析 <a href="../auto_examples/plot_multioutput_face_completion.html#sphx-glr-auto-examples-plot-multioutput-face-completion-py">Face completion with a multi-output estimators</a>。</th>
</tr>
</thead>
<tbody></tbody>
</table>
<p>在这个示例中，输入 X 是脸上半部分像素，输出 Y 是脸下半部分像素。</p>
<p>| target: | ../auto_examples/plot_multioutput_face_completion.html :scale: 75</p>
<p>&gt; &lt;colgroup&gt;&lt;col class="field-name"&gt; &lt;col class="field-body"&gt;&lt;/colgroup&gt; &gt; | align: | center | &gt; | --- | --- |</p>
<table>
<thead>
<tr>
<th></th>
</tr>
</thead>
<tbody></tbody>
</table>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/neighbors/plot_regression.html#sphx-glr-auto-examples-neighbors-plot-regression-py">Nearest Neighbors regression</a>: 使用最近邻进行回归的示例。</li>
<li><a href="../auto_examples/plot_multioutput_face_completion.html#sphx-glr-auto-examples-plot-multioutput-face-completion-py">Face completion with a multi-output estimators</a>: 使用最近邻进行多输出回归的示例。</li>
</ul>
<h2 id="164-最近邻算法">1.6.4. 最近邻算法</h2>
<h3 id="1641-暴力计算">1.6.4.1. 暴力计算</h3>
<p>最近邻的快速计算是机器学习中一个活跃的研究领域。最简单的近邻搜索的实现涉及数据集中所有成对点之间距离的暴力计算： 对于 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e03066df748abd9273db055cb79f0f01.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e03066df748abd9273db055cb79f0f01.jpg" alt="D"></a> 维度中的 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" alt="N"></a> 个样本来说, 这个方法的复杂度是 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2d3029206649000f40ed9f51bbeceafb.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2d3029206649000f40ed9f51bbeceafb.jpg" alt="OD N^2"></a>。 对于小数据样本，高效的暴力近邻搜索是非常有竞争力的。 然而，随着样本数 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" alt="N"></a> 的增长，暴力方法很快变得不切实际了。在 <a href="classes.html#module-sklearn.neighbors" title="sklearn.neighbors"><code>sklearn.neighbors</code></a> 类中， 暴力近邻搜索通过关键字 <code>algorithm = 'brute'</code> 来指定，并通过 <a href="classes.html#module-sklearn.metrics.pairwise" title="sklearn.metrics.pairwise"><code>sklearn.metrics.pairwise</code></a> 中的例程来进行计算。</p>
<h3 id="1642-k-d-树">1.6.4.2. K-D 树</h3>
<p>为了解决效率低下的暴力计算方法，已经发明了大量的基于树的数据结构。总的来说， 这些结构试图通过有效地编码样本的 aggregate distance (聚合距离) 信息来减少所需的距离计算量。 基本思想是，若 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/eeaf066f8cca5064b706ccfc4728323d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/eeaf066f8cca5064b706ccfc4728323d.jpg" alt="A"></a> 点距离 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/502926bb104c175c6f3e809b0207830c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/502926bb104c175c6f3e809b0207830c.jpg" alt="B"></a> 点非常远，<a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/502926bb104c175c6f3e809b0207830c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/502926bb104c175c6f3e809b0207830c.jpg" alt="B"></a> 点距离 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4b6d782a67ac392e97215c46b7590bf7.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4b6d782a67ac392e97215c46b7590bf7.jpg" alt="C"></a> 点非常近， 可知 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/eeaf066f8cca5064b706ccfc4728323d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/eeaf066f8cca5064b706ccfc4728323d.jpg" alt="A"></a> 点与 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4b6d782a67ac392e97215c46b7590bf7.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4b6d782a67ac392e97215c46b7590bf7.jpg" alt="C"></a> 点很遥远，<em>不需要明确计算它们的距离</em>。 通过这样的方式，近邻搜索的计算成本可以降低为 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a98f0fb22381bfc1d14fc1e3f7e737e5.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a98f0fb22381bfc1d14fc1e3f7e737e5.jpg" alt="OD N \log(N)"></a> 或更低。 这是对于暴力搜索在大样本数 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" alt="N"></a> 中表现的显著改善。</p>
<p>利用这种聚合信息的早期方法是 <em>KD tree</em> 数据结构（* K-dimensional tree* 的简写）, 它将二维 <em>Quad-trees</em> 和三维 <em>Oct-trees</em> 推广到任意数量的维度. KD 树是一个二叉树结构，它沿着数据轴递归地划分参数空间，将其划分为嵌入数据点的嵌套的各向异性区域。 KD 树的构造非常快：因为只需沿数据轴执行分区, 无需计算 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e03066df748abd9273db055cb79f0f01.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e03066df748abd9273db055cb79f0f01.jpg" alt="D"></a>-dimensional 距离。 一旦构建完成, 查询点的最近邻距离计算复杂度仅为 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7b6cebf625d680ab33eba86d34885910.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7b6cebf625d680ab33eba86d34885910.jpg" alt="O\log(N)"></a> 。 虽然 KD 树的方法对于低维度 (<a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7fb5b8aaa79d55e35332a1f02a5aee04.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7fb5b8aaa79d55e35332a1f02a5aee04.jpg" alt="D < 20"></a>) 近邻搜索非常快, 当 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e03066df748abd9273db055cb79f0f01.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e03066df748abd9273db055cb79f0f01.jpg" alt="D"></a> 增长到很大时, 效率变低: 这就是所谓的 “维度灾难” 的一种体现。 在 scikit-learn 中, KD 树近邻搜索可以使用关键字 <code>algorithm = 'kd_tree'</code> 来指定, 并且使用类 <a href="generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code>KDTree</code></a> 来计算。</p>
<p>References:</p>
<ul>
<li><a href="http://dl.acm.org/citation.cfm?doid=361002.361007">“Multidimensional binary search trees used for associative searching”</a>, Bentley, J.L., Communications of the ACM (1975)</li>
</ul>
<h3 id="1643-ball-树">1.6.4.3. Ball 树</h3>
<p>为了解决 KD 树在高维上效率低下的问题, <em>ball 树</em> 数据结构就被研发出来了. 其中 KD 树沿卡迪尔轴（即坐标轴）分割数据, ball 树在沿着一系列的 hyper-spheres 来分割数据. 通过这种方法构建的树要比 KD 树消耗更多的时间, 但是这种数据结构对于高结构化的数据是非常有效的, 即使在高维度上也是一样.</p>
<pre><code class="language-py">ball 树将数据递归地划分为由质心  和半径  定义的节点,
</code></pre>
<p>使得节点中的每个点位于由 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/451ef7ed1a14a6cdc38324c8a5c7c683.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/451ef7ed1a14a6cdc38324c8a5c7c683.jpg" alt="r"></a> 和 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4b6d782a67ac392e97215c46b7590bf7.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4b6d782a67ac392e97215c46b7590bf7.jpg" alt="C"></a> 定义的 hyper-sphere 内. 通过使用 <em>triangle inequality（三角不等式）</em> 减少近邻搜索的候选点数:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5df8f915c528f34f0ada91db5228605f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5df8f915c528f34f0ada91db5228605f.jpg" alt="|x+y| \leq |x| + |y|"></a></p>
<p>通过这种设置, 测试点和质心之间的单一距离计算足以确定距节点内所有点的距离的下限和上限. 由于 ball 树节点的球形几何, 它在高维度上的性能超出 <em>KD-tree</em>, 尽管实际的性能高度依赖于训练数据的结构. 在 scikit-learn 中, 基于 ball 树的近邻搜索可以使用关键字 <code>algorithm = 'ball_tree'</code> 来指定, 并且使用类 <a href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code>sklearn.neighbors.BallTree</code></a> 来计算. 或者, 用户可以直接使用 <a href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code>BallTree</code></a> 类.</p>
<p>参考:</p>
<ul>
<li><a href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.91.8209">“Five balltree construction algorithms”</a>, Omohundro, S.M., International Computer Science Institute Technical Report (1989)</li>
</ul>
<h3 id="1644-最近邻算法的选择">1.6.4.4. 最近邻算法的选择</h3>
<p>对于给定数据集的最优算法是一个复杂的选择, 并且取决于多个因素:</p>
<ul>
<li> <p>样本数量 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" alt="N"></a> (i.e. <code>n_samples</code>) 和维度 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e03066df748abd9273db055cb79f0f01.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e03066df748abd9273db055cb79f0f01.jpg" alt="D"></a> (例如. <code>n_features</code>).</p>
<ul>
<li> <p><em>Brute force</em> 查询时间以 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/cf8cc964dfa6df1a7473fe033f9fb642.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/cf8cc964dfa6df1a7473fe033f9fb642.jpg" alt="OD N"></a> 增长</p> </li>
<li> <p><em>Ball tree</em> 查询时间大约以 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/70abd4aa320170aa6dbe8204a5ed846e.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/70abd4aa320170aa6dbe8204a5ed846e.jpg" alt="OD \log(N)"></a> 增长</p> </li>
<li> <pre><code class="language-py">KD tree 的查询时间  的变化是很难精确描述的.
</code></pre> <p>对于较小的 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e03066df748abd9273db055cb79f0f01.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e03066df748abd9273db055cb79f0f01.jpg" alt="D"></a> (小于20) 的成本大约是 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f211ed45608192b0763ed51c85b60811.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f211ed45608192b0763ed51c85b60811.jpg" alt="OD\log(N)"></a>, 并且 KD 树更加有效.</p> <p>对于较大的 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e03066df748abd9273db055cb79f0f01.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e03066df748abd9273db055cb79f0f01.jpg" alt="D"></a> 成本的增加接近 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c5b0e465d16add1d02594ec434515c04.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c5b0e465d16add1d02594ec434515c04.jpg" alt="ODN"></a>, 由于树结构引起的开销会导致查询效率比暴力还要低.</p> </li>
</ul> <pre><code class="language-py">对于小数据集 ( 小于30),  相当于 , 暴力算法比基于树的算法更加有效.
</code></pre> <p><a href="generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code>KDTree</code></a> 和 <a href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code>BallTree</code></a> 通过提供一个 <em>leaf size</em> 参数来解决这个问题:</p> <p>这控制了查询切换到暴力计算样本数量. 使得两种算法的效率都能接近于对较小的 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" alt="N"></a> 的暴力计算的效率.</p> </li>
<li> <p>数据结构: 数据的 <em>intrinsic dimensionality</em> (本征维数) 和/或数据的 <em>sparsity</em> (稀疏度). 本征维数是指数据所在的流形的维数 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/20310556eb1fb84146ff2584e166fd9c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/20310556eb1fb84146ff2584e166fd9c.jpg" alt="d \le D"></a>, 在参数空间可以是线性或非线性的. 稀疏度指的是数据填充参数空间的程度(这与“稀疏”矩阵中使用的概念不同, 数据矩阵可能没有零项, 但是从这个意义上来讲,它的 <strong>structure</strong> 仍然是 “稀疏” 的)。</p>
<ul>
<li><em>Brute force</em> (暴力查询)时间不受数据结构的影响。</li>
<li><em>Ball tree</em> 和 <em>KD tree</em> 的数据结构对查询时间影响很大. 一般地, 小维度的 sparser (稀疏) 数据会使查询更快. 因为 KD 树的内部表现形式是与参数轴对齐的, 对于任意的结构化数据它通常不会表现的像 ball tree 那样好.</li>
</ul> <p>在机器学习中往往使用的数据集是非常结构化的, 而且非常适合基于树结构的查询。</p> </li>
<li> <p>query point（查询点）所需的近邻数 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> 。</p> <p>&gt; * <em>Brute force</em> 查询时间几乎不受 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> 值的影响. &gt; * <em>Ball tree</em> 和 <em>KD tree</em> 的查询时间会随着 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> 的增加而变慢. 这是由于两个影响: 首先, <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> 的值越大在参数空间中搜索的部分就越大. 其次, 使用 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a036c2c31320cfaea7959236c1b81d4c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a036c2c31320cfaea7959236c1b81d4c.jpg" alt="k > 1"></a> 进行树的遍历时, 需要对内部结果进行排序.</p> <p>当 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> 相比 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" alt="N"></a> 变大时, 在基于树的查询中修剪树枝的能力是减弱的. 在这种情况下, 暴力查询会更加有效.</p> </li>
<li> <p>query points（查询点）数. ball tree 和 KD Tree 都需要一个构建阶段. 在许多查询中分摊时，这种结构的成本可以忽略不计。 如果只执行少量的查询, 可是构建成本却占总成本的很大一部分. 如果仅需查询很少的点, 暴力方法会比基于树的方法更好.</p> </li>
</ul>
<p>一般地, <code>algorithm = 'auto'</code> 选择 <code>'kd_tree'</code> 如果 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f8f807bd22e1f9f3c4271c78c8cb33fa.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f8f807bd22e1f9f3c4271c78c8cb33fa.jpg" alt="k < N/2"></a> 并且 <code>'effective_metric_'</code> 在 <code>'kd_tree'</code> 的列表 <code>'VALID_METRICS'</code> 中. 它选择 <code>'ball_tree'</code> 如果 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f8f807bd22e1f9f3c4271c78c8cb33fa.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f8f807bd22e1f9f3c4271c78c8cb33fa.jpg" alt="k < N/2"></a> 并且 <code>'effective_metric_'</code> 在 <code>'ball_tree'</code> 的列表 <code>'VALID_METRICS'</code> 中. 它选择 <code>'brute'</code> 如果 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f8f807bd22e1f9f3c4271c78c8cb33fa.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f8f807bd22e1f9f3c4271c78c8cb33fa.jpg" alt="k < N/2"></a> 并且 <code>'effective_metric_'</code> 不在 <code>'kd_tree'</code> 或 <code>'ball_tree'</code> 的列表 <code>'VALID_METRICS'</code> 中. 它选择 <code>'brute'</code> 如果 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/bdc1e4261347e1c74950e91fa4f2230f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/bdc1e4261347e1c74950e91fa4f2230f.jpg" alt="k >= N/2"></a>.</p>
<p>这种选择基于以下假设: 查询点的数量与训练点的数量至少在相同的数量级, 并且 <code>leaf_size</code> 接近其默认值 <code>30</code>.</p>
<h3 id="1645-leaf-size-的影响">1.6.4.5. <code>leaf_size</code> 的影响</h3>
<p>如上所述, 对于小样本暴力搜索是比基于数的搜索更有效的方法. 这一事实在 ball 树和 KD 树中被解释为在叶节点内部切换到蛮力搜索. 该开关的级别可以使用参数 <code>leaf_size</code> 来指定. 这个参数选择有很多的效果:</p>
<pre><code class="language-py">构造时间
</code></pre>
<p>更大的 <code>leaf_size</code> 会导致更快的树构建时间, 因为需要创建更少的节点.</p>
<pre><code class="language-py">查询时间
</code></pre>
<p>一个大或小的 <code>leaf_size</code> 可能会导致次优查询成本. 当 <code>leaf_size</code> 接近 1 时, 遍历节点所涉及的开销大大减慢了查询时间. 当 <code>leaf_size</code>, 接近训练集的大小，查询变得本质上是暴力的. 这些之间的一个很好的妥协是 <code>leaf_size = 30</code>, 这是该参数的默认值.</p>
<p><strong>内存</strong> 随着leaf_size的增加，存储树结构所需的内存减少。 对于存储每个节点的D维质心的ball tree，这点至关重要。 针对 <a href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code>BallTree</code></a> 所需的存储空间近似于 <code>1 / leaf_size</code> 乘以训练集的大小.</p>
<p><code>leaf_size</code> 不被 brute force queries（暴力查询）所引用.</p>
<h2 id="165-最近质心分类">1.6.5. 最近质心分类</h2>
<p>该 <a href="generated/sklearn.neighbors.NearestCentroid.html#sklearn.neighbors.NearestCentroid" title="sklearn.neighbors.NearestCentroid"><code>NearestCentroid</code></a> 分类器是一个简单的算法, 通过其成员的质心来表示每个类。 实际上, 这使得它类似于 <code>sklearn.KMeans</code> 算法的标签更新阶段. 它也没有参数选择, 使其成为良好的基准分类器. 然而，它确实受到非凸类的影响，即当类有显著不同的方差时。所以这个分类器假设所有维度的方差都是相等的。 对于没有做出这个假设的更复杂的方法, 请参阅线性判别分析 (<a href="generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis" title="sklearn.discriminant_analysis.LinearDiscriminantAnalysis"><code>sklearn.discriminant_analysis.LinearDiscriminantAnalysis</code></a>) 和二次判别分析 (<a href="generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis" title="sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis"><code>sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis</code></a>). 默认的 <a href="generated/sklearn.neighbors.NearestCentroid.html#sklearn.neighbors.NearestCentroid" title="sklearn.neighbors.NearestCentroid"><code>NearestCentroid</code></a> 用法示例如下:</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn.neighbors.nearest_centroid import NearestCentroid
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
&gt;&gt;&gt; y = np.array([1, 1, 1, 2, 2, 2])
&gt;&gt;&gt; clf = NearestCentroid()
&gt;&gt;&gt; clf.fit(X, y)
NearestCentroid(metric='euclidean', shrink_threshold=None)
&gt;&gt;&gt; print(clf.predict([[-0.8, -1]]))
[1]

</code></pre>
<h3 id="1651-最近缩小质心">1.6.5.1. 最近缩小质心</h3>
<p>该 <a href="generated/sklearn.neighbors.NearestCentroid.html#sklearn.neighbors.NearestCentroid" title="sklearn.neighbors.NearestCentroid"><code>NearestCentroid</code></a> 分类器有一个 <code>shrink_threshold</code> 参数, 它实现了 nearest shrunken centroid 分类器. 实际上, 每个质心的每个特征的值除以该特征的类中的方差. 然后通过 <code>shrink_threshold</code> 来减小特征值. 最值得注意的是, 如果特定特征值过0, 则将其设置为0. 实际上，这个方法移除了影响分类器的特征。 这很有用, 例如, 去除噪声特征.</p>
<p>在以下例子中, 使用一个较小的 shrink 阀值将模型的准确度从 0.81 提高到 0.82.</p>
<table>
<thead>
<tr>
<th>target:</th>
<th>../auto_examples/neighbors/plot_nearest_centroid.html</th>
</tr>
</thead>
<tbody>
<tr>
<td>scale:</td>
<td>50</td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>target:</th>
<th>../auto_examples/neighbors/plot_nearest_centroid.html</th>
</tr>
</thead>
<tbody>
<tr>
<td>scale:</td>
<td>50</td>
</tr>
<tr>
<td>---</td>
<td>---</td>
</tr>
</tbody>
</table>
<p><strong><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/27eaae520bfaa9c4bdbef494c5029741.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/27eaae520bfaa9c4bdbef494c5029741.jpg" alt="nearest_centroid_1"></a> <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a561362ff63affeb799b9d33423235a3.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a561362ff63affeb799b9d33423235a3.jpg" alt="nearest_centroid_2"></a></strong></p>
<p>例子:</p>
<ul>
<li><a href="../auto_examples/neighbors/plot_nearest_centroid.html#sphx-glr-auto-examples-neighbors-plot-nearest-centroid-py">Nearest Centroid Classification</a>: 一个分类的例子, 它使用了不同 shrink 阀值的最近质心.</li>
</ul>
</div>
<hr class="uk-article-divider">
<div class="uk-block uk-block-muted uk-padding-top-remove uk-padding-bottom-remove uk-margin-large-top  book-recommend-wrap">
<div class="uk-margin-top uk-margin-bottom uk-margin-left uk-margin-right">
<div class="uk-margin uk-text-muted "><i class="uk-icon-outdent uk-icon-justify uk-margin-small-right"></i>书籍推荐</div>
<div class="books">
<ul class="uk-book-list">
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/74/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/74/index.html">Python进阶</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/46.html">东滨社</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">73页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月8日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 2664个">2664</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/96/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/96/index.html">零基础学Python</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/59.html">qiwsir</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">80页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月29日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 1635个">1635</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/162/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/162/index.html">Python方向综合面试题</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/92.html">jackfrued</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">115页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月26日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 35个">35</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/173/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/code_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/173/index.html">HBase中文参考指南 3.0</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/18.html">ApacheCN</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="code">code</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">33页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月26日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 51个">51</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/61/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/javascript_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/61/index.html">前端开发者手册</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/37.html">dwqs</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="html5">html5</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="javascript">javascript</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="css3">css3</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">92页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月5日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 548个">548</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/107/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/laravel_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/107/index.html">Laravel 源码详解</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/62.html">tzivanmoe</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="laravel">laravel</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">42页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年7月1日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 4个">4</span>
</div>
</div>
</div>
</li>
<hr>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
<nav class="tm-navbar uk-navbar uk-navbar-attached reader-nav">
<div class="uk-float-left uk-margin-small-top">
<a href="javascript:;" title="目录菜单" class="show-menu  uk-icon-hover  uk-icon-align-justify uk-margin-right"></a>
<div data-uk-dropdown="{mode:'click',pos:'bottom-left'}" class="font-setting-wrap">
<a class="uk-icon-hover uk-icon-font uk-margin-right" aria-label="字体设置" href="javascript:;"></a>
<div class="uk-dropdown dropdown-menu">
<div class="dropdown-caret"><span class="caret-outer"></span><span class="caret-inner"></span></div>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-reduce">小字</button>
<button class="uk-button-link button size-2 font-enlarge">大字</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-1 ">宋体</button>
<button class="uk-button-link button size-2 font-2 ">黑体</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-3 color-theme-sun "><i class="uk-icon-sun-o"></i>白天</button>
<button class="uk-button-link button size-3 color-theme-eye "><i class="uk-icon-eye"></i>护眼</button>
<button class="uk-button-link button size-3 color-theme-moon "><i class="uk-icon-moon-o"></i>夜晚</button></div>
</div>
</div>
<a class="logo uk-margin-right" href="../../../" title="返回首页"><img class="" src="../../../static/components/images/icon_32.png" /></a>
</div>
<div class="uk-navbar-flip  uk-hidden-small">
<div id="share-box"></div>
</div>
</nav>
<div id="menu-id" class="uk-offcanvas reader-offcanvas">
<div class="uk-offcanvas-bar">
<ul class="book-menu-bar uk-nav uk-nav-offcanvas" data-uk-nav>
<li>
<a href="../../../book/170/index.html" data-book-page-rel-url="index.html" data-book-page-id="0" title="封面">封面</a>
</li>
<li>
<a class="pjax" href="../../../book/170/readme.html" data-book-page-rel-url="readme.html" data-book-page-id="0" title="简介">简介</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/79.html" title="安装 scikit-learn" data-book-page-rel-url="docs/79.html" data-book-page-id="11491">安装 scikit-learn</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/1.html" title="1. 监督学习" data-book-page-rel-url="docs/1.html" data-book-page-id="11492">1. 监督学习</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/2.html" title="1.1. 广义线性模型" data-book-page-rel-url="docs/2.html" data-book-page-id="11493">1.1. 广义线性模型</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/3.html" title="1.2. 线性和二次判别分析" data-book-page-rel-url="docs/3.html" data-book-page-id="11494">1.2. 线性和二次判别分析</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/4.html" title="1.3. 内核岭回归" data-book-page-rel-url="docs/4.html" data-book-page-id="11495">1.3. 内核岭回归</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/5.html" title="1.4. 支持向量机" data-book-page-rel-url="docs/5.html" data-book-page-id="11496">1.4. 支持向量机</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/6.html" title="1.5. 随机梯度下降" data-book-page-rel-url="docs/6.html" data-book-page-id="11497">1.5. 随机梯度下降</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/7.html" title="1.6. 最近邻" data-book-page-rel-url="docs/7.html" data-book-page-id="11498">1.6. 最近邻</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/8.html" title="1.7. 高斯过程" data-book-page-rel-url="docs/8.html" data-book-page-id="11499">1.7. 高斯过程</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/9.html" title="1.8. 交叉分解" data-book-page-rel-url="docs/9.html" data-book-page-id="11500">1.8. 交叉分解</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/10.html" title="1.9. 朴素贝叶斯" data-book-page-rel-url="docs/10.html" data-book-page-id="11501">1.9. 朴素贝叶斯</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/11.html" title="1.10. 决策树" data-book-page-rel-url="docs/11.html" data-book-page-id="11502">1.10. 决策树</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/12.html" title="1.11. 集成方法" data-book-page-rel-url="docs/12.html" data-book-page-id="11503">1.11. 集成方法</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/13.html" title="1.12. 多类和多标签算法" data-book-page-rel-url="docs/13.html" data-book-page-id="11504">1.12. 多类和多标签算法</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/14.html" title="1.13. 特征选择" data-book-page-rel-url="docs/14.html" data-book-page-id="11505">1.13. 特征选择</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/15.html" title="1.14. 半监督学习" data-book-page-rel-url="docs/15.html" data-book-page-id="11506">1.14. 半监督学习</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/16.html" title="1.15. 等式回归" data-book-page-rel-url="docs/16.html" data-book-page-id="11507">1.15. 等式回归</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/17.html" title="1.16. 概率校准" data-book-page-rel-url="docs/17.html" data-book-page-id="11508">1.16. 概率校准</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/18.html" title="1.17. 神经网络模型（有监督）" data-book-page-rel-url="docs/18.html" data-book-page-id="11509">1.17. 神经网络模型（有监督）</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/19.html" title="2. 无监督学习" data-book-page-rel-url="docs/19.html" data-book-page-id="11510">2. 无监督学习</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/20.html" title="2.1. 高斯混合模型" data-book-page-rel-url="docs/20.html" data-book-page-id="11511">2.1. 高斯混合模型</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/21.html" title="2.2. 流形学习" data-book-page-rel-url="docs/21.html" data-book-page-id="11512">2.2. 流形学习</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/22.html" title="2.3. 聚类" data-book-page-rel-url="docs/22.html" data-book-page-id="11513">2.3. 聚类</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/23.html" title="2.4. 双聚类" data-book-page-rel-url="docs/23.html" data-book-page-id="11514">2.4. 双聚类</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/24.html" title="2.5. 分解成分中的信号（矩阵分解问题）" data-book-page-rel-url="docs/24.html" data-book-page-id="11515">2.5. 分解成分中的信号（矩阵分解问题）</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/25.html" title="2.6. 协方差估计" data-book-page-rel-url="docs/25.html" data-book-page-id="11516">2.6. 协方差估计</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/26.html" title="2.7. 经验协方差" data-book-page-rel-url="docs/26.html" data-book-page-id="11517">2.7. 经验协方差</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/27.html" title="2.8. 收敛协方差" data-book-page-rel-url="docs/27.html" data-book-page-id="11518">2.8. 收敛协方差</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/28.html" title="2.9. 稀疏逆协方差" data-book-page-rel-url="docs/28.html" data-book-page-id="11519">2.9. 稀疏逆协方差</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/29.html" title="2.10. Robust 协方差估计" data-book-page-rel-url="docs/29.html" data-book-page-id="11520">2.10. Robust 协方差估计</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/30.html" title="2.11. 新奇和异常值检测" data-book-page-rel-url="docs/30.html" data-book-page-id="11521">2.11. 新奇和异常值检测</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/31.html" title="2.12. 密度估计" data-book-page-rel-url="docs/31.html" data-book-page-id="11522">2.12. 密度估计</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/32.html" title="2.13. 神经网络模型（无监督）" data-book-page-rel-url="docs/32.html" data-book-page-id="11523">2.13. 神经网络模型（无监督）</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/33.html" title="3. 模型选择和评估" data-book-page-rel-url="docs/33.html" data-book-page-id="11524">3. 模型选择和评估</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/34.html" title="3.1. 交叉验证：评估估算器的表现" data-book-page-rel-url="docs/34.html" data-book-page-id="11525">3.1. 交叉验证：评估估算器的表现</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/35.html" title="3.2. 调整估计器的超参数" data-book-page-rel-url="docs/35.html" data-book-page-id="11526">3.2. 调整估计器的超参数</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/53.html" title="3.3. 模型评估: 量化预测的质量" data-book-page-rel-url="docs/53.html" data-book-page-id="11527">3.3. 模型评估: 量化预测的质量</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/54.html" title="3.4. 模型持久化" data-book-page-rel-url="docs/54.html" data-book-page-id="11528">3.4. 模型持久化</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/55.html" title="3.5. 验证曲线: 绘制分数以评估模型" data-book-page-rel-url="docs/55.html" data-book-page-id="11529">3.5. 验证曲线: 绘制分数以评估模型</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/56.html" title="4. 数据集转换" data-book-page-rel-url="docs/56.html" data-book-page-id="11530">4. 数据集转换</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/57.html" title="4.1. Pipeline（管道）和 FeatureUnion（特征联合）: 合并的评估器" data-book-page-rel-url="docs/57.html" data-book-page-id="11531">4.1. Pipeline（管道）和 FeatureUnion（特征联合）: 合并的评估器</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/58.html" title="4.2. 特征提取" data-book-page-rel-url="docs/58.html" data-book-page-id="11532">4.2. 特征提取</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/59.html" title="4.3. 预处理数据" data-book-page-rel-url="docs/59.html" data-book-page-id="11533">4.3. 预处理数据</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/60.html" title="4.4. 无监督降维" data-book-page-rel-url="docs/60.html" data-book-page-id="11534">4.4. 无监督降维</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/61.html" title="4.5. 随机投影" data-book-page-rel-url="docs/61.html" data-book-page-id="11535">4.5. 随机投影</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/62.html" title="4.6. 内核近似" data-book-page-rel-url="docs/62.html" data-book-page-id="11536">4.6. 内核近似</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/63.html" title="4.7. 成对的矩阵, 类别和核函数" data-book-page-rel-url="docs/63.html" data-book-page-id="11537">4.7. 成对的矩阵, 类别和核函数</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/64.html" title="4.8. 预测目标 (`y`.html) 的转换" data-book-page-rel-url="docs/64.html" data-book-page-id="11538">4.8. 预测目标 (`y`.html) 的转换</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/65.html" title="5. 数据集加载工具" data-book-page-rel-url="docs/65.html" data-book-page-id="11539">5. 数据集加载工具</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/66.html" title="6. 大规模计算的策略: 更大量的数据" data-book-page-rel-url="docs/66.html" data-book-page-id="11540">6. 大规模计算的策略: 更大量的数据</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/67.html" title="7. 计算性能" data-book-page-rel-url="docs/67.html" data-book-page-id="11541">7. 计算性能</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/68.html" title="使用 scikit-learn 介绍机器学习" data-book-page-rel-url="docs/68.html" data-book-page-id="11542">使用 scikit-learn 介绍机器学习</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/69.html" title="关于科学数据处理的统计学习教程" data-book-page-rel-url="docs/69.html" data-book-page-id="11543">关于科学数据处理的统计学习教程</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/70.html" title="机器学习: scikit-learn 中的设置以及预估对象" data-book-page-rel-url="docs/70.html" data-book-page-id="11544">机器学习: scikit-learn 中的设置以及预估对象</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/71.html" title="监督学习：从高维观察预测输出变量" data-book-page-rel-url="docs/71.html" data-book-page-id="11545">监督学习：从高维观察预测输出变量</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/72.html" title="模型选择：选择估计量及其参数" data-book-page-rel-url="docs/72.html" data-book-page-id="11546">模型选择：选择估计量及其参数</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/73.html" title="无监督学习: 寻求数据表示" data-book-page-rel-url="docs/73.html" data-book-page-id="11547">无监督学习: 寻求数据表示</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/74.html" title="把它们放在一起" data-book-page-rel-url="docs/74.html" data-book-page-id="11548">把它们放在一起</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/75.html" title="寻求帮助" data-book-page-rel-url="docs/75.html" data-book-page-id="11549">寻求帮助</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/76.html" title="处理文本数据" data-book-page-rel-url="docs/76.html" data-book-page-id="11550">处理文本数据</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/77.html" title="选择正确的评估器(estimator.html)" data-book-page-rel-url="docs/77.html" data-book-page-id="11551">选择正确的评估器(estimator.html)</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/78.html" title="外部资源，视频和谈话" data-book-page-rel-url="docs/78.html" data-book-page-id="11552">外部资源，视频和谈话</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/80.html" title="常见问题" data-book-page-rel-url="docs/80.html" data-book-page-id="11553">常见问题</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/81.html" title="时光轴" data-book-page-rel-url="docs/81.html" data-book-page-id="11554">时光轴</a>
</li>
</ul>
</div>
</div>
<script src="https://cdn.staticfile.net/jquery/1.12.4/jquery.min.js"></script>
<script type="text/javascript" src="../../../static/components/uikit-2.27.5/js/uikit.reader.js"></script>
<script type="text/javascript" src="../../../static/components/social-share/social-share.min.js"></script>
<script>(function(){var bp =document.createElement('script');var curProtocol =window.location.protocol.split(':')[0];if (curProtocol ==='https') {bp.src ='https://zz.bdstatic.com/linksubmit/push.js';}
else {bp.src ='http://push.zhanzhang.baidu.com/push.js';}
var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(bp,s);})();</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
<script src="https://cdn.staticfile.net/highlight.js/9.12.0/highlight.min.js"></script>
<script src="https://cdn.staticfile.net/jquery.pjax/2.0.1/jquery.pjax.min.js"></script>
<script src="https://cdn.staticfile.net/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="https://cdn.staticfile.net/uikit/2.27.5/js/components/lightbox.min.js"></script>
<link rel="dns-prefetch" href="../../..//cdn.mathjax.org" />
<script type="text/x-mathjax-config">
 function initMathJax() {
    var mathId = $("book-content-section")[0];
    MathJax.Hub.Config({
        tex2jax: {skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code','a']},
        showProcessingMessages: false,
        messageStyle: "none"
    });
    MathJax.Hub.Queue(["Typeset",MathJax.Hub,mathId]);
 };
initMathJax();
</script>
<script src='https://cdn.staticfile.net/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML' async></script>
<style>
	.MathJax_Display{display:inline!important;}
</style>
<script type="text/javascript" src="../../../static/components/js/reader.js"></script>
<script type="text/javascript">var bookId =170;var bookPageId =11498;var bookPageRelUrl ='docs/7.html';</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
</body>
</html>