
<!DOCTYPE html>
<html lang="zh-CN">
<head>
<title>1.4. 支持向量机-scikit-learn (sklearn) 官方文档中文版</title>
<meta content='1.4. 支持向量机,scikit-learn (sklearn) 官方文档中文版' name='keywords'>
<meta content='1.4. 支持向量机,scikit-learn (sklearn) 官方文档中文版' name='description'>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta http-equiv="Content-Language" content="zh-CN" />
<meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1, maximum-scale=1, user-scalable=no"../../../>
<meta name="applicable-device" content="pc,mobile">
<link rel="shortcut icon" href="../../../favicon.ico" type="image/x-icon" />
<meta name="renderer" content="webkit">
<link rel="stylesheet" href="../../../static/components/uikit-2.27.5/css/uikit.custom.css">
<link rel="stylesheet" href="../../../static/components/social-share/social-share.min.css">
<link rel="stylesheet" href="../../../static/components/highlight/styles/custom.css">
<link rel="stylesheet" href="../../../static/components/css/base.css">
<link rel="stylesheet" href="../../../static/components/css/reader.css">
<link rel="stylesheet" href="../../../static/components/css/markdown.css">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5313208362165053" crossorigin="anonymous"></script>
</head>
<body>
<div class=" book-main-wrap uk-container uk-container-center uk-margin-top ">
<div class="uk-grid">
<div class="uk-width-1-1 reader-wrap ">
<div class=" bottom-nav uk-clearfix ">
<div class="uk-align-left ">
<a href="../../../book/170/docs/4.html">
<i class="nav-icon-left uk-icon-small  uk-icon-caret-left"></i>
<span class="">1.3. 内核岭回归</span>
</a>
</div>
<div class="uk-align-right ">
<a href="../../../book/170/docs/6.html">
<span class="">1.5. 随机梯度下降</span>
<i class="nav-icon-right uk-icon-small  uk-icon-caret-right"></i>
</a>
</div>
</div>
<div class="uk-text-center">
<h2 class="book-page-title uk-container-center">
<a href="../../../book/170/index.html">scikit-learn (sklearn) 官方文档中文版</a>
<a target="_blank" rel="nofollow" href="https://github.com/apachecn/scikit-learn-doc-zh" class="uk-icon-button uk-icon-github" title="github项目地址"></a>
</h2>
</div>
<script type="text/javascript" src="../../../static/components/js/app_intro.js"></script>
<ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-5313208362165053" data-ad-slot="1328047120"></ins>
<script>(adsbygoogle =window.adsbygoogle ||[]).push({});</script>
<hr class="uk-article-divider">
<div class="book-content-section  md-content-section  uk-margin-bottom">
<h1 id="14-支持向量机">1.4. 支持向量机</h1>
<p>校验者: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/weiyd">@尔了个达</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@维</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@子浪</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@小瑶</a> 翻译者: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@Damon</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@Leon晋</a></p>
<p><strong>支持向量机 (SVMs)</strong> 可用于以下监督学习算法 <a href="#svm-classification">分类</a>, <a href="#svm-regression">回归</a> 和 <a href="#svm-outlier-detection">异常检测</a>.</p>
<p>支持向量机的优势在于:</p>
<blockquote>
<ul>
<li>在高维空间中非常高效.</li>
<li>即使在数据维度比样本数量大的情况下仍然有效.</li>
<li>在决策函数（称为支持向量）中使用训练集的子集,因此它也是高效利用内存的.</li>
<li>通用性: 不同的核函数 <a href="#svm-kernels">核函数</a> 与特定的决策函数一一对应.常见的 kernel 已</li>
</ul>
<p>经提供,也可以指定定制的内核.</p>
</blockquote>
<p>支持向量机的缺点包括:</p>
<blockquote>
<ul>
<li>如果特征数量比样本数量大得多,在选择核函数 <a href="#svm-kernels">核函数</a> 时要避免过拟合,</li>
</ul>
<p>而且正则化项是非常重要的.</p>
<ul>
<li>支持向量机不直接提供概率估计,这些都是使用昂贵的五次交叉验算计算的. (详情见 <a href="#scores-probabilities">Scores and probabilities</a>, 在下文中).</li>
</ul>
</blockquote>
<p>在 scikit-learn 中,支持向量机提供 dense(<code>numpy.ndarray</code> ,可以通过 <code>numpy.asarray</code> 进行转换) 和 sparse (任何 <code>scipy.sparse</code>) 样例向量作为输出.然而,要使用支持向量机来对 sparse 数据作预测,它必须已经拟合这样的数据.使用 C 代码的 <code>numpy.ndarray</code> (dense) 或者带有 <code>dtype=float64</code> 的 <code>scipy.sparse.csr_matrix</code> (sparse) 来优化性能.</p>
<h2 id="141-分类">1.4.1. 分类</h2>
<p><a href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code>SVC</code></a>, <a href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><code>NuSVC</code></a> 和 <a href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code>LinearSVC</code></a> 能在数据集中实现多元分类.</p>
<p><a href="../auto_examples/svm/plot_iris.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/75d98860b528f3fcd2b060ad5e624ca0.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/75d98860b528f3fcd2b060ad5e624ca0.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_iris_0012.png"></a></a></p>
<p><a href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code>SVC</code></a> 和 <a href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><code>NuSVC</code></a> 是相似的方法, 但是接受稍许不同的参数设置并且有不同的数学方程(在这部分看 <a href="#svm-mathematical-formulation">数学公式</a>). 另一方面, <a href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code>LinearSVC</code></a> 是另一个实现线性核函数的支持向量分类. 记住 <a href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code>LinearSVC</code></a> 不接受关键词 <code>kernel</code>, 因为它被假设为线性的. 它也缺少一些 <a href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code>SVC</code></a> 和 <a href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><code>NuSVC</code></a> 的成员(members) 比如 <code>support_</code> .</p>
<p>和其他分类器一样, <a href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code>SVC</code></a>, <a href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><code>NuSVC</code></a> 和 <a href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code>LinearSVC</code></a> 将两个数组作为输入: <code>[n_samples, n_features]</code> 大小的数组 X 作为训练样本, <code>[n_samples]</code> 大小的数组 y 作为类别标签(字符串或者整数):</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn import svm
&gt;&gt;&gt; X = [[0, 0], [1, 1]]
&gt;&gt;&gt; y = [0, 1]
&gt;&gt;&gt; clf = svm.SVC()
&gt;&gt;&gt; clf.fit(X, y)  
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
 decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
 max_iter=-1, probability=False, random_state=None, shrinking=True,
 tol=0.001, verbose=False)

</code></pre>
<p>在拟合后, 这个模型可以用来预测新的值:</p>
<pre><code class="language-py">&gt;&gt;&gt; clf.predict([[2., 2.]])
array([1])

</code></pre>
<p>SVMs 决策函数取决于训练集的一些子集, 称作支持向量. 这些支持向量的部分特性可以在 <code>support_vectors_</code>, <code>support_</code> 和 <code>n_support</code> 找到:</p>
<pre><code class="language-py">&gt;&gt;&gt; # 获得支持向量
&gt;&gt;&gt; clf.support_vectors_
array([[ 0.,  0.],
 [ 1.,  1.]])
&gt;&gt;&gt; # 获得支持向量的索引get indices of support vectors
&gt;&gt;&gt; clf.support_ 
array([0, 1]...)
&gt;&gt;&gt; # 为每一个类别获得支持向量的数量
&gt;&gt;&gt; clf.n_support_ 
array([1, 1]...)

</code></pre>
<h3 id="1411-多元分类">1.4.1.1. 多元分类</h3>
<p><a href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code>SVC</code></a> 和 <a href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><code>NuSVC</code></a> 为多元分类实现了 “one-against-one” 的方法 (Knerr et al., 1990) 如果 <code>n_class</code> 是类别的数量, 那么 <code>n_class * (n_class - 1) / 2</code> 分类器被重构, 而且每一个从两个类别中训练数据. 为了给其他分类器提供一致的交互, <code>decision_function_shape</code> 选项允许聚合 “one-against-one” 分类器的结果成 <code>(n_samples, n_classes)</code> 的大小到决策函数:</p>
<pre><code class="language-py">&gt;&gt;&gt; X = [[0], [1], [2], [3]]
&gt;&gt;&gt; Y = [0, 1, 2, 3]
&gt;&gt;&gt; clf = svm.SVC(decision_function_shape='ovo')
&gt;&gt;&gt; clf.fit(X, Y) 
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
 decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',
 max_iter=-1, probability=False, random_state=None, shrinking=True,
 tol=0.001, verbose=False)
&gt;&gt;&gt; dec = clf.decision_function([[1]])
&gt;&gt;&gt; dec.shape[1] # 4 classes: 4*3/2 = 6
6
&gt;&gt;&gt; clf.decision_function_shape = "ovr"
&gt;&gt;&gt; dec = clf.decision_function([[1]])
&gt;&gt;&gt; dec.shape[1] # 4 classes
4

</code></pre>
<p>另一方面, <a href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code>LinearSVC</code></a> 实现 “one-vs-the-rest” 多类别策略, 从而训练 n 类别的模型. 如果只有两类, 只训练一个模型.:</p>
<pre><code class="language-py">&gt;&gt;&gt; lin_clf = svm.LinearSVC()
&gt;&gt;&gt; lin_clf.fit(X, Y) 
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
 intercept_scaling=1, loss='squared_hinge', max_iter=1000,
 multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
 verbose=0)
&gt;&gt;&gt; dec = lin_clf.decision_function([[1]])
&gt;&gt;&gt; dec.shape[1]
4

</code></pre>
<p>参见 <a href="#svm-mathematical-formulation">数学公式</a> 查看决策函数的完整描述.</p>
<p>记住 <a href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code>LinearSVC</code></a> 也实现了可选择的多类别策略, 通过使用选项 <code>multi_class='crammer_singer'</code>, 所谓的多元 SVM 由 Crammer 和 Singer 明确表达. 这个方法是一致的, 对于 one-vs-rest 是不正确的. 实际上, one-vs-rest 分类通常受到青睐, 因为结果大多数是相似的, 但是运行时间却显著减少.</p>
<p>对于 “one-vs-rest” <a href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code>LinearSVC</code></a>, 属性 <code>coef_</code> 和 <code>intercept_</code> 分别具有 <code>[n_class, n_features]</code> 和 <code>[n_class]</code> 尺寸. 系数的每一行符合 <code>n_class</code> 的许多 one-vs-rest 分类器之一, 并且就以这一类的顺序与拦截器(intercepts)相似.</p>
<p>至于 one-vs-one <a href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code>SVC</code></a>, 属性特征的布局(layout)有少多些复杂. 考虑到有一种线性核函数, <code>coef_</code> 和 <code>intercept_</code> 的布局(layout)与上文描述成 <a href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code>LinearSVC</code></a> 相似, 除了 <code>coef_</code> 的形状 <code>[n_class * (n_class - 1) / 2, n_features]</code>, 与许多二元的分类器相似. 0到n的类别顺序是 “0 vs 1”, “0 vs 2” , … “0 vs n”, “1 vs 2”, “1 vs 3”, “1 vs n”, … “n-1 vs n”.</p>
<p><code>dual_coef_</code> 的 shape 是 <code>[n_class-1, n_SV]</code>, 这个结构有些难以理解. 对应于支持向量的列与 <code>n_class * (n_class - 1) / 2</code> “one-vs-one” 分类器相关. 每一个支持向量用于 <code>n_class - 1</code> 分类器中.对于这些分类器,每一行的 <code>n_class - 1</code> 条目对应于对偶系数(dual coefficients).</p>
<p>通过这个例子更容易说明:</p>
<p>考虑一个三类的问题,类0有三个支持向量 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0992b23a98660c7b2102695e74407be2.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0992b23a98660c7b2102695e74407be2.jpg" alt="v{0}_0, v{1}_0, v^{2}_0"></a> 而类 1 和 2 分别有 如下两个支持向量 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/60337a9162822d71dc32e68952b4e02a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/60337a9162822d71dc32e68952b4e02a.jpg" alt="v{0}_1, v{1}_1"></a> and <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5c0b2807058791d6069327b709fae60c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5c0b2807058791d6069327b709fae60c.jpg" alt="v{0}_2, v{1}_2"></a>.对于每个支持 向量 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ed6a1db8527fda759b14943c1b36d88e.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ed6a1db8527fda759b14943c1b36d88e.jpg" alt="v^{j}_i"></a>, 有两个对偶系数.在类别 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" alt="i"></a> 和 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6f0cdccb5dc60bae6e7a303075ddbdf6.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6f0cdccb5dc60bae6e7a303075ddbdf6.jpg" alt="\alpha^{j}_{i,k}"></a> 中, 我们将支持向量的系数记录为 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ed6a1db8527fda759b14943c1b36d88e.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ed6a1db8527fda759b14943c1b36d88e.jpg" alt="v^{j}_i"></a> 那么 <code>dual_coef_</code> 可以表示为:</p>
<p>| <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4f8138b00b37d9734bb93aec7e00ac5e.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4f8138b00b37d9734bb93aec7e00ac5e.jpg" alt="\alpha^{0}_{0,1}"></a> | <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d6127761ddbd135a1317ec14f1ddfac4.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d6127761ddbd135a1317ec14f1ddfac4.jpg" alt="\alpha^{0}_{0,2}"></a> | Coefficients for SVs of class 0 | | <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7ff85adefbea266b138eec7868e87fa9.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7ff85adefbea266b138eec7868e87fa9.jpg" alt="\alpha^{1}_{0,1}"></a> | <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ba5932f77767fa05771311d1f926e3ee.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ba5932f77767fa05771311d1f926e3ee.jpg" alt="\alpha^{1}_{0,2}"></a> | | <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8c0b5b6a48349ecfb1f20d9168d166b7.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8c0b5b6a48349ecfb1f20d9168d166b7.jpg" alt="\alpha^{2}_{0,1}"></a> | <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/48a36c240dcfa54de5ea4cc6250087fa.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/48a36c240dcfa54de5ea4cc6250087fa.jpg" alt="\alpha^{2}_{0,2}"></a> | | <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c9a9a1cec953f556f78c5f400277b422.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c9a9a1cec953f556f78c5f400277b422.jpg" alt="\alpha^{0}_{1,0}"></a> | <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/beee3f6e512d1e3caf1d1f6cfff468ae.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/beee3f6e512d1e3caf1d1f6cfff468ae.jpg" alt="\alpha^{0}_{1,2}"></a> | Coefficients for SVs of class 1 | | <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0243e3516b65d89a7e3da13680c1a1b7.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0243e3516b65d89a7e3da13680c1a1b7.jpg" alt="\alpha^{1}_{1,0}"></a> | <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2238e05d9e3ae45b81577c9902a9cfbb.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2238e05d9e3ae45b81577c9902a9cfbb.jpg" alt="\alpha^{1}_{1,2}"></a> | | <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6c281997fc8d9f34a530a7e2bc854adf.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6c281997fc8d9f34a530a7e2bc854adf.jpg" alt="\alpha^{0}_{2,0}"></a> | <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/505ffca1dc9570f24fd66272d18abb1f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/505ffca1dc9570f24fd66272d18abb1f.jpg" alt="\alpha^{0}_{2,1}"></a> | Coefficients for SVs of class 2 | | <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2c90187d36ba884ee9ae4c99334fb3b4.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2c90187d36ba884ee9ae4c99334fb3b4.jpg" alt="\alpha^{1}_{2,0}"></a> | <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/319e234a072e86b6b55ce431ca56b43e.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/319e234a072e86b6b55ce431ca56b43e.jpg" alt="\alpha^{1}_{2,1}"></a> |</p>
<h3 id="1412-得分和概率">1.4.1.2. 得分和概率</h3>
<p><a href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code>SVC</code></a> 方法的 <code>decision_function</code> 给每一个样例每一个类别分值(scores)(或者在一个二元类中每一个样例一个分值). 当构造器(constructor)选项 <code>probability</code> 设置为 <code>True</code> 的时候, 类成员可能性评估开启.(来自 <code>predict_proba</code> 和 <code>predict_log_proba</code> 方法) 在二元分类中,概率使用 Platt scaling 进行标准化: 在 SVM 分数上的逻辑回归,在训练集上用额外的交叉验证来拟合.在多类情况下,这可以扩展为 per Wu et al.(2004)</p>
<p>不用说,对于大数据集来说,在 Platt scaling 中进行交叉验证是一项昂贵的操作. 另外,可能性预测可能与 scores 不一致,因为 scores 的 “argmax” 可能不是可能性的 argmax. (例如,在二元分类中,一个样本可能被标记为一个有可能性的类 <code>predict</code> &lt;½ according to <code>predict_proba</code>.) Platt 的方法也有理论问题. 如果 confidence scores 必要,但是这些没必要是可能性, 那么建议设置 <code>probability=False</code> 并使用 <code>decision_function</code> 而不是 <code>predict_proba</code>.</p>
<p>参考:</p>
<ul>
<li>Wu, Lin and Weng, <a href="#id13"><code>"Probability estimates for multi-class classification by pairwise coupling（成对耦合的多类分类的概率估计）"&amp;lt;http://www.csie.ntu.edu.tw/~cjlin/papers/svmprob/svmprob.pdf&amp;gt;</code>_</a>, JMLR 5:975-1005, 2004.</li>
<li>Platt <a href="#id15"><code>"Probabilistic outputs for SVMs and comparisons to regularized likelihood methods（SVMs 的概率输出和与规则化似然方法的比较）"&amp;lt;http://www.cs.colorado.edu/~mozer/Teaching/syllabi/6622/papers/Platt1999.pdf&amp;gt;</code>_</a> .</li>
</ul>
<h3 id="1413-非均衡问题">1.4.1.3. 非均衡问题</h3>
<p>这个问题期望给予某一类或某个别样例能使用的关键词 <code>class_weight</code> 和 <code>sample_weight</code> 提高权重(importance).</p>
<p><a href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code>SVC</code></a> (而不是 <a href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><code>NuSVC</code></a>) 在 <code>fit</code> 方法中生成了一个关键词 <code>class_weight</code>. 它是形如 <code>{class_label : value}</code> 的字典, value 是浮点数大于 0 的值, 把类 <code>class_label</code> 的参数 <code>C</code> 设置为 <code>C * value</code>.</p>
<p><a href="../auto_examples/svm/plot_separating_hyperplane_unbalanced.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9b6c97851ffb568abc5688d5c9e81800.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9b6c97851ffb568abc5688d5c9e81800.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_separating_hyperplane_unbalanced_0011.png"></a></a></p>
<p><a href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code>SVC</code></a>, <a href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><code>NuSVC</code></a>, <a href="generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><code>SVR</code></a>, <a href="generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR" title="sklearn.svm.NuSVR"><code>NuSVR</code></a> 和 <a href="generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><code>OneClassSVM</code></a> 在 <code>fit</code> 方法中通过关键词 <code>sample_weight</code> 为单一样例实现权重weights.与 <code>class_weight</code> 相似, 这些把第i个样例的参数 <code>C</code> 换成 <code>C * sample_weight[i]</code>.</p>
<p><a href="../auto_examples/svm/plot_weighted_samples.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f298c2b42dd32bed6f02df3c6d4f7cf9.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f298c2b42dd32bed6f02df3c6d4f7cf9.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_weighted_samples_0011.png"></a></a></p>
<p>例子:</p>
<ul>
<li><a href="../auto_examples/svm/plot_iris.html#sphx-glr-auto-examples-svm-plot-iris-py">Plot different SVM classifiers in the iris dataset</a>,</li>
<li><a href="../auto_examples/svm/plot_separating_hyperplane.html#sphx-glr-auto-examples-svm-plot-separating-hyperplane-py">SVM: Maximum margin separating hyperplane</a>,</li>
<li><a href="../auto_examples/svm/plot_separating_hyperplane_unbalanced.html#sphx-glr-auto-examples-svm-plot-separating-hyperplane-unbalanced-py">SVM: Separating hyperplane for unbalanced classes</a></li>
<li><a href="../auto_examples/svm/plot_svm_anova.html#sphx-glr-auto-examples-svm-plot-svm-anova-py">SVM-Anova: SVM with univariate feature selection</a>,</li>
<li><a href="../auto_examples/svm/plot_svm_nonlinear.html#sphx-glr-auto-examples-svm-plot-svm-nonlinear-py">Non-linear SVM</a></li>
<li><a href="../auto_examples/svm/plot_weighted_samples.html#sphx-glr-auto-examples-svm-plot-weighted-samples-py">SVM: Weighted samples</a>,</li>
</ul>
<h2 id="142-回归">1.4.2. 回归</h2>
<p>支持向量分类的方法可以被扩展用作解决回归问题. 这个方法被称作支持向量回归.</p>
<p>支持向量分类生成的模型(如前描述)只依赖于训练集的子集,因为构建模型的 cost function 不在乎边缘之外的训练点. 类似的,支持向量回归生成的模型只依赖于训练集的子集, 因为构建模型的 cost function 忽略任何接近于模型预测的训练数据.</p>
<p>支持向量分类有三种不同的实现形式: <a href="generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><code>SVR</code></a>, <a href="generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR" title="sklearn.svm.NuSVR"><code>NuSVR</code></a> 和 <a href="generated/sklearn.svm.LinearSVR.html#sklearn.svm.LinearSVR" title="sklearn.svm.LinearSVR"><code>LinearSVR</code></a>. 在只考虑线性核的情况下, <a href="generated/sklearn.svm.LinearSVR.html#sklearn.svm.LinearSVR" title="sklearn.svm.LinearSVR"><code>LinearSVR</code></a> 比 <a href="generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><code>SVR</code></a> 提供一个更快的实现形式, 然而比起 <a href="generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><code>SVR</code></a> 和 <a href="generated/sklearn.svm.LinearSVR.html#sklearn.svm.LinearSVR" title="sklearn.svm.LinearSVR"><code>LinearSVR</code></a>, <a href="generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR" title="sklearn.svm.NuSVR"><code>NuSVR</code></a> 实现一个稍微不同的构思(formulation).细节参见 <a href="#svm-implementation-details">实现细节</a>.</p>
<p>与分类的类别一样, fit方法会调用参数向量 X, y, 只在 y 是浮点数而不是整数型.:</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn import svm
&gt;&gt;&gt; X = [[0, 0], [2, 2]]
&gt;&gt;&gt; y = [0.5, 2.5]
&gt;&gt;&gt; clf = svm.SVR()
&gt;&gt;&gt; clf.fit(X, y) 
SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',
 kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)
&gt;&gt;&gt; clf.predict([[1, 1]])
array([ 1.5])

</code></pre>
<p>样例:</p>
<ul>
<li><a href="../auto_examples/svm/plot_svm_regression.html#sphx-glr-auto-examples-svm-plot-svm-regression-py">Support Vector Regression (SVR) using linear and non-linear kernels</a></li>
</ul>
<h2 id="143-密度估计-异常novelty检测">1.4.3. 密度估计, 异常（novelty）检测</h2>
<p>但类别的 SVM 用于异常检测, 即给予一个样例集, 它会检测这个样例集的 soft boundary 以便给新的数据点分类, 看它是否属于这个样例集. 生成的类称作 <a href="generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><code>OneClassSVM</code></a>.</p>
<p>这种情况下, 因为它属于非监督学习的一类, 没有类标签, fit 方法只会考虑输入数组X.</p>
<p>在章节 <a href="outlier_detection.html#outlier-detection">新奇和异常值检测</a> 查看这个应用的更多细节.</p>
<p><a href="../auto_examples/svm/plot_oneclass.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b29b59eca5c581c3f54d92c1671f2b2e.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b29b59eca5c581c3f54d92c1671f2b2e.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_oneclass_0011.png"></a></a></p>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/svm/plot_oneclass.html#sphx-glr-auto-examples-svm-plot-oneclass-py">One-class SVM with non-linear kernel (RBF)</a></li>
<li><a href="../auto_examples/applications/plot_species_distribution_modeling.html#sphx-glr-auto-examples-applications-plot-species-distribution-modeling-py">Species distribution modeling</a></li>
</ul>
<h2 id="144-复杂度">1.4.4. 复杂度</h2>
<p>支持向量机是个强大的工具，不过它的计算和存储空间要求也会随着要训练向量的数目增加而快速增加。 SVM的核心是一个二次规划问题(Quadratic Programming, QP)，是将支持向量和训练数据的其余部分分离开来。 在实践中(数据集相关)，会根据 <a href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/">libsvm</a> 的缓存有多效，在 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/25a334612cb5a1736ebcc7eec00c7b29.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/25a334612cb5a1736ebcc7eec00c7b29.jpg" alt="O(n_{features} \times n_{samples}^2)"></a> 和 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d3d363339c8708bf2058b128facd0aea.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d3d363339c8708bf2058b128facd0aea.jpg" alt="O(n_{features} \times n_{samples}^3)"></a> 之间基于 <a href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/">libsvm</a> 的缩放操作才会调用这个 QP 解析器。 如果数据是非常稀疏，那 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/07921ae49a32570fd5559004f1cca103.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/07921ae49a32570fd5559004f1cca103.jpg" alt="n_{features}"></a> 就用样本向量中非零特征的平均数量去替换。</p>
<p>另外请注意，在线性情况下，由 <a href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/">liblinear</a> 操作的 <a href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code>LinearSVC</code></a> 算法要比由它的 <a href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/">libsvm</a> 对应的 <a href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code>SVC</code></a> 更为高效，并且它几乎可以线性缩放到数百万样本或者特征。</p>
<h2 id="145-使用诀窍">1.4.5. 使用诀窍</h2>
<blockquote>
<ul>
<li> <p><strong>避免数据复制</strong>: 对于 <a href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code>SVC</code></a>， <a href="generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><code>SVR</code></a>， <a href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><code>NuSVC</code></a> 和 <a href="generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR" title="sklearn.svm.NuSVR"><code>NuSVR</code></a>， 如果数据是通过某些方法而不是用 C 有序的连续双精度，那它先会调用底层的 C 命令再复制。 您可以通过检查它的 <code>flags</code> 属性，来确定给定的 numpy 数组是不是 C 连续的。</p> <p>对于 <a href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code>LinearSVC</code></a> (和 <a href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code>LogisticRegression</code></a>) 的任何输入，都会以 numpy 数组形式，被复制和转换为 用 liblinear 内部稀疏数据去表达（双精度浮点型 float 和非零部分的 int32 索引）。 如果您想要一个适合大规模的线性分类器，又不打算复制一个密集的 C-contiguous 双精度 numpy 数组作为输入， 那我们建议您去使用 <a href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code>SGDClassifier</code></a> 类作为替代。目标函数可以配置为和 <a href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code>LinearSVC</code></a> 模型差不多相同的。</p> </li>
<li> <p><strong>内核的缓存大小</strong>: 在大规模问题上，对于 <a href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code>SVC</code></a>, <a href="generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><code>SVR</code></a>, <code>nuSVC</code> 和 <a href="generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR" title="sklearn.svm.NuSVR"><code>NuSVR</code></a>, 内核缓存的大小会特别影响到运行时间。如果您有足够可用的 RAM，不妨把它的 <code>缓存大小</code> 设得比默认的 200(MB) 要高，例如为 500(MB) 或者 1000(MB)。</p> </li>
<li> <p><strong>惩罚系数C的设置</strong>:在合理的情况下， <code>C</code> 的默认选择为 <code>1</code> 。如果您有很多混杂的观察数据， 您应该要去调小它。 <code>C</code> 越小，就能更好地去正规化估计。</p> </li>
<li> <p>支持向量机算法本身不是用来扩大不变性，所以 <strong>我们强烈建议您去扩大数据量</strong>. 举个例子，对于输入向量 X， 规整它的每个数值范围为 [0, 1] 或 [-1, +1] ，或者标准化它的为均值为0方差为1的数据分布。请注意， 相同的缩放标准必须要应用到所有的测试向量，从而获得有意义的结果。 请参考章节 <a href="preprocessing.html#preprocessing">预处理数据</a> ，那里会提供到更多关于缩放和规整。</p> </li>
<li> <p>在 <a href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><code>NuSVC</code></a>/<a href="generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><code>OneClassSVM</code></a>/<a href="generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR" title="sklearn.svm.NuSVR"><code>NuSVR</code></a> 内的参数 <code>nu</code> ， 近似是训练误差和支持向量的比值。</p> </li>
<li> <p>在 <a href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code>SVC</code></a>, ，如果分类器的数据不均衡（就是说，很多正例很少负例），设置 <code>class_weight='balanced'</code> 与/或尝试不同的惩罚系数 <code>C</code> 。</p> </li>
<li> <p>在拟合模型时，底层 <a href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code>LinearSVC</code></a> 操作使用了随机数生成器去选择特征。 所以不要感到意外，对于相同的数据输入，也会略有不同的输出结果。如果这个发生了， 尝试用更小的 tol 参数。</p> </li>
<li> <p>使用由 <code>LinearSVC(loss='l2', penalty='l1', dual=False)</code> 提供的 L1 惩罚去产生稀疏解，也就是说，特征权重的子集不同于零，这样做有助于决策函数。 随着增加 <code>C</code> 会产生一个更复杂的模型（要做更多的特征选择）。可以使用 <a href="generated/sklearn.svm.l1_min_c.html#sklearn.svm.l1_min_c" title="sklearn.svm.l1_min_c"><code>l1_min_c</code></a> 去计算 <code>C</code> 的数值，去产生一个”null” 模型（所有的权重等于零）。</p> </li>
</ul>
</blockquote>
<h2 id="146-核函数">1.4.6. 核函数</h2>
<p><em>核函数</em> 可以是以下任何形式：:</p>
<blockquote>
<ul>
<li>线性: <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/587f27ca8cf947779c1929d65c697e0c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/587f27ca8cf947779c1929d65c697e0c.jpg" alt="\langle x, x'\rangle"></a>.</li>
<li>多项式: <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/58d86a5573e0796f320435a8ce8346ea.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/58d86a5573e0796f320435a8ce8346ea.jpg" alt="(\gamma \langle x, x'\rangle + r)^d"></a>. <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/adf83056bc2bd05628e24c40cb728b3d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/adf83056bc2bd05628e24c40cb728b3d.jpg" alt="d"></a> 是关键词 <code>degree</code>, <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/451ef7ed1a14a6cdc38324c8a5c7c683.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/451ef7ed1a14a6cdc38324c8a5c7c683.jpg" alt="r"></a> 指定 <code>coef0</code>。</li>
<li>rbf: <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ba90d4193a98b9023d3d8526a20fe1ac.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ba90d4193a98b9023d3d8526a20fe1ac.jpg" alt="\exp(-\gamma \|x-x'\|^2)"></a>. <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6552bde3d3999c1a9728016416932af7.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6552bde3d3999c1a9728016416932af7.jpg" alt="\gamma"></a> 是关键词 <code>gamma</code>, 必须大于 0。</li>
<li>sigmoid (<a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ed407df5e0211da2859805a96e271751.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ed407df5e0211da2859805a96e271751.jpg" alt="\tanh(\gamma \langle x,x'\rangle + r)"></a>), 其中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/451ef7ed1a14a6cdc38324c8a5c7c683.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/451ef7ed1a14a6cdc38324c8a5c7c683.jpg" alt="r"></a> 指定 <code>coef0</code>。</li>
</ul>
</blockquote>
<p>初始化时，不同内核由不同的函数名调用:</p>
<pre><code class="language-py">&gt;&gt;&gt; linear_svc = svm.SVC(kernel='linear')
&gt;&gt;&gt; linear_svc.kernel
'linear'
&gt;&gt;&gt; rbf_svc = svm.SVC(kernel='rbf')
&gt;&gt;&gt; rbf_svc.kernel
'rbf'

</code></pre>
<h3 id="1461-自定义核">1.4.6.1. 自定义核</h3>
<p>您可以自定义自己的核，通过使用python函数作为内核或者通过预计算 Gram 矩阵。</p>
<p>自定义内核的分类器和别的分类器一样，除了下面这几点:</p>
<blockquote>
<ul>
<li>空间 <code>support_vectors_</code> 现在不是空的, 只有支持向量的索引被存储在 <code>support_</code></li>
<li>请把 <code>fit()</code> 模型中的第一个参数的引用（不是副本）存储为将来的引用。 如果在 <code>fit()</code> 和 <code>predict()</code> 之间有数组发生改变，您将会碰到意料外的结果。</li>
</ul>
</blockquote>
<h4 id="14611-使用-python-函数作为内核">1.4.6.1.1. 使用 python 函数作为内核</h4>
<p>在构造时，您同样可以通过一个函数传递到关键词 <code>kernel</code> ，来使用您自己定义的内核。</p>
<p>您的内核必须要以两个矩阵作为参数，大小分别是 <code>(n_samples_1, n_features)</code>, <code>(n_samples_2, n_features)</code> 和返回一个内核矩阵，shape 是 <code>(n_samples_1, n_samples_2)</code>.</p>
<p>以下代码定义一个线性核，和构造一个使用该内核的分类器例子:</p>
<pre><code class="language-py">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn import svm
&gt;&gt;&gt; def my_kernel(X, Y):
...     return np.dot(X, Y.T)
...
&gt;&gt;&gt; clf = svm.SVC(kernel=my_kernel)

</code></pre>
<p>例子:</p>
<ul>
<li><a href="../auto_examples/svm/plot_custom_kernel.html#sphx-glr-auto-examples-svm-plot-custom-kernel-py">SVM with custom kernel</a>.</li>
</ul>
<h4 id="14612-使用-gram-矩阵">1.4.6.1.2. 使用 Gram 矩阵</h4>
<p>在适应算法中，设置 <code>kernel='precomputed'</code> 和把 X 替换为 Gram 矩阵。 此时，必须要提供在 <em>所有</em> 训练矢量和测试矢量中的内核值。</p>
<pre><code class="language-py">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn import svm
&gt;&gt;&gt; X = np.array([[0, 0], [1, 1]])
&gt;&gt;&gt; y = [0, 1]
&gt;&gt;&gt; clf = svm.SVC(kernel='precomputed')
&gt;&gt;&gt; # 线性内核计算
&gt;&gt;&gt; gram = np.dot(X, X.T)
&gt;&gt;&gt; clf.fit(gram, y) 
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
 decision_function_shape='ovr', degree=3, gamma='auto',
 kernel='precomputed', max_iter=-1, probability=False,
 random_state=None, shrinking=True, tol=0.001, verbose=False)
&gt;&gt;&gt; # 预测训练样本
&gt;&gt;&gt; clf.predict(gram)
array([0, 1])

</code></pre>
<h4 id="14613-rbf-内核参数">1.4.6.1.3. RBF 内核参数</h4>
<p>当用 <em>径向基</em> (RBF) 内核去训练 SVM，有两个参数必须要去考虑： <code>C</code> 惩罚系数和 <code>gamma</code> 。参数 <code>C</code> ， 通用在所有 SVM 内核，与决策表面的简单性相抗衡，可以对训练样本的误分类进行有价转换。 较小的 <code>C</code> 会使决策表面更平滑，同时较高的 <code>C</code> 旨在正确地分类所有训练样本。 <code>Gamma</code> 定义了单一 训练样本能起到多大的影响。较大的 <code>gamma</code> 会更让其他样本受到影响。</p>
<p>选择合适的 <code>C</code> 和 <code>gamma</code> ，对SVM的性能起到很关键的作用。建议一点是 使用 &nbsp;<a href="generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV" title="sklearn.model_selection.GridSearchCV"><code>sklearn.model_selection.GridSearchCV</code></a> 与 <code>C</code> 和 <code>gamma</code> 相隔 成倍差距从而选择到好的数值。</p>
<p>例子:</p>
<ul>
<li><a href="../auto_examples/svm/plot_rbf_parameters.html#sphx-glr-auto-examples-svm-plot-rbf-parameters-py">RBF SVM parameters</a></li>
</ul>
<h2 id="147-数学公式">1.4.7. 数学公式</h2>
<p>支持向量机在高维度或无穷维度空间中，构建一个超平面或者一系列的超平面，可以用于分类、回归或者别的任务。 直观地看，借助超平面去实现一个好的分割， 能在任意类别中使最为接近的训练数据点具有最大的间隔距离（即所 谓的函数余量），这样做是因为通常更大的余量能有更低的分类器泛化误差。</p>
<p><a href="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_separating_hyperplane_0011.png"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7d9b5103fb50fe740fbc421247d2a5c7.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7d9b5103fb50fe740fbc421247d2a5c7.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_separating_hyperplane_0011.png"></a></a></p>
<h3 id="1471-svc">1.4.7.1. SVC</h3>
<p>在两类中，给定训练向量 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2bd24ed32bcf24db79058c3cc81f5331.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2bd24ed32bcf24db79058c3cc81f5331.jpg" alt="x_i \in \mathbb{R}^p"></a>, i=1,…, n, 和一个向量 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/73658f99647e50786817b44416d09df1.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/73658f99647e50786817b44416d09df1.jpg" alt="y \in \{1, -1\}^n"></a>, SVC能解决 如下主要问题:</p>
<pre><code class="language-py">
![\min_ {w, b, \zeta} \frac{1}{2} w^T w + C \sum_{i=1}^{n} \zeta_i

\textrm {subject to } &amp; y_i (w^T \phi (x_i) + b) \geq 1 - \zeta_i,\\
&amp; \zeta_i \geq 0, i=1, ..., n](img/ee78ab463ea8dc72594f270f5193a7a6.jpg)

</code></pre>
<p>它的对偶是</p>
<pre><code class="language-py">
![\min_{\alpha} \frac{1}{2} \alpha^T Q \alpha - e^T \alpha

\textrm {subject to } &amp; y^T \alpha = 0\\
&amp; 0 \leq \alpha_i \leq C, i=1, ..., n](img/61b05c3bf030b831f23f257ca8182f51.jpg)

</code></pre>
<p>其中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1f9000a4bf057edcb9b87d7a4abb8e8d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1f9000a4bf057edcb9b87d7a4abb8e8d.jpg" alt="e"></a> 是所有的向量， <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2edeef5a5007d4bd8b4f43fe2670cf85.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2edeef5a5007d4bd8b4f43fe2670cf85.jpg" alt="C > 0"></a> 是上界，<a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/87dfb2676632ee8a92713f4861ccc84e.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/87dfb2676632ee8a92713f4861ccc84e.jpg" alt="Q"></a> 是一个 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c87d9110f3d32ffa5fa08671e4af11fb.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c87d9110f3d32ffa5fa08671e4af11fb.jpg" alt="n"></a> 由 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c87d9110f3d32ffa5fa08671e4af11fb.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c87d9110f3d32ffa5fa08671e4af11fb.jpg" alt="n"></a> 个半正定矩阵， 而 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/36c2dba9ae7680cd09eff62c73e37963.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/36c2dba9ae7680cd09eff62c73e37963.jpg" alt="Q_{ij} \equiv y_i y_j K(x_i, x_j)"></a> ，其中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/51fa9007646861e0569f8f66731c64e7.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/51fa9007646861e0569f8f66731c64e7.jpg" alt="K(x_i, x_j) = \phi (x_i)^T \phi (x_j)"></a> 是内核。所以训练向量是通过函数 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ff5e98366afa13070d3b410c55a80db1.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ff5e98366afa13070d3b410c55a80db1.jpg" alt="\phi"></a>，间接反映到一个更高维度的（无穷的）空间。</p>
<p>决策函数是:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/53f6b3d47807f65fe25b4fa232cd7abc.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/53f6b3d47807f65fe25b4fa232cd7abc.jpg" alt="\operatorname{sgn}(\sum_{i=1}^n y_i \alpha_i K(x_i, x) + \rho)"></a></p>
<p>注意:</p>
<p>虽然这些SVM模型是从 <a href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/">libsvm</a> 和 <a href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/">liblinear</a> 中派生出来，使用了 <code>C</code> 作为调整参数，但是大多数的 攻击使用了 <code>alpha</code>。两个模型的正则化量之间的精确等价，取决于模型优化的准确目标函数。举 个例子，当使用的估计器是 <code>sklearn.linear_model.Ridge</code> 做回归时，他们之间的相关性是 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c74889dd434ec9a5f4e1b57a549263e7.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c74889dd434ec9a5f4e1b57a549263e7.jpg" alt="C = \frac{1}{alpha}"></a>。</p>
<p>这些参数能通过成员 <code>dual_coef_</code>、 <code>support_vectors_</code> 、 <code>intercept_</code> 去访问，这些成员分别控制了输出 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/bf9baf4863bf6d025348b7d91c888066.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/bf9baf4863bf6d025348b7d91c888066.jpg" alt="y_i \alpha_i"></a>、支持向量和无关项 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b91e4507d9fd7068b02f689d697f8714.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b91e4507d9fd7068b02f689d697f8714.jpg" alt="\rho"></a> ：</p>
<p>参考文献:</p>
<ul>
<li><a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.7215">“Automatic Capacity Tuning of Very Large VC-dimension Classifiers”</a>, I. Guyon, B. Boser, V. Vapnik - Advances in neural information processing 1993.</li>
<li><a href="http://link.springer.com/article/10.1007%2FBF00994018">“Support-vector networks”</a>, C. Cortes, V. Vapnik - Machine Learning, 20, 273-297 (1995).</li>
</ul>
<h3 id="1472-nusvc">1.4.7.2. NuSVC</h3>
<p>我们引入一个新的参数 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f996477bc9806499e6b6a1ea4d9ae8eb.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f996477bc9806499e6b6a1ea4d9ae8eb.jpg" alt="\nu"></a> 来控制支持向量的数量和训练误差。参数 ![\nu \in (0, 1]](img/50eda5a92ebcfda1468e1508393b748a.jpg) 是训练误差分数的上限和支持向量分数的下限。</p>
<p>可以看出， <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f996477bc9806499e6b6a1ea4d9ae8eb.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f996477bc9806499e6b6a1ea4d9ae8eb.jpg" alt="\nu"></a>-SVC 公式是 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4b6d782a67ac392e97215c46b7590bf7.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4b6d782a67ac392e97215c46b7590bf7.jpg" alt="C"></a>-SVC 的再参数化，所以数学上是等效的。</p>
<h3 id="1473-svr">1.4.7.3. SVR</h3>
<p>给定训练向量 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2bd24ed32bcf24db79058c3cc81f5331.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2bd24ed32bcf24db79058c3cc81f5331.jpg" alt="x_i \in \mathbb{R}^p"></a>, i=1,…, n，向量 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6653de9b4dea7e5e9a897b5f34e7a4f0.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6653de9b4dea7e5e9a897b5f34e7a4f0.jpg" alt="y \in \mathbb{R}^n"></a> <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/bf9fb1354c2e0ea50d37e5cad7866314.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/bf9fb1354c2e0ea50d37e5cad7866314.jpg" alt="\varepsilon"></a>-SVR 能解决以下的主要问题：</p>
<pre><code class="language-py">
![\min_ {w, b, \zeta, \zeta^*} \frac{1}{2} w^T w + C \sum_{i=1}^{n} (\zeta_i + \zeta_i^*)

\textrm {subject to } &amp; y_i - w^T \phi (x_i) - b \leq \varepsilon + \zeta_i,\\
                      &amp; w^T \phi (x_i) + b - y_i \leq \varepsilon + \zeta_i^*,\\
                      &amp; \zeta_i, \zeta_i^* \geq 0, i=1, ..., n](img/23dac8b2be31a1cbe914b59ff2670dbf.jpg)

</code></pre>
<p>它的对偶是</p>
<pre><code class="language-py">
![\min_{\alpha, \alpha^*} \frac{1}{2} (\alpha - \alpha^*)^T Q (\alpha - \alpha^*) + \varepsilon e^T (\alpha + \alpha^*) - y^T (\alpha - \alpha^*)

\textrm {subject to } &amp; e^T (\alpha - \alpha^*) = 0\\
&amp; 0 \leq \alpha_i, \alpha_i^* \leq C, i=1, ..., n](img/e996da94de858e5248f145e01733ed9d.jpg)

</code></pre>
<p>其中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1f9000a4bf057edcb9b87d7a4abb8e8d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1f9000a4bf057edcb9b87d7a4abb8e8d.jpg" alt="e"></a> 是所有的向量， <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2edeef5a5007d4bd8b4f43fe2670cf85.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2edeef5a5007d4bd8b4f43fe2670cf85.jpg" alt="C > 0"></a> 是上界，<a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/87dfb2676632ee8a92713f4861ccc84e.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/87dfb2676632ee8a92713f4861ccc84e.jpg" alt="Q"></a> 是一个 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c87d9110f3d32ffa5fa08671e4af11fb.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c87d9110f3d32ffa5fa08671e4af11fb.jpg" alt="n"></a> 由 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c87d9110f3d32ffa5fa08671e4af11fb.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c87d9110f3d32ffa5fa08671e4af11fb.jpg" alt="n"></a> 个半正定矩阵， 而 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b019b19dda07f07208f1bd2576ebad30.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b019b19dda07f07208f1bd2576ebad30.jpg" alt="Q_{ij} \equiv K(x_i, x_j) = \phi (x_i)^T \phi (x_j)"></a> 是内核。 所以训练向量是通过函数 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ff5e98366afa13070d3b410c55a80db1.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ff5e98366afa13070d3b410c55a80db1.jpg" alt="\phi"></a>，间接反映到一个更高维度的（无穷的）空间。</p>
<p>决策函数是:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/db24e5f707f974690c4334cfa218bbee.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/db24e5f707f974690c4334cfa218bbee.jpg" alt="\sum_{i=1}n (\alpha_i - \alpha_i*) K(x_i, x) + \rho"></a></p>
<p>这些参数能通过成员 <code>dual_coef_</code>、 <code>support_vectors_</code> 、 <code>intercept_</code> 去访问，这些 成员分别控制了不同的 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1a78828504944887ab23097011f807d5.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1a78828504944887ab23097011f807d5.jpg" alt="\alpha_i - \alpha_i^*"></a>、支持向量和无关项 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b91e4507d9fd7068b02f689d697f8714.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b91e4507d9fd7068b02f689d697f8714.jpg" alt="\rho"></a>：</p>
<p>参考文献:</p>
<ul>
<li><a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.114.4288">“A Tutorial on Support Vector Regression”</a>, Alex J. Smola, Bernhard Schölkopf - Statistics and Computing archive Volume 14 Issue 3, August 2004, p. 199-222.</li>
</ul>
<h2 id="148-实现细节">1.4.8. 实现细节</h2>
<p>在底层里，我们使用 <a href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/">libsvm</a> 和 <a href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/">liblinear</a> 去处理所有的计算。这些库都使用了 C 和 Cython 去包装。</p>
<p>参考文献:</p>
<p>有关实现的描述和使用算法的细节，请参考</p>
<blockquote>
<ul>
<li><a href="http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf">LIBSVM: A Library for Support Vector Machines</a>.</li>
<li><a href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/">LIBLINEAR – A Library for Large Linear Classification</a>.</li>
</ul>
</blockquote>
</div>
<hr class="uk-article-divider">
<div class="uk-block uk-block-muted uk-padding-top-remove uk-padding-bottom-remove uk-margin-large-top  book-recommend-wrap">
<div class="uk-margin-top uk-margin-bottom uk-margin-left uk-margin-right">
<div class="uk-margin uk-text-muted "><i class="uk-icon-outdent uk-icon-justify uk-margin-small-right"></i>书籍推荐</div>
<div class="books">
<ul class="uk-book-list">
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/166/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/166/index.html">What the f*ck Python中文版</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/95.html">leisurelicht</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">70页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月26日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 7300个">7300</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/21/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/21/index.html">笨办法学 Python 3</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/16.html">yammgao</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">63页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年5月3日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 12个">12</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/130/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/130/index.html">进击的Python</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/68.html">HuberTRoy</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">23页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年7月8日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 169个">169</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/115/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/code_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/115/index.html">ANTLR 4简明教程</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/62.html">tzivanmoe</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="code">code</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">19页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年7月1日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 0个">0</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/16/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/go_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/16/index.html">Go语言web编程</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/9.html">astaxie</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="go">go</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">97页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年5月3日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 23774个">23774</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/197/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/ubuntu_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/197/index.html">手把手教你，搭建内网穿透服务</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/112.html">frank-lam</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="ubuntu">ubuntu</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">45页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2021年10月24日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 189个">189</span>
</div>
</div>
</div>
</li>
<hr>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
<nav class="tm-navbar uk-navbar uk-navbar-attached reader-nav">
<div class="uk-float-left uk-margin-small-top">
<a href="javascript:;" title="目录菜单" class="show-menu  uk-icon-hover  uk-icon-align-justify uk-margin-right"></a>
<div data-uk-dropdown="{mode:'click',pos:'bottom-left'}" class="font-setting-wrap">
<a class="uk-icon-hover uk-icon-font uk-margin-right" aria-label="字体设置" href="javascript:;"></a>
<div class="uk-dropdown dropdown-menu">
<div class="dropdown-caret"><span class="caret-outer"></span><span class="caret-inner"></span></div>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-reduce">小字</button>
<button class="uk-button-link button size-2 font-enlarge">大字</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-1 ">宋体</button>
<button class="uk-button-link button size-2 font-2 ">黑体</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-3 color-theme-sun "><i class="uk-icon-sun-o"></i>白天</button>
<button class="uk-button-link button size-3 color-theme-eye "><i class="uk-icon-eye"></i>护眼</button>
<button class="uk-button-link button size-3 color-theme-moon "><i class="uk-icon-moon-o"></i>夜晚</button></div>
</div>
</div>
<a class="logo uk-margin-right" href="../../../" title="返回首页"><img class="" src="../../../static/components/images/icon_32.png" /></a>
</div>
<div class="uk-navbar-flip  uk-hidden-small">
<div id="share-box"></div>
</div>
</nav>
<div id="menu-id" class="uk-offcanvas reader-offcanvas">
<div class="uk-offcanvas-bar">
<ul class="book-menu-bar uk-nav uk-nav-offcanvas" data-uk-nav>
<li>
<a href="../../../book/170/index.html" data-book-page-rel-url="index.html" data-book-page-id="0" title="封面">封面</a>
</li>
<li>
<a class="pjax" href="../../../book/170/readme.html" data-book-page-rel-url="readme.html" data-book-page-id="0" title="简介">简介</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/79.html" title="安装 scikit-learn" data-book-page-rel-url="docs/79.html" data-book-page-id="11491">安装 scikit-learn</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/1.html" title="1. 监督学习" data-book-page-rel-url="docs/1.html" data-book-page-id="11492">1. 监督学习</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/2.html" title="1.1. 广义线性模型" data-book-page-rel-url="docs/2.html" data-book-page-id="11493">1.1. 广义线性模型</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/3.html" title="1.2. 线性和二次判别分析" data-book-page-rel-url="docs/3.html" data-book-page-id="11494">1.2. 线性和二次判别分析</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/4.html" title="1.3. 内核岭回归" data-book-page-rel-url="docs/4.html" data-book-page-id="11495">1.3. 内核岭回归</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/5.html" title="1.4. 支持向量机" data-book-page-rel-url="docs/5.html" data-book-page-id="11496">1.4. 支持向量机</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/6.html" title="1.5. 随机梯度下降" data-book-page-rel-url="docs/6.html" data-book-page-id="11497">1.5. 随机梯度下降</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/7.html" title="1.6. 最近邻" data-book-page-rel-url="docs/7.html" data-book-page-id="11498">1.6. 最近邻</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/8.html" title="1.7. 高斯过程" data-book-page-rel-url="docs/8.html" data-book-page-id="11499">1.7. 高斯过程</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/9.html" title="1.8. 交叉分解" data-book-page-rel-url="docs/9.html" data-book-page-id="11500">1.8. 交叉分解</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/10.html" title="1.9. 朴素贝叶斯" data-book-page-rel-url="docs/10.html" data-book-page-id="11501">1.9. 朴素贝叶斯</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/11.html" title="1.10. 决策树" data-book-page-rel-url="docs/11.html" data-book-page-id="11502">1.10. 决策树</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/12.html" title="1.11. 集成方法" data-book-page-rel-url="docs/12.html" data-book-page-id="11503">1.11. 集成方法</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/13.html" title="1.12. 多类和多标签算法" data-book-page-rel-url="docs/13.html" data-book-page-id="11504">1.12. 多类和多标签算法</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/14.html" title="1.13. 特征选择" data-book-page-rel-url="docs/14.html" data-book-page-id="11505">1.13. 特征选择</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/15.html" title="1.14. 半监督学习" data-book-page-rel-url="docs/15.html" data-book-page-id="11506">1.14. 半监督学习</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/16.html" title="1.15. 等式回归" data-book-page-rel-url="docs/16.html" data-book-page-id="11507">1.15. 等式回归</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/17.html" title="1.16. 概率校准" data-book-page-rel-url="docs/17.html" data-book-page-id="11508">1.16. 概率校准</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/18.html" title="1.17. 神经网络模型（有监督）" data-book-page-rel-url="docs/18.html" data-book-page-id="11509">1.17. 神经网络模型（有监督）</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/19.html" title="2. 无监督学习" data-book-page-rel-url="docs/19.html" data-book-page-id="11510">2. 无监督学习</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/20.html" title="2.1. 高斯混合模型" data-book-page-rel-url="docs/20.html" data-book-page-id="11511">2.1. 高斯混合模型</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/21.html" title="2.2. 流形学习" data-book-page-rel-url="docs/21.html" data-book-page-id="11512">2.2. 流形学习</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/22.html" title="2.3. 聚类" data-book-page-rel-url="docs/22.html" data-book-page-id="11513">2.3. 聚类</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/23.html" title="2.4. 双聚类" data-book-page-rel-url="docs/23.html" data-book-page-id="11514">2.4. 双聚类</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/24.html" title="2.5. 分解成分中的信号（矩阵分解问题）" data-book-page-rel-url="docs/24.html" data-book-page-id="11515">2.5. 分解成分中的信号（矩阵分解问题）</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/25.html" title="2.6. 协方差估计" data-book-page-rel-url="docs/25.html" data-book-page-id="11516">2.6. 协方差估计</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/26.html" title="2.7. 经验协方差" data-book-page-rel-url="docs/26.html" data-book-page-id="11517">2.7. 经验协方差</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/27.html" title="2.8. 收敛协方差" data-book-page-rel-url="docs/27.html" data-book-page-id="11518">2.8. 收敛协方差</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/28.html" title="2.9. 稀疏逆协方差" data-book-page-rel-url="docs/28.html" data-book-page-id="11519">2.9. 稀疏逆协方差</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/29.html" title="2.10. Robust 协方差估计" data-book-page-rel-url="docs/29.html" data-book-page-id="11520">2.10. Robust 协方差估计</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/30.html" title="2.11. 新奇和异常值检测" data-book-page-rel-url="docs/30.html" data-book-page-id="11521">2.11. 新奇和异常值检测</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/31.html" title="2.12. 密度估计" data-book-page-rel-url="docs/31.html" data-book-page-id="11522">2.12. 密度估计</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/32.html" title="2.13. 神经网络模型（无监督）" data-book-page-rel-url="docs/32.html" data-book-page-id="11523">2.13. 神经网络模型（无监督）</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/33.html" title="3. 模型选择和评估" data-book-page-rel-url="docs/33.html" data-book-page-id="11524">3. 模型选择和评估</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/34.html" title="3.1. 交叉验证：评估估算器的表现" data-book-page-rel-url="docs/34.html" data-book-page-id="11525">3.1. 交叉验证：评估估算器的表现</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/35.html" title="3.2. 调整估计器的超参数" data-book-page-rel-url="docs/35.html" data-book-page-id="11526">3.2. 调整估计器的超参数</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/53.html" title="3.3. 模型评估: 量化预测的质量" data-book-page-rel-url="docs/53.html" data-book-page-id="11527">3.3. 模型评估: 量化预测的质量</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/54.html" title="3.4. 模型持久化" data-book-page-rel-url="docs/54.html" data-book-page-id="11528">3.4. 模型持久化</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/55.html" title="3.5. 验证曲线: 绘制分数以评估模型" data-book-page-rel-url="docs/55.html" data-book-page-id="11529">3.5. 验证曲线: 绘制分数以评估模型</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/56.html" title="4. 数据集转换" data-book-page-rel-url="docs/56.html" data-book-page-id="11530">4. 数据集转换</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/57.html" title="4.1. Pipeline（管道）和 FeatureUnion（特征联合）: 合并的评估器" data-book-page-rel-url="docs/57.html" data-book-page-id="11531">4.1. Pipeline（管道）和 FeatureUnion（特征联合）: 合并的评估器</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/58.html" title="4.2. 特征提取" data-book-page-rel-url="docs/58.html" data-book-page-id="11532">4.2. 特征提取</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/59.html" title="4.3. 预处理数据" data-book-page-rel-url="docs/59.html" data-book-page-id="11533">4.3. 预处理数据</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/60.html" title="4.4. 无监督降维" data-book-page-rel-url="docs/60.html" data-book-page-id="11534">4.4. 无监督降维</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/61.html" title="4.5. 随机投影" data-book-page-rel-url="docs/61.html" data-book-page-id="11535">4.5. 随机投影</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/62.html" title="4.6. 内核近似" data-book-page-rel-url="docs/62.html" data-book-page-id="11536">4.6. 内核近似</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/63.html" title="4.7. 成对的矩阵, 类别和核函数" data-book-page-rel-url="docs/63.html" data-book-page-id="11537">4.7. 成对的矩阵, 类别和核函数</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/64.html" title="4.8. 预测目标 (`y`.html) 的转换" data-book-page-rel-url="docs/64.html" data-book-page-id="11538">4.8. 预测目标 (`y`.html) 的转换</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/65.html" title="5. 数据集加载工具" data-book-page-rel-url="docs/65.html" data-book-page-id="11539">5. 数据集加载工具</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/66.html" title="6. 大规模计算的策略: 更大量的数据" data-book-page-rel-url="docs/66.html" data-book-page-id="11540">6. 大规模计算的策略: 更大量的数据</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/67.html" title="7. 计算性能" data-book-page-rel-url="docs/67.html" data-book-page-id="11541">7. 计算性能</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/68.html" title="使用 scikit-learn 介绍机器学习" data-book-page-rel-url="docs/68.html" data-book-page-id="11542">使用 scikit-learn 介绍机器学习</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/69.html" title="关于科学数据处理的统计学习教程" data-book-page-rel-url="docs/69.html" data-book-page-id="11543">关于科学数据处理的统计学习教程</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/70.html" title="机器学习: scikit-learn 中的设置以及预估对象" data-book-page-rel-url="docs/70.html" data-book-page-id="11544">机器学习: scikit-learn 中的设置以及预估对象</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/71.html" title="监督学习：从高维观察预测输出变量" data-book-page-rel-url="docs/71.html" data-book-page-id="11545">监督学习：从高维观察预测输出变量</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/72.html" title="模型选择：选择估计量及其参数" data-book-page-rel-url="docs/72.html" data-book-page-id="11546">模型选择：选择估计量及其参数</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/73.html" title="无监督学习: 寻求数据表示" data-book-page-rel-url="docs/73.html" data-book-page-id="11547">无监督学习: 寻求数据表示</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/74.html" title="把它们放在一起" data-book-page-rel-url="docs/74.html" data-book-page-id="11548">把它们放在一起</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/75.html" title="寻求帮助" data-book-page-rel-url="docs/75.html" data-book-page-id="11549">寻求帮助</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/76.html" title="处理文本数据" data-book-page-rel-url="docs/76.html" data-book-page-id="11550">处理文本数据</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/77.html" title="选择正确的评估器(estimator.html)" data-book-page-rel-url="docs/77.html" data-book-page-id="11551">选择正确的评估器(estimator.html)</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/78.html" title="外部资源，视频和谈话" data-book-page-rel-url="docs/78.html" data-book-page-id="11552">外部资源，视频和谈话</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/80.html" title="常见问题" data-book-page-rel-url="docs/80.html" data-book-page-id="11553">常见问题</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/81.html" title="时光轴" data-book-page-rel-url="docs/81.html" data-book-page-id="11554">时光轴</a>
</li>
</ul>
</div>
</div>
<script src="https://cdn.staticfile.net/jquery/1.12.4/jquery.min.js"></script>
<script type="text/javascript" src="../../../static/components/uikit-2.27.5/js/uikit.reader.js"></script>
<script type="text/javascript" src="../../../static/components/social-share/social-share.min.js"></script>
<script>(function(){var bp =document.createElement('script');var curProtocol =window.location.protocol.split(':')[0];if (curProtocol ==='https') {bp.src ='https://zz.bdstatic.com/linksubmit/push.js';}
else {bp.src ='http://push.zhanzhang.baidu.com/push.js';}
var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(bp,s);})();</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
<script src="https://cdn.staticfile.net/highlight.js/9.12.0/highlight.min.js"></script>
<script src="https://cdn.staticfile.net/jquery.pjax/2.0.1/jquery.pjax.min.js"></script>
<script src="https://cdn.staticfile.net/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="https://cdn.staticfile.net/uikit/2.27.5/js/components/lightbox.min.js"></script>
<link rel="dns-prefetch" href="../../..//cdn.mathjax.org" />
<script type="text/x-mathjax-config">
 function initMathJax() {
    var mathId = $("book-content-section")[0];
    MathJax.Hub.Config({
        tex2jax: {skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code','a']},
        showProcessingMessages: false,
        messageStyle: "none"
    });
    MathJax.Hub.Queue(["Typeset",MathJax.Hub,mathId]);
 };
initMathJax();
</script>
<script src='https://cdn.staticfile.net/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML' async></script>
<style>
	.MathJax_Display{display:inline!important;}
</style>
<script type="text/javascript" src="../../../static/components/js/reader.js"></script>
<script type="text/javascript">var bookId =170;var bookPageId =11496;var bookPageRelUrl ='docs/5.html';</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
</body>
</html>