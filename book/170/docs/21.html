
<!DOCTYPE html>
<html lang="zh-CN">
<head>
<title>2.2. 流形学习-scikit-learn (sklearn) 官方文档中文版</title>
<meta content='2.2. 流形学习,scikit-learn (sklearn) 官方文档中文版' name='keywords'>
<meta content='2.2. 流形学习,scikit-learn (sklearn) 官方文档中文版' name='description'>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta http-equiv="Content-Language" content="zh-CN" />
<meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1, maximum-scale=1, user-scalable=no"../../../>
<meta name="applicable-device" content="pc,mobile">
<link rel="shortcut icon" href="../../../favicon.ico" type="image/x-icon" />
<meta name="renderer" content="webkit">
<link rel="stylesheet" href="../../../static/components/uikit-2.27.5/css/uikit.custom.css">
<link rel="stylesheet" href="../../../static/components/social-share/social-share.min.css">
<link rel="stylesheet" href="../../../static/components/highlight/styles/custom.css">
<link rel="stylesheet" href="../../../static/components/css/base.css">
<link rel="stylesheet" href="../../../static/components/css/reader.css">
<link rel="stylesheet" href="../../../static/components/css/markdown.css">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5313208362165053" crossorigin="anonymous"></script>
</head>
<body>
<div class=" book-main-wrap uk-container uk-container-center uk-margin-top ">
<div class="uk-grid">
<div class="uk-width-1-1 reader-wrap ">
<div class=" bottom-nav uk-clearfix ">
<div class="uk-align-left ">
<a href="../../../book/170/docs/20.html">
<i class="nav-icon-left uk-icon-small  uk-icon-caret-left"></i>
<span class="">2.1. 高斯混合模型</span>
</a>
</div>
<div class="uk-align-right ">
<a href="../../../book/170/docs/22.html">
<span class="">2.3. 聚类</span>
<i class="nav-icon-right uk-icon-small  uk-icon-caret-right"></i>
</a>
</div>
</div>
<div class="uk-text-center">
<h2 class="book-page-title uk-container-center">
<a href="../../../book/170/index.html">scikit-learn (sklearn) 官方文档中文版</a>
<a target="_blank" rel="nofollow" href="https://github.com/apachecn/scikit-learn-doc-zh" class="uk-icon-button uk-icon-github" title="github项目地址"></a>
</h2>
</div>
<script type="text/javascript" src="../../../static/components/js/app_intro.js"></script>
<ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-5313208362165053" data-ad-slot="1328047120"></ins>
<script>(adsbygoogle =window.adsbygoogle ||[]).push({});</script>
<hr class="uk-article-divider">
<div class="book-content-section  md-content-section  uk-margin-bottom">
<h1 id="22-流形学习">2.2. 流形学习</h1>
<p>校验者: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/XuJianzhi">@XuJianzhi</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/RyanZhiNie">@RyanZhiNie</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@羊三</a> 翻译者: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/XuJianzhi">@XuJianzhi</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@羊三</a> Look for the bare necessitiesThe simple bare necessitiesForget about your worries and your strifeI mean the bare necessitiesOld Mother Nature’s recipesThat bring the bare necessities of life– Baloo的歌 [奇幻森林]<a href="../auto_examples/manifold/plot_compare_methods.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/fe9e5bb155154914f761d6497915e9cb.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/fe9e5bb155154914f761d6497915e9cb.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_compare_methods_0011.png"></a></a></p>
<p>流形学习是一种非线性降维方法。其算法基于的思想是：许多数据集的维度过高只是由人为导致的。</p>
<h2 id="221-介绍">2.2.1. 介绍</h2>
<p>高维数据集会非常难以可视化。 虽然可以绘制两维或三维的数据来显示数据的固有结构，但与之等效的高维图不太直观。 为了帮助数据集结构的可视化，必须以某种方式降低维度。</p>
<p>通过对数据的随机投影来实现降维是最简单的方法。 虽然这样做能实现数据结构一定程度的可视化，但随机选择投影仍有许多有待改进之处。 在随机投影中，数据中更有趣的结构很可能会丢失。</p>
<p><strong><a href="../auto_examples/manifold/plot_lle_digits.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/35a2693b8dbfe5cf9335dc2659c6ef96.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/35a2693b8dbfe5cf9335dc2659c6ef96.jpg" alt="digits_img"></a></a> <a href="../auto_examples/manifold/plot_lle_digits.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/015fcf78112c08948e66bb51171ae137.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/015fcf78112c08948e66bb51171ae137.jpg" alt="projected_img"></a></a></strong></p>
<p>为了解决这一问题，一些监督和无监督的线性降维框架被设计出来，如主成分分析（PCA），独立成分分析，线性判别分析等。 这些算法定义了明确的规定来选择数据的“有趣的”线性投影。 它们虽然强大，但是会经常错失数据中重要的非线性结构。</p>
<p><strong><a href="../auto_examples/manifold/plot_lle_digits.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/93d2f2876517637396e99e36132252f3.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/93d2f2876517637396e99e36132252f3.jpg" alt="PCA_img"></a></a> <a href="../auto_examples/manifold/plot_lle_digits.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4953c9da8999e3eb76b63a4dd0432896.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4953c9da8999e3eb76b63a4dd0432896.jpg" alt="LDA_img"></a></a></strong></p>
<p>流形学习可以被认为是一种将线性框架（如 PCA ）推广为对数据中非线性结构敏感的尝试。 虽然存在监督变量，但是典型的流形学习问题是无监督的：它从数据本身学习数据的高维结构，而不使用预定的分类。</p>
<p>例子:</p>
<ul>
<li>参见 <a href="../auto_examples/manifold/plot_lle_digits.html#sphx-glr-auto-examples-manifold-plot-lle-digits-py">Manifold learning on handwritten digits: Locally Linear Embedding, Isomap…</a> ,手写数字降维的例子。</li>
<li>参见 <a href="../auto_examples/manifold/plot_compare_methods.html#sphx-glr-auto-examples-manifold-plot-compare-methods-py">Comparison of Manifold Learning methods</a> ,玩具 “S曲线” 数据集降维的例子。</li>
</ul>
<p>以下概述了 scikit-learn 中可用的流形学习实现</p>
<h2 id="222-isomap">2.2.2. Isomap</h2>
<p>流形学习的最早方法之一是 Isomap 算法，等距映射（Isometric Mapping）的缩写。 Isomap 可以被视为多维缩放（Multi-dimensional Scaling：MDS）或核主成分分析（Kernel PCA）的扩展。 Isomap 寻求一个较低维度的嵌入，它保持所有点之间的测量距离。 Isomap 可以通过 <a href="generated/sklearn.manifold.Isomap.html#sklearn.manifold.Isomap" title="sklearn.manifold.Isomap"><code>Isomap</code></a> 对象执行。</p>
<p><a href="../auto_examples/manifold/plot_lle_digits.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/21937e85250a7aaa8aea86e4fbf93452.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/21937e85250a7aaa8aea86e4fbf93452.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_lle_digits_0051.png"></a></a></p>
<h3 id="2221-复杂度">2.2.2.1. 复杂度</h3>
<p>Isomap 算法包括三个阶段:</p>
<ol>
<li><strong>最近邻搜索.</strong> Isomap 使用 <a href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code>sklearn.neighbors.BallTree</code></a> 进行有效的近邻搜索。 对于 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e03066df748abd9273db055cb79f0f01.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e03066df748abd9273db055cb79f0f01.jpg" alt="D"></a> 维中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" alt="N"></a> 个点的 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> 个最近邻，代价约为 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7b0e2ed0273c0a1650cc9f78eabe93c4.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7b0e2ed0273c0a1650cc9f78eabe93c4.jpg" alt="OD \log(k) N \log(N)"></a></li>
<li><strong>最短路径图搜索.</strong> 最有效的已知算法是 Dijkstra 算法或 Floyd-Warshall 算法，其复杂度分别是约 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d459482314974b92f7f44cc36d6eae3e.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d459482314974b92f7f44cc36d6eae3e.jpg" alt="ON^2(k + \log(N))"></a> 和 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1a1bc66f06af187108d4250f068748c9.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1a1bc66f06af187108d4250f068748c9.jpg" alt="ON^3"></a> 。 用户可通过使用 isomap 的 path_method 关键字选择该算法。 如果未指定，则代码尝试为输入数据选择最佳算法。</li>
<li><strong>部分特征值分解.</strong> 对应于 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3bdd2a9b74f6a2e0db32e159c63ffec0.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3bdd2a9b74f6a2e0db32e159c63ffec0.jpg" alt="N \times N"></a> isomap核中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/adf83056bc2bd05628e24c40cb728b3d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/adf83056bc2bd05628e24c40cb728b3d.jpg" alt="d"></a> 个最大特征值的特征向量，进行嵌入编码。 对于密集求解器，代价约为 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9642d01a97f06869baba6159e3438677.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9642d01a97f06869baba6159e3438677.jpg" alt="Od N^2"></a> 。 通常可以使用 ARPACK 求解器来减少代价。 用户可通过使用 isomap 的 path_method 关键字指定特征求解器。 如果未指定，则代码尝试为输入数据选择最佳算法。</li>
</ol>
<p>Isomap 的整体复杂度是 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ccd727d4b039d28f8146546bd5f614b3.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ccd727d4b039d28f8146546bd5f614b3.jpg" alt="OD \log(k) N \log(N) + ON^2(k + \log(N)) + Od N^2"></a>.</p>
<ul>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" alt="N"></a> : 训练数据点的个数</li>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e03066df748abd9273db055cb79f0f01.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e03066df748abd9273db055cb79f0f01.jpg" alt="D"></a> : 输入维度</li>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> : 最近邻的个数</li>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/adf83056bc2bd05628e24c40cb728b3d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/adf83056bc2bd05628e24c40cb728b3d.jpg" alt="d"></a> : 输出维度</li>
</ul>
<p>参考文献:</p>
<ul>
<li><a href="http://science.sciencemag.org/content/290/5500/2319.full">“A global geometric framework for nonlinear dimensionality reduction”</a> Tenenbaum, J.B.; De Silva, V.; &amp; Langford, J.C. Science 290 (5500)</li>
</ul>
<h2 id="223-局部线性嵌入">2.2.3. 局部线性嵌入</h2>
<p>局部线性嵌入（LLE）通过保留局部邻域内的距离来寻求数据的低维投影。 它可以被认为是一系列的局部主成分分析，与全局相比，找到最优的局部非线性嵌入。</p>
<p>局部线性嵌入可以使用 <a href="generated/sklearn.manifold.locally_linear_embedding.html#sklearn.manifold.locally_linear_embedding" title="sklearn.manifold.locally_linear_embedding"><code>locally_linear_embedding</code></a> 函数或其面向对象的等效方法 <a href="generated/sklearn.manifold.LocallyLinearEmbedding.html#sklearn.manifold.LocallyLinearEmbedding" title="sklearn.manifold.LocallyLinearEmbedding"><code>LocallyLinearEmbedding</code></a> 来实现。</p>
<p><a href="../auto_examples/manifold/plot_lle_digits.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/017a1400b81bc9ef956adc43050bb5c8.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/017a1400b81bc9ef956adc43050bb5c8.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_lle_digits_0061.png"></a></a></p>
<h3 id="2231-复杂度">2.2.3.1. 复杂度</h3>
<p>标准的 LLE 算法包括三个阶段:</p>
<ol>
<li><strong>最邻近搜索</strong>. 参见上述 Isomap 讨论。</li>
<li><strong>构造权重矩阵</strong>. <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ca22762150e0516b4847c03efd5ebf6d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ca22762150e0516b4847c03efd5ebf6d.jpg" alt="OD N k^3"></a>. LLE 权重矩阵的构造包括每个 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" alt="N"></a> 局部邻域的 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2a96390b6e7eb8fc07579c2f9066fc4d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2a96390b6e7eb8fc07579c2f9066fc4d.jpg" alt="k \times k"></a> 线性方程的解</li>
<li><strong>部分特征值分解</strong>. 参见上述 Isomap 讨论。</li>
</ol>
<p>标准 LLE 的整体复杂度是 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5daa1b5d6a3d63020722cb0f4b41eee2.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5daa1b5d6a3d63020722cb0f4b41eee2.jpg" alt="OD \log(k) N \log(N) + OD N k^3 + Od N^2"></a>.</p>
<ul>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" alt="N"></a> : 训练数据点的个数</li>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e03066df748abd9273db055cb79f0f01.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e03066df748abd9273db055cb79f0f01.jpg" alt="D"></a> : 输入维度</li>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> : 最近邻的个数</li>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/adf83056bc2bd05628e24c40cb728b3d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/adf83056bc2bd05628e24c40cb728b3d.jpg" alt="d"></a> : 输出维度</li>
</ul>
<p>参考文献:</p>
<ul>
<li><a href="http://www.sciencemag.org/content/290/5500/2323.full">“Nonlinear dimensionality reduction by locally linear embedding”</a> Roweis, S. &amp; Saul, L. Science 290:2323 (2000)</li>
</ul>
<h2 id="224-改进型局部线性嵌入mlle">2.2.4. 改进型局部线性嵌入（MLLE）</h2>
<p>关于局部线性嵌入（LLE）的一个众所周知的问题是正则化问题。当 neighbors（邻域）的数量多于输入的维度数量时，定义每个局部邻域的矩阵是不满秩的。为解决这个问题，标准的局部线性嵌入算法使用一个任意正则化参数 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/451ef7ed1a14a6cdc38324c8a5c7c683.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/451ef7ed1a14a6cdc38324c8a5c7c683.jpg" alt="r"></a> ，它的取值受局部权重矩阵的迹的影响。虽然可以认为 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ab81f225a7e452d651b4888d437d07d2.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ab81f225a7e452d651b4888d437d07d2.jpg" alt="r \to 0"></a> ，即解收敛于嵌入情况，但是不保证最优解情况下 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8cddd8c0c85ca4a1b6dce8bbf145a8aa.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8cddd8c0c85ca4a1b6dce8bbf145a8aa.jpg" alt="r > 0"></a> 。此问题说明，在嵌入时此问题会扭曲流形的内部几何形状，使其失真。</p>
<p>解决正则化问题的一种方法是对邻域使用多个权重向量。这就是改进型局部线性嵌入（MLLE）算法的精髓。MLLE 可被执行于函数 <a href="generated/sklearn.manifold.locally_linear_embedding.html#sklearn.manifold.locally_linear_embedding" title="sklearn.manifold.locally_linear_embedding"><code>locally_linear_embedding</code></a> ，或者面向对象的副本 <a href="generated/sklearn.manifold.LocallyLinearEmbedding.html#sklearn.manifold.LocallyLinearEmbedding" title="sklearn.manifold.LocallyLinearEmbedding"><code>LocallyLinearEmbedding</code></a> ，附带关键词 <code>method = 'modified'</code> 。它需要满足 <code>n_neighbors &amp;gt; n_components</code> 。</p>
<p><a href="../auto_examples/manifold/plot_lle_digits.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ca04f56b8f8c29e1eec03620f0f601b0.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ca04f56b8f8c29e1eec03620f0f601b0.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_lle_digits_0071.png"></a></a></p>
<h3 id="2241-复杂度">2.2.4.1. 复杂度</h3>
<p>MLLE 算法分为三部分：</p>
<ol>
<li><strong>近邻搜索</strong>。与标准 LLE 的相同。</li>
<li><strong>权重矩阵构造</strong>。大约是 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d7f26dee1f8849176f6438863fb775fb.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d7f26dee1f8849176f6438863fb775fb.jpg" alt="OD N k^3 + ON (k-D) k^2"></a> 。该式第一项恰好等于标准 LLE 算法的复杂度。该式第二项与由多个权重来构造权重矩阵相关。在实践中，（在第二步中）构造 MLLE 权重矩阵（对复杂度）增加的影响，比第一步和第三步的小。</li>
<li><strong>部分特征值分解</strong>。与标准 LLE 的相同。</li>
</ol>
<p>综上，MLLE 的复杂度为 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/efb0c43ded3d4bdfb4b1d2092c8ee446.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/efb0c43ded3d4bdfb4b1d2092c8ee446.jpg" alt="OD \log(k) N \log(N) + OD N k^3 + ON (k-D) k^2 + Od N^2"></a> 。</p>
<ul>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" alt="N"></a> : 训练集数据点的个数</li>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e03066df748abd9273db055cb79f0f01.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e03066df748abd9273db055cb79f0f01.jpg" alt="D"></a> : 输入维度</li>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> : 最近邻域的个数</li>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/adf83056bc2bd05628e24c40cb728b3d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/adf83056bc2bd05628e24c40cb728b3d.jpg" alt="d"></a> : 输出的维度</li>
</ul>
<p>参考文献:</p>
<ul>
<li><a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382">“MLLE: Modified Locally Linear Embedding Using Multiple Weights”</a> Zhang, Z. &amp; Wang, J.</li>
</ul>
<h2 id="225-黑塞特征映射he">2.2.5. 黑塞特征映射（HE）</h2>
<p>黑塞特征映射 (也称作基于黑塞的 LLE: HLLE ）是解决 LLE 正则化问题的另一种方法。在每个用于恢复局部线性结构的邻域内，它会围绕一个基于黑塞的二次型展开。虽然其他实现表明它对数据大小缩放较差，但是 sklearn 实现了一些算法改进，使得在输出低维度时它的损耗可与其他 LLE 变体相媲美。HLLE 可实现为函数 <a href="generated/sklearn.manifold.locally_linear_embedding.html#sklearn.manifold.locally_linear_embedding" title="sklearn.manifold.locally_linear_embedding"><code>locally_linear_embedding</code></a> 或其面向对象的形式 <a href="generated/sklearn.manifold.LocallyLinearEmbedding.html#sklearn.manifold.LocallyLinearEmbedding" title="sklearn.manifold.LocallyLinearEmbedding"><code>LocallyLinearEmbedding</code></a> ，附带关键词 <code>method = 'hessian'</code> 。它需满足 <code>n_neighbors &amp;gt; n_components * (n_components + 3) / 2</code> 。</p>
<p><a href="../auto_examples/manifold/plot_lle_digits.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/01e7c74ccc13a6832f6bfcd46b442a1b.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/01e7c74ccc13a6832f6bfcd46b442a1b.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_lle_digits_0081.png"></a></a></p>
<h3 id="2251-复杂度">2.2.5.1. 复杂度</h3>
<p>HLLE 算法分为三部分:</p>
<ol>
<li><strong>近邻搜索</strong>。与标准 LLE 的相同。</li>
<li><strong>权重矩阵构造</strong>. 大约是 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f8e0c6c9a82bcbf369e2d0b7fc7aba8d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f8e0c6c9a82bcbf369e2d0b7fc7aba8d.jpg" alt="OD N k^3 + ON d^6"></a> 。其中第一项与标准 LLE 相似。第二项来自于局部黑塞估计量的一个 QR 分解。</li>
<li><strong>部分特征值分解</strong>。与标准 LLE 的相同。</li>
</ol>
<p>综上，HLLE 的复杂度为 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2ad6b07024498864a0ce275913a42d9f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2ad6b07024498864a0ce275913a42d9f.jpg" alt="OD \log(k) N \log(N) + OD N k^3 + ON d^6 + Od N^2"></a> 。</p>
<ul>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" alt="N"></a> : 训练集数据点的个数</li>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e03066df748abd9273db055cb79f0f01.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e03066df748abd9273db055cb79f0f01.jpg" alt="D"></a> : 输入维度</li>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> : 最近邻域的个数</li>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/adf83056bc2bd05628e24c40cb728b3d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/adf83056bc2bd05628e24c40cb728b3d.jpg" alt="d"></a> : 输出的维度</li>
</ul>
<p>参考文献:</p>
<ul>
<li><a href="http://www.pnas.org/content/100/10/5591">“Hessian Eigenmaps: Locally linear embedding techniques for high-dimensional data”</a> Donoho, D. &amp; Grimes, C. Proc Natl Acad Sci USA. 100:5591 (2003)</li>
</ul>
<h2 id="226-谱嵌入">2.2.6. 谱嵌入</h2>
<p>谱嵌入是计算非线性嵌入的一种方法。scikit-learn 执行拉普拉斯特征映射，该映射是用图拉普拉斯的谱分解的方法把数据进行低维表达。这个生成的图可认为是低维流形在高维空间里的离散近似值。基于图的代价函数的最小化确保流形上彼此临近的点被映射后在低维空间也彼此临近，低维空间保持了局部距离。谱嵌入可执行为函数 <a href="generated/sklearn.manifold.spectral_embedding.html#sklearn.manifold.spectral_embedding" title="sklearn.manifold.spectral_embedding"><code>spectral_embedding</code></a> 或它的面向对象的对应形式 <a href="generated/sklearn.manifold.SpectralEmbedding.html#sklearn.manifold.SpectralEmbedding" title="sklearn.manifold.SpectralEmbedding"><code>SpectralEmbedding</code></a> 。</p>
<h3 id="2261-复杂度">2.2.6.1. 复杂度</h3>
<p>谱嵌入（拉普拉斯特征映射）算法含三部分：</p>
<ol>
<li><strong>加权图结构</strong>。把原输入数据转换为用相似（邻接）矩阵表达的图表达。</li>
<li><strong>图拉普拉斯结构</strong>。非规格化的图拉普拉斯是按 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/49b0512284893ed2ca56a2b8c0b7d0b5.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/49b0512284893ed2ca56a2b8c0b7d0b5.jpg" alt="L = D - A"></a> 构造，并按 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4bb6ac59e053fd48275c31c9af35b2d1.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4bb6ac59e053fd48275c31c9af35b2d1.jpg" alt="L = D{-\frac{1}{2}} (D - A) D{-\frac{1}{2}}"></a> 规格化的。</li>
<li><strong>部分特征值分解</strong>。在图拉普拉斯上进行特征值分解。</li>
</ol>
<p>综上，谱嵌入的复杂度是 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5daa1b5d6a3d63020722cb0f4b41eee2.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5daa1b5d6a3d63020722cb0f4b41eee2.jpg" alt="OD \log(k) N \log(N) + OD N k^3 + Od N^2"></a> 。</p>
<ul>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" alt="N"></a> : 训练集数据点的个数</li>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e03066df748abd9273db055cb79f0f01.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e03066df748abd9273db055cb79f0f01.jpg" alt="D"></a> : 输入维度</li>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> : 最近邻域的个数</li>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/adf83056bc2bd05628e24c40cb728b3d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/adf83056bc2bd05628e24c40cb728b3d.jpg" alt="d"></a> : 输出的维度</li>
</ul>
<p>参考文献:</p>
<ul>
<li><a href="http://web.cse.ohio-state.edu/~mbelkin/papers/LEM_NC_03.pdf">“Laplacian Eigenmaps for Dimensionality Reduction and Data Representation”</a> M. Belkin, P. Niyogi, Neural Computation, June 2003; 15 (6):1373-1396</li>
</ul>
<h2 id="227-局部切空间对齐ltsa">2.2.7. 局部切空间对齐（LTSA）</h2>
<p>尽管局部切空间对齐（LTSA）在技术上并不是 LLE 的变体，但它与 LLE 足够相近，可以放入这个目录。与 LLE 算法关注于保持临点距离不同，LTSA 寻求通过切空间来描述局部几何形状，并（通过）实现全局最优化来对其这些局部切空间，从而学会嵌入。 LTSA 可执行为函数 <a href="generated/sklearn.manifold.locally_linear_embedding.html#sklearn.manifold.locally_linear_embedding" title="sklearn.manifold.locally_linear_embedding"><code>locally_linear_embedding</code></a> 或它的面向对象的对应形式 <a href="generated/sklearn.manifold.LocallyLinearEmbedding.html#sklearn.manifold.LocallyLinearEmbedding" title="sklearn.manifold.LocallyLinearEmbedding"><code>LocallyLinearEmbedding</code></a> ，附带关键词 <code>method = 'ltsa'</code> 。</p>
<p><a href="../auto_examples/manifold/plot_lle_digits.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b74decc4f9ee591a92a5281d0187f05a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b74decc4f9ee591a92a5281d0187f05a.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_lle_digits_0091.png"></a></a></p>
<h3 id="2271-复杂度">2.2.7.1. 复杂度</h3>
<p>LTSA 算法含三部分:</p>
<ol>
<li><strong>近邻搜索</strong>。与标准 LLE 的相同。</li>
<li><strong>加权矩阵构造</strong>。大约是 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8c187292cd29fea23a4983db349e7545.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8c187292cd29fea23a4983db349e7545.jpg" alt="OD N k^3 + Ok^2 d"></a> 。其中第一项与标准 LLE 相似。</li>
<li><strong>部分特征值分解</strong>。同于标准 LLE 。</li>
</ol>
<p>综上，复杂度是 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/fa48fa696e5242bb078fb786e6dc24c3.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/fa48fa696e5242bb078fb786e6dc24c3.jpg" alt="OD \log(k) N \log(N) + OD N k^3 + Ok^2 d + Od N^2"></a> 。</p>
<ul>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" alt="N"></a> : 训练集数据点的个数</li>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e03066df748abd9273db055cb79f0f01.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e03066df748abd9273db055cb79f0f01.jpg" alt="D"></a> : 输入维度</li>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> : 最近邻域的个数</li>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/adf83056bc2bd05628e24c40cb728b3d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/adf83056bc2bd05628e24c40cb728b3d.jpg" alt="d"></a> : 输出的维度</li>
</ul>
<p>参考文献:</p>
<ul>
<li><a href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.4.3693">“Principal manifolds and nonlinear dimensionality reduction via tangent space alignment”</a> Zhang, Z. &amp; Zha, H. Journal of Shanghai Univ. 8:406 (2004)</li>
</ul>
<h2 id="228-多维尺度分析mds">2.2.8. 多维尺度分析（MDS）</h2>
<p><a href="https://en.wikipedia.org/wiki/Multidimensional_scaling">多维尺度分析 Multidimensional scaling</a> （ <a href="generated/sklearn.manifold.MDS.html#sklearn.manifold.MDS" title="sklearn.manifold.MDS"><code>MDS</code></a> ） 寻求数据的低维表示，（低维下的）它的距离保持了在初始高维空间中的距离。</p>
<p>一般来说，（MDS）是一种用来分析在几何空间距离相似或相异数据的技术。MDS 尝试将相似或相异的数据建模为几何空间距离。这些数据可以是物体间的相似等级，也可是分子的作用频率，还可以是国家简单贸易指数。</p>
<p>MDS算法有2类：度量和非度量。在 scikit-learn 中， <a href="generated/sklearn.manifold.MDS.html#sklearn.manifold.MDS" title="sklearn.manifold.MDS"><code>MDS</code></a> 类中二者都有。在度量 MDS 中，输入相似度矩阵源自度量(并因此遵从三角形不等式)，输出两点之间的距离被设置为尽可能接近相似度或相异度的数据。在非度量版本中，算法尝试保持距离的控制，并因此寻找在所嵌入空间中的距离和相似/相异之间的单调关系。</p>
<p><a href="../auto_examples/manifold/plot_lle_digits.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/12ab1980b4b3f069be032c0d4f1184ed.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/12ab1980b4b3f069be032c0d4f1184ed.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_lle_digits_0101.png"></a></a></p>
<p>设 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/12ecd862769bee1e71c75c134b6423bb.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/12ecd862769bee1e71c75c134b6423bb.jpg" alt="S"></a> 是相似度矩阵，<a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43c1fea57579e54f80c0535bc582626f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43c1fea57579e54f80c0535bc582626f.jpg" alt="X"></a> 是 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c87d9110f3d32ffa5fa08671e4af11fb.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c87d9110f3d32ffa5fa08671e4af11fb.jpg" alt="n"></a> 个输入点的坐标。差异 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/223988a8bef489edcaa2f198e5e3a9a5.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/223988a8bef489edcaa2f198e5e3a9a5.jpg" alt="\hat{d}_{ij}"></a> 是以某种最佳方式选择的相似度的转换。然后，通过 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ae82b9adb507cb166d4721c004ae5f40.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ae82b9adb507cb166d4721c004ae5f40.jpg" alt="sum_{i < j} d_{ij}(X) - \hat{d}_{ij}(X)"></a> 定义称为 Stress （应力值）的对象。</p>
<h3 id="2281-度量-mds">2.2.8.1. 度量 MDS</h3>
<p>最简单的度量 <a href="generated/sklearn.manifold.MDS.html#sklearn.manifold.MDS" title="sklearn.manifold.MDS"><code>MDS</code></a> 模型称为 <em>absolute MDS（绝对MDS）</em>，差异由 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/446d6d36c20a79508f1cc84c737a597b.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/446d6d36c20a79508f1cc84c737a597b.jpg" alt="\hat{d}{ij} = S{ij}"></a> 定义。对于绝对 MDS，值 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/bf95d88f4f17676409c7bab64ba036dc.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/bf95d88f4f17676409c7bab64ba036dc.jpg" alt="S_{ij}"></a> 应精确地对应于嵌入点的点 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" alt="i"></a> 和 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7b215f2882ce8aaa33a97e43ad626314.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7b215f2882ce8aaa33a97e43ad626314.jpg" alt="j"></a> 之间的距离。</p>
<p>大多数情况下，差异应设置为 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/de55c53f911184b6ad3e562a4d694c01.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/de55c53f911184b6ad3e562a4d694c01.jpg" alt="\hat{d}{ij} = b S{ij}"></a> 。</p>
<h3 id="2282-非度量-mds">2.2.8.2. 非度量 MDS</h3>
<p>非度量 <a href="generated/sklearn.manifold.MDS.html#sklearn.manifold.MDS" title="sklearn.manifold.MDS"><code>MDS</code></a> 关注数据的排序。如果 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7ab8c51f211ad5aea8e4e78337ca3624.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7ab8c51f211ad5aea8e4e78337ca3624.jpg" alt="S_{ij} < S_{kl}"></a> ，则嵌入应执行 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6a0cf5d5f1d5ad90f9713a46fa55111f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6a0cf5d5f1d5ad90f9713a46fa55111f.jpg" alt="d_{ij} < d_{jk}"></a> 。这样执行的一个简单算法是在 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/bf95d88f4f17676409c7bab64ba036dc.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/bf95d88f4f17676409c7bab64ba036dc.jpg" alt="S_{ij}"></a> 上使用 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e0d8dbb9574d5eb264279927dcf8baaf.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e0d8dbb9574d5eb264279927dcf8baaf.jpg" alt="d_{ij}"></a> 的单调回归，产生与 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/bf95d88f4f17676409c7bab64ba036dc.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/bf95d88f4f17676409c7bab64ba036dc.jpg" alt="S_{ij}"></a> 相同顺序的差异 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/223988a8bef489edcaa2f198e5e3a9a5.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/223988a8bef489edcaa2f198e5e3a9a5.jpg" alt="\hat{d}_{ij}"></a> 。</p>
<p>此问题的 a trivial solution（一个平凡解）是把所有点设置到原点上。为了避免这种情况，将差异 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/bf95d88f4f17676409c7bab64ba036dc.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/bf95d88f4f17676409c7bab64ba036dc.jpg" alt="S_{ij}"></a> 标准化。</p>
<p><a href="../auto_examples/manifold/plot_mds.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/fe62193b4391c9f60e373f03623696ac.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/fe62193b4391c9f60e373f03623696ac.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_mds_0011.png"></a></a></p>
<p>参考文献:</p>
<ul>
<li><a href="http://www.springer.com/fr/book/9780387251509">“Modern Multidimensional Scaling - Theory and Applications”</a> Borg, I.; Groenen P. Springer Series in Statistics (1997)</li>
<li><a href="http://link.springer.com/article/10.1007%2FBF02289694">“Nonmetric multidimensional scaling: a numerical method”</a> Kruskal, J. Psychometrika, 29 (1964)</li>
<li><a href="http://link.springer.com/article/10.1007%2FBF02289565">“Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis”</a> Kruskal, J. Psychometrika, 29, (1964)</li>
</ul>
<h2 id="229-t-分布随机邻域嵌入t-sne">2.2.9. t 分布随机邻域嵌入（t-SNE）</h2>
<p>t-SNE（ <a href="generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE" title="sklearn.manifold.TSNE"><code>TSNE</code></a> ）将数据点的相似性转换为概率。原始空间中的相似性表示为高斯联合概率，嵌入空间中的相似性表示为 “学生” 的 t 分布。这允许 t-SNE 对局部结构特别敏感，并且有超过现有技术的一些其它优点:</p>
<ul>
<li>在一个单一映射上以多种比例显示结构</li>
<li>显示位于多个、不同的流形或聚类中的数据</li>
<li>减轻在中心聚集的趋势</li>
</ul>
<p>Isomap、LLE 和其它变体最适合展开单个连续低维流形，而 t-SNE 将侧重于数据的局部结构，并倾向于提取聚类的局部样本组，就像S曲线示例中突出显示的那样。这种基于局部结构对样本进行分组的能力可能有助于在视觉上同时解开包括多个流形的数据集，如数字数据集中的情况。</p>
<p>原始空间和嵌入空间中的联合概率的 Kullback-Leibler（KL） 散度将通过梯度下降而最小化。注意，KL 发散不是凸的，即具有不同初始化的多次重新开始将以KL发散的局部最小值结束。因此，尝试不同的开始值并选择具有最低KL散度的嵌入有时是有用的。</p>
<p>使用 t - SNE 的缺点大致如下:</p>
<ul>
<li>t-SNE 的计算成本很高，在百万样本数据集上可能需要几个小时，而PCA将在几秒或几分钟内完成同样工作。</li>
<li>Barnes-Hut t-SNE 方法仅限于二维或三维嵌入。</li>
<li>该算法是随机的，不同种子的多次重新开始可以产生不同的嵌入。然而，以最小的误差选择嵌入是完全合理的。</li>
<li>未明确保留全局结构。用PCA初始化点(使用 &lt;cite&gt;init=’pca’&lt;/cite&gt; )，可以减轻此问题。</li>
</ul>
<p><a href="../auto_examples/manifold/plot_lle_digits.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/afcad7956ba0a3a4a6771ee9810280c2.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/afcad7956ba0a3a4a6771ee9810280c2.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_lle_digits_0131.png"></a></a></p>
<h3 id="2291-优化-t-sne">2.2.9.1. 优化 t-SNE</h3>
<p>t-SNE 的主要目的是实现高维数据的可视化。因此，当数据将嵌入到二维或三维时，它效果最好。</p>
<p>优化KL发散有时可能有点棘手。有五个参数控制 t-SNE 的优化，因此可能也控制最终嵌入的质量:</p>
<ul>
<li>复杂度</li>
<li>早期增长因子</li>
<li>学习率</li>
<li>最大迭代次数</li>
<li>角度（不在精确方法中使用）</li>
</ul>
<p>复杂度（perplexity）定义为 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/81dfab5bd4f0d37601684acb3d714e9d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/81dfab5bd4f0d37601684acb3d714e9d.jpg" alt="k=2^(S)"></a> ，其中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/12ecd862769bee1e71c75c134b6423bb.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/12ecd862769bee1e71c75c134b6423bb.jpg" alt="S"></a> 是条件概率分布的香农熵。k 面色子的复杂度是 k ，因此 k 实际上是生成条件概率时 t-SNE 考虑的最近邻域的个数。复杂度越大导致有越多的近邻域，则对小结构越不敏感。相反地，越低的复杂度考虑越少的邻域，并因此忽略越多的全局信息而越关注局部邻域。当数据集的大小变大时，需要更多的点来获得局部邻域的合理样本，因此可能需要更大的复杂度。类似地，噪声越大的数据集需要越大的复杂度来包含足够的局部邻域，以超出背景噪声。</p>
<p>最大迭代次数通常足够高，不需要任何调整。优化分为两个阶段:早期增长阶段和最终优化阶段。在早期增长中，原始空间中的联合概率将通过与给定因子相乘而被人为地增加。越大的因子导致数据中的自然聚类之间的差距越大。如果因子过高，KL 发散可能在此阶段增加。通常不需要对其进行调谐。学习率是一个关键参数。如果梯度太低，下降会陷入局部极小值。如果过高，KL发散将在优化阶段增加。可以在 Laurens van derMaaten 的常见问题解答中找到更多提示(见参考资料)。最后一个参数角度是性能和精度之间的折衷。角度越大意味着我们可以通过单个点来近似的区域越大，从而导致越快的速度，但结果越不准确。</p>
<p><a href="http://distill.pub/2016/misread-tsne/">“如何高效使用 t-SNE”</a> 提供了一个关于各种参数效果的很好的讨论，以及用来探索不同参数效果的交互式绘图。</p>
<h3 id="2292-barnes-hut-t-sne">2.2.9.2. Barnes-Hut t-SNE</h3>
<p>在此实现的 Barnes-Hut t-SNE 通常比其他流形学习算法慢得多。优化是很困难的，梯度的计算是 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f60c0101ae8f649bb02ed8b24b30fd83.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f60c0101ae8f649bb02ed8b24b30fd83.jpg" alt="Od N log(N)"></a> ，其中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/adf83056bc2bd05628e24c40cb728b3d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/adf83056bc2bd05628e24c40cb728b3d.jpg" alt="d"></a> 是输出维数，<a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" alt="N"></a> 是样本个数。Barnes-Hut 方法在 t-SNE 复杂度为 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9642d01a97f06869baba6159e3438677.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9642d01a97f06869baba6159e3438677.jpg" alt="Od N^2"></a> 的精确方法上有所改进，但有其他几个显著区别:</p>
<ul>
<li>Barnes-Hut 实现仅在目标维度为3或更小时才有效。构建可视化时，2D 案例是典型的。</li>
<li>Barnes-Hut 仅适用于密集的输入数据。稀疏数据矩阵只能用精确方法嵌入，或者可以通过密集的低阶投影来近似，例如使用 <a href="generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD" title="sklearn.decomposition.TruncatedSVD"><code>sklearn.decomposition.TruncatedSVD</code></a></li>
<li>Barnes-Hut 是精确方法的近似。近似使用 angle 作为参数，因此当参数 method=”exact” 时，angle 参数无效。</li>
<li>Barnes-Hut 的拓展性很高。Barnes-Hut 可用于嵌入数十万个数据点，而精确方法只能处理数千个样本，再多就很困难了。</li>
</ul>
<p>出于可视化目的（ t-SNE 的主要使用情况），强烈建议使用 Barnes-Hut 方法。精确的 t-SNE 方法可用于检验高维空间中嵌入的理论性质，但由于计算约束而仅限于小数据集。</p>
<p>还要注意，数字 label 与 t-SNE 发现的自然聚类大致匹配，而 PCA 模型的线性 2D 投影产生标签区域在很大程度上重叠的表示。这是一个强有力的线索，表明该数据可以通过关注局部结构的非线性方法（例如，具有高斯 RBF 核的 SVM ）很好地分离。然而，如果不能在二维中用 t-SNE 来可视化分离良好的均匀标记组，并不一定意味着数据不能被监督模型正确地分类。可能是因为 2 维不够低，无法准确表示数据的内部结构。</p>
<p>参考文献:</p>
<ul>
<li><a href="http://jmlr.org/papers/v9/vandermaaten08a.html">“Visualizing High-Dimensional Data Using t-SNE”</a> van der Maaten, L.J.P.; Hinton, G. Journal of Machine Learning Research (2008)</li>
<li><a href="http://lvdmaaten.github.io/tsne/">“t-Distributed Stochastic Neighbor Embedding”</a> van der Maaten, L.J.P.</li>
<li><a href="https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf">“Accelerating t-SNE using Tree-Based Algorithms.”</a> L.J.P. van der Maaten. Journal of Machine Learning Research 15(Oct):3221-3245, 2014.</li>
</ul>
<h2 id="2210-实用技巧">2.2.10. 实用技巧</h2>
<ul>
<li>确保对所有特征使用相同的缩放。因为流形学习方法是基于最近邻搜索的，否则算法的性能可能很差。有关缩放异构数据的方便方法，请参阅 <a href="preprocessing.html#preprocessing-scaler">StandardScaler</a> 。</li>
<li>由每个例程计算的重构误差可用于选择最佳输出维度。对于嵌入在 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/adf83056bc2bd05628e24c40cb728b3d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/adf83056bc2bd05628e24c40cb728b3d.jpg" alt="d"></a> 维参数空间中的 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/adf83056bc2bd05628e24c40cb728b3d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/adf83056bc2bd05628e24c40cb728b3d.jpg" alt="d"></a> 维流形，重构误差将随着 <code>n_components</code> 的增加而减小，直到 <code>n_components == d</code> 。</li>
<li>注意，噪声数据可以对流形造成“短路”，其实质上充当了一个桥梁，用于连接流形的不同部分，否则（没有这样的“桥梁”）这些流形将被很好地划分开。噪声和/或不完全数据的流形学习是一个活跃的研究领域。</li>
<li>某些输入配置可能导致奇异加权矩阵，例如，当数据集中的两个以上点完全相同时，或者当数据被分割成不连续的组时。在这种情况下， <code>solver='arpack'</code> 将无法找到空空间。解决这一问题的最简单方法是使用 <code>solver='dense'</code> ，它将在一个奇异矩阵上进行，尽管它可能因为输入点的数量而非常缓慢。或者，人们可以尝试理解奇异的来源:如果它是由于不相交的集合，增加 <code>n_neighbors</code> 可能有所帮助；如果是由于数据集中的相同点，则删除这些点可能有所帮助。</li>
</ul>
<p>See also</p>
<p><a href="ensemble.html#random-trees-embedding">完全随机树嵌入</a> 也可以用于得到特征空间的非线性表示，另外它不用降维。</p>
</div>
<hr class="uk-article-divider">
<div class="uk-block uk-block-muted uk-padding-top-remove uk-padding-bottom-remove uk-margin-large-top  book-recommend-wrap">
<div class="uk-margin-top uk-margin-bottom uk-margin-left uk-margin-right">
<div class="uk-margin uk-text-muted "><i class="uk-icon-outdent uk-icon-justify uk-margin-small-right"></i>书籍推荐</div>
<div class="books">
<ul class="uk-book-list">
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/97/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/97/index.html">Twisted与异步编程入门</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/60.html">likebeta</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">23页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月29日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 158个">158</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/127/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/127/index.html">aiohttp 中文文档</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/68.html">HuberTRoy</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">124页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年7月8日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 34个">34</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/96/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/96/index.html">零基础学Python</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/59.html">qiwsir</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">80页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月29日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 1635个">1635</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/113/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/react_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/113/index.html">React Native中文文档0.51版</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/62.html">tzivanmoe</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="react">react</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">118页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年7月1日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 0个">0</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/44/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/linux_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/44/index.html">Shell 编程范例</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/23.html">泰晓科技</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="linux">linux</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">15页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年5月30日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 296个">296</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/3/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/go_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/3/index.html">深入解析Go</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/3.html">tiancaiamao</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="go">go</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">41页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年5月1日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 1018个">1018</span>
</div>
</div>
</div>
</li>
<hr>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
<nav class="tm-navbar uk-navbar uk-navbar-attached reader-nav">
<div class="uk-float-left uk-margin-small-top">
<a href="javascript:;" title="目录菜单" class="show-menu  uk-icon-hover  uk-icon-align-justify uk-margin-right"></a>
<div data-uk-dropdown="{mode:'click',pos:'bottom-left'}" class="font-setting-wrap">
<a class="uk-icon-hover uk-icon-font uk-margin-right" aria-label="字体设置" href="javascript:;"></a>
<div class="uk-dropdown dropdown-menu">
<div class="dropdown-caret"><span class="caret-outer"></span><span class="caret-inner"></span></div>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-reduce">小字</button>
<button class="uk-button-link button size-2 font-enlarge">大字</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-1 ">宋体</button>
<button class="uk-button-link button size-2 font-2 ">黑体</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-3 color-theme-sun "><i class="uk-icon-sun-o"></i>白天</button>
<button class="uk-button-link button size-3 color-theme-eye "><i class="uk-icon-eye"></i>护眼</button>
<button class="uk-button-link button size-3 color-theme-moon "><i class="uk-icon-moon-o"></i>夜晚</button></div>
</div>
</div>
<a class="logo uk-margin-right" href="../../../" title="返回首页"><img class="" src="../../../static/components/images/icon_32.png" /></a>
</div>
<div class="uk-navbar-flip  uk-hidden-small">
<div id="share-box"></div>
</div>
</nav>
<div id="menu-id" class="uk-offcanvas reader-offcanvas">
<div class="uk-offcanvas-bar">
<ul class="book-menu-bar uk-nav uk-nav-offcanvas" data-uk-nav>
<li>
<a href="../../../book/170/index.html" data-book-page-rel-url="index.html" data-book-page-id="0" title="封面">封面</a>
</li>
<li>
<a class="pjax" href="../../../book/170/readme.html" data-book-page-rel-url="readme.html" data-book-page-id="0" title="简介">简介</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/79.html" title="安装 scikit-learn" data-book-page-rel-url="docs/79.html" data-book-page-id="11491">安装 scikit-learn</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/1.html" title="1. 监督学习" data-book-page-rel-url="docs/1.html" data-book-page-id="11492">1. 监督学习</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/2.html" title="1.1. 广义线性模型" data-book-page-rel-url="docs/2.html" data-book-page-id="11493">1.1. 广义线性模型</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/3.html" title="1.2. 线性和二次判别分析" data-book-page-rel-url="docs/3.html" data-book-page-id="11494">1.2. 线性和二次判别分析</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/4.html" title="1.3. 内核岭回归" data-book-page-rel-url="docs/4.html" data-book-page-id="11495">1.3. 内核岭回归</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/5.html" title="1.4. 支持向量机" data-book-page-rel-url="docs/5.html" data-book-page-id="11496">1.4. 支持向量机</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/6.html" title="1.5. 随机梯度下降" data-book-page-rel-url="docs/6.html" data-book-page-id="11497">1.5. 随机梯度下降</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/7.html" title="1.6. 最近邻" data-book-page-rel-url="docs/7.html" data-book-page-id="11498">1.6. 最近邻</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/8.html" title="1.7. 高斯过程" data-book-page-rel-url="docs/8.html" data-book-page-id="11499">1.7. 高斯过程</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/9.html" title="1.8. 交叉分解" data-book-page-rel-url="docs/9.html" data-book-page-id="11500">1.8. 交叉分解</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/10.html" title="1.9. 朴素贝叶斯" data-book-page-rel-url="docs/10.html" data-book-page-id="11501">1.9. 朴素贝叶斯</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/11.html" title="1.10. 决策树" data-book-page-rel-url="docs/11.html" data-book-page-id="11502">1.10. 决策树</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/12.html" title="1.11. 集成方法" data-book-page-rel-url="docs/12.html" data-book-page-id="11503">1.11. 集成方法</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/13.html" title="1.12. 多类和多标签算法" data-book-page-rel-url="docs/13.html" data-book-page-id="11504">1.12. 多类和多标签算法</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/14.html" title="1.13. 特征选择" data-book-page-rel-url="docs/14.html" data-book-page-id="11505">1.13. 特征选择</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/15.html" title="1.14. 半监督学习" data-book-page-rel-url="docs/15.html" data-book-page-id="11506">1.14. 半监督学习</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/16.html" title="1.15. 等式回归" data-book-page-rel-url="docs/16.html" data-book-page-id="11507">1.15. 等式回归</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/17.html" title="1.16. 概率校准" data-book-page-rel-url="docs/17.html" data-book-page-id="11508">1.16. 概率校准</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/18.html" title="1.17. 神经网络模型（有监督）" data-book-page-rel-url="docs/18.html" data-book-page-id="11509">1.17. 神经网络模型（有监督）</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/19.html" title="2. 无监督学习" data-book-page-rel-url="docs/19.html" data-book-page-id="11510">2. 无监督学习</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/20.html" title="2.1. 高斯混合模型" data-book-page-rel-url="docs/20.html" data-book-page-id="11511">2.1. 高斯混合模型</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/21.html" title="2.2. 流形学习" data-book-page-rel-url="docs/21.html" data-book-page-id="11512">2.2. 流形学习</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/22.html" title="2.3. 聚类" data-book-page-rel-url="docs/22.html" data-book-page-id="11513">2.3. 聚类</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/23.html" title="2.4. 双聚类" data-book-page-rel-url="docs/23.html" data-book-page-id="11514">2.4. 双聚类</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/24.html" title="2.5. 分解成分中的信号（矩阵分解问题）" data-book-page-rel-url="docs/24.html" data-book-page-id="11515">2.5. 分解成分中的信号（矩阵分解问题）</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/25.html" title="2.6. 协方差估计" data-book-page-rel-url="docs/25.html" data-book-page-id="11516">2.6. 协方差估计</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/26.html" title="2.7. 经验协方差" data-book-page-rel-url="docs/26.html" data-book-page-id="11517">2.7. 经验协方差</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/27.html" title="2.8. 收敛协方差" data-book-page-rel-url="docs/27.html" data-book-page-id="11518">2.8. 收敛协方差</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/28.html" title="2.9. 稀疏逆协方差" data-book-page-rel-url="docs/28.html" data-book-page-id="11519">2.9. 稀疏逆协方差</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/29.html" title="2.10. Robust 协方差估计" data-book-page-rel-url="docs/29.html" data-book-page-id="11520">2.10. Robust 协方差估计</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/30.html" title="2.11. 新奇和异常值检测" data-book-page-rel-url="docs/30.html" data-book-page-id="11521">2.11. 新奇和异常值检测</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/31.html" title="2.12. 密度估计" data-book-page-rel-url="docs/31.html" data-book-page-id="11522">2.12. 密度估计</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/32.html" title="2.13. 神经网络模型（无监督）" data-book-page-rel-url="docs/32.html" data-book-page-id="11523">2.13. 神经网络模型（无监督）</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/33.html" title="3. 模型选择和评估" data-book-page-rel-url="docs/33.html" data-book-page-id="11524">3. 模型选择和评估</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/34.html" title="3.1. 交叉验证：评估估算器的表现" data-book-page-rel-url="docs/34.html" data-book-page-id="11525">3.1. 交叉验证：评估估算器的表现</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/35.html" title="3.2. 调整估计器的超参数" data-book-page-rel-url="docs/35.html" data-book-page-id="11526">3.2. 调整估计器的超参数</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/53.html" title="3.3. 模型评估: 量化预测的质量" data-book-page-rel-url="docs/53.html" data-book-page-id="11527">3.3. 模型评估: 量化预测的质量</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/54.html" title="3.4. 模型持久化" data-book-page-rel-url="docs/54.html" data-book-page-id="11528">3.4. 模型持久化</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/55.html" title="3.5. 验证曲线: 绘制分数以评估模型" data-book-page-rel-url="docs/55.html" data-book-page-id="11529">3.5. 验证曲线: 绘制分数以评估模型</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/56.html" title="4. 数据集转换" data-book-page-rel-url="docs/56.html" data-book-page-id="11530">4. 数据集转换</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/57.html" title="4.1. Pipeline（管道）和 FeatureUnion（特征联合）: 合并的评估器" data-book-page-rel-url="docs/57.html" data-book-page-id="11531">4.1. Pipeline（管道）和 FeatureUnion（特征联合）: 合并的评估器</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/58.html" title="4.2. 特征提取" data-book-page-rel-url="docs/58.html" data-book-page-id="11532">4.2. 特征提取</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/59.html" title="4.3. 预处理数据" data-book-page-rel-url="docs/59.html" data-book-page-id="11533">4.3. 预处理数据</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/60.html" title="4.4. 无监督降维" data-book-page-rel-url="docs/60.html" data-book-page-id="11534">4.4. 无监督降维</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/61.html" title="4.5. 随机投影" data-book-page-rel-url="docs/61.html" data-book-page-id="11535">4.5. 随机投影</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/62.html" title="4.6. 内核近似" data-book-page-rel-url="docs/62.html" data-book-page-id="11536">4.6. 内核近似</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/63.html" title="4.7. 成对的矩阵, 类别和核函数" data-book-page-rel-url="docs/63.html" data-book-page-id="11537">4.7. 成对的矩阵, 类别和核函数</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/64.html" title="4.8. 预测目标 (`y`.html) 的转换" data-book-page-rel-url="docs/64.html" data-book-page-id="11538">4.8. 预测目标 (`y`.html) 的转换</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/65.html" title="5. 数据集加载工具" data-book-page-rel-url="docs/65.html" data-book-page-id="11539">5. 数据集加载工具</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/66.html" title="6. 大规模计算的策略: 更大量的数据" data-book-page-rel-url="docs/66.html" data-book-page-id="11540">6. 大规模计算的策略: 更大量的数据</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/67.html" title="7. 计算性能" data-book-page-rel-url="docs/67.html" data-book-page-id="11541">7. 计算性能</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/68.html" title="使用 scikit-learn 介绍机器学习" data-book-page-rel-url="docs/68.html" data-book-page-id="11542">使用 scikit-learn 介绍机器学习</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/69.html" title="关于科学数据处理的统计学习教程" data-book-page-rel-url="docs/69.html" data-book-page-id="11543">关于科学数据处理的统计学习教程</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/70.html" title="机器学习: scikit-learn 中的设置以及预估对象" data-book-page-rel-url="docs/70.html" data-book-page-id="11544">机器学习: scikit-learn 中的设置以及预估对象</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/71.html" title="监督学习：从高维观察预测输出变量" data-book-page-rel-url="docs/71.html" data-book-page-id="11545">监督学习：从高维观察预测输出变量</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/72.html" title="模型选择：选择估计量及其参数" data-book-page-rel-url="docs/72.html" data-book-page-id="11546">模型选择：选择估计量及其参数</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/73.html" title="无监督学习: 寻求数据表示" data-book-page-rel-url="docs/73.html" data-book-page-id="11547">无监督学习: 寻求数据表示</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/74.html" title="把它们放在一起" data-book-page-rel-url="docs/74.html" data-book-page-id="11548">把它们放在一起</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/75.html" title="寻求帮助" data-book-page-rel-url="docs/75.html" data-book-page-id="11549">寻求帮助</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/76.html" title="处理文本数据" data-book-page-rel-url="docs/76.html" data-book-page-id="11550">处理文本数据</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/77.html" title="选择正确的评估器(estimator.html)" data-book-page-rel-url="docs/77.html" data-book-page-id="11551">选择正确的评估器(estimator.html)</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/78.html" title="外部资源，视频和谈话" data-book-page-rel-url="docs/78.html" data-book-page-id="11552">外部资源，视频和谈话</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/80.html" title="常见问题" data-book-page-rel-url="docs/80.html" data-book-page-id="11553">常见问题</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/81.html" title="时光轴" data-book-page-rel-url="docs/81.html" data-book-page-id="11554">时光轴</a>
</li>
</ul>
</div>
</div>
<script src="https://cdn.staticfile.net/jquery/1.12.4/jquery.min.js"></script>
<script type="text/javascript" src="../../../static/components/uikit-2.27.5/js/uikit.reader.js"></script>
<script type="text/javascript" src="../../../static/components/social-share/social-share.min.js"></script>
<script>(function(){var bp =document.createElement('script');var curProtocol =window.location.protocol.split(':')[0];if (curProtocol ==='https') {bp.src ='https://zz.bdstatic.com/linksubmit/push.js';}
else {bp.src ='http://push.zhanzhang.baidu.com/push.js';}
var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(bp,s);})();</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
<script src="https://cdn.staticfile.net/highlight.js/9.12.0/highlight.min.js"></script>
<script src="https://cdn.staticfile.net/jquery.pjax/2.0.1/jquery.pjax.min.js"></script>
<script src="https://cdn.staticfile.net/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="https://cdn.staticfile.net/uikit/2.27.5/js/components/lightbox.min.js"></script>
<link rel="dns-prefetch" href="../../..//cdn.mathjax.org" />
<script type="text/x-mathjax-config">
 function initMathJax() {
    var mathId = $("book-content-section")[0];
    MathJax.Hub.Config({
        tex2jax: {skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code','a']},
        showProcessingMessages: false,
        messageStyle: "none"
    });
    MathJax.Hub.Queue(["Typeset",MathJax.Hub,mathId]);
 };
initMathJax();
</script>
<script src='https://cdn.staticfile.net/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML' async></script>
<style>
	.MathJax_Display{display:inline!important;}
</style>
<script type="text/javascript" src="../../../static/components/js/reader.js"></script>
<script type="text/javascript">var bookId =170;var bookPageId =11512;var bookPageRelUrl ='docs/21.html';</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
</body>
</html>