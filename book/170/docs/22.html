
<!DOCTYPE html>
<html lang="zh-CN">
<head>
<title>2.3. 聚类-scikit-learn (sklearn) 官方文档中文版</title>
<meta content='2.3. 聚类,scikit-learn (sklearn) 官方文档中文版' name='keywords'>
<meta content='2.3. 聚类,scikit-learn (sklearn) 官方文档中文版' name='description'>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta http-equiv="Content-Language" content="zh-CN" />
<meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1, maximum-scale=1, user-scalable=no"../../../>
<meta name="applicable-device" content="pc,mobile">
<link rel="shortcut icon" href="../../../favicon.ico" type="image/x-icon" />
<meta name="renderer" content="webkit">
<link rel="stylesheet" href="../../../static/components/uikit-2.27.5/css/uikit.custom.css">
<link rel="stylesheet" href="../../../static/components/social-share/social-share.min.css">
<link rel="stylesheet" href="../../../static/components/highlight/styles/custom.css">
<link rel="stylesheet" href="../../../static/components/css/base.css">
<link rel="stylesheet" href="../../../static/components/css/reader.css">
<link rel="stylesheet" href="../../../static/components/css/markdown.css">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5313208362165053" crossorigin="anonymous"></script>
</head>
<body>
<div class=" book-main-wrap uk-container uk-container-center uk-margin-top ">
<div class="uk-grid">
<div class="uk-width-1-1 reader-wrap ">
<div class=" bottom-nav uk-clearfix ">
<div class="uk-align-left ">
<a href="../../../book/170/docs/21.html">
<i class="nav-icon-left uk-icon-small  uk-icon-caret-left"></i>
<span class="">2.2. 流形学习</span>
</a>
</div>
<div class="uk-align-right ">
<a href="../../../book/170/docs/23.html">
<span class="">2.4. 双聚类</span>
<i class="nav-icon-right uk-icon-small  uk-icon-caret-right"></i>
</a>
</div>
</div>
<div class="uk-text-center">
<h2 class="book-page-title uk-container-center">
<a href="../../../book/170/index.html">scikit-learn (sklearn) 官方文档中文版</a>
<a target="_blank" rel="nofollow" href="https://github.com/apachecn/scikit-learn-doc-zh" class="uk-icon-button uk-icon-github" title="github项目地址"></a>
</h2>
</div>
<script type="text/javascript" src="../../../static/components/js/app_intro.js"></script>
<ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-5313208362165053" data-ad-slot="1328047120"></ins>
<script>(adsbygoogle =window.adsbygoogle ||[]).push({});</script>
<hr class="uk-article-divider">
<div class="book-content-section  md-content-section  uk-margin-bottom">
<h1 id="23-聚类">2.3. 聚类</h1>
<p>校验者: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@花开无声</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@小瑶</a> 翻译者: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@小瑶</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@krokyin</a></p>
<p>未标记的数据的 <a href="https://en.wikipedia.org/wiki/Cluster_analysis">Clustering（聚类）</a> 可以使用模块 <a href="classes.html#module-sklearn.cluster" title="sklearn.cluster"><code>sklearn.cluster</code></a> 来实现。</p>
<p>每个 clustering algorithm （聚类算法）有两个变体: 一个是 class, 它实现了 <code>fit</code> 方法来学习 train data（训练数据）的 clusters（聚类），还有一个 function（函数），是给定 train data（训练数据），返回与不同 clusters（聚类）对应的整数标签 array（数组）。对于 class（类），training data（训练数据）上的标签可以在 <code>labels_</code> 属性中找到。</p>
<p>输入数据</p>
<p>需要注意的一点是，该模块中实现的算法可以采用不同种类的 matrix （矩阵）作为输入。所有这些都接受 shape <code>[n_samples, n_features]</code> 的标准数据矩阵。 这些可以从以下的 <a href="classes.html#module-sklearn.feature_extraction" title="sklearn.feature_extraction"><code>sklearn.feature_extraction</code></a> 模块的 classes （类）中获得。对于 <a href="generated/sklearn.cluster.AffinityPropagation.html#sklearn.cluster.AffinityPropagation" title="sklearn.cluster.AffinityPropagation"><code>AffinityPropagation</code></a>, <a href="generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering" title="sklearn.cluster.SpectralClustering"><code>SpectralClustering</code></a> 和 <a href="generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN" title="sklearn.cluster.DBSCAN"><code>DBSCAN</code></a> 也可以输入 shape <code>[n_samples, n_samples]</code> 的相似矩阵。这些可以从 <a href="classes.html#module-sklearn.metrics.pairwise" title="sklearn.metrics.pairwise"><code>sklearn.metrics.pairwise</code></a> 模块中的函数获得。</p>
<h2 id="231-聚类方法概述">2.3.1. 聚类方法概述</h2>
<p><a href="../auto_examples/cluster/plot_cluster_comparison.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/153aceb3cdac953277c6c840339ac023.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/153aceb3cdac953277c6c840339ac023.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_cluster_comparison_0011.png"></a></a></p>
<p>在 scikit-learn 中的 clustering algorithms （聚类算法）的比较</p>
<table>
<thead>
<tr>
<th>Method name（方法名称）</th>
<th>Parameters（参数）</th>
<th>Scalability（可扩展性）</th>
<th>Usecase（使用场景）</th>
<th>Geometry (metric used)（几何图形（公制使用））</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="#k-means">K-Means（K-均值）</a></td>
<td>number of clusters（聚类形成的簇的个数）</td>
<td>非常大的 <code>n_samples</code>, 中等的 <code>n_clusters</code> 使用 <a href="#mini-batch-kmeans">MiniBatch code（MiniBatch 代码）</a></td>
<td>通用, 均匀的 cluster size（簇大小）, flat geometry（平面几何）, 不是太多的 clusters（簇）</td>
<td>Distances between points（点之间的距离）</td>
</tr>
<tr>
<td><a href="#affinity-propagation">Affinity propagation</a></td>
<td>damping（阻尼）, sample preference（样本偏好）</td>
<td>Not scalable with n_samples（n_samples 不可扩展）</td>
<td>Many clusters, uneven cluster size, non-flat geometry（许多簇，不均匀的簇大小，非平面几何）</td>
<td>Graph distance (e.g. nearest-neighbor graph)（图形距离（例如，最近邻图））</td>
</tr>
<tr>
<td><a href="#mean-shift">Mean-shift</a></td>
<td>bandwidth（带宽）</td>
<td>Not scalable with <code>n_samples</code> （不可扩展的 <code>n_samples</code>）</td>
<td>Many clusters, uneven cluster size, non-flat geometry（许多簇，不均匀的簇大小，非平面几何）</td>
<td>Distances between points（点之间的距离）</td>
</tr>
<tr>
<td><a href="#spectral-clustering">Spectral clustering</a></td>
<td>number of clusters（簇的个数）</td>
<td>中等的 <code>n_samples</code>, 小的 <code>n_clusters</code></td>
<td>Few clusters, even cluster size, non-flat geometry（几个簇，均匀的簇大小，非平面几何）</td>
<td>Graph distance (e.g. nearest-neighbor graph)（图形距离（例如最近邻图））</td>
</tr>
<tr>
<td><a href="#hierarchical-clustering">Ward hierarchical clustering</a></td>
<td>number of clusters（簇的个数）</td>
<td>大的 <code>n_samples</code> 和 <code>n_clusters</code></td>
<td>Many clusters, possibly connectivity constraints（很多的簇，可能连接限制）</td>
<td>Distances between points（点之间的距离）</td>
</tr>
<tr>
<td><a href="#hierarchical-clustering">Agglomerative clustering</a></td>
<td>number of clusters（簇的个数）, linkage type（链接类型）, distance（距离）</td>
<td>大的 <code>n_samples</code> 和 <code>n_clusters</code></td>
<td>Many clusters, possibly connectivity constraints, non Euclidean distances（很多簇，可能连接限制，非欧几里得距离）</td>
<td>Any pairwise distance（任意成对距离）</td>
</tr>
<tr>
<td><a href="#dbscan">DBSCAN</a></td>
<td>neighborhood size（neighborhood 的大小）</td>
<td>非常大的 <code>n_samples</code>, 中等的 <code>n_clusters</code></td>
<td>Non-flat geometry, uneven cluster sizes（非平面几何，不均匀的簇大小）</td>
<td>Distances between nearest points（最近点之间的距离）</td>
</tr>
<tr>
<td><a href="mixture.html#mixture">Gaussian mixtures（高斯混合）</a></td>
<td>many（很多）</td>
<td>Not scalable（不可扩展）</td>
<td>Flat geometry, good for density estimation（平面几何，适用于密度估计）</td>
<td>Mahalanobis distances to centers（Mahalanobis 与中心的距离）</td>
</tr>
<tr>
<td><a href="#birch">Birch</a></td>
<td>branching factor（分支因子）, threshold（阈值）, optional global clusterer（可选全局簇）.</td>
<td>大的 <code>n_clusters</code> 和 <code>n_samples</code></td>
<td>Large dataset, outlier removal, data reduction.（大数据集，异常值去除，数据简化）</td>
<td>Euclidean distance between points（点之间的欧式距离）</td>
</tr>
</tbody>
</table>
<p>当 clusters （簇）具有 specific shape （特殊的形状），即 non-flat manifold（非平面 manifold），并且标准欧几里得距离不是正确的 metric （度量标准）时，Non-flat geometry clustering （非平面几何聚类）是非常有用的。这种情况出现在上图的两个顶行中。</p>
<p>用于 clustering （聚类）的 Gaussian mixture models （高斯混合模型），专用于 mixture models （混合模型）描述在 <a href="mixture.html#mixture">文档的另一章节</a> 。可以将 KMeans 视为具有 equal covariance per component （每个分量相等协方差）的 Gaussian mixture model （高斯混合模型）的特殊情况。</p>
<h2 id="232-k-means">2.3.2. K-means</h2>
<p><a href="generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans" title="sklearn.cluster.KMeans"><code>KMeans</code></a> 算法通过试图分离 n groups of equal variance（n 个相等方差组）的样本来聚集数据，minimizing （最小化）称为 <a href="inertia">inertia</a> 或者 within-cluster sum-of-squares （簇内和平方）的 criterion （标准）。 该算法需要指定 number of clusters （簇的数量）。它可以很好地 scales （扩展）到 large number of samples（大量样本），并已经被广泛应用于许多不同领域的应用领域。</p>
<p>k-means 算法将一组 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" alt="N"></a> 样本 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43c1fea57579e54f80c0535bc582626f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43c1fea57579e54f80c0535bc582626f.jpg" alt="X"></a> 划分成 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e279b8169ddd6581c5606c868ba52fae.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e279b8169ddd6581c5606c868ba52fae.jpg" alt="K"></a> 不相交的 clusters （簇） <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4b6d782a67ac392e97215c46b7590bf7.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4b6d782a67ac392e97215c46b7590bf7.jpg" alt="C"></a>, 每个都用 cluster （该簇）中的样本的均值 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/38320089278fc639e640f3f772eac6b1.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/38320089278fc639e640f3f772eac6b1.jpg" alt="\mu_j"></a> 描述。 这个 means （均值）通常被称为 cluster（簇）的 “centroids（质心）”; 注意，它们一般不是从 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43c1fea57579e54f80c0535bc582626f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43c1fea57579e54f80c0535bc582626f.jpg" alt="X"></a> 中挑选出的点，虽然它们是处在同一个 space（空间）。 K-means（K-均值）算法旨在选择最小化 <em>inertia（惯性）</em> 或 within-cluster sum of squared（簇内和的平方和）的标准的 centroids（质心）:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c46633c42aaa3e030b14d90aadb323fc.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c46633c42aaa3e030b14d90aadb323fc.jpg" alt="\sum_{i=0}{n}\min_{\mu_j \in C}(||x_j - \mu_i||2)"></a></p>
<p>Inertia（惯性）, 或 the within-cluster sum of squares（簇内和平方差） criterion（标准）,可以被认为是 internally coherent clusters （内部想干聚类）的 measure （度量）。 它有各种缺点:</p>
<ul>
<li>Inertia（惯性）假设 clusters （簇）是 convex（凸）的和 isotropic （各项同性），这并不是总是这样。它对 elongated clusters （细长的簇）或具有不规则形状的 manifolds 反应不佳。</li>
<li>Inertia（惯性）不是一个 normalized metric（归一化度量）: 我们只知道 lower values （较低的值）是更好的，并且 零 是最优的。但是在 very high-dimensional spaces （非常高维的空间）中，欧几里得距离往往会变得 inflated （膨胀）（这就是所谓的 “curse of dimensionality （维度诅咒/维度惩罚）”）。在 k-means 聚类之前运行诸如 <a href="PCA">PCA</a> 之类的 dimensionality reduction algorithm （降维算法）可以减轻这个问题并加快计算速度。</li>
</ul>
<p><a href="../auto_examples/cluster/plot_kmeans_assumptions.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d97ae32100e54dfed8139aef0fcc9b68.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d97ae32100e54dfed8139aef0fcc9b68.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_kmeans_assumptions_0011.png"></a></a></p>
<p>K-means 通常被称为 Lloyd’s algorithm（劳埃德算法）。在基本术语中，算法有三个步骤。、 第一步是选择 initial centroids （初始质心），最基本的方法是从 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43c1fea57579e54f80c0535bc582626f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43c1fea57579e54f80c0535bc582626f.jpg" alt="X"></a> 数据集中选择 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> 个样本。初始化完成后，K-means 由两个其他步骤之间的循环组成。 第一步将每个样本分配到其 nearest centroid （最近的质心）。第二步通过取分配给每个先前质心的所有样本的平均值来创建新的质心。计算旧的和新的质心之间的差异，并且算法重复这些最后的两个步骤，直到该值小于阈值。换句话说，算法重复这个步骤，直到质心不再显著移动。</p>
<p><a href="../auto_examples/cluster/plot_kmeans_digits.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8a76c85f2de3d3777fe72f5d8e32e0cf.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8a76c85f2de3d3777fe72f5d8e32e0cf.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_kmeans_digits_0011.png"></a></a></p>
<p>K-means 相当于具有 small, all-equal, diagonal covariance matrix （小的全对称协方差矩阵）的 expectation-maximization algorithm （期望最大化算法）。</p>
<p>该算法也可以通过 <a href="#id42"><code>Voronoi diagrams（Voronoi图）&amp;lt;https://en.wikipedia.org/wiki/Voronoi_diagram&amp;gt;</code>_</a> 的概念来理解。首先使用 current centroids （当前质心）计算点的 Voronoi 图。 Voronoi 图中的每个 segment （段）都成为一个 separate cluster （单独的簇）。其次，centroids（质心）被更新为每个 segment （段）的 mean（平均值）。然后，该算法重复此操作，直到满足停止条件。 通常情况下，当 iterations （迭代）之间的 objective function （目标函数）的相对减小小于给定的 tolerance value （公差值）时，算法停止。在此实现中不是这样: 当质心移动小于 tolerance （公差）时，迭代停止。</p>
<p>给定足够的时间，K-means 将总是收敛的，但这可能是 local minimum （局部最小）的。这很大程度上取决于 initialization of the centroids （质心的初始化）。 因此，通常会进行几次 different initializations of the centroids （初始化不同质心）的计算。帮助解决这个问题的一种方法是 k-means++ 初始化方案，它已经在 scikit-learn 中实现（使用 <code>init='k-means++'</code> 参数）。 这将初始化 centroids （质心）（通常）彼此远离，导致比随机初始化更好的结果，如参考文献所示。</p>
<p>可以给出一个参数，以允许 K-means 并行运行，称为 <code>n_jobs</code>。给这个参数一个正值使用许多处理器（默认值: 1）。值 -1 使用所有可用的处理器，-2 使用一个，等等。Parallelization （并行化）通常以 cost of memory（内存的代价）加速计算（在这种情况下，需要存储多个质心副本，每个作业使用一个）。</p>
<p>Warning</p>
<p>当 &lt;cite&gt;numpy&lt;/cite&gt; 使用 &lt;cite&gt;Accelerate&lt;/cite&gt; 框架时，K-Means 的并行版本在 OS X 上损坏。这是 expected behavior （预期的行为）: &lt;cite&gt;Accelerate&lt;/cite&gt; 可以在 fork 之后调用，但是您需要使用 Python binary（二进制）（该多进程在 posix 下不执行）来执行子进程。</p>
<p>K-means 可用于 vector quantization （矢量量化）。这是使用以下类型的 trained model （训练模型）的变换方法实现的 <a href="generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans" title="sklearn.cluster.KMeans"><code>KMeans</code></a> 。</p>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/cluster/plot_kmeans_assumptions.html#sphx-glr-auto-examples-cluster-plot-kmeans-assumptions-py">Demonstration of k-means assumptions</a>: 演示 k-means 是否 performs intuitively （直观执行），何时不执行</li>
<li><a href="../auto_examples/cluster/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py">A demo of K-Means clustering on the handwritten digits data</a>: 聚类手写数字</li>
</ul>
<p>参考:</p>
<ul>
<li><a href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf">“k-means++: The advantages of careful seeding”</a> Arthur, David, and Sergei Vassilvitskii, <em>Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms</em>, Society for Industrial and Applied Mathematics (2007)</li>
</ul>
<h3 id="2321-小批量-k-means">2.3.2.1. 小批量 K-Means</h3>
<p><a href="generated/sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans" title="sklearn.cluster.MiniBatchKMeans"><code>MiniBatchKMeans</code></a> 是 <a href="generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans" title="sklearn.cluster.KMeans"><code>KMeans</code></a> 算法的一个变体，它使用 mini-batches （小批量）来减少计算时间，同时仍然尝试优化相同的 objective function （目标函数）。 Mini-batches（小批量）是输入数据的子集，在每次 training iteration （训练迭代）中 randomly sampled （随机抽样）。这些小批量大大减少了融合到本地解决方案所需的计算量。 与其他降低 k-means 收敛时间的算法相反，mini-batch k-means 产生的结果通常只比标准算法略差。</p>
<p>该算法在两个主要步骤之间进行迭代，类似于 vanilla k-means 。 在第一步， <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6ae91fb0f3221b92d2dd4e22204d8008.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6ae91fb0f3221b92d2dd4e22204d8008.jpg" alt="b"></a> 样本是从数据集中随机抽取的，形成一个 mini-batch （小批量）。然后将它们分配到最近的 centroid（质心）。 在第二步，centroids （质心）被更新。与 k-means 相反，这是在每个样本的基础上完成的。对于 mini-batch （小批量）中的每个样本，通过取样本的 streaming average （流平均值）和分配给该质心的所有先前样本来更新分配的质心。 这具有随时间降低 centroid （质心）的 rate （变化率）的效果。执行这些步骤直到达到收敛或达到预定次数的迭代。</p>
<p><a href="generated/sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans" title="sklearn.cluster.MiniBatchKMeans"><code>MiniBatchKMeans</code></a> 收敛速度比 <a href="generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans" title="sklearn.cluster.KMeans"><code>KMeans</code></a> ，但是结果的质量会降低。在实践中，质量差异可能相当小，如示例和引用的参考。</p>
<p><a href="../auto_examples/cluster/plot_mini_batch_kmeans.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c851b3cdef3493f17f70f7249928e34b.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c851b3cdef3493f17f70f7249928e34b.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_mini_batch_kmeans_0011.png"></a></a></p>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/cluster/plot_mini_batch_kmeans.html#sphx-glr-auto-examples-cluster-plot-mini-batch-kmeans-py">Comparison of the K-Means and MiniBatchKMeans clustering algorithms</a>: KMeans 与 MiniBatchKMeans 的比较</li>
<li><a href="../auto_examples/text/document_clustering.html#sphx-glr-auto-examples-text-document-clustering-py">Clustering text documents using k-means</a>: 使用 sparse MiniBatchKMeans （稀疏 MiniBatchKMeans）的文档聚类</li>
<li><a href="../auto_examples/cluster/plot_dict_face_patches.html#sphx-glr-auto-examples-cluster-plot-dict-face-patches-py">Online learning of a dictionary of parts of faces</a></li>
</ul>
<p>参考:</p>
<ul>
<li><a href="http://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf">“Web Scale K-Means clustering”</a> D. Sculley, <em>Proceedings of the 19th international conference on World wide web</em> (2010)</li>
</ul>
<h2 id="233-affinity-propagation">2.3.3. Affinity Propagation</h2>
<p><a href="generated/sklearn.cluster.AffinityPropagation.html#sklearn.cluster.AffinityPropagation" title="sklearn.cluster.AffinityPropagation"><code>AffinityPropagation</code></a> AP聚类是通过在样本对之间发送消息直到收敛来创建聚类。然后使用少量示例样本作为聚类中心来描述数据集， 聚类中心是数据集中最能代表一类数据的样本。在样本对之间发送的消息表示一个样本作为另一个样本的示例样本的 适合程度，适合程度值在根据通信的反馈不断更新。更新迭代直到收敛，完成聚类中心的选取，因此也给出了最终聚类。</p>
<p><a href="../auto_examples/cluster/plot_affinity_propagation.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/12867664a0e0e6047ee303c542b4deac.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/12867664a0e0e6047ee303c542b4deac.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_affinity_propagation_0011.png"></a></a></p>
<pre><code class="language-py">Affinity Propagation 算法比较有趣的是可以根据提供的数据决定聚类的数目。 因此有两个比较重要的参数
</code></pre>
<p><em>preference</em>, 决定使用多少个示例样本 <a href="#id6">*</a>damping factor*（阻尼因子） 减少吸引信息和归属信息以防止 更新减少吸引度和归属度信息时数据振荡。</p>
<p>AP聚类算法主要的缺点是算法的复杂度. AP聚类算法的时间复杂度是 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2af4d75ca07ede34c7d38b8f7708723d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2af4d75ca07ede34c7d38b8f7708723d.jpg" alt="O(N^2 T)"></a>, 其中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" alt="N"></a> 是样本的个数 ， <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c4373cf7ea98d1425608569103286d28.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c4373cf7ea98d1425608569103286d28.jpg" alt="T"></a> 是收敛之前迭代的次数. 如果使用密集的相似性矩阵空间复杂度是 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ff5428ca3c50ed06f5162ad194377188.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ff5428ca3c50ed06f5162ad194377188.jpg" alt="O(N^2)"></a> 如果使用稀疏的相似性矩阵空间复杂度可以降低。 这使得AP聚类最适合中小型数据集。</p>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/cluster/plot_affinity_propagation.html#sphx-glr-auto-examples-cluster-plot-affinity-propagation-py">Demo of affinity propagation clustering algorithm</a>: Affinity Propagation on a synthetic 2D datasets with 3 classes.</li>
<li><a href="../auto_examples/applications/plot_stock_market.html#sphx-glr-auto-examples-applications-plot-stock-market-py">Visualizing the stock market structure</a> Affinity Propagation on Financial time series to find groups of companies</li>
</ul>
<p><strong>Algorithm description(算法描述):</strong> 样本之间传递的信息有两种。 第一种是 responsibility(吸引信息) <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d7b279566c62332b11d20ca6ff026505.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d7b279566c62332b11d20ca6ff026505.jpg" alt="r(i, k)"></a>, 样本 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> 适合作为样本 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" alt="i"></a> 的聚类中心的程度。</p>
<p>第二种是 availability(归属信息) <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/725082a3e3f2eacec65e9c1435a6960d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/725082a3e3f2eacec65e9c1435a6960d.jpg" alt="a(i, k)"></a> 样本 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" alt="i"></a> 选择样本 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> 作为聚类中心的适合程度,并且考虑其他所有样本选取 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> 做为聚类中心的合适程度。 通过这个方法，选取示例样本作为聚类中心如果 (1) 该样本与其许多样本相似，并且 (2) 被许多样本选取 为它们自己的示例样本。</p>
<p>样本 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> 对样本 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" alt="i"></a> 吸引度计算公式:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8dab78bd2e80188f99e0c88c4c83472a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8dab78bd2e80188f99e0c88c4c83472a.jpg" alt="r(i, k) \leftarrow s(i, k) - max  a(i, k') + s(i, k') \forall k' \neq k "></a></p>
<p>其中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f8d66dde73704b8821db5322592a0cc2.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f8d66dde73704b8821db5322592a0cc2.jpg" alt="s(i, k)"></a> 是样本 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" alt="i"></a> 和样本 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> 之间的相似度。 样本 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> 作为样本 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" alt="i"></a> 的示例样本的合适程度:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/851c667ab0811688c25c6819aafacba0.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/851c667ab0811688c25c6819aafacba0.jpg" alt="a(i, k) \leftarrow min 0, r(k, k) + \sum_{i'~s.t.~i' \notin \{i, k\}}{r(i', k)}"></a></p>
<p>算法开始时 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/451ef7ed1a14a6cdc38324c8a5c7c683.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/451ef7ed1a14a6cdc38324c8a5c7c683.jpg" alt="r"></a> 和 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/578c95150175e4efdf851fe66d503079.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/578c95150175e4efdf851fe66d503079.jpg" alt="a"></a> 都被置 0,然后开始迭代计算直到收敛。 为了防止更新数据时出现数据振荡，在迭代过程中引入阻尼因子 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0f92bc682b050115d03c625ce770c77d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0f92bc682b050115d03c625ce770c77d.jpg" alt="\lambda"></a> :</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f1fa822436569807fdc9dca5d2879d99.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f1fa822436569807fdc9dca5d2879d99.jpg" alt="r_{t+1}(i, k) = \lambda\cdot r_{t}(i, k) + (1-\lambda)\cdot r_{t+1}(i, k)"></a></p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1ae6d373d81c5f3f50905f336b4a070a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1ae6d373d81c5f3f50905f336b4a070a.jpg" alt="a_{t+1}(i, k) = \lambda\cdot a_{t}(i, k) + (1-\lambda)\cdot a_{t+1}(i, k)"></a></p>
<p>其中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/12b2c1da1f9041738fa7153efc651372.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/12b2c1da1f9041738fa7153efc651372.jpg" alt="t"></a> 迭代的次数。</p>
<h2 id="234-mean-shift">2.3.4. Mean Shift</h2>
<p><a href="generated/sklearn.cluster.MeanShift.html#sklearn.cluster.MeanShift" title="sklearn.cluster.MeanShift"><code>MeanShift</code></a> 算法旨在于发现一个样本密度平滑的 <em>blobs</em> 。 均值漂移算法是基于质心的算法，通过更新质心的候选位置为所选定区域的偏移均值。 然后，这些候选者在后处理阶段被过滤以消除近似重复，从而形成最终质心集合。</p>
<p>给定第 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/12b2c1da1f9041738fa7153efc651372.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/12b2c1da1f9041738fa7153efc651372.jpg" alt="t"></a> 次迭代中的候选质心 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/cf52655ee609af9f3c27c06448a5bf67.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/cf52655ee609af9f3c27c06448a5bf67.jpg" alt="x_i"></a> , 候选质心的位置将被安装如下公式更新:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/be2b3bbef9fe377c6f748dd05355b58b.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/be2b3bbef9fe377c6f748dd05355b58b.jpg" alt="x_i{t+1} = x_it + m(x_i^t)"></a></p>
<p>其中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f35f174b5f70ab18c19107e3f0fbe889.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f35f174b5f70ab18c19107e3f0fbe889.jpg" alt="N(x_i)"></a> 是围绕 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/cf52655ee609af9f3c27c06448a5bf67.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/cf52655ee609af9f3c27c06448a5bf67.jpg" alt="x_i"></a> 周围一个给定距离范围内的样本空间 and <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/94156b879a7455cb0d516efa9c9c0991.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/94156b879a7455cb0d516efa9c9c0991.jpg" alt="m"></a> 是 <em>mean shift</em> vector（均值偏移向量） 是所有质心中指向 点密度增加最多的区域的偏移向量。使用以下等式计算，有效地将质心更新为其邻域内样本的平均值:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/97f450040417800904df33c9702d2c66.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/97f450040417800904df33c9702d2c66.jpg" alt="m(x_i) = \frac{\sum_{x_j \in N(x_i)}K(x_j - x_i)x_j}{\sum_{x_j \in N(x_i)}K(x_j - x_i)}"></a></p>
<p>算法自动设定聚类的数目，取代依赖参数 <code>bandwidth``（带宽）,带宽是决定搜索区域的size的参数。 这个参数可以手动设置，但是如果没有设置，可以使用提供的评估函数 ``estimate_bandwidth</code> 进行评估。</p>
<p>该算法不是高度可扩展的，因为在执行算法期间需要执行多个最近邻搜索。 该算法保证收敛，但是当 质心的变化较小时，算法将停止迭代。</p>
<p>通过找到给定样本的最近质心来给新样本打上标签。</p>
<p><a href="../auto_examples/cluster/plot_mean_shift.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7304e7fb0302be38d7fa1688bcd14df4.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7304e7fb0302be38d7fa1688bcd14df4.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_mean_shift_0011.png"></a></a></p>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/cluster/plot_mean_shift.html#sphx-glr-auto-examples-cluster-plot-mean-shift-py">A demo of the mean-shift clustering algorithm</a>: Mean Shift clustering on a synthetic 2D datasets with 3 classes.</li>
</ul>
<p>参考:</p>
<ul>
<li><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.76.8968&amp;rep=rep1&amp;type=pdf">“Mean shift: A robust approach toward feature space analysis.”</a> D. Comaniciu and P. Meer, <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> (2002)</li>
</ul>
<h2 id="235-spectral-clustering">2.3.5. Spectral clustering</h2>
<p><a href="generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering" title="sklearn.cluster.SpectralClustering"><code>SpectralClustering</code></a> 是在样本之间进行亲和力矩阵的低维度嵌入，其实是低维空间中的 KMeans。 如果亲和度矩阵稀疏，则这是非常有效的并且 <a href="http://pyamg.org/">pyamg</a> module 以及安装好。 SpectralClustering 需要指定聚类数。这个算法适用于聚类数少时，在聚类数多是不建议使用。</p>
<p>对于两个聚类，它解决了相似图上的 <a href="http://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf">normalised cuts</a> 问题: 将图形切割成两个，使得切割的边缘的重量比每个簇内的边缘的权重小。在图像处理时，这个标准是特别有趣的: 图像的顶点是像素，相似图的边缘是图像的渐变函数。</p>
<p><strong><a href="../auto_examples/cluster/plot_segmentation_toy.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/bb0a3257b0276e1ade46d7fa84c49ad0.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/bb0a3257b0276e1ade46d7fa84c49ad0.jpg" alt="noisy_img"></a></a> <a href="../auto_examples/cluster/plot_segmentation_toy.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5ec012661471fa940c27472afcce01a2.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5ec012661471fa940c27472afcce01a2.jpg" alt="segmented_img"></a></a></strong></p>
<p>Warning</p>
<p>Transforming distance to well-behaved similarities</p>
<p>请注意，如果你的相似矩阵的值分布不均匀，例如:存在负值或者距离矩阵并不表示相似性 spectral problem 将会变得奇异，并且不能解决。 在这种情况下，建议对矩阵的 entries 进行转换。比如在符号距离有符号的情况下通常使用 heat kernel:</p>
<pre><code class="language-py">similarity = np.exp(-beta * distance / distance.std())

</code></pre>
<p>请看这样一个应用程序的例子。</p>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/cluster/plot_segmentation_toy.html#sphx-glr-auto-examples-cluster-plot-segmentation-toy-py">Spectral clustering for image segmentation</a>: Segmenting objects from a noisy background using spectral clustering.</li>
<li><a href="../auto_examples/cluster/plot_face_segmentation.html#sphx-glr-auto-examples-cluster-plot-face-segmentation-py">Segmenting the picture of a raccoon face in regions</a>: Spectral clustering to split the image of the raccoon face in regions.</li>
</ul>
<h3 id="2351-不同的标记分配策略">2.3.5.1. 不同的标记分配策略</h3>
<p>可以使用不同的分配策略, 对应于 <code>assign_labels</code> 参数 <a href="generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering" title="sklearn.cluster.SpectralClustering"><code>SpectralClustering</code></a>。 <code>"kmeans"</code> 可以匹配更精细的数据细节，但是可能更加不稳定。 特别是，除非你设置 <code>random_state</code> 否则可能无法复现运行的结果 ，因为它取决于随机初始化。另一方， 使用 <code>"discretize"</code> 策略是 100% 可以复现的，但是它往往会产生相当均匀的几何形状的边缘。</p>
<table>
<thead>
<tr>
<th><code>assign_labels="kmeans"</code></th>
<th><code>assign_labels="discretize"</code></th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="../auto_examples/cluster/plot_face_segmentation.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3579ce0b5c145fb891d865367eeba3ac.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3579ce0b5c145fb891d865367eeba3ac.jpg" alt="face_kmeans"></a></a></td>
<td><a href="../auto_examples/cluster/plot_face_segmentation.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1ff26934befcf3ca9623f1e729a8824c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1ff26934befcf3ca9623f1e729a8824c.jpg" alt="face_discretize"></a></a></td>
</tr>
</tbody>
</table>
<p>参考:</p>
<ul>
<li><a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323">“A Tutorial on Spectral Clustering”</a> Ulrike von Luxburg, 2007</li>
<li><a href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324">“Normalized cuts and image segmentation”</a> Jianbo Shi, Jitendra Malik, 2000</li>
<li><a href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.33.1501">“A Random Walks View of Spectral Segmentation”</a> Marina Meila, Jianbo Shi, 2001</li>
<li><a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.19.8100">“On Spectral Clustering: Analysis and an algorithm”</a> Andrew Y. Ng, Michael I. Jordan, Yair Weiss, 2001</li>
</ul>
<h2 id="236-层次聚类">2.3.6. 层次聚类</h2>
<p>Hierarchical clustering 是一个常用的聚类算法，它通过不断的合并或者分割来构建聚类。 聚类的层次被表示成树（或者 dendrogram（树形图））。树根是拥有所有样本的唯一聚类，叶子是仅有一个样本的聚类。 请参照 <a href="https://en.wikipedia.org/wiki/Hierarchical_clustering">Wikipedia page</a> 查看更多细节。</p>
<p>The <a href="generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering" title="sklearn.cluster.AgglomerativeClustering"><code>AgglomerativeClustering</code></a> 使用自下而上的方法进行层次聚类:开始是每一个对象是一个聚类， 并且聚类别相继合并在一起。 linkage criteria 确定用于合并的策略的度量:</p>
<ul>
<li><strong>Ward</strong> 最小化所有聚类内的平方差总和。这是一种 variance-minimizing （方差最小化）的优化方向， 这是与k-means 的目标函数相似的优化方法，但是用 agglomerative hierarchical（聚类分层）的方法处理。</li>
<li><strong>Maximum</strong> 或 <strong>complete linkage</strong> 最小化聚类对两个样本之间的最大距离。</li>
<li><strong>Average linkage</strong> 最小化聚类两个聚类中样本距离的平均值。</li>
</ul>
<p><a href="generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering" title="sklearn.cluster.AgglomerativeClustering"><code>AgglomerativeClustering</code></a> 在于连接矩阵联合使用时，也可以扩大到大量的样本，但是 在样本之间没有添加连接约束时，计算代价很大:每一个步骤都要考虑所有可能的合并。</p>
<p><a href="generated/sklearn.cluster.FeatureAgglomeration.html#sklearn.cluster.FeatureAgglomeration" title="sklearn.cluster.FeatureAgglomeration"><code>FeatureAgglomeration</code></a></p>
<p>The <a href="generated/sklearn.cluster.FeatureAgglomeration.html#sklearn.cluster.FeatureAgglomeration" title="sklearn.cluster.FeatureAgglomeration"><code>FeatureAgglomeration</code></a> 使用 agglomerative clustering 将看上去相似的 特征组合在一起，从而减少特征的数量。这是一个降维工具, 请参照 <a href="unsupervised_reduction.html#data-reduction">无监督降维</a>。</p>
<h3 id="2361-different-linkage-type-ward-complete-and-average-linkage">2.3.6.1. Different linkage type: Ward, complete and average linkage</h3>
<p><a href="generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering" title="sklearn.cluster.AgglomerativeClustering"><code>AgglomerativeClustering</code></a> 支持 Ward, average, and complete linkage 策略.</p>
<p><a href="../auto_examples/cluster/plot_digits_linkage.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8dcf0f01f9d255c37e21948ad3821885.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8dcf0f01f9d255c37e21948ad3821885.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_digits_linkage_0011.png"></a></a> <a href="../auto_examples/cluster/plot_digits_linkage.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/eada6f59eaee0a758bddb97b44835751.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/eada6f59eaee0a758bddb97b44835751.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_digits_linkage_0021.png"></a></a> <a href="../auto_examples/cluster/plot_digits_linkage.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/165303a7d56136efa39130cd3cd9539e.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/165303a7d56136efa39130cd3cd9539e.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_digits_linkage_0031.png"></a></a></p>
<p>Agglomerative cluster 存在 “rich get richer” 现象导致聚类大小不均匀。这方面 complete linkage 是最坏的策略，Ward 给出了最规则的大小。然而，在 Ward 中 affinity (or distance used in clustering) 不能被改变，对于 non Euclidean metrics 来说 average linkage 是一个好的选择。</p>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/cluster/plot_digits_linkage.html#sphx-glr-auto-examples-cluster-plot-digits-linkage-py">Various Agglomerative Clustering on a 2D embedding of digits</a>: exploration of the different linkage strategies in a real dataset.</li>
</ul>
<h3 id="2362-添加连接约束">2.3.6.2. 添加连接约束</h3>
<p><a href="generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering" title="sklearn.cluster.AgglomerativeClustering"><code>AgglomerativeClustering</code></a> 中一个有趣的特点是可以使用 connectivity matrix（连接矩阵） 将连接约束添加到算法中（只有相邻的聚类可以合并到一起），连接矩阵为每一个样本给定了相邻的样本。 例如，在 swiss-roll 的例子中，连接约束禁止在不相邻的 swiss roll 上合并，从而防止形成在 roll 上 重复折叠的聚类。</p>
<p><strong><a href="../auto_examples/cluster/plot_ward_structured_vs_unstructured.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/63f146cd209ad922f402bf81bfdeb621.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/63f146cd209ad922f402bf81bfdeb621.jpg" alt="unstructured"></a></a> <a href="../auto_examples/cluster/plot_ward_structured_vs_unstructured.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/50bc02ed6fb21594c72e30d1a33bbf89.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/50bc02ed6fb21594c72e30d1a33bbf89.jpg" alt="structured"></a></a></strong></p>
<p>这些约束对于强加一定的局部结构是很有用的，但是这也使得算法更快，特别是当样本数量巨大时。</p>
<p>连通性的限制是通过连接矩阵来实现的:一个 scipy sparse matrix（稀疏矩阵），仅在一行和 一列的交集处具有应该连接在一起的数据集的索引。这个矩阵可以通过 a-priori information （先验信息） 构建:例如，你可能通过仅仅将从一个连接指向另一个的链接合并页面来聚类页面。也可以从数据中学习到,</p>
<blockquote>
<p>例如使用 <a href="generated/sklearn.neighbors.kneighbors_graph.html#sklearn.neighbors.kneighbors_graph" title="sklearn.neighbors.kneighbors_graph"><code>sklearn.neighbors.kneighbors_graph</code></a> 限制与最临近的合并 :ref:<a href="#id13">`</a>this example</p>
</blockquote>
<pre><code class="language-py">&lt;sphx_glr_auto_examples_cluster_plot_agglomerative_clustering.py&gt;`, 或者使用
</code></pre>
<p><a href="generated/sklearn.feature_extraction.image.grid_to_graph.html#sklearn.feature_extraction.image.grid_to_graph" title="sklearn.feature_extraction.image.grid_to_graph"><code>sklearn.feature_extraction.image.grid_to_graph</code></a> 仅合并图像上相邻的像素点， 例如 <a href="../auto_examples/cluster/plot_face_ward_segmentation.html#sphx-glr-auto-examples-cluster-plot-face-ward-segmentation-py">raccoon face</a> 。</p>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/cluster/plot_face_ward_segmentation.html#sphx-glr-auto-examples-cluster-plot-face-ward-segmentation-py">A demo of structured Ward hierarchical clustering on a raccoon face image</a>: Ward clustering to split the image of a raccoon face in regions.</li>
<li><a href="../auto_examples/cluster/plot_ward_structured_vs_unstructured.html#sphx-glr-auto-examples-cluster-plot-ward-structured-vs-unstructured-py">Hierarchical clustering: structured vs unstructured ward</a>: Example of Ward algorithm on a swiss-roll, comparison of structured approaches versus unstructured approaches.</li>
<li><a href="../auto_examples/cluster/plot_feature_agglomeration_vs_univariate_selection.html#sphx-glr-auto-examples-cluster-plot-feature-agglomeration-vs-univariate-selection-py">Feature agglomeration vs. univariate selection</a>: Example of dimensionality reduction with feature agglomeration based on Ward hierarchical clustering.</li>
<li><a href="../auto_examples/cluster/plot_agglomerative_clustering.html#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-py">Agglomerative clustering with and without structure</a></li>
</ul>
<p>Warning</p>
<p><strong>Connectivity constraints with average and complete linkage</strong></p>
<p>Connectivity constraints 和 complete or average linkage 可以增强 agglomerative clustering 中的 ‘rich getting richer’ 现象。特别是，如果建立的是 <a href="generated/sklearn.neighbors.kneighbors_graph.html#sklearn.neighbors.kneighbors_graph" title="sklearn.neighbors.kneighbors_graph"><code>sklearn.neighbors.kneighbors_graph</code></a>。 在少量聚类的限制中, 更倾向于给出一些 macroscopically occupied clusters 并且几乎是空的 (讨论内容请查看 <a href="../auto_examples/cluster/plot_agglomerative_clustering.html#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-py">Agglomerative clustering with and without structure</a>)。</p>
<p><a href="../auto_examples/cluster/plot_agglomerative_clustering.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1ce6ae6c075734e41812dc91b67d16e5.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1ce6ae6c075734e41812dc91b67d16e5.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_agglomerative_clustering_0011.png"></a></a> <a href="../auto_examples/cluster/plot_agglomerative_clustering.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/59420186f988199ba986eefc023fb637.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/59420186f988199ba986eefc023fb637.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_agglomerative_clustering_0021.png"></a></a> <a href="../auto_examples/cluster/plot_agglomerative_clustering.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1fc92e9d8efa5433f7346284592e9ea0.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1fc92e9d8efa5433f7346284592e9ea0.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_agglomerative_clustering_0031.png"></a></a> <a href="../auto_examples/cluster/plot_agglomerative_clustering.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d61124c62424b8a8d38adc3c41bb71f6.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d61124c62424b8a8d38adc3c41bb71f6.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_agglomerative_clustering_0041.png"></a></a></p>
<h3 id="2363-varying-the-metric">2.3.6.3. Varying the metric</h3>
<p>Average and complete linkage 可以使用各种距离 (or affinities), 特别是 Euclidean distance (<em>l2</em>), Manhattan distance（曼哈顿距离）(or Cityblock（城市区块距离）, or <em>l1</em>), cosine distance(余弦距离),</p>
<blockquote>
<p>或者任何预先计算的 affinity matrix（亲和度矩阵）.</p>
</blockquote>
<ul>
<li><em>l1</em> distance 有利于稀疏特征或者稀疏噪声: 例如很多特征都是0，就想在文本挖掘中使用 rare words 一样。</li>
<li><em>cosine</em> distance 非常有趣因为它对全局放缩是一样的。</li>
</ul>
<p>选择度量标准的方针是使得不同类样本之间距离最大化，并且最小化同类样本之间的距离。</p>
<p><a href="../auto_examples/cluster/plot_agglomerative_clustering_metrics.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/423f64b70bdfeba3566e0bbcca01c277.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/423f64b70bdfeba3566e0bbcca01c277.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_agglomerative_clustering_metrics_0051.png"></a></a> <a href="../auto_examples/cluster/plot_agglomerative_clustering_metrics.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5a5de287d8a2c74dd12f86219cc19697.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5a5de287d8a2c74dd12f86219cc19697.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_agglomerative_clustering_metrics_0061.png"></a></a> <a href="../auto_examples/cluster/plot_agglomerative_clustering_metrics.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/27449ee75d40c9391b04e2ca48c4d83b.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/27449ee75d40c9391b04e2ca48c4d83b.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_agglomerative_clustering_metrics_0071.png"></a></a></p>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/cluster/plot_agglomerative_clustering_metrics.html#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-metrics-py">Agglomerative clustering with different metrics</a></li>
</ul>
<h2 id="237-dbscan">2.3.7. DBSCAN</h2>
<p>The <a href="generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN" title="sklearn.cluster.DBSCAN"><code>DBSCAN</code></a> 算法将聚类视为被低密度区域分隔的高密度区域。由于这个相当普遍的观点， DBSCAN发现的聚类可以是任何形状的，与假设聚类是 convex shaped 的 K-means 相反。 DBSCAN 的核心概念是 <em>core samples</em>, 是指位于高密度区域的样本。 因此一个聚类是一组核心样本，每个核心样本彼此靠近（通过一定距离度量测量） 和一组接近核心样本的非核心样本（但本身不是核心样本）。算法中的两个参数, <code>min_samples</code> 和 <code>eps</code>,正式的定义了我们所说的 <a href="#id16">*</a>dense*（稠密）。较高的 <code>min_samples</code> 或者</p>
<blockquote>
<p>较低的 [<code>](#id18)eps</code>表示形成聚类所需的较高密度。</p>
</blockquote>
<p>更正式的,我们定义核心样本是指数据集中的一个样本，存在 <code>min_samples</code> 个其他样本在 <code>eps</code> 距离范围内，这些样本被定为为核心样本的邻居 <em>neighbors</em> 。这告诉我们核心样本在向量空间稠密的区域。 一个聚类是一个核心样本的集合，可以通过递归来构建，选取一个核心样本，查找它所有的 neighbors （邻居样本） 中的核心样本，然后查找 <em>their</em> （新获取的核心样本）的 neighbors （邻居样本）中的核心样本，递归这个过程。 聚类中还具有一组非核心样本，它们是集群中核心样本的邻居的样本，但本身并不是核心样本。 显然，这些样本位于聚类的边缘。</p>
<pre><code class="language-py">根据定义，任何核心样本都是聚类的一部分，任何不是核心样本并且和任意一个核心样本距离都大于
</code></pre>
<p><code>eps</code> 的样本将被视为异常值。</p>
<p>在下图中，颜色表示聚类成员属性，大圆圈表示算法发现的核心样本。 较小的圈子是仍然是群集的 一部分的非核心样本。 此外，异常值由下面的黑点表示。</p>
<p><strong><a href="../auto_examples/cluster/plot_dbscan.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9997b300f697e018f955724f7106ad09.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9997b300f697e018f955724f7106ad09.jpg" alt="dbscan_results"></a></a></strong></p>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/cluster/plot_dbscan.html#sphx-glr-auto-examples-cluster-plot-dbscan-py">Demo of DBSCAN clustering algorithm</a></li>
</ul>
<p>实现</p>
<p>DBSCAN 算法是具有确定性的，当以相同的顺序给出相同的数据时总是形成相同的聚类。 然而，当以不同的顺序提供数据时聚类的结果可能不相同。首先，即使核心样本总是被 分配给相同的聚类，这些集群的标签将取决于数据中遇到这些样本的顺序。第二个更重 要的是，非核心样本的聚类可能因数据顺序而有所不同。 当一个非核心样本距离两个核心样本的距离都小于 <code>eps</code> 时，就会发生这种情况。 通过三角不等式可知，这两个核心样本距离一定大于 <code>eps</code> 或者处于同一个聚类中。 非核心样本将被非配到首先查找到改样本的类别，因此结果将取决于数据的顺序。</p>
<p>当前版本使用 ball trees 和 kd-trees 来确定领域，这样避免了计算全部的距离矩阵 （0.14 之前的 scikit-learn 版本计算全部的距离矩阵）。保留使用 custom metrics （自定义指标）的可能性。细节请参照 <code>NearestNeighbors</code>。</p>
<p>大量样本的内存消耗</p>
<p>默认的实现方式并没有充分利用内存，因为在不使用 kd-trees 或者 ball-trees 的情况下构建一个 完整的相似度矩阵（e.g. 使用稀疏矩阵）。这个矩阵将消耗 n^2 个浮点数。 解决这个问题的几种机制:</p>
<ul>
<li> <p>A sparse radius neighborhood graph （稀疏半径邻域图）(其中缺少条目被假定为距离超出eps) 可以以高效的方式预先编译，并且可以使用 <code>metric='precomputed'</code> 来运行 dbscan。</p> </li>
<li> <p>数据可以压缩，当数据中存在准确的重复时，可以删除这些重复的数据，或者使用BIRCH。 任何。然后仅需要使用相对少量的样本来表示大量的点。当训练DBSCAN时，可以提供一个</p> <p>&gt; <code>sample_weight</code> 参数。</p> </li>
</ul>
<p>引用:</p>
<ul>
<li>“A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise” Ester, M., H. P. Kriegel, J. Sander, and X. Xu, In Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, Portland, OR, AAAI Press, pp. 226–231. 1996</li>
</ul>
<h2 id="238-birch">2.3.8. Birch</h2>
<p>The <a href="generated/sklearn.cluster.Birch.html#sklearn.cluster.Birch" title="sklearn.cluster.Birch"><code>Birch</code></a> 为提供的数据构建一颗 Characteristic Feature Tree (CFT，聚类特征树)。 数据实质上是被有损压缩成一组 Characteristic Feature nodes (CF Nodes，聚类特征节点)。 CF Nodes 中有一部分子聚类被称为 Characteristic Feature subclusters (CF Subclusters)， 并且这些位于非终端位置的CF Subclusters 可以拥有 CF Nodes 作为孩子节点。</p>
<p>CF Subclusters 保存用于聚类的必要信息，防止将整个输入数据保存在内存中。 这些信息包括:</p>
<ul>
<li>Number of samples in a subcluster（子聚类中样本数）.</li>
<li>Linear Sum - A n-dimensional vector holding the sum of all samples（保存所有样本和的n维向量）</li>
<li>Squared Sum - Sum of the squared L2 norm of all samples（所有样本的L2 norm的平方和）.</li>
<li>Centroids - To avoid recalculation linear sum / n_samples（为了防止重复计算 linear sum / n_samples）.</li>
<li>Squared norm of the centroids（质心的 Squared norm ）.</li>
</ul>
<p>Birch 算法有两个参数，即 threshold （阈值）和 branching factor 分支因子。Branching factor （分支因子） 限制了一个节点中的子集群的数量 ，threshold （簇半径阈值）限制了新加入的样本和存在与现有子集群中样本的最大距离。</p>
<p>该算法可以视为将一个实例或者数据简化的方法，因为它将输入的数据简化到可以直接从CFT的叶子结点中获取的一组子聚类。 这种简化的数据可以通过将其馈送到global clusterer（全局聚类）来进一步处理。Global clusterer（全局聚类）可以 通过 [<code>](#id21)n_clusters</code>来设置。</p>
<p>如果 <code>n_clusters</code> 被设置为 None，将直接读取叶子结点中的子聚类，否则，global clustering（全局聚类） 将逐步标记他的 subclusters 到 global clusters (labels) 中，样本将被映射到 距离最近的子聚类的global label中。</p>
<p><strong>算法描述:</strong></p>
<ul>
<li>一个新的样本作为一个CF Node 被插入到 CF Tree 的根节点。然后将其合并到根节点的子聚类中去，使得合并后子聚类 拥有最小的半径，子聚类的选取受 threshold 和 branching factor 的约束。如果子聚类也拥有孩子节点，则重复执 行这个步骤直到到达叶子结点。在叶子结点中找到最近的子聚类以后，递归的更新这个子聚类及其父聚类的属性。</li>
<li>如果合并了新样本和最近的子聚类获得的子聚类半径大约square of the threshold（阈值的平方）， 并且子聚类的数量大于branching factor（分支因子），则将为这个样本分配一个临时空间。 最远的两个子聚类被选取，剩下的子聚类按照之间的距离分为两组作为被选取的两个子聚类的孩子节点。</li>
<li>If this split node has a parent subcluster and there is room for a new subcluster, then the parent is split into two. If there is no room, then this node is again split into two and the process is continued recursively, till it reaches the root. 如果拆分的节点有一个 parent subcluster ，并且有一个容纳一个新的子聚类的空间，那么父聚类拆分成两个。 如果没有空间容纳一个新的聚类，那么这个节点将被再次拆分，依次向上检查父节点是否需要分裂， 如果需要按叶子节点方式相同分裂。</li>
</ul>
<p><strong>Birch or MiniBatchKMeans?</strong></p>
<blockquote>
<ul>
<li>Birch 在高维数据上表现不好。一条经验法则，如果 <code>n_features</code> 大于20，通常使用 MiniBatchKMeans 更好。</li>
<li>如果需要减少数据实例的数量，或者如果需要大量的子聚类作为预处理步骤或者其他， Birch 比 MiniBatchKMeans 更有用。</li>
</ul>
</blockquote>
<p><strong>How to use partial_fit?</strong></p>
<p>为了避免对 global clustering 的计算，每次调用建议使用 <code>partial_fit</code> 。</p>
<blockquote>
<ol>
<li>初始化 <code>n_clusters=None</code> 。</li>
<li>通过多次调用 partial_fit 训练所以的数据。</li>
<li>设置 <code>n_clusters</code> 为所需值，通过使用 <code>brc.set_params(n_clusters=n_clusters)</code> 。</li>
<li>最后不需要参数调用 <code>partial_fit</code> , 例如 <code>brc.partial_fit()</code> 执行全局聚类。</li>
</ol>
</blockquote>
<p><a href="../auto_examples/cluster/plot_birch_vs_minibatchkmeans.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d9ac7cfff134bd66e853020e32d76f5c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d9ac7cfff134bd66e853020e32d76f5c.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_birch_vs_minibatchkmeans_0011.png"></a></a></p>
<p>参考:</p>
<ul>
<li>Tian Zhang, Raghu Ramakrishnan, Maron Livny BIRCH: An efficient data clustering method for large databases. <a href="http://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf">http://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf</a></li>
<li>Roberto Perdisci JBirch - Java implementation of BIRCH clustering algorithm <a href="https://code.google.com/archive/p/jbirch">https://code.google.com/archive/p/jbirch</a></li>
</ul>
<h2 id="239-聚类性能度量">2.3.9. 聚类性能度量</h2>
<p>度量聚类算法的性能不是简单的统计错误的数量或计算监督分类算法中的 precision （准确率）和 recall （召回率）。 特别地，任何 evaluation metric （度量指标）不应该考虑到 cluster labels （簇标签）的绝对值，而是如果这个簇定义类似于某些 ground truth set of classes 或者满足某些假设，使得属于同一个类的成员更类似于根据某些 similarity metric （相似性度量）的不同类的成员。</p>
<h3 id="2391-调整后的-rand-指数">2.3.9.1. 调整后的 Rand 指数</h3>
<p>考虑到 the ground truth class 赋值 <code>labels_true</code> 和相同样本 <code>labels_pred</code> 的聚类算法分配的知识，<strong>adjusted Rand index</strong> 是一个函数，用于测量两个 assignments （任务）的 <strong>similarity（相似度）</strong> ，忽略 permutations （排列）和 <strong>with chance normalization（使用机会规范化）</strong>:</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn import metrics
&gt;&gt;&gt; labels_true = [0, 0, 0, 1, 1, 1]
&gt;&gt;&gt; labels_pred = [0, 0, 1, 1, 2, 2]

&gt;&gt;&gt; metrics.adjusted_rand_score(labels_true, labels_pred)  
0.24...

</code></pre>
<p>可以在预测的标签中 permute （排列） 0 和 1，重命名为 2 到 3， 得到相同的分数:</p>
<pre><code class="language-py">&gt;&gt;&gt; labels_pred = [1, 1, 0, 0, 3, 3]
&gt;&gt;&gt; metrics.adjusted_rand_score(labels_true, labels_pred)  
0.24...

</code></pre>
<p>另外， <a href="generated/sklearn.metrics.adjusted_rand_score.html#sklearn.metrics.adjusted_rand_score" title="sklearn.metrics.adjusted_rand_score"><code>adjusted_rand_score</code></a> 是 <strong>symmetric（对称的）</strong> : 交换参数不会改变 score （得分）。它可以作为 <strong>consensus measure（共识度量）</strong>:</p>
<pre><code class="language-py">&gt;&gt;&gt; metrics.adjusted_rand_score(labels_pred, labels_true)  
0.24...

</code></pre>
<p>完美的标签得分为 1.0</p>
<pre><code class="language-py">&gt;&gt;&gt; labels_pred = labels_true[:]
&gt;&gt;&gt; metrics.adjusted_rand_score(labels_true, labels_pred)
1.0

</code></pre>
<p>坏 (e.g. independent labelings（独立标签）) 有负数 或 接近于 0.0 分:</p>
<pre><code class="language-py">&gt;&gt;&gt; labels_true = [0, 1, 2, 0, 3, 4, 5, 1]
&gt;&gt;&gt; labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]
&gt;&gt;&gt; metrics.adjusted_rand_score(labels_true, labels_pred)  
-0.12...

</code></pre>
<h4 id="23911-优点">2.3.9.1.1. 优点</h4>
<ul>
<li><strong>Random (uniform) label assignments have a ARI score close to 0.0（随机（统一）标签分配的 ARI 评分接近于 0.0）</strong> 对于 <code>n_clusters</code> 和 <code>n_samples</code> 的任何值（这不是原始的 Rand index 或者 V-measure 的情况）。</li>
<li><strong>Bounded range（范围是有界的） [-1, 1]</strong>: negative values （负值）是坏的 (独立性标签), 类似的聚类有一个 positive ARI （正的 ARI）， 1.0 是完美的匹配得分。</li>
<li><strong>No assumption is made on the cluster structure（对簇的结构不需作出任何假设）</strong>: 可以用于比较聚类算法，例如 k-means，其假定 isotropic blob shapes 与可以找到具有 “folded” shapes 的聚类的 spectral clustering algorithms（谱聚类算法）的结果。</li>
</ul>
<h4 id="23912-缺点">2.3.9.1.2. 缺点</h4>
<ul>
<li> <p>与 inertia 相反，<strong>ARI requires knowledge of the ground truth classes（ARI 需要 ground truth classes 的相关知识）</strong> ，而在实践中几乎不可用，或者需要人工标注者手动分配（如在监督学习环境中）。</p> <p>然而，ARI 还可以在 purely unsupervised setting （纯粹无监督的设置中）作为可用于 聚类模型选择（TODO）的共识索引的构建块。</p> </li>
</ul>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/cluster/plot_adjusted_for_chance_measures.html#sphx-glr-auto-examples-cluster-plot-adjusted-for-chance-measures-py">Adjustment for chance in clustering performance evaluation</a>: 分析数据集大小对随机分配聚类度量值的影响。</li>
</ul>
<h4 id="23913-数学表达">2.3.9.1.3. 数学表达</h4>
<p>如果 C 是一个 ground truth class assignment（任务）， K 是簇的个数，我们定义 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/578c95150175e4efdf851fe66d503079.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/578c95150175e4efdf851fe66d503079.jpg" alt="a"></a> 和 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6ae91fb0f3221b92d2dd4e22204d8008.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6ae91fb0f3221b92d2dd4e22204d8008.jpg" alt="b"></a> 如:</p>
<ul>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/578c95150175e4efdf851fe66d503079.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/578c95150175e4efdf851fe66d503079.jpg" alt="a"></a>, 在 C 中的相同集合的与 K 中的相同集合中的元素的对数</li>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6ae91fb0f3221b92d2dd4e22204d8008.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6ae91fb0f3221b92d2dd4e22204d8008.jpg" alt="b"></a>, 在 C 中的不同集合与 K 中的不同集合中的元素的对数</li>
</ul>
<p>原始（未经调整）的 Rand index 则由下式给出:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/070018458bf56c0d94293de45828e878.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/070018458bf56c0d94293de45828e878.jpg" alt="\text{RI} = \frac{a + b}{C_2^{n_{samples}}}"></a></p>
<p>其中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0ce3ae4e9a8bbd17b08f5fae78d60f21.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0ce3ae4e9a8bbd17b08f5fae78d60f21.jpg" alt="C_2^{n_{samples}}"></a> 是数据集中可能的 pairs （数据对）的总数（不排序）。</p>
<p>然而，RI 评分不能保证 random label assignments （随机标签任务）将获得接近零的值（特别是如果簇的数量与采样数量相同的数量级）。</p>
<p>为了抵消这种影响，我们可以通过定义 adjusted Rand index （调整后的 Rand index）来 discount（折现） 随机标签的预期 RI <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7881dd425f1090aadc25eca46dc0daec.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7881dd425f1090aadc25eca46dc0daec.jpg" alt="E\text{RI}"></a> ,如下所示:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8f4f76678eb50ebccaba25e86961ff3e.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8f4f76678eb50ebccaba25e86961ff3e.jpg" alt="\text{ARI} = \frac{\text{RI} - E\text{RI}}{\max(\text{RI}) - E\text{RI}}"></a></p>
<p>参考</p>
<ul>
<li><a href="http://link.springer.com/article/10.1007%2FBF01908075">Comparing Partitions</a> L. Hubert and P. Arabie, Journal of Classification 1985</li>
<li><a href="https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index">Wikipedia entry for the adjusted Rand index</a></li>
</ul>
<h3 id="2392-基于-mutual-information-互信息的分数">2.3.9.2. 基于 Mutual Information （互信息）的分数</h3>
<p>考虑到 ground truth class assignments （标定过的真实数据类分配） <code>labels_true</code> 的知识和相同样本 <code>labels_pred</code> 的聚类算法分配， <strong>Mutual Information</strong> 是测量两者 <strong>agreement</strong> 分配的函数，忽略 permutations（排列）。 这种测量方案的两个不同的标准化版本可用，<strong>Normalized Mutual Information(NMI)</strong> 和 <strong>Adjusted Mutual Information(AMI)</strong>。NMI 经常在文献中使用，而 AMI 最近被提出，并且 <strong>normalized against chance</strong>:</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn import metrics
&gt;&gt;&gt; labels_true = [0, 0, 0, 1, 1, 1]
&gt;&gt;&gt; labels_pred = [0, 0, 1, 1, 2, 2]

&gt;&gt;&gt; metrics.adjusted_mutual_info_score(labels_true, labels_pred)  
0.22504...

</code></pre>
<p>可以在 predicted labels （预测的标签）中 permute （排列） 0 和 1, 重命名为 2 到 3 并得到相同的得分:</p>
<pre><code class="language-py">&gt;&gt;&gt; labels_pred = [1, 1, 0, 0, 3, 3]
&gt;&gt;&gt; metrics.adjusted_mutual_info_score(labels_true, labels_pred)  
0.22504...

</code></pre>
<p>全部的，<a href="generated/sklearn.metrics.mutual_info_score.html#sklearn.metrics.mutual_info_score" title="sklearn.metrics.mutual_info_score"><code>mutual_info_score</code></a>, <a href="generated/sklearn.metrics.adjusted_mutual_info_score.html#sklearn.metrics.adjusted_mutual_info_score" title="sklearn.metrics.adjusted_mutual_info_score"><code>adjusted_mutual_info_score</code></a> 和 <a href="generated/sklearn.metrics.normalized_mutual_info_score.html#sklearn.metrics.normalized_mutual_info_score" title="sklearn.metrics.normalized_mutual_info_score"><code>normalized_mutual_info_score</code></a> 是 symmetric（对称的）: 交换参数不会更改分数。因此，它们可以用作 <strong>consensus measure</strong>:</p>
<pre><code class="language-py">&gt;&gt;&gt; metrics.adjusted_mutual_info_score(labels_pred, labels_true)  
0.22504...

</code></pre>
<p>完美标签得分是 1.0:</p>
<pre><code class="language-py">&gt;&gt;&gt; labels_pred = labels_true[:]
&gt;&gt;&gt; metrics.adjusted_mutual_info_score(labels_true, labels_pred)
1.0

&gt;&gt;&gt; metrics.normalized_mutual_info_score(labels_true, labels_pred)
1.0

</code></pre>
<p>这对于 <code>mutual_info_score</code> 是不正确的，因此更难判断:</p>
<pre><code class="language-py">&gt;&gt;&gt; metrics.mutual_info_score(labels_true, labels_pred)  
0.69...

</code></pre>
<p>坏的 (例如 independent labelings（独立标签）) 具有非正分数:</p>
<pre><code class="language-py">&gt;&gt;&gt; labels_true = [0, 1, 2, 0, 3, 4, 5, 1]
&gt;&gt;&gt; labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]
&gt;&gt;&gt; metrics.adjusted_mutual_info_score(labels_true, labels_pred)  
-0.10526...

</code></pre>
<h4 id="23921-优点">2.3.9.2.1. 优点</h4>
<ul>
<li><strong>Random (uniform) label assignments have a AMI score close to 0.0（随机（统一）标签分配的AMI评分接近0.0）</strong> 对于 <code>n_clusters</code> 和 <code>n_samples</code> 的任何值（这不是原始 Mutual Information 或者 V-measure 的情况）。</li>
<li><strong>Bounded range（有界范围） [0, 1]</strong>: 接近 0 的值表示两个主要独立的标签分配，而接近 1 的值表示重要的一致性。此外，正好 0 的值表示 <strong>purely（纯粹）</strong> 独立标签分配，正好为 1 的 AMI 表示两个标签分配相等（有或者没有 permutation）。</li>
<li><strong>No assumption is made on the cluster structure（对簇的结构没有作出任何假设）</strong>: 可以用于比较聚类算法，例如 k-means，其假定 isotropic blob shapes 与可以找到具有 “folded” shapes 的聚类的 spectral clustering algorithms （频谱聚类算法）的结果。</li>
</ul>
<h4 id="23922-缺点">2.3.9.2.2. 缺点</h4>
<ul>
<li> <p>与 inertia 相反，<strong>MI-based measures require the knowledge of the ground truth classes（MI-based measures 需要了解 ground truth classes）</strong> ，而在实践中几乎不可用，或者需要人工标注或手动分配（如在监督学习环境中）。</p> <p>然而，基于 MI-based measures （基于 MI 的测量方式）也可用于纯无人监控的设置，作为可用于聚类模型选择的 Consensus Index （共识索引）的构建块。</p> </li>
<li> <p>NMI 和 MI 没有调整机会。</p> </li>
</ul>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/cluster/plot_adjusted_for_chance_measures.html#sphx-glr-auto-examples-cluster-plot-adjusted-for-chance-measures-py">Adjustment for chance in clustering performance evaluation</a>: 分析数据集大小对随机分配聚类度量值的影响。 此示例还包括 Adjusted Rand Index。</li>
</ul>
<h4 id="23923-数学公式">2.3.9.2.3. 数学公式</h4>
<p>假设两个标签分配（相同的 N 个对象），<a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/11c00539ec3e5944afd76511830591db.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/11c00539ec3e5944afd76511830591db.jpg" alt="U"></a> 和 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5303ecbc70bf5189b8785555c03c54ee.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5303ecbc70bf5189b8785555c03c54ee.jpg" alt="V"></a>。 它们的 entropy （熵）是一个 partition set （分区集合）的不确定性量，定义如下:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/07610ee9d3a524eb0a3fb7ae409614c1.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/07610ee9d3a524eb0a3fb7ae409614c1.jpg" alt="H(U) = - \sum_{i=1}^{|U|}P(i)\log(P(i))"></a></p>
<p>其中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/18906a7fe0c5d78e0a291e472ded58ce.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/18906a7fe0c5d78e0a291e472ded58ce.jpg" alt="P(i) = |U_i| / N"></a> 是从 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/11c00539ec3e5944afd76511830591db.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/11c00539ec3e5944afd76511830591db.jpg" alt="U"></a> 中随机选取的对象到类 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/59100a001bb4b110e00f7ddf1354cd5b.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/59100a001bb4b110e00f7ddf1354cd5b.jpg" alt="U_i"></a> 的概率。同样对于 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5303ecbc70bf5189b8785555c03c54ee.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5303ecbc70bf5189b8785555c03c54ee.jpg" alt="V"></a>:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b4e752f6314fe52f8c066964d26145a8.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b4e752f6314fe52f8c066964d26145a8.jpg" alt="H(V) = - \sum_{j=1}^{|V|}P'(j)\log(P'(j))"></a></p>
<p>使用 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e270fdc1fb7cabab295d31d189d77258.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e270fdc1fb7cabab295d31d189d77258.jpg" alt="P'(j) = |V_j| / N"></a>. <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/11c00539ec3e5944afd76511830591db.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/11c00539ec3e5944afd76511830591db.jpg" alt="U"></a> 和 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5303ecbc70bf5189b8785555c03c54ee.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5303ecbc70bf5189b8785555c03c54ee.jpg" alt="V"></a> 之间的 mutual information (MI) 由下式计算:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/352bc5f9f9d6aefcdaf8deca4f7964ff.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/352bc5f9f9d6aefcdaf8deca4f7964ff.jpg" alt="\text{MI}(U, V) = \sum_{i=1}{|U|}\sum_{j=1}{|V|}P(i, j)\log\left(\frac{P(i,j)}{P(i)P'(j)}\right)"></a></p>
<p>其中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b67ce2997477f658a6a39026c01e07c4.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b67ce2997477f658a6a39026c01e07c4.jpg" alt="P(i, j) = |U_i \cap V_j| / N"></a> 是随机选择的对象落入两个类的概率 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/59100a001bb4b110e00f7ddf1354cd5b.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/59100a001bb4b110e00f7ddf1354cd5b.jpg" alt="U_i"></a> 和 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/22f3a10ad9acceb77ea6193f945b11cf.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/22f3a10ad9acceb77ea6193f945b11cf.jpg" alt="V_j"></a> 。</p>
<p>也可以用设定的基数表达式表示:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/170bd587959dabf132e4e0f39fa0a7b7.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/170bd587959dabf132e4e0f39fa0a7b7.jpg" alt="\text{MI}(U, V) = \sum_{i=1}{|U|} \sum_{j=1}{|V|} \frac{|U_i \cap V_j|}{N}\log\left(\frac{N|U_i \cap V_j|}{|U_i||V_j|}\right)"></a></p>
<p>normalized (归一化) mutual information 被定义为</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7695a05e60c9dc0ec13f779fc19da966.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7695a05e60c9dc0ec13f779fc19da966.jpg" alt="\text{NMI}(U, V) = \frac{\text{MI}(U, V)}{\sqrt{H(U)H(V)}}"></a></p>
<p>mutual information 的价值以及 normalized variant （标准化变量）的值不会因 chance （机会）而被调整，随着不同标签（clusters（簇））的数量的增加，不管标签分配之间的 “mutual information” 的实际数量如何，都会趋向于增加。</p>
<p>mutual information 的期望值可以用 Vinh, Epps 和 Bailey,(2009) 的以下公式来计算。在这个方程式中, <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f3893160388ee4203c313659d729cef0.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f3893160388ee4203c313659d729cef0.jpg" alt="a_i = |U_i|"></a> (<a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/59100a001bb4b110e00f7ddf1354cd5b.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/59100a001bb4b110e00f7ddf1354cd5b.jpg" alt="U_i"></a> 中元素的数量) 和 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e2bd3aaa1586d4d17301f7fe016eefd7.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e2bd3aaa1586d4d17301f7fe016eefd7.jpg" alt="b_j = |V_j|"></a> (<a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/22f3a10ad9acceb77ea6193f945b11cf.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/22f3a10ad9acceb77ea6193f945b11cf.jpg" alt="V_j"></a> 中元素的数量).</p>
<pre><code class="language-py">
![E[\text{MI}(U,V)]=\sum_{i=1}^|U| \sum_{j=1}^|V| \sum_{n_{ij}=(a_i+b_j-N)^+
}^{\min(a_i, b_j)} \frac{n_{ij}}{N}\log \left( \frac{ N.n_{ij}}{a_i b_j}\right)
\frac{a_i!b_j!(N-a_i)!(N-b_j)!}{N!n_{ij}!(a_i-n_{ij})!(b_j-n_{ij})!
(N-a_i-b_j+n_{ij})!}](img/942734d190e4b1d2c51b0e2ee6c24428.jpg)

</code></pre>
<p>使用期望值, 然后可以使用与 adjusted Rand index 相似的形式来计算调整后的 mutual information:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/17689bafe240fb42feab1cca674b5b88.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/17689bafe240fb42feab1cca674b5b88.jpg" alt="\text{AMI} = \frac{\text{MI} - E\text{MI}}{\max(H(U), H(V)) - E\text{MI}}"></a></p>
<p>参考</p>
<ul>
<li>Strehl, Alexander, and Joydeep Ghosh (2002). “Cluster ensembles – a knowledge reuse framework for combining multiple partitions”. Journal of Machine Learning Research 3: 583–617. <a href="http://strehl.com/download/strehl-jmlr02.pdf">doi:10.1162/153244303321897735</a>.</li>
<li>Vinh, Epps, and Bailey, (2009). “Information theoretic measures for clusterings comparison”. Proceedings of the 26th Annual International Conference on Machine Learning - ICML ‘09. <a href="https://dl.acm.org/citation.cfm?doid=1553374.1553511">doi:10.1145/1553374.1553511</a>. ISBN 9781605585161.</li>
<li>Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance, JMLR <a href="http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf">http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf</a></li>
<li><a href="https://en.wikipedia.org/wiki/Mutual_Information">Wikipedia entry for the (normalized) Mutual Information</a></li>
<li><a href="https://en.wikipedia.org/wiki/Adjusted_Mutual_Information">Wikipedia entry for the Adjusted Mutual Information</a></li>
</ul>
<h3 id="2393-同质性完整性和-v-measure">2.3.9.3. 同质性，完整性和 V-measure</h3>
<p>鉴于样本的 ground truth class assignments （标定过的真实数据类分配）的知识，可以使用 conditional entropy （条件熵）分析来定义一些 intuitive metric（直观的度量）。</p>
<p>特别是 Rosenberg 和 Hirschberg (2007) 为任何 cluster （簇）分配定义了以下两个理想的目标:</p>
<ul>
<li><strong>homogeneity(同质性)</strong>: 每个簇只包含一个类的成员</li>
<li><strong>completeness(完整性)</strong>: 给定类的所有成员都分配给同一个簇。</li>
</ul>
<p>我们可以把这些概念作为分数 <a href="generated/sklearn.metrics.homogeneity_score.html#sklearn.metrics.homogeneity_score" title="sklearn.metrics.homogeneity_score"><code>homogeneity_score</code></a> 和 <a href="generated/sklearn.metrics.completeness_score.html#sklearn.metrics.completeness_score" title="sklearn.metrics.completeness_score"><code>completeness_score</code></a> 。两者均在 0.0 以下 和 1.0 以上（越高越好）:</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn import metrics
&gt;&gt;&gt; labels_true = [0, 0, 0, 1, 1, 1]
&gt;&gt;&gt; labels_pred = [0, 0, 1, 1, 2, 2]

&gt;&gt;&gt; metrics.homogeneity_score(labels_true, labels_pred)  
0.66...

&gt;&gt;&gt; metrics.completeness_score(labels_true, labels_pred) 
0.42...

</code></pre>
<p>称为 <strong>V-measure</strong> 的 harmonic mean 由以下函数计算 <a href="generated/sklearn.metrics.v_measure_score.html#sklearn.metrics.v_measure_score" title="sklearn.metrics.v_measure_score"><code>v_measure_score</code></a>:</p>
<pre><code class="language-py">&gt;&gt;&gt; metrics.v_measure_score(labels_true, labels_pred)    
0.51...

</code></pre>
<p>V-measure 实际上等于上面讨论的 mutual information (NMI) 由 label entropies <a href="B2011">B2011</a>(#b2011) （标准熵 <a href="B2011">B2011</a>(#b2011)） 的总和 normalized （归一化）。</p>
<p>Homogeneity（同质性）, completeness（完整性） and V-measure 可以立即计算 <a href="generated/sklearn.metrics.homogeneity_completeness_v_measure.html#sklearn.metrics.homogeneity_completeness_v_measure" title="sklearn.metrics.homogeneity_completeness_v_measure"><code>homogeneity_completeness_v_measure</code></a> 如下:</p>
<pre><code class="language-py">&gt;&gt;&gt; metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)
...                                                      
(0.66..., 0.42..., 0.51...)

</code></pre>
<p>以下聚类分配稍微好一些，因为它是同构但不完整的:</p>
<pre><code class="language-py">&gt;&gt;&gt; labels_pred = [0, 0, 0, 1, 2, 2]
&gt;&gt;&gt; metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)
...                                                      
(1.0, 0.68..., 0.81...)

</code></pre>
<p>Note</p>
<p><a href="generated/sklearn.metrics.v_measure_score.html#sklearn.metrics.v_measure_score" title="sklearn.metrics.v_measure_score"><code>v_measure_score</code></a> 是 <strong>symmetric（对称的）</strong>: 它可以用于评估同一数据集上两个 independent assignments （独立赋值）的 <strong>agreement（协议）</strong>。</p>
<p>这不是这样的 <a href="generated/sklearn.metrics.completeness_score.html#sklearn.metrics.completeness_score" title="sklearn.metrics.completeness_score"><code>completeness_score</code></a> 和 <a href="generated/sklearn.metrics.homogeneity_score.html#sklearn.metrics.homogeneity_score" title="sklearn.metrics.homogeneity_score"><code>homogeneity_score</code></a>: 两者的关系是被这样约束着:</p>
<pre><code class="language-py">homogeneity_score(a, b) == completeness_score(b, a)

</code></pre>
<h4 id="23931-优点">2.3.9.3.1. 优点</h4>
<ul>
<li><strong>Bounded scores（分数是有界的）</strong>: 0.0 是最坏的, 1.0 是一个完美的分数.</li>
<li>Intuitive interpretation（直观解释）: 具有不良 V-measure 的聚类可以在 <strong>qualitatively analyzed in terms of homogeneity and completeness（在同质性和完整性方面进行定性分析）</strong> 以更好地感知到作业完成的错误类型。</li>
<li><strong>No assumption is made on the cluster structure（对簇的结构没有作出任何假设）</strong>: 可以用于比较聚类算法，例如 k-means ，其假定 isotropic blob shapes 与可以找到具有 “folded” shapes 的聚类的 spectral clustering algorithms （频谱聚类算法）的结果。</li>
</ul>
<h4 id="23932-缺点">2.3.9.3.2. 缺点</h4>
<ul>
<li> <p>以前引入的 metrics （度量标准）<strong>not normalized with regards to random labeling（并不是随机标记的标准化的）</strong>: 这意味着，根据 number of samples （样本数量），clusters （簇）和 ground truth classes （标定过的真实数据类），完全随机的标签并不总是产生 homogeneity （同质性），completeness（完整性）和 hence v-measure 的相同值。特别是 <strong>random labeling won’t yield zero scores especially when the number of clusters is large（随机标记不会产生零分，特别是当集群数量大时）</strong>。</p> <p>当样本数量超过 1000，簇的数量小于 10 时，可以安全地忽略此问题。<strong>For smaller sample sizes or larger number of clusters it is safer to use an adjusted index such as the Adjusted Rand Index (ARI)（对于较小的样本数量或者较大数量的簇，使用 adjusted index 例如 Adjusted Rand Index (ARI)）</strong>。</p> </li>
</ul>
<p><a href="../auto_examples/cluster/plot_adjusted_for_chance_measures.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/77e9cd089beb314666ac8397f95afc0a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/77e9cd089beb314666ac8397f95afc0a.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_adjusted_for_chance_measures_0011.png"></a></a></p>
<ul>
<li>这些 metrics （指标） <strong>require the knowledge of the ground truth classes（需要标定过的真实数据类的知识）</strong>，而在实践中几乎不可用，或需要人工标注来人工分配（如在受监督的学习环境中）。</li>
</ul>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/cluster/plot_adjusted_for_chance_measures.html#sphx-glr-auto-examples-cluster-plot-adjusted-for-chance-measures-py">Adjustment for chance in clustering performance evaluation</a>: 分析数据集大小对随机分配聚类度量值的影响。</li>
</ul>
<h4 id="23933-数学表达">2.3.9.3.3. 数学表达</h4>
<p>Homogeneity（同质性） 和 completeness（完整性） 的得分由下面公式给出:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/148aed7690723555d32f36019c3d6948.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/148aed7690723555d32f36019c3d6948.jpg" alt="h = 1 - \frac{H(C|K)}{H(C)}"></a></p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b714492d7f23932738745c4ed05fe7ae.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b714492d7f23932738745c4ed05fe7ae.jpg" alt="c = 1 - \frac{H(K|C)}{H(K)}"></a></p>
<p>其中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c9f28da3986a32d6c1421f357d52b9fa.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c9f28da3986a32d6c1421f357d52b9fa.jpg" alt="H(C|K)"></a> 是 <strong>给定簇分配的类的 conditional entropy （条件熵）</strong> ，由下式给出:</p>
<pre><code class="language-py">
![H(C|K) = - \sum_{c=1}^{|C|} \sum_{k=1}^{|K|} \frac{n_{c,k}}{n}
\cdot \log\left(\frac{n_{c,k}}{n_k}\right)](img/e18ade3134bef595ea6ddf488ff9557a.jpg)

</code></pre>
<p>并且 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/be4190a760361bd7ae65c77218465778.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/be4190a760361bd7ae65c77218465778.jpg" alt="H(C)"></a> 是 <strong>entropy of the classes（类的熵）</strong>，并且由下式给出:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8c43dd6816e66709ef3f9d681ec3941a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8c43dd6816e66709ef3f9d681ec3941a.jpg" alt="H(C) = - \sum_{c=1}^{|C|} \frac{n_c}{n} \cdot \log\left(\frac{n_c}{n}\right)"></a></p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c87d9110f3d32ffa5fa08671e4af11fb.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c87d9110f3d32ffa5fa08671e4af11fb.jpg" alt="n"></a> 个样本总数， <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/21e9f42c5b6730d593e37a11c6ffb13a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/21e9f42c5b6730d593e37a11c6ffb13a.jpg" alt="n_c"></a> 和 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6042b714de932f6ed841e71bfe9acede.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6042b714de932f6ed841e71bfe9acede.jpg" alt="n_k"></a> 分别属于 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d5c9a11453ea30a1be50a1034052bd6b.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d5c9a11453ea30a1be50a1034052bd6b.jpg" alt="c"></a> 类和簇 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> 的样本数，最后 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0acf1512409eb0a9a90102698304fd52.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0acf1512409eb0a9a90102698304fd52.jpg" alt="n_{c,k}"></a> 分配给簇 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> 的类 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d5c9a11453ea30a1be50a1034052bd6b.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d5c9a11453ea30a1be50a1034052bd6b.jpg" alt="c"></a> 的样本数。</p>
<p><strong>conditional entropy of clusters given class（给定类的条件熵）</strong> <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/05588cdc4e82289930a92b0097f67d2d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/05588cdc4e82289930a92b0097f67d2d.jpg" alt="H(K|C)"></a> 和 <strong>entropy of clusters（类的熵）</strong> <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5062c88fba7988fa39aca3bc91857721.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5062c88fba7988fa39aca3bc91857721.jpg" alt="H(K)"></a> 以 symmetric manner （对称方式）定义。</p>
<p>Rosenberg 和 Hirschberg 进一步定义 <strong>V-measure</strong> 作为 <strong>harmonic mean of homogeneity and completeness（同质性和完整性的 harmonic mean）</strong>:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/611639bdcfd73c857a43842913d6e826.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/611639bdcfd73c857a43842913d6e826.jpg" alt="v = 2 \cdot \frac{h \cdot c}{h + c}"></a></p>
<p>参考</p>
<ul>
<li><a href="http://aclweb.org/anthology/D/D07/D07-1043.pdf">V-Measure: A conditional entropy-based external cluster evaluation measure</a> Andrew Rosenberg and Julia Hirschberg, 2007</li>
</ul>
<p>| [B2011] | <em>(<a href="#id30">1</a>, <a href="#id31">2</a>)</em> <a href="http://www.cs.columbia.edu/~hila/hila-thesis-distributed.pdf">Identication and Characterization of Events in Social Media</a>, Hila Becker, PhD Thesis. |</p>
<h3 id="2394-fowlkes-mallows-分数">2.3.9.4. Fowlkes-Mallows 分数</h3>
<p>当样本的已标定的真实数据的类别分配已知时，可以使用 Fowlkes-Mallows index （Fowlkes-Mallows 指数）(<a href="generated/sklearn.metrics.fowlkes_mallows_score.html#sklearn.metrics.fowlkes_mallows_score" title="sklearn.metrics.fowlkes_mallows_score"><code>sklearn.metrics.fowlkes_mallows_score</code></a>) 。Fowlkes-Mallows 分数 FMI 被定义为 geometric mean of the pairwise precision （成对的准确率）和 recall （召回率）的几何平均值:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/403595258114953d3411fd1bfbf335f8.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/403595258114953d3411fd1bfbf335f8.jpg" alt="\text{FMI} = \frac{\text{TP}}{\sqrt{(\text{TP} + \text{FP}) (\text{TP} + \text{FN})}}"></a></p>
<p>其中的 <code>TP</code> 是 <strong>True Positive（真正例）</strong> 的数量（即，真实标签和预测标签中属于相同簇的点对数），<code>FP</code> 是 <strong>False Positive（假正例）</strong> （即，在真实标签中属于同一簇的点对数，而不在预测标签中），<code>FN</code> 是 <strong>False Negative（假负例）</strong> 的数量（即，预测标签中属于同一簇的点对数，而不在真实标签中）。</p>
<p>score （分数）范围为 0 到 1。较高的值表示两个簇之间的良好相似性。</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn import metrics
&gt;&gt;&gt; labels_true = [0, 0, 0, 1, 1, 1]
&gt;&gt;&gt; labels_pred = [0, 0, 1, 1, 2, 2]

</code></pre>
<pre><code class="language-py">&gt;&gt;&gt; metrics.fowlkes_mallows_score(labels_true, labels_pred)  
0.47140...

</code></pre>
<p>可以在 predicted labels （预测的标签）中 permute （排列） 0 和 1 ，重命名为 2 到 3 并得到相同的得分:</p>
<pre><code class="language-py">&gt;&gt;&gt; labels_pred = [1, 1, 0, 0, 3, 3]

&gt;&gt;&gt; metrics.fowlkes_mallows_score(labels_true, labels_pred)  
0.47140...

</code></pre>
<p>完美的标签得分是 1.0:</p>
<pre><code class="language-py">&gt;&gt;&gt; labels_pred = labels_true[:]
&gt;&gt;&gt; metrics.fowlkes_mallows_score(labels_true, labels_pred)  
1.0

</code></pre>
<p>坏的（例如 independent labelings （独立标签））的标签得分为 0:</p>
<pre><code class="language-py">&gt;&gt;&gt; labels_true = [0, 1, 2, 0, 3, 4, 5, 1]
&gt;&gt;&gt; labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]
&gt;&gt;&gt; metrics.fowlkes_mallows_score(labels_true, labels_pred)  
0.0

</code></pre>
<h4 id="23941-优点">2.3.9.4.1. 优点</h4>
<ul>
<li><strong>Random (uniform) label assignments have a FMI score close to 0.0（随机（统一）标签分配 FMI 得分接近于 0.0）</strong> 对于 <code>n_clusters</code> 和 <code>n_samples</code> 的任何值（对于原始 Mutual Information 或例如 V-measure 而言）。</li>
<li><strong>Bounded range（有界范围） [0, 1]</strong>: 接近于 0 的值表示两个标签分配在很大程度上是独立的，而接近于 1 的值表示 significant agreement 。此外，正好为 0 的值表示 <strong>purely</strong> 独立标签分配，正好为 1 的 AMI 表示两个标签分配相等（有或者没有 permutation （排列））。</li>
<li><strong>No assumption is made on the cluster structure（对簇的结构没有作出任何假设）</strong>: 可以用于比较诸如 k-means 的聚类算法，其将假设 isotropic blob shapes 与能够找到具有 “folded” shapes 的簇的 spectral clustering algorithms （频谱聚类算法）的结果相结合。</li>
</ul>
<h4 id="23942-缺点">2.3.9.4.2. 缺点</h4>
<ul>
<li>与 inertia（习惯）相反，<strong>FMI-based measures require the knowledge of the ground truth classes（基于 FMI 的测量方案需要了解已标注的真是数据的类）</strong> ，而几乎不用于实践和需要人工标注者的手动任务（如在监督学习的学习环境中）。</li>
</ul>
<p>参考</p>
<ul>
<li>E. B. Fowkles and C. L. Mallows, 1983. “A method for comparing two hierarchical clusterings”. Journal of the American Statistical Association. <a href="http://wildfire.stat.ucla.edu/pdflibrary/fowlkes.pdf">http://wildfire.stat.ucla.edu/pdflibrary/fowlkes.pdf</a></li>
<li><a href="https://en.wikipedia.org/wiki/Fowlkes-Mallows_index">Wikipedia entry for the Fowlkes-Mallows Index</a></li>
</ul>
<h3 id="2395-silhouette-系数">2.3.9.5. Silhouette 系数</h3>
<p>如果标注过的真实数据的标签不知道，则必须使用模型本身进行度量。Silhouette Coefficient (<a href="generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score" title="sklearn.metrics.silhouette_score"><code>sklearn.metrics.silhouette_score</code></a>) 是一个这样的评估的例子，其中较高的 Silhouette Coefficient 得分与具有更好定义的聚类的模型相关。Silhouette Coefficient 是为每个样本定义的，由两个得分组成:</p>
<ul>
<li><strong>a</strong>: 样本与同一类别中所有其他点之间的平均距离。</li>
<li><strong>b</strong>: 样本与 <em>下一个距离最近的簇</em> 中的所有其他点之间的平均距离。</li>
</ul>
<p>然后将单个样本的 Silhouette 系数 <em>s</em> 给出为:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8f839ebe5b506fef19bd8cc121b3f557.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8f839ebe5b506fef19bd8cc121b3f557.jpg" alt="s = \frac{b - a}{max(a, b)}"></a></p>
<p>给定一组样本的 Silhouette 系数作为每个样本的 Silhouette 系数的平均值。</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn import metrics
&gt;&gt;&gt; from sklearn.metrics import pairwise_distances
&gt;&gt;&gt; from sklearn import datasets
&gt;&gt;&gt; dataset = datasets.load_iris()
&gt;&gt;&gt; X = dataset.data
&gt;&gt;&gt; y = dataset.target

</code></pre>
<p>在正常使用情况下，将 Silhouette 系数应用于聚类分析的结果。</p>
<pre><code class="language-py">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.cluster import KMeans
&gt;&gt;&gt; kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)
&gt;&gt;&gt; labels = kmeans_model.labels_
&gt;&gt;&gt; metrics.silhouette_score(X, labels, metric='euclidean')
...                                                      
0.55...

</code></pre>
<p>参考</p>
<ul>
<li>Peter J. Rousseeuw (1987). “Silhouettes: a Graphical Aid to the Interpretation and Validation of Cluster Analysis”. Computational and Applied Mathematics 20: 53–65. <a href="http://dx.doi.org/10.1016/0377-0427(87)90125-7">doi:10.1016/0377-0427(87)90125-7</a>.</li>
</ul>
<h4 id="23951-优点">2.3.9.5.1. 优点</h4>
<ul>
<li>对于不正确的 clustering （聚类），分数为 -1 ， highly dense clustering （高密度聚类）为 +1 。零点附近的分数表示 overlapping clusters （重叠的聚类）。</li>
<li>当 clusters （簇）密集且分离较好时，分数更高，这与 cluster （簇）的标准概念有关。</li>
</ul>
<h4 id="23952-缺点">2.3.9.5.2. 缺点</h4>
<ul>
<li>convex clusters（凸的簇）的 Silhouette Coefficient 通常比其他类型的 cluster （簇）更高，例如通过 DBSCAN 获得的基于密度的 cluster（簇）。</li>
</ul>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py">Selecting the number of clusters with silhouette analysis on KMeans clustering</a> : 在这个例子中，silhouette 分析用于为 n_clusters 选择最佳值.</li>
</ul>
<h3 id="2396-calinski-harabaz-指数">2.3.9.6. Calinski-Harabaz 指数</h3>
<p>如果不知道真实数据的类别标签，则可以使用 Calinski-Harabaz 指数 (<a href="generated/sklearn.metrics.calinski_harabaz_score.html#sklearn.metrics.calinski_harabaz_score" title="sklearn.metrics.calinski_harabaz_score"><code>sklearn.metrics.calinski_harabaz_score</code></a>) 来评估模型，其中较高的 Calinski-Harabaz 的得分与具有更好定义的聚类的模型相关。</p>
<p>对于 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> 簇，Calinski-Harabaz 得分 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0faa297883831c0432cf4d72960eeb6c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0faa297883831c0432cf4d72960eeb6c.jpg" alt="s"></a> 是作为 between-clusters dispersion mean （簇间色散平均值）与 within-cluster dispersion（群内色散之间）的比值给出的:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/af875bce0483bd18603c4d247e6a3745.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/af875bce0483bd18603c4d247e6a3745.jpg" alt="s(k) = \frac{\mathrm{Tr}(B_k)}{\mathrm{Tr}(W_k)} \times \frac{N - k}{k - 1}"></a></p>
<p>其中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/71581bfc44b992a82bd0bc7a6eee38f4.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/71581bfc44b992a82bd0bc7a6eee38f4.jpg" alt="B_K"></a> 是 between group dispersion matrix （组间色散矩阵）， <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9127c3e2b5748eee602354fed5570605.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9127c3e2b5748eee602354fed5570605.jpg" alt="W_K"></a> 是由以下定义的 within-cluster dispersion matrix （群内色散矩阵）:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a24e299927ed136dd98d6c87904c973d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a24e299927ed136dd98d6c87904c973d.jpg" alt="W_k = \sum_{q=1}k \sum_{x \in C_q} (x - c_q) (x - c_q)T"></a></p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7e8b544e8ce168b079607ff9674a2c91.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7e8b544e8ce168b079607ff9674a2c91.jpg" alt="B_k = \sum_q n_q (c_q - c) (c_q - c)^T"></a></p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a44a7c045f2217894a894c482861387a.jpg" alt="N"></a> 为数据中的点数，<a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/03aa3da890dedc42b04c1df154062257.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/03aa3da890dedc42b04c1df154062257.jpg" alt="C_q"></a> 为 cluster （簇） <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/dc074c105944810a277030dfab298376.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/dc074c105944810a277030dfab298376.jpg" alt="q"></a> 中的点集， <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/385a9104b38457eeb59acf86cf974472.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/385a9104b38457eeb59acf86cf974472.jpg" alt="c_q"></a> 为 cluster（簇） <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/dc074c105944810a277030dfab298376.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/dc074c105944810a277030dfab298376.jpg" alt="q"></a> 的中心， <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d5c9a11453ea30a1be50a1034052bd6b.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d5c9a11453ea30a1be50a1034052bd6b.jpg" alt="c"></a> 为 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5bb034cee5851ab5105aca4c40a4e16e.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5bb034cee5851ab5105aca4c40a4e16e.jpg" alt="E"></a> 的中心， <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/33b99b5c21f0cf5b03e92fe60cbe6ad0.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/33b99b5c21f0cf5b03e92fe60cbe6ad0.jpg" alt="n_q"></a> 为 cluster（簇） <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/dc074c105944810a277030dfab298376.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/dc074c105944810a277030dfab298376.jpg" alt="q"></a> 中的点数。</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn import metrics
&gt;&gt;&gt; from sklearn.metrics import pairwise_distances
&gt;&gt;&gt; from sklearn import datasets
&gt;&gt;&gt; dataset = datasets.load_iris()
&gt;&gt;&gt; X = dataset.data
&gt;&gt;&gt; y = dataset.target

</code></pre>
<p>在正常使用情况下，将 Calinski-Harabaz index （Calinski-Harabaz 指数）应用于 cluster analysis （聚类分析）的结果。</p>
<pre><code class="language-py">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.cluster import KMeans
&gt;&gt;&gt; kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)
&gt;&gt;&gt; labels = kmeans_model.labels_
&gt;&gt;&gt; metrics.calinski_harabaz_score(X, labels)  
560.39...

</code></pre>
<h4 id="23961-优点">2.3.9.6.1. 优点</h4>
<ul>
<li>当 cluster （簇）密集且分离较好时，分数更高，这与一个标准的 cluster（簇）有关。</li>
<li>得分计算很快</li>
</ul>
<h4 id="23962-缺点">2.3.9.6.2. 缺点</h4>
<ul>
<li>凸的簇的 Calinski-Harabaz index（Calinski-Harabaz 指数）通常高于其他类型的 cluster（簇），例如通过 DBSCAN 获得的基于密度的 cluster（簇）。</li>
</ul>
<p>参考</p>
<ul>
<li>Caliński, T., &amp; Harabasz, J. (1974). “A dendrite method for cluster analysis”. Communications in Statistics-theory and Methods 3: 1-27. <a href="http://dx.doi.org/10.1080/03610926.2011.560741">doi:10.1080/03610926.2011.560741</a>.</li>
</ul>
</div>
<hr class="uk-article-divider">
<div class="uk-block uk-block-muted uk-padding-top-remove uk-padding-bottom-remove uk-margin-large-top  book-recommend-wrap">
<div class="uk-margin-top uk-margin-bottom uk-margin-left uk-margin-right">
<div class="uk-margin uk-text-muted "><i class="uk-icon-outdent uk-icon-justify uk-margin-small-right"></i>书籍推荐</div>
<div class="books">
<ul class="uk-book-list">
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/74/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/74/index.html">Python进阶</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/46.html">东滨社</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">73页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月8日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 2664个">2664</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/156/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/156/index.html">pyspider中文文档</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/88.html">aaronhua123</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">18页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月12日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 1个">1</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/97/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/97/index.html">Twisted与异步编程入门</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/60.html">likebeta</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">23页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月29日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 158个">158</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/3/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/go_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/3/index.html">深入解析Go</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/3.html">tiancaiamao</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="go">go</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">41页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年5月1日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 1018个">1018</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/103/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/code_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/103/index.html">http2讲解中文版</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/63.html">ye11ow</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="code">code</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">15页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月29日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 254个">254</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/30/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/atom_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/30/index.html">Atom飞行手册</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/15.html">wizardforcel</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="atom">atom</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">41页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年5月3日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 45个">45</span>
</div>
</div>
</div>
</li>
<hr>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
<nav class="tm-navbar uk-navbar uk-navbar-attached reader-nav">
<div class="uk-float-left uk-margin-small-top">
<a href="javascript:;" title="目录菜单" class="show-menu  uk-icon-hover  uk-icon-align-justify uk-margin-right"></a>
<div data-uk-dropdown="{mode:'click',pos:'bottom-left'}" class="font-setting-wrap">
<a class="uk-icon-hover uk-icon-font uk-margin-right" aria-label="字体设置" href="javascript:;"></a>
<div class="uk-dropdown dropdown-menu">
<div class="dropdown-caret"><span class="caret-outer"></span><span class="caret-inner"></span></div>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-reduce">小字</button>
<button class="uk-button-link button size-2 font-enlarge">大字</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-1 ">宋体</button>
<button class="uk-button-link button size-2 font-2 ">黑体</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-3 color-theme-sun "><i class="uk-icon-sun-o"></i>白天</button>
<button class="uk-button-link button size-3 color-theme-eye "><i class="uk-icon-eye"></i>护眼</button>
<button class="uk-button-link button size-3 color-theme-moon "><i class="uk-icon-moon-o"></i>夜晚</button></div>
</div>
</div>
<a class="logo uk-margin-right" href="../../../" title="返回首页"><img class="" src="../../../static/components/images/icon_32.png" /></a>
</div>
<div class="uk-navbar-flip  uk-hidden-small">
<div id="share-box"></div>
</div>
</nav>
<div id="menu-id" class="uk-offcanvas reader-offcanvas">
<div class="uk-offcanvas-bar">
<ul class="book-menu-bar uk-nav uk-nav-offcanvas" data-uk-nav>
<li>
<a href="../../../book/170/index.html" data-book-page-rel-url="index.html" data-book-page-id="0" title="封面">封面</a>
</li>
<li>
<a class="pjax" href="../../../book/170/readme.html" data-book-page-rel-url="readme.html" data-book-page-id="0" title="简介">简介</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/79.html" title="安装 scikit-learn" data-book-page-rel-url="docs/79.html" data-book-page-id="11491">安装 scikit-learn</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/1.html" title="1. 监督学习" data-book-page-rel-url="docs/1.html" data-book-page-id="11492">1. 监督学习</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/2.html" title="1.1. 广义线性模型" data-book-page-rel-url="docs/2.html" data-book-page-id="11493">1.1. 广义线性模型</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/3.html" title="1.2. 线性和二次判别分析" data-book-page-rel-url="docs/3.html" data-book-page-id="11494">1.2. 线性和二次判别分析</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/4.html" title="1.3. 内核岭回归" data-book-page-rel-url="docs/4.html" data-book-page-id="11495">1.3. 内核岭回归</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/5.html" title="1.4. 支持向量机" data-book-page-rel-url="docs/5.html" data-book-page-id="11496">1.4. 支持向量机</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/6.html" title="1.5. 随机梯度下降" data-book-page-rel-url="docs/6.html" data-book-page-id="11497">1.5. 随机梯度下降</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/7.html" title="1.6. 最近邻" data-book-page-rel-url="docs/7.html" data-book-page-id="11498">1.6. 最近邻</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/8.html" title="1.7. 高斯过程" data-book-page-rel-url="docs/8.html" data-book-page-id="11499">1.7. 高斯过程</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/9.html" title="1.8. 交叉分解" data-book-page-rel-url="docs/9.html" data-book-page-id="11500">1.8. 交叉分解</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/10.html" title="1.9. 朴素贝叶斯" data-book-page-rel-url="docs/10.html" data-book-page-id="11501">1.9. 朴素贝叶斯</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/11.html" title="1.10. 决策树" data-book-page-rel-url="docs/11.html" data-book-page-id="11502">1.10. 决策树</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/12.html" title="1.11. 集成方法" data-book-page-rel-url="docs/12.html" data-book-page-id="11503">1.11. 集成方法</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/13.html" title="1.12. 多类和多标签算法" data-book-page-rel-url="docs/13.html" data-book-page-id="11504">1.12. 多类和多标签算法</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/14.html" title="1.13. 特征选择" data-book-page-rel-url="docs/14.html" data-book-page-id="11505">1.13. 特征选择</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/15.html" title="1.14. 半监督学习" data-book-page-rel-url="docs/15.html" data-book-page-id="11506">1.14. 半监督学习</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/16.html" title="1.15. 等式回归" data-book-page-rel-url="docs/16.html" data-book-page-id="11507">1.15. 等式回归</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/17.html" title="1.16. 概率校准" data-book-page-rel-url="docs/17.html" data-book-page-id="11508">1.16. 概率校准</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/18.html" title="1.17. 神经网络模型（有监督）" data-book-page-rel-url="docs/18.html" data-book-page-id="11509">1.17. 神经网络模型（有监督）</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/19.html" title="2. 无监督学习" data-book-page-rel-url="docs/19.html" data-book-page-id="11510">2. 无监督学习</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/20.html" title="2.1. 高斯混合模型" data-book-page-rel-url="docs/20.html" data-book-page-id="11511">2.1. 高斯混合模型</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/21.html" title="2.2. 流形学习" data-book-page-rel-url="docs/21.html" data-book-page-id="11512">2.2. 流形学习</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/22.html" title="2.3. 聚类" data-book-page-rel-url="docs/22.html" data-book-page-id="11513">2.3. 聚类</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/23.html" title="2.4. 双聚类" data-book-page-rel-url="docs/23.html" data-book-page-id="11514">2.4. 双聚类</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/24.html" title="2.5. 分解成分中的信号（矩阵分解问题）" data-book-page-rel-url="docs/24.html" data-book-page-id="11515">2.5. 分解成分中的信号（矩阵分解问题）</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/25.html" title="2.6. 协方差估计" data-book-page-rel-url="docs/25.html" data-book-page-id="11516">2.6. 协方差估计</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/26.html" title="2.7. 经验协方差" data-book-page-rel-url="docs/26.html" data-book-page-id="11517">2.7. 经验协方差</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/27.html" title="2.8. 收敛协方差" data-book-page-rel-url="docs/27.html" data-book-page-id="11518">2.8. 收敛协方差</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/28.html" title="2.9. 稀疏逆协方差" data-book-page-rel-url="docs/28.html" data-book-page-id="11519">2.9. 稀疏逆协方差</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/29.html" title="2.10. Robust 协方差估计" data-book-page-rel-url="docs/29.html" data-book-page-id="11520">2.10. Robust 协方差估计</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/30.html" title="2.11. 新奇和异常值检测" data-book-page-rel-url="docs/30.html" data-book-page-id="11521">2.11. 新奇和异常值检测</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/31.html" title="2.12. 密度估计" data-book-page-rel-url="docs/31.html" data-book-page-id="11522">2.12. 密度估计</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/32.html" title="2.13. 神经网络模型（无监督）" data-book-page-rel-url="docs/32.html" data-book-page-id="11523">2.13. 神经网络模型（无监督）</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/33.html" title="3. 模型选择和评估" data-book-page-rel-url="docs/33.html" data-book-page-id="11524">3. 模型选择和评估</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/34.html" title="3.1. 交叉验证：评估估算器的表现" data-book-page-rel-url="docs/34.html" data-book-page-id="11525">3.1. 交叉验证：评估估算器的表现</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/35.html" title="3.2. 调整估计器的超参数" data-book-page-rel-url="docs/35.html" data-book-page-id="11526">3.2. 调整估计器的超参数</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/53.html" title="3.3. 模型评估: 量化预测的质量" data-book-page-rel-url="docs/53.html" data-book-page-id="11527">3.3. 模型评估: 量化预测的质量</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/54.html" title="3.4. 模型持久化" data-book-page-rel-url="docs/54.html" data-book-page-id="11528">3.4. 模型持久化</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/55.html" title="3.5. 验证曲线: 绘制分数以评估模型" data-book-page-rel-url="docs/55.html" data-book-page-id="11529">3.5. 验证曲线: 绘制分数以评估模型</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/56.html" title="4. 数据集转换" data-book-page-rel-url="docs/56.html" data-book-page-id="11530">4. 数据集转换</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/57.html" title="4.1. Pipeline（管道）和 FeatureUnion（特征联合）: 合并的评估器" data-book-page-rel-url="docs/57.html" data-book-page-id="11531">4.1. Pipeline（管道）和 FeatureUnion（特征联合）: 合并的评估器</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/58.html" title="4.2. 特征提取" data-book-page-rel-url="docs/58.html" data-book-page-id="11532">4.2. 特征提取</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/59.html" title="4.3. 预处理数据" data-book-page-rel-url="docs/59.html" data-book-page-id="11533">4.3. 预处理数据</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/60.html" title="4.4. 无监督降维" data-book-page-rel-url="docs/60.html" data-book-page-id="11534">4.4. 无监督降维</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/61.html" title="4.5. 随机投影" data-book-page-rel-url="docs/61.html" data-book-page-id="11535">4.5. 随机投影</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/62.html" title="4.6. 内核近似" data-book-page-rel-url="docs/62.html" data-book-page-id="11536">4.6. 内核近似</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/63.html" title="4.7. 成对的矩阵, 类别和核函数" data-book-page-rel-url="docs/63.html" data-book-page-id="11537">4.7. 成对的矩阵, 类别和核函数</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/64.html" title="4.8. 预测目标 (`y`.html) 的转换" data-book-page-rel-url="docs/64.html" data-book-page-id="11538">4.8. 预测目标 (`y`.html) 的转换</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/65.html" title="5. 数据集加载工具" data-book-page-rel-url="docs/65.html" data-book-page-id="11539">5. 数据集加载工具</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/66.html" title="6. 大规模计算的策略: 更大量的数据" data-book-page-rel-url="docs/66.html" data-book-page-id="11540">6. 大规模计算的策略: 更大量的数据</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/67.html" title="7. 计算性能" data-book-page-rel-url="docs/67.html" data-book-page-id="11541">7. 计算性能</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/68.html" title="使用 scikit-learn 介绍机器学习" data-book-page-rel-url="docs/68.html" data-book-page-id="11542">使用 scikit-learn 介绍机器学习</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/69.html" title="关于科学数据处理的统计学习教程" data-book-page-rel-url="docs/69.html" data-book-page-id="11543">关于科学数据处理的统计学习教程</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/70.html" title="机器学习: scikit-learn 中的设置以及预估对象" data-book-page-rel-url="docs/70.html" data-book-page-id="11544">机器学习: scikit-learn 中的设置以及预估对象</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/71.html" title="监督学习：从高维观察预测输出变量" data-book-page-rel-url="docs/71.html" data-book-page-id="11545">监督学习：从高维观察预测输出变量</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/72.html" title="模型选择：选择估计量及其参数" data-book-page-rel-url="docs/72.html" data-book-page-id="11546">模型选择：选择估计量及其参数</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/73.html" title="无监督学习: 寻求数据表示" data-book-page-rel-url="docs/73.html" data-book-page-id="11547">无监督学习: 寻求数据表示</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/74.html" title="把它们放在一起" data-book-page-rel-url="docs/74.html" data-book-page-id="11548">把它们放在一起</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/75.html" title="寻求帮助" data-book-page-rel-url="docs/75.html" data-book-page-id="11549">寻求帮助</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/76.html" title="处理文本数据" data-book-page-rel-url="docs/76.html" data-book-page-id="11550">处理文本数据</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/77.html" title="选择正确的评估器(estimator.html)" data-book-page-rel-url="docs/77.html" data-book-page-id="11551">选择正确的评估器(estimator.html)</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/78.html" title="外部资源，视频和谈话" data-book-page-rel-url="docs/78.html" data-book-page-id="11552">外部资源，视频和谈话</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/80.html" title="常见问题" data-book-page-rel-url="docs/80.html" data-book-page-id="11553">常见问题</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/81.html" title="时光轴" data-book-page-rel-url="docs/81.html" data-book-page-id="11554">时光轴</a>
</li>
</ul>
</div>
</div>
<script src="https://cdn.staticfile.net/jquery/1.12.4/jquery.min.js"></script>
<script type="text/javascript" src="../../../static/components/uikit-2.27.5/js/uikit.reader.js"></script>
<script type="text/javascript" src="../../../static/components/social-share/social-share.min.js"></script>
<script>(function(){var bp =document.createElement('script');var curProtocol =window.location.protocol.split(':')[0];if (curProtocol ==='https') {bp.src ='https://zz.bdstatic.com/linksubmit/push.js';}
else {bp.src ='http://push.zhanzhang.baidu.com/push.js';}
var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(bp,s);})();</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
<script src="https://cdn.staticfile.net/highlight.js/9.12.0/highlight.min.js"></script>
<script src="https://cdn.staticfile.net/jquery.pjax/2.0.1/jquery.pjax.min.js"></script>
<script src="https://cdn.staticfile.net/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="https://cdn.staticfile.net/uikit/2.27.5/js/components/lightbox.min.js"></script>
<link rel="dns-prefetch" href="../../..//cdn.mathjax.org" />
<script type="text/x-mathjax-config">
 function initMathJax() {
    var mathId = $("book-content-section")[0];
    MathJax.Hub.Config({
        tex2jax: {skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code','a']},
        showProcessingMessages: false,
        messageStyle: "none"
    });
    MathJax.Hub.Queue(["Typeset",MathJax.Hub,mathId]);
 };
initMathJax();
</script>
<script src='https://cdn.staticfile.net/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML' async></script>
<style>
	.MathJax_Display{display:inline!important;}
</style>
<script type="text/javascript" src="../../../static/components/js/reader.js"></script>
<script type="text/javascript">var bookId =170;var bookPageId =11513;var bookPageRelUrl ='docs/22.html';</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
</body>
</html>