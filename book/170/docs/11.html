
<!DOCTYPE html>
<html lang="zh-CN">
<head>
<title>1.10. 决策树-scikit-learn (sklearn) 官方文档中文版</title>
<meta content='1.10. 决策树,scikit-learn (sklearn) 官方文档中文版' name='keywords'>
<meta content='1.10. 决策树,scikit-learn (sklearn) 官方文档中文版' name='description'>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta http-equiv="Content-Language" content="zh-CN" />
<meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1, maximum-scale=1, user-scalable=no"../../../>
<meta name="applicable-device" content="pc,mobile">
<link rel="shortcut icon" href="../../../favicon.ico" type="image/x-icon" />
<meta name="renderer" content="webkit">
<link rel="stylesheet" href="../../../static/components/uikit-2.27.5/css/uikit.custom.css">
<link rel="stylesheet" href="../../../static/components/social-share/social-share.min.css">
<link rel="stylesheet" href="../../../static/components/highlight/styles/custom.css">
<link rel="stylesheet" href="../../../static/components/css/base.css">
<link rel="stylesheet" href="../../../static/components/css/reader.css">
<link rel="stylesheet" href="../../../static/components/css/markdown.css">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5313208362165053" crossorigin="anonymous"></script>
</head>
<body>
<div class=" book-main-wrap uk-container uk-container-center uk-margin-top ">
<div class="uk-grid">
<div class="uk-width-1-1 reader-wrap ">
<div class=" bottom-nav uk-clearfix ">
<div class="uk-align-left ">
<a href="../../../book/170/docs/10.html">
<i class="nav-icon-left uk-icon-small  uk-icon-caret-left"></i>
<span class="">1.9. 朴素贝叶斯</span>
</a>
</div>
<div class="uk-align-right ">
<a href="../../../book/170/docs/12.html">
<span class="">1.11. 集成方法</span>
<i class="nav-icon-right uk-icon-small  uk-icon-caret-right"></i>
</a>
</div>
</div>
<div class="uk-text-center">
<h2 class="book-page-title uk-container-center">
<a href="../../../book/170/index.html">scikit-learn (sklearn) 官方文档中文版</a>
<a target="_blank" rel="nofollow" href="https://github.com/apachecn/scikit-learn-doc-zh" class="uk-icon-button uk-icon-github" title="github项目地址"></a>
</h2>
</div>
<script type="text/javascript" src="../../../static/components/js/app_intro.js"></script>
<ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-5313208362165053" data-ad-slot="1328047120"></ins>
<script>(adsbygoogle =window.adsbygoogle ||[]).push({});</script>
<hr class="uk-article-divider">
<div class="book-content-section  md-content-section  uk-margin-bottom">
<h1 id="110-决策树">1.10. 决策树</h1>
<p>校验者: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@文谊</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@皮卡乒的皮卡乓</a> 翻译者: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh"></a><a href="https://github.com/I"><strong>@I</strong></a> Remember</p>
<p><strong>Decision Trees (DTs)</strong> 是一种用来 <a href="#tree-classification">classification</a> 和 <a href="#tree-regression">regression</a> 的无参监督学习方法。其目的是创建一种模型从数据特征中学习简单的决策规则来预测一个目标变量的值。</p>
<p>例如，在下面的图片中，决策树通过if-then-else的决策规则来学习数据从而估测数一个正弦图像。决策树越深入，决策规则就越复杂并且对数据的拟合越好。</p>
<p><a href="../auto_examples/tree/plot_tree_regression.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f0b72920659961ba27aec1da59f3019c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f0b72920659961ba27aec1da59f3019c.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_tree_regression_0011.png"></a></a></p>
<p>决策树的优势:</p>
<blockquote>
<ul>
<li>便于理解和解释。树的结构可以可视化出来。</li>
</ul>
<p>&gt; * 训练需要的数据少。其他机器学习模型通常需要数据规范化，比如构建虚拟变量和移除缺失值,不过请注意，这种模型不支持缺失值。</p>
<ul>
<li>由于训练决策树的数据点的数量导致了决策树的使用开销呈指数分布(训练树模型的时间复杂度是参与训练数据点的对数值)。</li>
<li>能够处理数值型数据和分类数据。其他的技术通常只能用来专门分析某一种变量类型的数据集。详情请参阅算法。</li>
<li>能够处理多路输出的问题。</li>
<li>使用白盒模型。如果某种给定的情况在该模型中是可以观察的，那么就可以轻易的通过布尔逻辑来解释这种情况。相比之下，在黑盒模型中的结果就是很难说明清 楚地。</li>
<li>可以通过数值统计测试来验证该模型。这对事解释验证该模型的可靠性成为可能。</li>
<li>即使该模型假设的结果与真实模型所提供的数据有些违反，其表现依旧良好。</li>
</ul>
</blockquote>
<p>决策树的缺点包括:</p>
<blockquote>
<ul>
<li>决策树模型容易产生一个过于复杂的模型,这样的模型对数据的泛化性能会很差。这就是所谓的过拟合.一些策略像剪枝、设置叶节点所需的最小样本数或设置数的最大深度是避免出现 该问题最为有效地方法。</li>
<li>决策树可能是不稳定的，因为数据中的微小变化可能会导致完全不同的树生成。这个问题可以通过决策树的集成来得到缓解</li>
<li>在多方面性能最优和简单化概念的要求下，学习一棵最优决策树通常是一个NP难问题。因此，实际的决策树学习算法是基于启发式算法，例如在每个节点进 行局部最优决策的贪心算法。这样的算法不能保证返回全局最优决策树。这个问题可以通过集成学习来训练多棵决策树来缓解,这多棵决策树一般通过对特征和样本有放回的随机采样来生成。</li>
<li>有些概念很难被决策树学习到,因为决策树很难清楚的表述这些概念。例如XOR，奇偶或者复用器的问题。</li>
<li>如果某些类在问题中占主导地位会使得创建的决策树有偏差。因此，我们建议在拟合前先对数据集进行平衡。</li>
</ul>
</blockquote>
<h2 id="1101-分类">1.10.1. 分类</h2>
<p><a href="generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier" title="sklearn.tree.DecisionTreeClassifier"><code>DecisionTreeClassifier</code></a> 是能够在数据集上执行多分类的类,与其他分类器一样，<a href="generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier" title="sklearn.tree.DecisionTreeClassifier"><code>DecisionTreeClassifier</code></a> 采用输入两个数组：数组X，用 <code>[n_samples, n_features]</code> 的方式来存放训练样本。整数值数组Y，用 <code>[n_samples]</code> 来保存训练样本的类标签:</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn import tree
&gt;&gt;&gt; X = [[0, 0], [1, 1]]
&gt;&gt;&gt; Y = [0, 1]
&gt;&gt;&gt; clf = tree.DecisionTreeClassifier()
&gt;&gt;&gt; clf = clf.fit(X, Y)

</code></pre>
<p>执行通过之后，可以使用该模型来预测样本类别:</p>
<pre><code class="language-py">&gt;&gt;&gt; clf.predict([[2., 2.]])
array([1])

</code></pre>
<p>另外，也可以预测每个类的概率，这个概率是叶中相同类的训练样本的分数:</p>
<pre><code class="language-py">&gt;&gt;&gt; clf.predict_proba([[2., 2.]])
array([[ 0.,  1.]])

</code></pre>
<p><a href="generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier" title="sklearn.tree.DecisionTreeClassifier"><code>DecisionTreeClassifier</code></a> 既能用于二分类（其中标签为[-1,1]）也能用于多分类（其中标签为[0,…,k-1]）。使用Lris数据集，我们可以构造一个决策树，如下所示:</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn.datasets import load_iris
&gt;&gt;&gt; from sklearn import tree
&gt;&gt;&gt; iris = load_iris()
&gt;&gt;&gt; clf = tree.DecisionTreeClassifier()
&gt;&gt;&gt; clf = clf.fit(iris.data, iris.target)

</code></pre>
<p>经过训练，我们可以使用 <a href="generated/sklearn.tree.export_graphviz.html#sklearn.tree.export_graphviz" title="sklearn.tree.export_graphviz"><code>export_graphviz</code></a> 导出器以 <a href="http://www.graphviz.org/">Graphviz</a> 格式导出决策树. 如果你是用 <a href="http://conda.io">conda</a> 来管理包，那么安装 graphviz 二进制文件和 python 包可以用以下指令安装</p>
<blockquote>
<p>conda install python-graphviz</p>
</blockquote>
<p>或者，可以从 graphviz 项目主页下载 graphviz 的二进制文件，并从 pypi 安装 Python 包装器，并安装 &lt;cite&gt;pip install graphviz&lt;/cite&gt; .以下是在整个 iris 数据集上训练的上述树的 graphviz 导出示例; 其结果被保存在 &lt;cite&gt;iris.pdf&lt;/cite&gt; 中:</p>
<pre><code class="language-py">   &gt;&gt;&gt; import graphviz # doctest: +SKIP
   &gt;&gt;&gt; dot_data = tree.export_graphviz(clf, out_file=None) # doctest: +SKIP
   &gt;&gt;&gt; graph = graphviz.Source(dot_data) # doctest: +SKIP
   &gt;&gt;&gt; graph.render("iris") # doctest: +SKIP

:func:`export_graphviz` 出导出还支持各种美化，包括通过他们的类着色节点（或回归值），如果需要，使用显式变量和类名。Jupyter notebook也可以自动找出相同的模块::

   &gt;&gt;&gt; dot_data = tree.export_graphviz(clf, out_file=None, # doctest: +SKIP
                            feature_names=iris.feature_names,  # doctest: +SKIP
                            class_names=iris.target_names,  # doctest: +SKIP
                            filled=True, rounded=True,  # doctest: +SKIP
                            special_characters=True)  # doctest: +SKIP
   &gt;&gt;&gt; graph = graphviz.Source(dot_data)  # doctest: +SKIP
   &gt;&gt;&gt; graph # doctest: +SKIP

</code></pre>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/eac4cdf0a783ddcd7098023e25bb16ef.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/eac4cdf0a783ddcd7098023e25bb16ef.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/iris.svg"></a></p>
<p>执行通过之后，可以使用该模型预测样品类别:</p>
<pre><code class="language-py">&gt;&gt;&gt; clf.predict(iris.data[:1, :])
array([0])

</code></pre>
<p>或者，可以根据决策树叶子树里训练样本中的相同类的分数，使得类预测成为可能:</p>
<pre><code class="language-py">&gt;&gt;&gt; clf.predict_proba(iris.data[:1, :])
array([[ 1.,  0.,  0.]])

</code></pre>
<p><a href="../auto_examples/tree/plot_iris.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/cba233fc4178da6d3fe0b177cbbb6318.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/cba233fc4178da6d3fe0b177cbbb6318.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_iris_0013.png"></a></a></p>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/tree/plot_iris.html#sphx-glr-auto-examples-tree-plot-iris-py">Plot the decision surface of a decision tree on the iris dataset</a></li>
</ul>
<h2 id="1102-回归">1.10.2. 回归</h2>
<p><a href="../auto_examples/tree/plot_tree_regression.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f0b72920659961ba27aec1da59f3019c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f0b72920659961ba27aec1da59f3019c.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_tree_regression_0011.png"></a></a></p>
<p>决策树通过使用 <a href="generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor" title="sklearn.tree.DecisionTreeRegressor"><code>DecisionTreeRegressor</code></a> 类也可以用来解决回归问题。如在分类设置中，拟合方法将数组X和数组y作为参数，只有在这种情况下，y数组预期才是浮点值:</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn import tree
&gt;&gt;&gt; X = [[0, 0], [2, 2]]
&gt;&gt;&gt; y = [0.5, 2.5]
&gt;&gt;&gt; clf = tree.DecisionTreeRegressor()
&gt;&gt;&gt; clf = clf.fit(X, y)
&gt;&gt;&gt; clf.predict([[1, 1]])
array([ 0.5])

</code></pre>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/tree/plot_tree_regression.html#sphx-glr-auto-examples-tree-plot-tree-regression-py">Decision Tree Regression</a></li>
</ul>
<h2 id="1103-多值输出问题">1.10.3. 多值输出问题</h2>
<p>一个多值输出问题是一个类似当 Y 是大小为 <code>[n_samples, n_outputs]</code> 的2d数组时，有多个输出值需要预测的监督学习问题。</p>
<p>当输出值之间没有关联时，一个很简单的处理该类型的方法是建立一个n独立模型，即每个模型对应一个输出，然后使用这些模型来独立地预测n个输出中的每一个。然而，由于可能与相同输入相关的输出值本身是相关的，所以通常更好的方法是构建能够同时预测所有n个输出的单个模型。首先，因为仅仅是建立了一个模型所以训练时间会更短。第二，最终模型的泛化性能也会有所提升。对于决策树，这一策略可以很容易地用于多输出问题。 这需要以下更改：</p>
<blockquote>
<ul>
<li>在叶中存储n个输出值，而不是一个;</li>
<li>通过计算所有n个输出的平均减少量来作为分裂标准.</li>
</ul>
</blockquote>
<p>该模块通过在 <code>DecisionTreeClassifier</code>和 :class:<code>DecisionTreeRegressor</code> 中实现该策略来支持多输出问题。如果决策树与大小为 <code>[n_samples, n_outputs]</code> 的输出数组Y向匹配，则得到的估计器将:</p>
<pre><code class="language-py">* ``predict`` 是输出n_output的值

* 在 ``predict_proba`` 上输出 n_output 数组列表

</code></pre>
<p>用多输出决策树进行回归分析 <a href="../auto_examples/tree/plot_tree_regression_multioutput.html#sphx-glr-auto-examples-tree-plot-tree-regression-multioutput-py">Multi-output Decision Tree Regression</a> 。 在该示例中，输入X是单个实数值，并且输出Y是X的正弦和余弦。</p>
<p><a href="../auto_examples/tree/plot_tree_regression_multioutput.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c6b27df44672e7fa50d1d81ffbbebfbd.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c6b27df44672e7fa50d1d81ffbbebfbd.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_tree_regression_multioutput_0011.png"></a></a></p>
<p>使用多输出树进行分类，在 <a href="../auto_examples/plot_multioutput_face_completion.html#sphx-glr-auto-examples-plot-multioutput-face-completion-py">Face completion with a multi-output estimators</a> 中进行了演示。 在该示例中，输入X是面的上半部分的像素，并且输出Y是这些面的下半部分的像素。</p>
<p><a href="../auto_examples/plot_multioutput_face_completion.html"></a></p>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/tree/plot_tree_regression_multioutput.html#sphx-glr-auto-examples-tree-plot-tree-regression-multioutput-py">Multi-output Decision Tree Regression</a></li>
<li><a href="../auto_examples/plot_multioutput_face_completion.html#sphx-glr-auto-examples-plot-multioutput-face-completion-py">Face completion with a multi-output estimators</a></li>
</ul>
<p>参考:</p>
<ul>
<li>M. Dumont et al, <a href="http://www.montefiore.ulg.ac.be/services/stochastic/pubs/2009/DMWG09/dumont-visapp09-shortpaper.pdf">Fast multi-class image annotation with random subwindows and multiple output randomized trees</a>, International Conference on Computer Vision Theory and Applications 2009</li>
</ul>
<h2 id="1104-复杂度分析">1.10.4. 复杂度分析</h2>
<p>总体来说，用来构建平衡二叉树的运行时间为 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/264ba68c53c2e2867b9e307e8b940d49.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/264ba68c53c2e2867b9e307e8b940d49.jpg" alt="O(n_{samples}n_{features}\log(n_{samples}))"></a> 查询时间为 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ec29f705a6be2ce512a10c266dd755f0.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ec29f705a6be2ce512a10c266dd755f0.jpg" alt="O(\log(n_{samples}))"></a> 。尽管树的构造算法尝试生成平衡树，但它们并不总能保持平衡。假设子树能大概保持平衡，每个节点的成本包括通过 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/92570652e9c52768c76f5b9cf8f97a28.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/92570652e9c52768c76f5b9cf8f97a28.jpg" alt="O(n_{features})"></a> 时间复杂度来搜索找到提供熵减小最大的特征。每个节点的花费为 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6c8db7614197ace6a4bf0f437c085e6d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6c8db7614197ace6a4bf0f437c085e6d.jpg" alt="O(n_{features}n_{samples}\log(n_{samples}))"></a> ，从而使得整个决策树的构造成本为 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2bf67de05d37f31bd2154fdc96690102.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2bf67de05d37f31bd2154fdc96690102.jpg" alt="O(n_{features}n_{samples}^{2}\log(n_{samples}))"></a> 。</p>
<p>Scikit-learn提供了更多有效的方法来创建决策树。初始实现（如上所述）将重新计算沿着给定特征的每个新分割点的类标签直方图（用于分类）或平均值（用于回归）。与分类所有的样本特征，然后再次训练时运行标签计数，可将每个节点的复杂度降低为 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/93cdc140e4de4cbb0d093fc7ad5c6a36.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/93cdc140e4de4cbb0d093fc7ad5c6a36.jpg" alt="O(n_{features}\log(n_{samples}))"></a> ，则总的成本花费为 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6c8db7614197ace6a4bf0f437c085e6d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6c8db7614197ace6a4bf0f437c085e6d.jpg" alt="O(n_{features}n_{samples}\log(n_{samples}))"></a> 。这是一种对所有基于树的算法的改进选项。默认情况下，对于梯度提升模型该算法是打开的，一般来说它会让训练速度更快。但对于所有其他算法默认是关闭的，当训练深度很深的树时往往会减慢训练速度。</p>
<h2 id="1105-实际使用技巧">1.10.5. 实际使用技巧</h2>
<blockquote>
<p>&gt; * 对于拥有大量特征的数据决策树会出现过拟合的现象。获得一个合适的样本比例和特征数量十分重要，因为在高维空间中只有少量的样本的树是十分容易过拟合的。 &gt; * 考虑事先进行降维( <a href="decomposition.html#pca">PCA</a> , <a href="decomposition.html#ica">ICA</a> ，使您的树更好地找到具有分辨性的特征。 &gt; * 通过 <code>export</code> 功能可以可视化您的决策树。使用 <code>max_depth=3</code> 作为初始树深度，让决策树知道如何适应您的数据，然后再增加树的深度。 &gt; * 请记住，填充树的样本数量会增加树的每个附加级别。使用 <code>max_depth</code> 来控制输的大小防止过拟合。 &gt; * 通过使用 <code>min_samples_split</code> 和 <code>min_samples_leaf</code> 来控制叶节点上的样本数量。当这个值很小时意味着生成的决策树将会过拟合，然而当这个值很大时将会不利于决策树的对样本的学习。所以尝试 <code>min_samples_leaf=5</code> 作为初始值。如果样本的变化量很大，可以使用浮点数作为这两个参数中的百分比。两者之间的主要区别在于 <code>min_samples_leaf</code> 保证叶结点中最少的采样数，而 <code>min_samples_split</code> 可以创建任意小的叶子，尽管在文献中 <code>min_samples_split</code> 更常见。 &gt; * 在训练之前平衡您的数据集，以防止决策树偏向于主导类.可以通过从每个类中抽取相等数量的样本来进行类平衡，或者优选地通过将每个类的样本权重 (<code>sample_weight</code>) 的和归一化为相同的值。还要注意的是，基于权重的预修剪标准 (<code>min_weight_fraction_leaf</code>) 对于显性类别的偏倚偏小，而不是不了解样本权重的标准，如 <code>min_samples_leaf</code> 。</p>
<ul>
<li>如果样本被加权，则使用基于权重的预修剪标准 <code>min_weight_fraction_leaf</code> 来优化树结构将更容易，这确保叶节点包含样本权重的总和的至少一部分。</li>
<li>所有的决策树内部使用 <code>np.float32</code> 数组 ，如果训练数据不是这种格式，将会复制数据集。</li>
<li>如果输入的矩阵X为稀疏矩阵，建议您在调用fit之前将矩阵X转换为稀疏的<code>csc_matrix</code> ,在调用predict之前将 <code>csr_matrix</code> 稀疏。当特征在大多数样本中具有零值时，与密集矩阵相比，稀疏矩阵输入的训练时间可以快几个数量级。</li>
</ul>
</blockquote>
<h2 id="1106-决策树算法-id3-c45-c50-和-cart">1.10.6. 决策树算法: ID3, C4.5, C5.0 和 CART</h2>
<p>所有种类的决策树算法有哪些以及它们之间的区别？scikit-learn 中实现何种算法呢？</p>
<p>ID3（Iterative Dichotomiser 3）由 Ross Quinlan 在1986年提出。该算法创建一个多路树，找到每个节点（即以贪心的方式）分类特征，这将产生分类目标的最大信息增益。决策树发展到其最大尺寸，然后通常利用剪枝来提高树对未知数据的泛华能力。</p>
<p>C4.5 是 ID3 的后继者，并且通过动态定义将连续属性值分割成一组离散间隔的离散属性（基于数字变量），消除了特征必须被明确分类的限制。C4.5 将训练的树（即，ID3算法的输出）转换成 if-then 规则的集合。然后评估每个规则的这些准确性，以确定应用它们的顺序。如果规则的准确性没有改变，则需要决策树的树枝来解决。</p>
<p>C5.0 是 Quinlan 根据专有许可证发布的最新版本。它使用更少的内存，并建立比 C4.5 更小的规则集，同时更准确。</p>
<p>CART（Classification and Regression Trees （分类和回归树））与 C4.5 非常相似，但它不同之处在于它支持数值目标变量（回归），并且不计算规则集。CART 使用在每个节点产生最大信息增益的特征和阈值来构造二叉树。</p>
<p>scikit-learn 使用 CART 算法的优化版本。</p>
<h2 id="1107-数学表达">1.10.7. 数学表达</h2>
<p>给定训练向量 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/02848ebe72029503696b6523e4052c0c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/02848ebe72029503696b6523e4052c0c.jpg" alt="x_i \in R^n"></a>, i=1,…, l 和标签向量 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/405095229d24f3525298dc6f99077666.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/405095229d24f3525298dc6f99077666.jpg" alt="y \in R^l"></a>。决策树递归地分割空间，例如将有相同标签的样本归为一组。</p>
<p>将 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/94156b879a7455cb0d516efa9c9c0991.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/94156b879a7455cb0d516efa9c9c0991.jpg" alt="m"></a> 节点上的数据用 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/87dfb2676632ee8a92713f4861ccc84e.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/87dfb2676632ee8a92713f4861ccc84e.jpg" alt="Q"></a> 来表示。每一个候选组 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c3567127ff1f678758b338a50e9c4880.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c3567127ff1f678758b338a50e9c4880.jpg" alt="\theta = (j, t_m)"></a> 包含一个特征 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7b215f2882ce8aaa33a97e43ad626314.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7b215f2882ce8aaa33a97e43ad626314.jpg" alt="j"></a> 和阈值 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/264dc5b617a5aa98151c4ea6975e9a90.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/264dc5b617a5aa98151c4ea6975e9a90.jpg" alt="t_m"></a> 将,数据分成 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/32246af90101d1607825a589ebea6880.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/32246af90101d1607825a589ebea6880.jpg" alt="Q_{left}(\theta)"></a> 和 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6c70b46b88f05e00e292f1a0f98d2aa8.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6c70b46b88f05e00e292f1a0f98d2aa8.jpg" alt="Q_{right}(\theta)"></a> 子集。</p>
<pre><code class="language-py">
![Q_{left}(\theta) = {(x, y) | x_j &amp;lt;= t_m}

Q_{right}(\theta) = Q \setminus Q_{left}(\theta)](img/d5a26fae0e652d4e951d9ec9ae1a01e5.jpg)

</code></pre>
<p>使用不纯度函数 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b382a1d99ddfadf17b35d32b0b156b5b.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b382a1d99ddfadf17b35d32b0b156b5b.jpg" alt="H()"></a> 计算 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/94156b879a7455cb0d516efa9c9c0991.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/94156b879a7455cb0d516efa9c9c0991.jpg" alt="m"></a> 处的不纯度,其选择取决于正在解决的任务（分类或回归）</p>
<pre><code class="language-py">
![G(Q, \theta) = \frac{n_{left}}{N_m} H(Q_{left}(\theta))
+ \frac{n_{right}}{N_m} H(Q_{right}(\theta))](img/c57c1c5b116586e218fdaa3d0696d246.jpg)

</code></pre>
<p>选择使不纯度最小化的参数</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/af78ae4aadd0f0961cf4d9564897b1ff.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/af78ae4aadd0f0961cf4d9564897b1ff.jpg" alt="\theta^* = \operatorname{argmin}_\theta  G(Q, \theta)"></a></p>
<p>重新计算子集 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/33eb0dacfcc0df16c84bfaed52d31859.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/33eb0dacfcc0df16c84bfaed52d31859.jpg" alt="Q_{left}(\theta^*)"></a> 和 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a1bd06d03e764db224f0e10b4f024bdd.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a1bd06d03e764db224f0e10b4f024bdd.jpg" alt="Q_{right}(\theta^*)"></a> ，直到达到最大允许深度，<a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0fec9d3a9833abc417480a03be883b3e.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0fec9d3a9833abc417480a03be883b3e.jpg" alt="N_m < \min_{samples}"></a> 或 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/be71aa00cd62f24b4657f7993d1b3a45.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/be71aa00cd62f24b4657f7993d1b3a45.jpg" alt="N_m = 1"></a>。</p>
<h3 id="11071-分类标准">1.10.7.1. 分类标准</h3>
<p>对于节点 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/94156b879a7455cb0d516efa9c9c0991.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/94156b879a7455cb0d516efa9c9c0991.jpg" alt="m"></a> ，表示具有 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ee68d82006856c6355aa0dca42cd5054.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ee68d82006856c6355aa0dca42cd5054.jpg" alt="N_m"></a> 个观测值的区域 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/018a312145ba4dee4c257135644ced91.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/018a312145ba4dee4c257135644ced91.jpg" alt="R_m"></a> ，如果分类结果采用值是 0,1,…,K-1 的值，让</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/17430579d0bcbef3e2d99eba470792c8.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/17430579d0bcbef3e2d99eba470792c8.jpg" alt="p_{mk} = 1/ N_m \sum_{x_i \in R_m} I(y_i = k)"></a></p>
<p>是节点 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/94156b879a7455cb0d516efa9c9c0991.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/94156b879a7455cb0d516efa9c9c0991.jpg" alt="m"></a> 中k类观测的比例通常用来处理杂质的方法是Gini</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c236b4ae30d04ba7fae7fa499a2ba9ea.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c236b4ae30d04ba7fae7fa499a2ba9ea.jpg" alt="H(X_m) = \sum_k p_{mk} (1 - p_{mk})"></a></p>
<p>Cross-Entropy （交叉熵）</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a539f6901c50549f8d30f6d7f6f3e177.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a539f6901c50549f8d30f6d7f6f3e177.jpg" alt="H(X_m) = - \sum_k p_{mk} \log(p_{mk})"></a></p>
<p>和 Misclassification （错误分类）</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b146555411b22bcf0ad73d4720455038.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b146555411b22bcf0ad73d4720455038.jpg" alt="H(X_m) = 1 - \max(p_{mk})"></a></p>
<p>在 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/fe5ed835e0d3407e3f2d694d8bc049a1.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/fe5ed835e0d3407e3f2d694d8bc049a1.jpg" alt="X_m"></a> 训练 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/94156b879a7455cb0d516efa9c9c0991.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/94156b879a7455cb0d516efa9c9c0991.jpg" alt="m"></a> 节点上的数据时。</p>
<h3 id="11072-回归标准">1.10.7.2. 回归标准</h3>
<p>如果目标是连续性的值，那么对于节点 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/94156b879a7455cb0d516efa9c9c0991.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/94156b879a7455cb0d516efa9c9c0991.jpg" alt="m"></a> ,表示具有 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ee68d82006856c6355aa0dca42cd5054.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ee68d82006856c6355aa0dca42cd5054.jpg" alt="N_m"></a> 个观测值的区域 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/018a312145ba4dee4c257135644ced91.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/018a312145ba4dee4c257135644ced91.jpg" alt="R_m"></a> ，对于以后的分裂节点的位置的决定常用的最小化标准是均方差和平均绝对误差，前者使用终端节点处的平均值来最小化L2误差，后者使用终端节点处的中值来最小化 L1 误差。</p>
<p>Mean Squared Error （均方误差）:</p>
<pre><code class="language-py">
![c_m = \frac{1}{N_m} \sum_{i \in N_m} y_i

H(X_m) = \frac{1}{N_m} \sum_{i \in N_m} (y_i - c_m)^2](img/0cd05229735908f0f99e59deb90a4434.jpg)

</code></pre>
<p>Mean Absolute Error（平均绝对误差）:</p>
<pre><code class="language-py">
![\bar{y_m} = \frac{1}{N_m} \sum_{i \in N_m} y_i

H(X_m) = \frac{1}{N_m} \sum_{i \in N_m} |y_i - \bar{y_m}|](img/3b70a99c882760b6d8ed230e145ed742.jpg)

</code></pre>
<p>在 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/fe5ed835e0d3407e3f2d694d8bc049a1.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/fe5ed835e0d3407e3f2d694d8bc049a1.jpg" alt="X_m"></a> 训练 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/94156b879a7455cb0d516efa9c9c0991.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/94156b879a7455cb0d516efa9c9c0991.jpg" alt="m"></a> 节点上的数据时。</p>
<p>示例:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Decision_tree_learning">https://en.wikipedia.org/wiki/Decision_tree_learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Predictive_analytics">https://en.wikipedia.org/wiki/Predictive_analytics</a></li>
<li>L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984.</li>
<li>J.R. Quinlan. C4. 5: programs for machine learning. Morgan Kaufmann, 1993.</li>
<li>T. Hastie, R. Tibshirani and J. Friedman. Elements of Statistical Learning, Springer, 2009.</li>
</ul>
</div>
<hr class="uk-article-divider">
<div class="uk-block uk-block-muted uk-padding-top-remove uk-padding-bottom-remove uk-margin-large-top  book-recommend-wrap">
<div class="uk-margin-top uk-margin-bottom uk-margin-left uk-margin-right">
<div class="uk-margin uk-text-muted "><i class="uk-icon-outdent uk-icon-justify uk-margin-small-right"></i>书籍推荐</div>
<div class="books">
<ul class="uk-book-list">
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/21/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/21/index.html">笨办法学 Python 3</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/16.html">yammgao</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">63页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年5月3日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 12个">12</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/172/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/172/index.html">Seaborn 0.9 中文文档</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/18.html">ApacheCN</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">76页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月26日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 32个">32</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/74/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/74/index.html">Python进阶</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/46.html">东滨社</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">73页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月8日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 2664个">2664</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/28/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/linux_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/28/index.html">笨办法学 Linux</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/15.html">wizardforcel</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="linux">linux</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">34页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年5月3日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 326个">326</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/190/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/nginx_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/190/index.html">Nginx 管理员手册</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/107.html">trimstray</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="nginx">nginx</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">307页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月26日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 8424个">8424</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/74/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/74/index.html">Python进阶</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/46.html">东滨社</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">73页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月8日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 2664个">2664</span>
</div>
</div>
</div>
</li>
<hr>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
<nav class="tm-navbar uk-navbar uk-navbar-attached reader-nav">
<div class="uk-float-left uk-margin-small-top">
<a href="javascript:;" title="目录菜单" class="show-menu  uk-icon-hover  uk-icon-align-justify uk-margin-right"></a>
<div data-uk-dropdown="{mode:'click',pos:'bottom-left'}" class="font-setting-wrap">
<a class="uk-icon-hover uk-icon-font uk-margin-right" aria-label="字体设置" href="javascript:;"></a>
<div class="uk-dropdown dropdown-menu">
<div class="dropdown-caret"><span class="caret-outer"></span><span class="caret-inner"></span></div>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-reduce">小字</button>
<button class="uk-button-link button size-2 font-enlarge">大字</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-1 ">宋体</button>
<button class="uk-button-link button size-2 font-2 ">黑体</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-3 color-theme-sun "><i class="uk-icon-sun-o"></i>白天</button>
<button class="uk-button-link button size-3 color-theme-eye "><i class="uk-icon-eye"></i>护眼</button>
<button class="uk-button-link button size-3 color-theme-moon "><i class="uk-icon-moon-o"></i>夜晚</button></div>
</div>
</div>
<a class="logo uk-margin-right" href="../../../" title="返回首页"><img class="" src="../../../static/components/images/icon_32.png" /></a>
</div>
<div class="uk-navbar-flip  uk-hidden-small">
<div id="share-box"></div>
</div>
</nav>
<div id="menu-id" class="uk-offcanvas reader-offcanvas">
<div class="uk-offcanvas-bar">
<ul class="book-menu-bar uk-nav uk-nav-offcanvas" data-uk-nav>
<li>
<a href="../../../book/170/index.html" data-book-page-rel-url="index.html" data-book-page-id="0" title="封面">封面</a>
</li>
<li>
<a class="pjax" href="../../../book/170/readme.html" data-book-page-rel-url="readme.html" data-book-page-id="0" title="简介">简介</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/79.html" title="安装 scikit-learn" data-book-page-rel-url="docs/79.html" data-book-page-id="11491">安装 scikit-learn</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/1.html" title="1. 监督学习" data-book-page-rel-url="docs/1.html" data-book-page-id="11492">1. 监督学习</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/2.html" title="1.1. 广义线性模型" data-book-page-rel-url="docs/2.html" data-book-page-id="11493">1.1. 广义线性模型</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/3.html" title="1.2. 线性和二次判别分析" data-book-page-rel-url="docs/3.html" data-book-page-id="11494">1.2. 线性和二次判别分析</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/4.html" title="1.3. 内核岭回归" data-book-page-rel-url="docs/4.html" data-book-page-id="11495">1.3. 内核岭回归</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/5.html" title="1.4. 支持向量机" data-book-page-rel-url="docs/5.html" data-book-page-id="11496">1.4. 支持向量机</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/6.html" title="1.5. 随机梯度下降" data-book-page-rel-url="docs/6.html" data-book-page-id="11497">1.5. 随机梯度下降</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/7.html" title="1.6. 最近邻" data-book-page-rel-url="docs/7.html" data-book-page-id="11498">1.6. 最近邻</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/8.html" title="1.7. 高斯过程" data-book-page-rel-url="docs/8.html" data-book-page-id="11499">1.7. 高斯过程</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/9.html" title="1.8. 交叉分解" data-book-page-rel-url="docs/9.html" data-book-page-id="11500">1.8. 交叉分解</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/10.html" title="1.9. 朴素贝叶斯" data-book-page-rel-url="docs/10.html" data-book-page-id="11501">1.9. 朴素贝叶斯</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/11.html" title="1.10. 决策树" data-book-page-rel-url="docs/11.html" data-book-page-id="11502">1.10. 决策树</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/12.html" title="1.11. 集成方法" data-book-page-rel-url="docs/12.html" data-book-page-id="11503">1.11. 集成方法</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/13.html" title="1.12. 多类和多标签算法" data-book-page-rel-url="docs/13.html" data-book-page-id="11504">1.12. 多类和多标签算法</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/14.html" title="1.13. 特征选择" data-book-page-rel-url="docs/14.html" data-book-page-id="11505">1.13. 特征选择</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/15.html" title="1.14. 半监督学习" data-book-page-rel-url="docs/15.html" data-book-page-id="11506">1.14. 半监督学习</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/16.html" title="1.15. 等式回归" data-book-page-rel-url="docs/16.html" data-book-page-id="11507">1.15. 等式回归</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/17.html" title="1.16. 概率校准" data-book-page-rel-url="docs/17.html" data-book-page-id="11508">1.16. 概率校准</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/18.html" title="1.17. 神经网络模型（有监督）" data-book-page-rel-url="docs/18.html" data-book-page-id="11509">1.17. 神经网络模型（有监督）</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/19.html" title="2. 无监督学习" data-book-page-rel-url="docs/19.html" data-book-page-id="11510">2. 无监督学习</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/20.html" title="2.1. 高斯混合模型" data-book-page-rel-url="docs/20.html" data-book-page-id="11511">2.1. 高斯混合模型</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/21.html" title="2.2. 流形学习" data-book-page-rel-url="docs/21.html" data-book-page-id="11512">2.2. 流形学习</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/22.html" title="2.3. 聚类" data-book-page-rel-url="docs/22.html" data-book-page-id="11513">2.3. 聚类</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/23.html" title="2.4. 双聚类" data-book-page-rel-url="docs/23.html" data-book-page-id="11514">2.4. 双聚类</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/24.html" title="2.5. 分解成分中的信号（矩阵分解问题）" data-book-page-rel-url="docs/24.html" data-book-page-id="11515">2.5. 分解成分中的信号（矩阵分解问题）</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/25.html" title="2.6. 协方差估计" data-book-page-rel-url="docs/25.html" data-book-page-id="11516">2.6. 协方差估计</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/26.html" title="2.7. 经验协方差" data-book-page-rel-url="docs/26.html" data-book-page-id="11517">2.7. 经验协方差</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/27.html" title="2.8. 收敛协方差" data-book-page-rel-url="docs/27.html" data-book-page-id="11518">2.8. 收敛协方差</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/28.html" title="2.9. 稀疏逆协方差" data-book-page-rel-url="docs/28.html" data-book-page-id="11519">2.9. 稀疏逆协方差</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/29.html" title="2.10. Robust 协方差估计" data-book-page-rel-url="docs/29.html" data-book-page-id="11520">2.10. Robust 协方差估计</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/30.html" title="2.11. 新奇和异常值检测" data-book-page-rel-url="docs/30.html" data-book-page-id="11521">2.11. 新奇和异常值检测</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/31.html" title="2.12. 密度估计" data-book-page-rel-url="docs/31.html" data-book-page-id="11522">2.12. 密度估计</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/32.html" title="2.13. 神经网络模型（无监督）" data-book-page-rel-url="docs/32.html" data-book-page-id="11523">2.13. 神经网络模型（无监督）</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/33.html" title="3. 模型选择和评估" data-book-page-rel-url="docs/33.html" data-book-page-id="11524">3. 模型选择和评估</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/34.html" title="3.1. 交叉验证：评估估算器的表现" data-book-page-rel-url="docs/34.html" data-book-page-id="11525">3.1. 交叉验证：评估估算器的表现</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/35.html" title="3.2. 调整估计器的超参数" data-book-page-rel-url="docs/35.html" data-book-page-id="11526">3.2. 调整估计器的超参数</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/53.html" title="3.3. 模型评估: 量化预测的质量" data-book-page-rel-url="docs/53.html" data-book-page-id="11527">3.3. 模型评估: 量化预测的质量</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/54.html" title="3.4. 模型持久化" data-book-page-rel-url="docs/54.html" data-book-page-id="11528">3.4. 模型持久化</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/55.html" title="3.5. 验证曲线: 绘制分数以评估模型" data-book-page-rel-url="docs/55.html" data-book-page-id="11529">3.5. 验证曲线: 绘制分数以评估模型</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/56.html" title="4. 数据集转换" data-book-page-rel-url="docs/56.html" data-book-page-id="11530">4. 数据集转换</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/57.html" title="4.1. Pipeline（管道）和 FeatureUnion（特征联合）: 合并的评估器" data-book-page-rel-url="docs/57.html" data-book-page-id="11531">4.1. Pipeline（管道）和 FeatureUnion（特征联合）: 合并的评估器</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/58.html" title="4.2. 特征提取" data-book-page-rel-url="docs/58.html" data-book-page-id="11532">4.2. 特征提取</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/59.html" title="4.3. 预处理数据" data-book-page-rel-url="docs/59.html" data-book-page-id="11533">4.3. 预处理数据</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/60.html" title="4.4. 无监督降维" data-book-page-rel-url="docs/60.html" data-book-page-id="11534">4.4. 无监督降维</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/61.html" title="4.5. 随机投影" data-book-page-rel-url="docs/61.html" data-book-page-id="11535">4.5. 随机投影</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/62.html" title="4.6. 内核近似" data-book-page-rel-url="docs/62.html" data-book-page-id="11536">4.6. 内核近似</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/63.html" title="4.7. 成对的矩阵, 类别和核函数" data-book-page-rel-url="docs/63.html" data-book-page-id="11537">4.7. 成对的矩阵, 类别和核函数</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/64.html" title="4.8. 预测目标 (`y`.html) 的转换" data-book-page-rel-url="docs/64.html" data-book-page-id="11538">4.8. 预测目标 (`y`.html) 的转换</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/65.html" title="5. 数据集加载工具" data-book-page-rel-url="docs/65.html" data-book-page-id="11539">5. 数据集加载工具</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/66.html" title="6. 大规模计算的策略: 更大量的数据" data-book-page-rel-url="docs/66.html" data-book-page-id="11540">6. 大规模计算的策略: 更大量的数据</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/67.html" title="7. 计算性能" data-book-page-rel-url="docs/67.html" data-book-page-id="11541">7. 计算性能</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/68.html" title="使用 scikit-learn 介绍机器学习" data-book-page-rel-url="docs/68.html" data-book-page-id="11542">使用 scikit-learn 介绍机器学习</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/69.html" title="关于科学数据处理的统计学习教程" data-book-page-rel-url="docs/69.html" data-book-page-id="11543">关于科学数据处理的统计学习教程</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/70.html" title="机器学习: scikit-learn 中的设置以及预估对象" data-book-page-rel-url="docs/70.html" data-book-page-id="11544">机器学习: scikit-learn 中的设置以及预估对象</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/71.html" title="监督学习：从高维观察预测输出变量" data-book-page-rel-url="docs/71.html" data-book-page-id="11545">监督学习：从高维观察预测输出变量</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/72.html" title="模型选择：选择估计量及其参数" data-book-page-rel-url="docs/72.html" data-book-page-id="11546">模型选择：选择估计量及其参数</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/73.html" title="无监督学习: 寻求数据表示" data-book-page-rel-url="docs/73.html" data-book-page-id="11547">无监督学习: 寻求数据表示</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/74.html" title="把它们放在一起" data-book-page-rel-url="docs/74.html" data-book-page-id="11548">把它们放在一起</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/75.html" title="寻求帮助" data-book-page-rel-url="docs/75.html" data-book-page-id="11549">寻求帮助</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/76.html" title="处理文本数据" data-book-page-rel-url="docs/76.html" data-book-page-id="11550">处理文本数据</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/77.html" title="选择正确的评估器(estimator.html)" data-book-page-rel-url="docs/77.html" data-book-page-id="11551">选择正确的评估器(estimator.html)</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/78.html" title="外部资源，视频和谈话" data-book-page-rel-url="docs/78.html" data-book-page-id="11552">外部资源，视频和谈话</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/80.html" title="常见问题" data-book-page-rel-url="docs/80.html" data-book-page-id="11553">常见问题</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/81.html" title="时光轴" data-book-page-rel-url="docs/81.html" data-book-page-id="11554">时光轴</a>
</li>
</ul>
</div>
</div>
<script src="https://cdn.staticfile.net/jquery/1.12.4/jquery.min.js"></script>
<script type="text/javascript" src="../../../static/components/uikit-2.27.5/js/uikit.reader.js"></script>
<script type="text/javascript" src="../../../static/components/social-share/social-share.min.js"></script>
<script>(function(){var bp =document.createElement('script');var curProtocol =window.location.protocol.split(':')[0];if (curProtocol ==='https') {bp.src ='https://zz.bdstatic.com/linksubmit/push.js';}
else {bp.src ='http://push.zhanzhang.baidu.com/push.js';}
var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(bp,s);})();</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
<script src="https://cdn.staticfile.net/highlight.js/9.12.0/highlight.min.js"></script>
<script src="https://cdn.staticfile.net/jquery.pjax/2.0.1/jquery.pjax.min.js"></script>
<script src="https://cdn.staticfile.net/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="https://cdn.staticfile.net/uikit/2.27.5/js/components/lightbox.min.js"></script>
<link rel="dns-prefetch" href="../../..//cdn.mathjax.org" />
<script type="text/x-mathjax-config">
 function initMathJax() {
    var mathId = $("book-content-section")[0];
    MathJax.Hub.Config({
        tex2jax: {skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code','a']},
        showProcessingMessages: false,
        messageStyle: "none"
    });
    MathJax.Hub.Queue(["Typeset",MathJax.Hub,mathId]);
 };
initMathJax();
</script>
<script src='https://cdn.staticfile.net/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML' async></script>
<style>
	.MathJax_Display{display:inline!important;}
</style>
<script type="text/javascript" src="../../../static/components/js/reader.js"></script>
<script type="text/javascript">var bookId =170;var bookPageId =11502;var bookPageRelUrl ='docs/11.html';</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
</body>
</html>