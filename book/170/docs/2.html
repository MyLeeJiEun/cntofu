
<!DOCTYPE html>
<html lang="zh-CN">
<head>
<title>1.1. 广义线性模型-scikit-learn (sklearn) 官方文档中文版</title>
<meta content='1.1. 广义线性模型,scikit-learn (sklearn) 官方文档中文版' name='keywords'>
<meta content='1.1. 广义线性模型,scikit-learn (sklearn) 官方文档中文版' name='description'>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta http-equiv="Content-Language" content="zh-CN" />
<meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1, maximum-scale=1, user-scalable=no"../../../>
<meta name="applicable-device" content="pc,mobile">
<link rel="shortcut icon" href="../../../favicon.ico" type="image/x-icon" />
<meta name="renderer" content="webkit">
<link rel="stylesheet" href="../../../static/components/uikit-2.27.5/css/uikit.custom.css">
<link rel="stylesheet" href="../../../static/components/social-share/social-share.min.css">
<link rel="stylesheet" href="../../../static/components/highlight/styles/custom.css">
<link rel="stylesheet" href="../../../static/components/css/base.css">
<link rel="stylesheet" href="../../../static/components/css/reader.css">
<link rel="stylesheet" href="../../../static/components/css/markdown.css">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5313208362165053" crossorigin="anonymous"></script>
</head>
<body>
<div class=" book-main-wrap uk-container uk-container-center uk-margin-top ">
<div class="uk-grid">
<div class="uk-width-1-1 reader-wrap ">
<div class=" bottom-nav uk-clearfix ">
<div class="uk-align-left ">
<a href="../../../book/170/docs/1.html">
<i class="nav-icon-left uk-icon-small  uk-icon-caret-left"></i>
<span class="">1. 监督学习</span>
</a>
</div>
<div class="uk-align-right ">
<a href="../../../book/170/docs/3.html">
<span class="">1.2. 线性和二次判..</span>
<i class="nav-icon-right uk-icon-small  uk-icon-caret-right"></i>
</a>
</div>
</div>
<div class="uk-text-center">
<h2 class="book-page-title uk-container-center">
<a href="../../../book/170/index.html">scikit-learn (sklearn) 官方文档中文版</a>
<a target="_blank" rel="nofollow" href="https://github.com/apachecn/scikit-learn-doc-zh" class="uk-icon-button uk-icon-github" title="github项目地址"></a>
</h2>
</div>
<script type="text/javascript" src="../../../static/components/js/app_intro.js"></script>
<ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-5313208362165053" data-ad-slot="1328047120"></ins>
<script>(adsbygoogle =window.adsbygoogle ||[]).push({});</script>
<hr class="uk-article-divider">
<div class="book-content-section  md-content-section  uk-margin-bottom">
<h1 id="11-广义线性模型">1.1. 广义线性模型</h1>
<p>校验者: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@专业吹牛逼的小明</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@Gladiator</a> 翻译者: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@瓜牛</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@年纪大了反应慢了</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@Hazekiah</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@BWM-蜜蜂</a></p>
<p>下面是一组用于回归的方法，其中目标值 y 是输入变量 x 的线性组合。 在数学概念中，如果 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/047826f1c2e6f2687b304cb5217be8d8.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/047826f1c2e6f2687b304cb5217be8d8.jpg" alt="\hat{y}"></a> 是预测值。</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4ee9f6c666393981b6458e54c3ec89d0.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4ee9f6c666393981b6458e54c3ec89d0.jpg" alt="\hat{y}(w, x) = w_0 + w_1 x_1 + ... + w_p x_p"></a></p>
<p>在整个模块中，我们定义向量 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b003858334d1ad594207911e84219151.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b003858334d1ad594207911e84219151.jpg" alt="w = (w_1,..., w_p)"></a> 作为 <code>coef_</code> ，定义 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/57e15e43b846791e47a202e1a9a5d8ce.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/57e15e43b846791e47a202e1a9a5d8ce.jpg" alt="w_0"></a> 作为 <code>intercept_</code> 。</p>
<p>如果需要使用广义线性模型进行分类，请参阅 <a href="#logistic-regression">logistic 回归</a> 。</p>
<h2 id="111-普通最小二乘法">1.1.1. 普通最小二乘法</h2>
<p><a href="generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression" title="sklearn.linear_model.LinearRegression"><code>LinearRegression</code></a> 拟合一个带有系数 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3f5adc0c9b0e51a0759ed6ac49f94431.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3f5adc0c9b0e51a0759ed6ac49f94431.jpg" alt="w = (w_1, ..., w_p)"></a> 的线性模型，使得数据集实际观测数据和预测数据（估计值）之间的残差平方和最小。其数学表达式为:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1b6228a71a038f66ac7b8a2743adf4e7.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1b6228a71a038f66ac7b8a2743adf4e7.jpg" alt="\underset{w}{min\,} {|| X w - y||_2}^2"></a></p>
<p><a href="../auto_examples/linear_model/plot_ols.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/af7b81123e6cdf0b42acec802041beef.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/af7b81123e6cdf0b42acec802041beef.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_ols_0011.png"></a></a></p>
<p><a href="generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression" title="sklearn.linear_model.LinearRegression"><code>LinearRegression</code></a> 会调用 <code>fit</code> 方法来拟合数组 X， y，并且将线性模型的系数 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8a58e8df6a985a3273e39bac7dd72b1f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8a58e8df6a985a3273e39bac7dd72b1f.jpg" alt="w"></a> 存储在其成员变量 <code>coef_</code> 中:</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn import linear_model
&gt;&gt;&gt; reg = linear_model.LinearRegression()
&gt;&gt;&gt; reg.fit ([[0, 0], [1, 1], [2, 2]], [0, 1, 2])
LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
&gt;&gt;&gt; reg.coef_
array([ 0.5,  0.5])

</code></pre>
<p>然而，对于普通最小二乘的系数估计问题，其依赖于模型各项的相互独立性。当各项是相关的，且设计矩阵 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43c1fea57579e54f80c0535bc582626f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43c1fea57579e54f80c0535bc582626f.jpg" alt="X"></a> 的各列近似线性相关，那么，设计矩阵会趋向于奇异矩阵，这会导致最小二乘估计对于随机误差非常敏感，产生很大的方差。例如，在没有实验设计的情况下收集到的数据，这种多重共线性（multicollinearity）的情况可能真的会出现。</p>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py">Linear Regression Example</a></li>
</ul>
<h3 id="1111-普通最小二乘法复杂度">1.1.1.1. 普通最小二乘法复杂度</h3>
<p>该方法使用 X 的奇异值分解来计算最小二乘解。如果 X 是一个 size 为 (n, p) 的矩阵，设 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7a7a32bd2dd0da3d117c39efc7e35dd3.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7a7a32bd2dd0da3d117c39efc7e35dd3.jpg" alt="n \geq p"></a> ，则该方法的复杂度为 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e761380fca8200d40164e77965652982.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e761380fca8200d40164e77965652982.jpg" alt="O(n p^2)"></a></p>
<h2 id="112-岭回归">1.1.2. 岭回归</h2>
<p><a href="generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code>Ridge</code></a> 回归通过对系数的大小施加惩罚来解决 <a href="#ordinary-least-squares">普通最小二乘法</a> 的一些问题。 岭系数最小化的是带罚项的残差平方和，</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c7e49892dca2f0df35d1261a276693f2.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c7e49892dca2f0df35d1261a276693f2.jpg" alt="\underset{w}{min\,} {{|| X w - y||_2}2 + \alpha {||w||_2}2}"></a></p>
<p>其中， <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a4775baaa990a4fbffcfc2688e3b5578.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a4775baaa990a4fbffcfc2688e3b5578.jpg" alt="\alpha \geq 0"></a> 是控制系数收缩量的复杂性参数： <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d8b3d5242d513369a44f8bf0c6112744.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d8b3d5242d513369a44f8bf0c6112744.jpg" alt="\alpha"></a> 的值越大，收缩量越大，这样系数对共线性的鲁棒性也更强。</p>
<p><a href="../auto_examples/linear_model/plot_ridge_path.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a4a20739f22e7059a927ba615ec373da.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a4a20739f22e7059a927ba615ec373da.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_ridge_path_0011.png"></a></a></p>
<p>与其他线性模型一样， <a href="generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code>Ridge</code></a> 用 <code>fit</code> 方法将模型系数 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8a58e8df6a985a3273e39bac7dd72b1f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8a58e8df6a985a3273e39bac7dd72b1f.jpg" alt="w"></a> 存储在其 <code>coef_</code> 成员中:</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn import linear_model
&gt;&gt;&gt; reg = linear_model.Ridge (alpha = .5)
&gt;&gt;&gt; reg.fit ([[0, 0], [0, 0], [1, 1]], [0, .1, 1]) 
Ridge(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=None,
 normalize=False, random_state=None, solver='auto', tol=0.001)
&gt;&gt;&gt; reg.coef_
array([ 0.34545455,  0.34545455])
&gt;&gt;&gt; reg.intercept_ 
0.13636...

</code></pre>
<p>示例:</p>
<ul>
<li>:ref:[<code>](#id5)sphx_glr_auto_examples_linear_model_plot_ridge_path.py</code>( 作为正则化的函数，绘制岭系数 )</li>
<li>:ref:[<code>](#id7)sphx_glr_auto_examples_text_document_classification_20newsgroups.py</code>( 使用稀疏特征的文本文档分类 )</li>
</ul>
<h3 id="1121-岭回归的复杂度">1.1.2.1. 岭回归的复杂度</h3>
<p>这种方法与 <a href="#ordinary-least-squares">普通最小二乘法</a> 的复杂度是相同的.</p>
<h3 id="1122-设置正则化参数广义交叉验证">1.1.2.2. 设置正则化参数：广义交叉验证</h3>
<p><a href="generated/sklearn.linear_model.RidgeCV.html#sklearn.linear_model.RidgeCV" title="sklearn.linear_model.RidgeCV"><code>RidgeCV</code></a> 通过内置的 Alpha 参数的交叉验证来实现岭回归。 该对象与 GridSearchCV 的使用方法相同，只是它默认为 Generalized Cross-Validation(广义交叉验证 GCV)，这是一种有效的留一验证方法（LOO-CV）:</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn import linear_model
&gt;&gt;&gt; reg = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])
&gt;&gt;&gt; reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])       
RidgeCV(alphas=[0.1, 1.0, 10.0], cv=None, fit_intercept=True, scoring=None,
 normalize=False)
&gt;&gt;&gt; reg.alpha_                                      
0.1

</code></pre>
<p>参考</p>
<ul>
<li>“Notes on Regularized Least Squares”, Rifkin &amp; Lippert (<a href="http://cbcl.mit.edu/projects/cbcl/publications/ps/MIT-CSAIL-TR-2007-025.pdf">technical report</a>, <a href="http://www.mit.edu/~9.520/spring07/Classes/rlsslides.pdf">course slides</a>).</li>
</ul>
<h2 id="113-lasso">1.1.3. Lasso</h2>
<p>The <a href="generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso" title="sklearn.linear_model.Lasso"><code>Lasso</code></a> 是估计稀疏系数的线性模型。 它在一些情况下是有用的，因为它倾向于使用具有较少参数值的情况，有效地减少给定解决方案所依赖变量的数量。 因此，Lasso 及其变体是压缩感知领域的基础。 在一定条件下，它可以恢复一组非零权重的精确集（见 <a href="../auto_examples/applications/plot_tomography_l1_reconstruction.html#sphx-glr-auto-examples-applications-plot-tomography-l1-reconstruction-py">Compressive sensing: tomography reconstruction with L1 prior (Lasso)</a> ）。</p>
<p>在数学公式表达上，它由一个带有 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3bf32d926cdf24f440b6b831f0d9cc37.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3bf32d926cdf24f440b6b831f0d9cc37.jpg" alt="\ell_1"></a> 先验的正则项的线性模型组成。 其最小化的目标函数是:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/187eddee2de4e12860dc001c5f74b2b4.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/187eddee2de4e12860dc001c5f74b2b4.jpg" alt="\underset{w}{min\,} { \frac{1}{2n_{samples}} ||X w - y||_2 ^ 2 + \alpha ||w||_1}"></a></p>
<p>lasso estimate 解决了加上罚项 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b2ea359213f8f5b01eead0821e29e856.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b2ea359213f8f5b01eead0821e29e856.jpg" alt="\alpha ||w||_1"></a> 的最小二乘法的最小化，其中， <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d8b3d5242d513369a44f8bf0c6112744.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d8b3d5242d513369a44f8bf0c6112744.jpg" alt="\alpha"></a> 是一个常数， <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/62eb544f1f6e234c61099fea1517300b.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/62eb544f1f6e234c61099fea1517300b.jpg" alt="||w||_1"></a> 是参数向量的 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3bf32d926cdf24f440b6b831f0d9cc37.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3bf32d926cdf24f440b6b831f0d9cc37.jpg" alt="\ell_1"></a>-norm 范数。</p>
<p><a href="generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso" title="sklearn.linear_model.Lasso"><code>Lasso</code></a> 类的实现使用了 coordinate descent （坐标下降算法）来拟合系数。 查看 <a href="#least-angle-regression">最小角回归</a> ，这是另一种方法:</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn import linear_model
&gt;&gt;&gt; reg = linear_model.Lasso(alpha = 0.1)
&gt;&gt;&gt; reg.fit([[0, 0], [1, 1]], [0, 1])
Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,
 normalize=False, positive=False, precompute=False, random_state=None,
 selection='cyclic', tol=0.0001, warm_start=False)
&gt;&gt;&gt; reg.predict([[1, 1]])
array([ 0.8])

</code></pre>
<p>对于较低级别的任务，同样有用的是函数 <a href="generated/sklearn.linear_model.lasso_path.html#sklearn.linear_model.lasso_path" title="sklearn.linear_model.lasso_path"><code>lasso_path</code></a> 。它能够通过搜索所有可能的路径上的值来计算系数。</p>
<p>举例:</p>
<ul>
<li><a href="../auto_examples/linear_model/plot_lasso_and_elasticnet.html#sphx-glr-auto-examples-linear-model-plot-lasso-and-elasticnet-py">Lasso and Elastic Net for Sparse Signals</a> （稀疏信号的 lasso 和弹性网）</li>
<li><a href="../auto_examples/applications/plot_tomography_l1_reconstruction.html#sphx-glr-auto-examples-applications-plot-tomography-l1-reconstruction-py">Compressive sensing: tomography reconstruction with L1 prior (Lasso)</a> （压缩感知：L1 先验（Lasso）的断层扫描重建）</li>
</ul>
<p>Note</p>
<p><strong>Feature selection with Lasso（使用 Lasso 进行特征选择）</strong></p>
<p>由于 Lasso 回归产生稀疏模型，因此可以用于执行特征选择，详见 <a href="feature_selection.html#l1-feature-selection">基于 L1 的特征选取</a> （基于 L1 的特征选择）。</p>
<h3 id="1131-设置正则化参数">1.1.3.1. 设置正则化参数</h3>
<blockquote>
<p><code>alpha</code> 参数控制估计系数的稀疏度。</p>
</blockquote>
<h4 id="11311-使用交叉验证">1.1.3.1.1. 使用交叉验证</h4>
<p>scikit-learn 通过交叉验证来公开设置 Lasso <code>alpha</code> 参数的对象: <a href="generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV" title="sklearn.linear_model.LassoCV"><code>LassoCV</code></a> 和 <a href="generated/sklearn.linear_model.LassoLarsCV.html#sklearn.linear_model.LassoLarsCV" title="sklearn.linear_model.LassoLarsCV"><code>LassoLarsCV</code></a>。 <a href="generated/sklearn.linear_model.LassoLarsCV.html#sklearn.linear_model.LassoLarsCV" title="sklearn.linear_model.LassoLarsCV"><code>LassoLarsCV</code></a> 是基于下面解释的 <a href="#least-angle-regression">最小角回归</a> 算法。</p>
<p>对于具有许多线性回归的高维数据集， <a href="generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV" title="sklearn.linear_model.LassoCV"><code>LassoCV</code></a> 最常见。 然而，<a href="generated/sklearn.linear_model.LassoLarsCV.html#sklearn.linear_model.LassoLarsCV" title="sklearn.linear_model.LassoLarsCV"><code>LassoLarsCV</code></a> 在寻找 &lt;cite&gt;alpha&lt;/cite&gt; 参数值上更具有优势，而且如果样本数量与特征数量相比非常小时，通常 <a href="generated/sklearn.linear_model.LassoLarsCV.html#sklearn.linear_model.LassoLarsCV" title="sklearn.linear_model.LassoLarsCV"><code>LassoLarsCV</code></a> 比 <a href="generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV" title="sklearn.linear_model.LassoCV"><code>LassoCV</code></a> 要快。</p>
<p><strong><a href="../auto_examples/linear_model/plot_lasso_model_selection.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/035f009eecfdebf82b493f797843a919.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/035f009eecfdebf82b493f797843a919.jpg" alt="lasso_cv_1"></a></a> <a href="../auto_examples/linear_model/plot_lasso_model_selection.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ab2096ee4087e644cca732d92d241edf.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ab2096ee4087e644cca732d92d241edf.jpg" alt="lasso_cv_2"></a></a></strong></p>
<h4 id="11312-基于信息标准的模型选择">1.1.3.1.2. 基于信息标准的模型选择</h4>
<p>有多种选择时，估计器 <a href="generated/sklearn.linear_model.LassoLarsIC.html#sklearn.linear_model.LassoLarsIC" title="sklearn.linear_model.LassoLarsIC"><code>LassoLarsIC</code></a> 建议使用 Akaike information criterion （Akaike 信息准则）（AIC）和 Bayes Information criterion （贝叶斯信息准则）（BIC）。 当使用 k-fold 交叉验证时，正则化路径只计算一次而不是 k + 1 次，所以找到 α 的最优值是一种计算上更便宜的替代方法。 然而，这样的标准需要对解决方案的自由度进行适当的估计，对于大样本（渐近结果）导出，并假设模型是正确的，即数据实际上是由该模型生成的。 当问题严重受限（比样本更多的特征）时，他们也倾向于打破。</p>
<p><a href="../auto_examples/linear_model/plot_lasso_model_selection.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e7a07569c8c6af174aa061b9f8921065.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e7a07569c8c6af174aa061b9f8921065.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_lasso_model_selection_0011.png"></a></a></p>
<p>示例:</p>
<ul>
<li>:ref:[<code>](#id15)sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py</code>(Lasso 型号选择：交叉验证/AIC/BIC)</li>
</ul>
<h4 id="11313-与-svm-的正则化参数的比较">1.1.3.1.3. 与 SVM 的正则化参数的比较</h4>
<p><code>alpha</code> 和 SVM 的正则化参数<code>C</code> 之间的等式关系是 <code>alpha = 1 / C</code> 或者 <code>alpha = 1 / (n_samples * C)</code> ，并依赖于估计器和模型优化的确切的目标函数。 .. _multi_task_lasso:</p>
<h2 id="114-多任务-lasso">1.1.4. 多任务 Lasso</h2>
<blockquote>
<p><a href="generated/sklearn.linear_model.MultiTaskLasso.html#sklearn.linear_model.MultiTaskLasso" title="sklearn.linear_model.MultiTaskLasso"><code>MultiTaskLasso</code></a> 是一个估计多元回归稀疏系数的线性模型： <code>y</code> 是一个 <code>(n_samples, n_tasks)</code> 的二维数组，其约束条件和其他回归问题（也称为任务）是一样的，都是所选的特征值。</p>
</blockquote>
<p>下图比较了通过使用简单的 Lasso 或 MultiTaskLasso 得到的 W 中非零的位置。 Lasso 估计产生分散的非零值，而 MultiTaskLasso 的一整列都是非零的。</p>
<p><strong><a href="../auto_examples/linear_model/plot_multi_task_lasso_support.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0449a2a9bce6d759e7253da7d17fa938.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0449a2a9bce6d759e7253da7d17fa938.jpg" alt="multi_task_lasso_1"></a></a> <a href="../auto_examples/linear_model/plot_multi_task_lasso_support.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3b1e10150e98ef95e977c12ad0607620.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3b1e10150e98ef95e977c12ad0607620.jpg" alt="multi_task_lasso_2"></a></a></strong></p>
<p><strong>拟合 time-series model （时间序列模型），强制任何活动的功能始终处于活动状态。</strong></p>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/linear_model/plot_multi_task_lasso_support.html#sphx-glr-auto-examples-linear-model-plot-multi-task-lasso-support-py">Joint feature selection with multi-task Lasso</a> （联合功能选择与多任务 Lasso）</li>
</ul>
<p>在数学上，它由一个线性模型组成，以混合的 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3bf32d926cdf24f440b6b831f0d9cc37.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3bf32d926cdf24f440b6b831f0d9cc37.jpg" alt="\ell_1"></a> <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8851bd0fe9749b4841b30cee41fb040d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8851bd0fe9749b4841b30cee41fb040d.jpg" alt="\ell_2"></a> 作为正则化器进行训练。目标函数最小化是：</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/aba64ff85b1f99c5d1c4f8e1ace15f89.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/aba64ff85b1f99c5d1c4f8e1ace15f89.jpg" alt="\underset{w}{min\,} { \frac{1}{2n_{samples}} ||X W - Y||{Fro} ^ 2 + \alpha ||W||{21}}"></a></p>
<p>其中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/987fc6b717a40e57a95fb79a8e809309.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/987fc6b717a40e57a95fb79a8e809309.jpg" alt="Fro"></a> 表示 Frobenius 标准：</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9259b19a18f30f67db9e45b8c0b361c7.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9259b19a18f30f67db9e45b8c0b361c7.jpg" alt="||A||{Fro} = \sqrt{\sum{ij} a_{ij}^2}"></a></p>
<p>并且 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3bf32d926cdf24f440b6b831f0d9cc37.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3bf32d926cdf24f440b6b831f0d9cc37.jpg" alt="\ell_1"></a> <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8851bd0fe9749b4841b30cee41fb040d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8851bd0fe9749b4841b30cee41fb040d.jpg" alt="\ell_2"></a> 读取为:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2f373d871220ee042a8c2ee44e6fff3a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2f373d871220ee042a8c2ee44e6fff3a.jpg" alt="||A||{2 1} = \sum_i \sqrt{\sum_j a{ij}^2}"></a></p>
<p><a href="generated/sklearn.linear_model.MultiTaskLasso.html#sklearn.linear_model.MultiTaskLasso" title="sklearn.linear_model.MultiTaskLasso"><code>MultiTaskLasso</code></a> 类的实现使用了坐标下降作为拟合系数的算法。</p>
<h2 id="115-弹性网络">1.1.5. 弹性网络</h2>
<p><code>弹性网络</code> 是一种使用 L1， L2 范数作为先验正则项训练的线性回归模型。 这种组合允许学习到一个只有少量参数是非零稀疏的模型，就像 <a href="generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso" title="sklearn.linear_model.Lasso"><code>Lasso</code></a> 一样，但是它仍然保持 一些像 <a href="generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code>Ridge</code></a> 的正则性质。我们可利用 <code>l1_ratio</code> 参数控制 L1 和 L2 的凸组合。</p>
<p>弹性网络在很多特征互相联系的情况下是非常有用的。Lasso 很可能只随机考虑这些特征中的一个，而弹性网络更倾向于选择两个。</p>
<p>在实践中，Lasso 和 Ridge 之间权衡的一个优势是它允许在循环过程（Under rotate）中继承 Ridge 的稳定性。</p>
<p>在这里，最小化的目标函数是</p>
<pre><code class="language-py">
![\underset{w}{min\,} { \frac{1}{2n_{samples}} ||X w - y||_2 ^ 2 + \alpha \rho ||w||_1 +
\frac{\alpha(1-\rho)}{2} ||w||_2 ^ 2}](img/9b9ee41d276ad49322856b95cb6c7e43.jpg)

</code></pre>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/Images/aa0c61cd560f0fdab4fe10c7b12e5082.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/Images/aa0c61cd560f0fdab4fe10c7b12e5082.jpg" alt="https://scikit-learn.org/stable/_images/sphx_glr_plot_lasso_coordinate_descent_path_001.png../auto_examples/linear_model/plot_lasso_coordinate_descent_path.htmlcenter:scale:50%"></a></p>
<p><a href="generated/sklearn.linear_model.ElasticNetCV.html#sklearn.linear_model.ElasticNetCV" title="sklearn.linear_model.ElasticNetCV"><code>ElasticNetCV</code></a> 类可以通过交叉验证来设置参数 <code>alpha</code> （ <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d8b3d5242d513369a44f8bf0c6112744.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d8b3d5242d513369a44f8bf0c6112744.jpg" alt="\alpha"></a> ） 和 <code>l1_ratio</code> （ <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b91e4507d9fd7068b02f689d697f8714.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b91e4507d9fd7068b02f689d697f8714.jpg" alt="\rho"></a> ） 。</p>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/linear_model/plot_lasso_and_elasticnet.html#sphx-glr-auto-examples-linear-model-plot-lasso-and-elasticnet-py">Lasso and Elastic Net for Sparse Signals</a></li>
<li><a href="../auto_examples/linear_model/plot_lasso_coordinate_descent_path.html#sphx-glr-auto-examples-linear-model-plot-lasso-coordinate-descent-path-py">Lasso and Elastic Net</a></li>
</ul>
<h2 id="116-多任务弹性网络">1.1.6. 多任务弹性网络</h2>
<blockquote>
<p><a href="generated/sklearn.linear_model.MultiTaskElasticNet.html#sklearn.linear_model.MultiTaskElasticNet" title="sklearn.linear_model.MultiTaskElasticNet"><code>MultiTaskElasticNet</code></a> 是一个对多回归问题估算稀疏参数的弹性网络: <code>Y</code> 是一个二维数组，形状是 <code>(n_samples,n_tasks)</code>。 其限制条件是和其他回归问题一样，是选择的特征，也称为 tasks 。</p>
</blockquote>
<p>从数学上来说， 它包含一个混合的 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3bf32d926cdf24f440b6b831f0d9cc37.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3bf32d926cdf24f440b6b831f0d9cc37.jpg" alt="\ell_1"></a> <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8851bd0fe9749b4841b30cee41fb040d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8851bd0fe9749b4841b30cee41fb040d.jpg" alt="\ell_2"></a> 先验和 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8851bd0fe9749b4841b30cee41fb040d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8851bd0fe9749b4841b30cee41fb040d.jpg" alt="\ell_2"></a> 先验为正则项训练的线性模型 目标函数就是最小化:</p>
<pre><code class="language-py">
![\underset{W}{min\,} { \frac{1}{2n_{samples}} ||X W - Y||_{Fro}^2 + \alpha \rho ||W||_{2 1} +
\frac{\alpha(1-\rho)}{2} ||W||_{Fro}^2}](img/a1670c1fcb5b7ad10830f43812ed50da.jpg)

</code></pre>
<p>在 <a href="generated/sklearn.linear_model.MultiTaskElasticNet.html#sklearn.linear_model.MultiTaskElasticNet" title="sklearn.linear_model.MultiTaskElasticNet"><code>MultiTaskElasticNet</code></a> 类中的实现采用了坐标下降法求解参数。</p>
<p>在 <a href="generated/sklearn.linear_model.MultiTaskElasticNetCV.html#sklearn.linear_model.MultiTaskElasticNetCV" title="sklearn.linear_model.MultiTaskElasticNetCV"><code>MultiTaskElasticNetCV</code></a> 中可以通过交叉验证来设置参数 <code>alpha</code> （ <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d8b3d5242d513369a44f8bf0c6112744.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d8b3d5242d513369a44f8bf0c6112744.jpg" alt="\alpha"></a> ） 和 <code>l1_ratio</code> （ <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b91e4507d9fd7068b02f689d697f8714.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b91e4507d9fd7068b02f689d697f8714.jpg" alt="\rho"></a> ） 。</p>
<h2 id="117-最小角回归">1.1.7. 最小角回归</h2>
<p>最小角回归 （LARS） 是对高维数据的回归算法， 由 Bradley Efron, Trevor Hastie, Iain Johnstone 和 Robert Tibshirani 开发完成。 LARS 和逐步回归很像。在每一步，它寻找与响应最有关联的 预测。当有很多预测有相同的关联时，它没有继续利用相同的预测，而是在这些预测中找出应该等角的方向。</p>
<p>LARS的优点:</p>
<blockquote>
<ul>
<li>当 p &gt;&gt; n，该算法数值运算上非常有效。(例如当维度的数目远超点的个数)</li>
<li>它在计算上和前向选择一样快，和普通最小二乘法有相同的运算复杂度。</li>
<li>它产生了一个完整的分段线性的解决路径，在交叉验证或者其他相似的微调模型的方法上非常有用。</li>
<li>如果两个变量对响应几乎有相等的联系，则它们的系数应该有相似的增长率。因此这个算法和我们直觉 上的判断一样，而且还更加稳定。</li>
<li>它很容易修改并为其他估算器生成解，比如Lasso。</li>
</ul>
</blockquote>
<p>LARS 的缺点:</p>
<blockquote>
<ul>
<li>因为 LARS 是建立在循环拟合剩余变量上的，所以它对噪声非常敏感。这个问题，在 2004 年统计年鉴的文章由 Weisberg 详细讨论。</li>
</ul>
</blockquote>
<p>LARS 模型可以在 <a href="generated/sklearn.linear_model.Lars.html#sklearn.linear_model.Lars" title="sklearn.linear_model.Lars"><code>Lars</code></a> ，或者它的底层实现 <a href="generated/sklearn.linear_model.lars_path.html#sklearn.linear_model.lars_path" title="sklearn.linear_model.lars_path"><code>lars_path</code></a> 中被使用。</p>
<h2 id="118-lars-lasso">1.1.8. LARS Lasso</h2>
<p><a href="generated/sklearn.linear_model.LassoLars.html#sklearn.linear_model.LassoLars" title="sklearn.linear_model.LassoLars"><code>LassoLars</code></a> 是一个使用 LARS 算法的 lasso 模型，不同于基于坐标下降法的实现，它可以得到一个精确解，也就是一个关于自身参数标准化后的一个分段线性解。</p>
<p><a href="../auto_examples/linear_model/plot_lasso_lars.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/05459a925be9207abbb2f72203e48cf2.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/05459a925be9207abbb2f72203e48cf2.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_lasso_lars_0011.png"></a></a></p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn import linear_model
&gt;&gt;&gt; reg = linear_model.LassoLars(alpha=.1)
&gt;&gt;&gt; reg.fit([[0, 0], [1, 1]], [0, 1])  
LassoLars(alpha=0.1, copy_X=True, eps=..., fit_intercept=True,
 fit_path=True, max_iter=500, normalize=True, positive=False,
 precompute='auto', verbose=False)
&gt;&gt;&gt; reg.coef_    
array([ 0.717157...,  0\.        ])

</code></pre>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/linear_model/plot_lasso_lars.html#sphx-glr-auto-examples-linear-model-plot-lasso-lars-py">Lasso path using LARS</a></li>
</ul>
<p>Lars 算法提供了一个几乎无代价的沿着正则化参数的系数的完整路径，因此常利用函数 <a href="generated/sklearn.linear_model.lars_path.html#sklearn.linear_model.lars_path" title="sklearn.linear_model.lars_path"><code>lars_path</code></a> 来取回路径。</p>
<h3 id="1181-数学表达式">1.1.8.1. 数学表达式</h3>
<p>该算法和逐步回归非常相似，但是它没有在每一步包含变量，它估计的参数是根据与 其他剩余变量的联系来增加的。</p>
<p>在 LARS 的解中，没有给出一个向量的结果，而是给出一条曲线，显示参数向量的 L1 范式的每个值的解。 完全的参数路径存在 <code>coef_path_</code> 下。它的 size 是 (n_features, max_features+1)。 其中第一列通常是全 0 列。</p>
<p>参考文献:</p>
<ul>
<li>Original Algorithm is detailed in the paper <a href="http://www-stat.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf">Least Angle Regression</a> by Hastie et al.</li>
</ul>
<h2 id="119-正交匹配追踪法omp">1.1.9. 正交匹配追踪法（OMP）</h2>
<blockquote>
<p><a href="generated/sklearn.linear_model.OrthogonalMatchingPursuit.html#sklearn.linear_model.OrthogonalMatchingPursuit" title="sklearn.linear_model.OrthogonalMatchingPursuit"><code>OrthogonalMatchingPursuit</code></a> (正交匹配追踪法)和 <a href="generated/sklearn.linear_model.orthogonal_mp.html#sklearn.linear_model.orthogonal_mp" title="sklearn.linear_model.orthogonal_mp"><code>orthogonal_mp</code></a></p>
</blockquote>
<p>使用了 OMP 算法近似拟合了一个带限制的线性模型，该限制影响于模型的非 0 系数(例：L0 范数)。</p>
<p>就像最小角回归一样，作为一个前向特征选择方法，正交匹配追踪法可以近似一个固定非 0 元素的最优向量解:</p>
<pre><code class="language-py">
![\text{arg\,min\,} ||y - X\gamma||_2^2 \text{ subject to } \
||\gamma||_0 \leq n_{nonzero\_coefs}](img/ed70b000f50fb169ffe20ca2979e4a75.jpg)

</code></pre>
<p>正交匹配追踪法也可以针对一个特殊的误差而不是一个特殊的非零系数的个数。可以表示为:</p>
<pre><code class="language-py">
![\text{arg\,min\,} ||\gamma||_0 \text{ subject to } ||y-X\gamma||_2^2 \
\leq \text{tol}](img/6b7248d635f4161b925734dbc60de37a.jpg)

</code></pre>
<p>OMP 是基于每一步的贪心算法，其每一步元素都是与当前残差高度相关的。它跟较为简单的匹配追踪（MP）很相似，但是相比 MP 更好，在每一次迭代中，可以利用正交投影到之前选择的字典元素重新计算残差。</p>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/linear_model/plot_omp.html#sphx-glr-auto-examples-linear-model-plot-omp-py">Orthogonal Matching Pursuit</a></li>
</ul>
<p>参考文献:</p>
<ul>
<li><a href="http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf">http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf</a></li>
<li><a href="http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf">Matching pursuits with time-frequency dictionaries</a>, S. G. Mallat, Z. Zhang,</li>
</ul>
<h2 id="1110-贝叶斯回归">1.1.10. 贝叶斯回归</h2>
<p>贝叶斯回归可以用于在预估阶段的参数正则化: 正则化参数的选择不是通过人为的选择，而是通过手动调节数据值来实现。</p>
<p>上述过程可以通过引入 <a href="https://en.wikipedia.org/wiki/Non-informative_prior#Uninformative_priors">无信息先验</a> 于模型中的超参数来完成。 在 &lt;cite&gt;岭回归&lt;/cite&gt; 中使用的 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e473a2606f078eaa7b86800b11f4d62b.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e473a2606f078eaa7b86800b11f4d62b.jpg" alt="\ell_{2}"></a> 正则项相当于在 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8a58e8df6a985a3273e39bac7dd72b1f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8a58e8df6a985a3273e39bac7dd72b1f.jpg" alt="w"></a> 为高斯先验条件下，且此先验的精确度为 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7f35ead97a9f7be07b87ff7b860bcab9.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7f35ead97a9f7be07b87ff7b860bcab9.jpg" alt="\lambda^{-1}"></a> 求最大后验估计。在这里，我们没有手工调参数 lambda ，而是让他作为一个变量，通过数据中估计得到。</p>
<p>为了得到一个全概率模型，输出 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0775c03fc710a24df297dedcec515aaf.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0775c03fc710a24df297dedcec515aaf.jpg" alt="y"></a> 也被认为是关于 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/827de4e90947894fc96dd0432ff0d7dd.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/827de4e90947894fc96dd0432ff0d7dd.jpg" alt="X w"></a> 的高斯分布。</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/eaf558e2c8d1fbd5426664c1698d80bd.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/eaf558e2c8d1fbd5426664c1698d80bd.jpg" alt="p(y|X,w,\alpha) = \mathcal{N}(y|X w,\alpha)"></a></p>
<p>Alpha 在这里也是作为一个变量，通过数据中估计得到。</p>
<p>贝叶斯回归有如下几个优点:</p>
<blockquote>
<ul>
<li>它能根据已有的数据进行改变。</li>
<li>它能在估计过程中引入正则项。</li>
</ul>
</blockquote>
<p>贝叶斯回归有如下缺点:</p>
<blockquote>
<ul>
<li>它的推断过程是非常耗时的。</li>
</ul>
</blockquote>
<p>参考文献</p>
<ul>
<li>一个对于贝叶斯方法的很好的介绍 C. Bishop: Pattern Recognition and Machine learning</li>
<li>详细介绍原创算法的一本书 &lt;cite&gt;Bayesian learning for neural networks&lt;/cite&gt; by Radford M. Neal</li>
</ul>
<h3 id="11101-贝叶斯岭回归">1.1.10.1. 贝叶斯岭回归</h3>
<blockquote>
<p><a href="generated/sklearn.linear_model.BayesianRidge.html#sklearn.linear_model.BayesianRidge" title="sklearn.linear_model.BayesianRidge"><code>BayesianRidge</code></a> 利用概率模型估算了上述的回归问题，其先验参数 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8a58e8df6a985a3273e39bac7dd72b1f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8a58e8df6a985a3273e39bac7dd72b1f.jpg" alt="w"></a> 是由以下球面高斯公式得出的：</p>
</blockquote>
<pre><code class="language-py">
![p(w|\lambda) =
\mathcal{N}(w|0,\lambda^{-1}\bold{I_{p}})](img/971b86cde9801a3bb1a80af70bd05466.jpg)

</code></pre>
<p>先验参数 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d8b3d5242d513369a44f8bf0c6112744.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d8b3d5242d513369a44f8bf0c6112744.jpg" alt="\alpha"></a> 和 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0f92bc682b050115d03c625ce770c77d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0f92bc682b050115d03c625ce770c77d.jpg" alt="\lambda"></a> 一般是服从 <a href="https://en.wikipedia.org/wiki/Gamma_distribution">gamma 分布</a> ， 这个分布与高斯成共轭先验关系。</p>
<p>得到的模型一般称为 <em>贝叶斯岭回归</em>， 并且这个与传统的 <a href="generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code>Ridge</code></a> 非常相似。参数 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8a58e8df6a985a3273e39bac7dd72b1f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8a58e8df6a985a3273e39bac7dd72b1f.jpg" alt="w"></a> ， <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d8b3d5242d513369a44f8bf0c6112744.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d8b3d5242d513369a44f8bf0c6112744.jpg" alt="\alpha"></a> 和 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0f92bc682b050115d03c625ce770c77d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0f92bc682b050115d03c625ce770c77d.jpg" alt="\lambda"></a> 是在模型拟合的时候一起被估算出来的。 剩下的超参数就是 关于:math:&lt;cite&gt;alpha&lt;/cite&gt; 和 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0f92bc682b050115d03c625ce770c77d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0f92bc682b050115d03c625ce770c77d.jpg" alt="\lambda"></a> 的 gamma 分布的先验了。 它们通常被选择为 <em>无信息先验</em> 。模型参数的估计一般利用最大 <em>边缘似然对数估计</em> 。</p>
<p>默认 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/76530e85b09bd8385fad05337b968caf.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/76530e85b09bd8385fad05337b968caf.jpg" alt="\alpha_1 = \alpha_2 =  \lambda_1 = \lambda_2 = 10^{-6}"></a>.</p>
<p><a href="../auto_examples/linear_model/plot_bayesian_ridge.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/947ae691edda29c53c3b962665b052c6.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/947ae691edda29c53c3b962665b052c6.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_bayesian_ridge_0011.png"></a></a></p>
<p>贝叶斯岭回归用来解决回归问题:</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn import linear_model
&gt;&gt;&gt; X = [[0., 0.], [1., 1.], [2., 2.], [3., 3.]]
&gt;&gt;&gt; Y = [0., 1., 2., 3.]
&gt;&gt;&gt; reg = linear_model.BayesianRidge()
&gt;&gt;&gt; reg.fit(X, Y)
BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,
 fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,
 normalize=False, tol=0.001, verbose=False)

</code></pre>
<p>在模型训练完成后，可以用来预测新值:</p>
<pre><code class="language-py">&gt;&gt;&gt; reg.predict ([[1, 0.]])
array([ 0.50000013])

</code></pre>
<p>权值 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8a58e8df6a985a3273e39bac7dd72b1f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8a58e8df6a985a3273e39bac7dd72b1f.jpg" alt="w"></a> 可以被这样访问:</p>
<pre><code class="language-py">&gt;&gt;&gt; reg.coef_
array([ 0.49999993,  0.49999993])

</code></pre>
<p>由于贝叶斯框架的缘故，权值与 <a href="#ordinary-least-squares">普通最小二乘法</a> 产生的不太一样。 但是，贝叶斯岭回归对病态问题（ill-posed）的鲁棒性要更好。</p>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/linear_model/plot_bayesian_ridge.html#sphx-glr-auto-examples-linear-model-plot-bayesian-ridge-py">Bayesian Ridge Regression</a></li>
</ul>
<p>参考文献</p>
<ul>
<li>更多细节可以参考 <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.27.9072&amp;rep=rep1&amp;type=pdf">Bayesian Interpolation</a> by MacKay, David J. C.</li>
</ul>
<h3 id="11102-主动相关决策理论---ard">1.1.10.2. 主动相关决策理论 - ARD</h3>
<blockquote>
<p><a href="generated/sklearn.linear_model.ARDRegression.html#sklearn.linear_model.ARDRegression" title="sklearn.linear_model.ARDRegression"><code>ARDRegression</code></a> （主动相关决策理论）和 <a href="#id51"><code>Bayesian Ridge Regression</code>_</a> 非常相似，</p>
</blockquote>
<pre><code class="language-py">但是会导致一个更加稀疏的权重  [1] [2] 。
</code></pre>
<p><a href="generated/sklearn.linear_model.ARDRegression.html#sklearn.linear_model.ARDRegression" title="sklearn.linear_model.ARDRegression"><code>ARDRegression</code></a> 提出了一个不同的 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8a58e8df6a985a3273e39bac7dd72b1f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8a58e8df6a985a3273e39bac7dd72b1f.jpg" alt="w"></a> 的先验假设。具体来说，就是弱化了高斯分布为球形的假设。</p>
<p>它采用 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8a58e8df6a985a3273e39bac7dd72b1f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8a58e8df6a985a3273e39bac7dd72b1f.jpg" alt="w"></a> 分布是与轴平行的椭圆高斯分布。</p>
<p>也就是说，每个权值 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/848835d5b40c5bd74a6e592a65eed5d6.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/848835d5b40c5bd74a6e592a65eed5d6.jpg" alt="w_{i}"></a> 从一个中心在 0 点，精度为 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/fcf31635bf1c46833111df71ab92b68e.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/fcf31635bf1c46833111df71ab92b68e.jpg" alt="\lambda_{i}"></a> 的高斯分布中采样得到的。</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7afe3c56e3473a3a7f18cf983ed5e79c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7afe3c56e3473a3a7f18cf983ed5e79c.jpg" alt="p(w|\lambda) = \mathcal{N}(w|0,A^{-1})"></a></p>
<p>并且 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5db611c8f58fbd9a9776c013656a16ff.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5db611c8f58fbd9a9776c013656a16ff.jpg" alt="diag \; (A) = \lambda = \{\lambda_{1},...,\lambda_{p}\}"></a>.</p>
<p>与 <a href="#id53"><code>Bayesian Ridge Regression</code>_</a> 不同， 每个 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/848835d5b40c5bd74a6e592a65eed5d6.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/848835d5b40c5bd74a6e592a65eed5d6.jpg" alt="w_{i}"></a> 都有一个标准差 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/fc333385a9012524b39bc23303de30d4.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/fc333385a9012524b39bc23303de30d4.jpg" alt="\lambda_i"></a> 。所有 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/fc333385a9012524b39bc23303de30d4.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/fc333385a9012524b39bc23303de30d4.jpg" alt="\lambda_i"></a> 的先验分布 由超参数 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a6334506478d6feb4025038294ccfa00.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a6334506478d6feb4025038294ccfa00.jpg" alt="\lambda_1"></a> 、 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/535f86af715e90b9c394e3cbf53d99eb.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/535f86af715e90b9c394e3cbf53d99eb.jpg" alt="\lambda_2"></a> 确定的相同的 gamma 分布确定。</p>
<p><a href="../auto_examples/linear_model/plot_ard.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8aca5aa85ff13bf8e8687220b137f9d3.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8aca5aa85ff13bf8e8687220b137f9d3.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_ard_0011.png"></a></a></p>
<p>ARD 也被称为 <em>稀疏贝叶斯学习</em> 或 <em>相关向量机</em> <a href="3">3</a>(#id32) <a href="4">4</a>(#id33) 。</p>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/linear_model/plot_ard.html#sphx-glr-auto-examples-linear-model-plot-ard-py">Automatic Relevance Determination Regression (ARD)</a></li>
</ul>
<p>参考文献:</p>
<p>| <a href="1">1</a>(#id26) | Christopher M. Bishop: Pattern Recognition and Machine Learning, Chapter 7.2.1 |</p>
<p>| <a href="2">2</a>(#id27) | David Wipf and Srikantan Nagarajan: <a href="http://papers.nips.cc/paper/3372-a-new-view-of-automatic-relevance-determination.pdf">A new view of automatic relevance determination</a> |</p>
<p>| <a href="3">3</a>(#id28) | Michael E. Tipping: <a href="http://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf">Sparse Bayesian Learning and the Relevance Vector Machine</a> |</p>
<p>| <a href="4">4</a>(#id29) | Tristan Fletcher: <a href="http://www.tristanfletcher.co.uk/RVM Explained.pdf">Relevance Vector Machines explained</a> |</p>
<h2 id="1111-logistic-回归">1.1.11. logistic 回归</h2>
<p>logistic 回归，虽然名字里有 “回归” 二字，但实际上是解决分类问题的一类线性模型。在某些文献中，logistic 回归又被称作 logit 回归，maximum-entropy classification（MaxEnt，最大熵分类），或 log-linear classifier（对数线性分类器）。该模型利用函数 <a href="https://en.wikipedia.org/wiki/Logistic_function">logistic function</a> 将单次试验（single trial）的可能结果输出为概率。</p>
<p>scikit-learn 中 logistic 回归在 <a href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code>LogisticRegression</code></a> 类中实现了二分类（binary）、一对多分类（one-vs-rest）及多项式 logistic 回归，并带有可选的 L1 和 L2 正则化。</p>
<p>作为优化问题，带 L2 罚项的二分类 logistic 回归要最小化以下代价函数（cost function）：</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/203c5a2c58d6567a86dbc86faa92209e.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/203c5a2c58d6567a86dbc86faa92209e.jpg" alt="\underset{w, c}{min\,} \frac{1}{2}wT w + C \sum_{i=1}n \log(\exp(- y_i (X_i^T w + c)) + 1) ."></a></p>
<p>类似地，带 L1 正则的 logistic 回归解决的是如下优化问题：</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d7ff3091308658ce388554d420581459.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d7ff3091308658ce388554d420581459.jpg" alt="\underset{w, c}{min\,} \|w\|1 + C \sum{i=1}n \log(\exp(- y_i (X_iT w + c)) + 1) ."></a></p>
<p>在 <a href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code>LogisticRegression</code></a> 类中实现了这些优化算法: “liblinear”， “newton-cg”， “lbfgs”， “sag” 和 “saga”。</p>
<p>“liblinear” 应用了坐标下降算法（Coordinate Descent, CD），并基于 scikit-learn 内附的高性能 C++ 库 <a href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/">LIBLINEAR library</a> 实现。不过 CD 算法训练的模型不是真正意义上的多分类模型，而是基于 “one-vs-rest” 思想分解了这个优化问题，为每个类别都训练了一个二元分类器。因为实现在底层使用该求解器的 <a href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code>LogisticRegression</code></a> 实例对象表面上看是一个多元分类器。 <a href="generated/sklearn.svm.l1_min_c.html#sklearn.svm.l1_min_c" title="sklearn.svm.l1_min_c"><code>sklearn.svm.l1_min_c</code></a> 可以计算使用 L1 罚项时 C 的下界，以避免模型为空（即全部特征分量的权重为零）。</p>
<p>“lbfgs”, “sag” 和 “newton-cg” solvers （求解器）只支持 L2 惩罚项，对某些高维数据收敛更快。这些求解器的参数 [<code>](#id34)multi_class</code>设为 “multinomial” 即可训练一个真正的多项式 logistic 回归 <a href="5">5</a>(#id39) ，其预测的概率比默认的 “one-vs-rest” 设定更为准确。</p>
<p>“sag” 求解器基于平均随机梯度下降算法（Stochastic Average Gradient descent） <a href="6">6</a>(#id40)。在大数据集上的表现更快，大数据集指样本量大且特征数多。</p>
<p>“saga” 求解器 <a href="7">7</a>(#id41) 是 “sag” 的一类变体，它支持非平滑（non-smooth）的 L1 正则选项 <code>penalty="l1"</code> 。因此对于稀疏多项式 logistic 回归 ，往往选用该求解器。</p>
<p>一言以蔽之，选用求解器可遵循如下规则:</p>
<table>
<thead>
<tr>
<th>Case</th>
<th>Solver</th>
</tr>
</thead>
<tbody>
<tr>
<td>L1正则</td>
<td>“liblinear” or “saga”</td>
</tr>
<tr>
<td>多项式损失（multinomial loss）</td>
<td>“lbfgs”, “sag”, “saga” or “newton-cg”</td>
</tr>
<tr>
<td>大数据集（&lt;cite&gt;n_samples&lt;/cite&gt;）</td>
<td>“sag” or “saga”</td>
</tr>
</tbody>
</table>
<p>“saga” 一般都是最佳的选择，但出于一些历史遗留原因默认的是 “liblinear” 。</p>
<p>对于大数据集，还可以用 <a href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code>SGDClassifier</code></a> ，并使用对数损失（’log’ loss）</p>
<p>示例：</p>
<ul>
<li><a href="../auto_examples/linear_model/plot_logistic_l1_l2_sparsity.html#sphx-glr-auto-examples-linear-model-plot-logistic-l1-l2-sparsity-py">L1 Penalty and Sparsity in Logistic Regression</a></li>
<li><a href="../auto_examples/linear_model/plot_logistic_path.html#sphx-glr-auto-examples-linear-model-plot-logistic-path-py">Path with L1- Logistic Regression</a></li>
<li><a href="../auto_examples/linear_model/plot_logistic_multinomial.html#sphx-glr-auto-examples-linear-model-plot-logistic-multinomial-py">Plot multinomial and One-vs-Rest Logistic Regression</a></li>
<li><a href="../auto_examples/linear_model/plot_sparse_logistic_regression_20newsgroups.html#sphx-glr-auto-examples-linear-model-plot-sparse-logistic-regression-20newsgroups-py">Multiclass sparse logisitic regression on newgroups20</a></li>
<li><a href="../auto_examples/linear_model/plot_sparse_logistic_regression_mnist.html#sphx-glr-auto-examples-linear-model-plot-sparse-logistic-regression-mnist-py">MNIST classfification using multinomial logistic + L1</a></li>
</ul>
<p>与 liblinear 的区别:</p>
<p>当 <code>fit_intercept=False</code> 拟合得到的 <code>coef_</code> 或者待预测的数据为零时，用 <code>solver=liblinear</code> 的 <a href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code>LogisticRegression</code></a> 或 <code>LinearSVC</code> 与直接使用外部 liblinear 库预测得分会有差异。这是因为， 对于 <code>decision_function</code> 为零的样本， <a href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code>LogisticRegression</code></a> 和 <code>LinearSVC</code> 将预测为负类，而 liblinear 预测为正类。 注意，设定了 <code>fit_intercept=False</code> ，又有很多样本使得 <code>decision_function</code> 为零的模型，很可能会欠拟合，其表现往往比较差。建议您设置 <code>fit_intercept=True</code> 并增大 <code>intercept_scaling</code> 。</p>
<p>Note</p>
<p><strong>利用稀疏 logistic 回归进行特征选择</strong></p>
<blockquote>
<p>带 L1 罚项的 logistic 回归 将得到稀疏模型（sparse model），相当于进行了特征选择（feature selection），详情参见 <a href="feature_selection.html#l1-feature-selection">基于 L1 的特征选取</a> 。</p>
</blockquote>
<p><a href="generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV" title="sklearn.linear_model.LogisticRegressionCV"><code>LogisticRegressionCV</code></a> 对 logistic 回归 的实现内置了交叉验证（cross-validation），可以找出最优的参数 C 。”newton-cg”， “sag”， “saga” 和 “lbfgs” 在高维数据上更快，因为采用了热启动（warm-starting）。 在多分类设定下，若 &lt;cite&gt;multi_class&lt;/cite&gt; 设为 “ovr” ，会为每类求一个最佳的 C 值；若 &lt;cite&gt;multi_class&lt;/cite&gt; 设为 “multinomial” ，会通过交叉熵损失（cross-entropy loss）求出一个最佳 C 值。</p>
<p>参考文献：</p>
<p>| <a href="5">5</a>(#id36) | Christopher M. Bishop: Pattern Recognition and Machine Learning, Chapter 4.3.4 |</p>
<p>| <a href="6">6</a>(#id37) | Mark Schmidt, Nicolas Le Roux, and Francis Bach: <a href="https://hal.inria.fr/hal-00860051/document">Minimizing Finite Sums with the Stochastic Average Gradient.</a> |</p>
<p>| <a href="7">7</a>(#id38) | Aaron Defazio, Francis Bach, Simon Lacoste-Julien: <a href="https://arxiv.org/abs/1407.0202">SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives.</a> |</p>
<h2 id="1112-随机梯度下降-sgd">1.1.12. 随机梯度下降， SGD</h2>
<p>随机梯度下降是拟合线性模型的一个简单而高效的方法。在样本量（和特征数）很大时尤为有用。 方法 <code>partial_fit</code> 可用于 online learning （在线学习）或基于 out-of-core learning （外存的学习）</p>
<p><a href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code>SGDClassifier</code></a> 和 <a href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code>SGDRegressor</code></a> 分别用于拟合分类问题和回归问题的线性模型，可使用不同的（凸）损失函数，支持不同的罚项。 例如，设定 <code>loss="log"</code> ，则 <a href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code>SGDClassifier</code></a> 拟合一个逻辑斯蒂回归模型，而 <code>loss="hinge"</code> 拟合线性支持向量机（SVM）。</p>
<p>参考文献</p>
<ul>
<li><a href="sgd.html#sgd">随机梯度下降</a></li>
</ul>
<h2 id="1113-perceptron感知器">1.1.13. Perceptron（感知器）</h2>
<p><a href="generated/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron" title="sklearn.linear_model.Perceptron"><code>Perceptron</code></a> 是适用于大规模学习的一种简单算法。默认情况下：</p>
<blockquote>
<ul>
<li>不需要设置学习率（learning rate）。</li>
<li>不需要正则化处理。</li>
<li>仅使用错误样本更新模型。</li>
</ul>
</blockquote>
<p>最后一点表明使用合页损失（hinge loss）的感知机比 SGD 略快，所得模型更稀疏。</p>
<h2 id="1114-passive-aggressive-algorithms被动攻击算法">1.1.14. Passive Aggressive Algorithms（被动攻击算法）</h2>
<p>被动攻击算法是大规模学习的一类算法。和感知机类似，它也不需要设置学习率，不过比感知机多出一个正则化参数 <code>C</code> 。</p>
<p>对于分类问题， <a href="generated/sklearn.linear_model.PassiveAggressiveClassifier.html#sklearn.linear_model.PassiveAggressiveClassifier" title="sklearn.linear_model.PassiveAggressiveClassifier"><code>PassiveAggressiveClassifier</code></a> 可设定 <code>loss='hinge'</code> （PA-I）或 <code>loss='squared_hinge'</code> （PA-II）。对于回归问题， <a href="generated/sklearn.linear_model.PassiveAggressiveRegressor.html#sklearn.linear_model.PassiveAggressiveRegressor" title="sklearn.linear_model.PassiveAggressiveRegressor"><code>PassiveAggressiveRegressor</code></a> 可设置 <code>loss='epsilon_insensitive'</code> （PA-I）或 <code>loss='squared_epsilon_insensitive'</code> （PA-II）。</p>
<p>参考文献：</p>
<ul>
<li><a href="http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf">“Online Passive-Aggressive Algorithms”</a> K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR 7 (2006)</li>
</ul>
<h2 id="1115-稳健回归robustness-regression-处理离群点outliers和模型错误">1.1.15. 稳健回归（Robustness regression）: 处理离群点（outliers）和模型错误</h2>
<p>稳健回归（robust regression）特别适用于回归模型包含损坏数据（corrupt data）的情况，如离群点或模型中的错误。</p>
<p><a href="../auto_examples/linear_model/plot_theilsen.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9d2e3befcfa08a4b6a7cfed8dadbd5c0.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9d2e3befcfa08a4b6a7cfed8dadbd5c0.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_theilsen_0011.png"></a></a></p>
<h3 id="11151-各种使用场景与相关概念">1.1.15.1. 各种使用场景与相关概念</h3>
<p>处理包含离群点的数据时牢记以下几点:</p>
<ul>
<li> <p><strong>离群值在 X 上还是在 y 方向上</strong>?</p>
<table>
<thead>
<tr>
<th>离群值在 y 方向上</th>
<th>离群值在 X 方向上</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="../auto_examples/linear_model/plot_robust_fit.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/51d70ae60903891457d75099cc46e450.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/51d70ae60903891457d75099cc46e450.jpg" alt="y_outliers"></a></a></td>
<td><a href="../auto_examples/linear_model/plot_robust_fit.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/bee2cb707f91d8e36ae11638b6698fe4.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/bee2cb707f91d8e36ae11638b6698fe4.jpg" alt="X_outliers"></a></a></td>
</tr>
</tbody>
</table> </li>
<li> <p><strong>离群点的比例 vs. 错误的量级（amplitude）</strong></p> <p>离群点的数量很重要，离群程度也同样重要。</p> </li>
</ul>
<p>稳健拟合（robust fitting）的一个重要概念是崩溃点（breakdown point），即拟合模型（仍准确预测）所能承受的离群值最大比例。</p>
<p>注意，在高维数据条件下（ &lt;cite&gt;n_features&lt;/cite&gt; 大），一般而言很难完成稳健拟合，很可能完全不起作用。</p>
<p><strong>折中： 预测器的选择</strong></p>
<blockquote>
<p>Scikit-learn提供了三种稳健回归的预测器（estimator）: <a href="#ransac-regression">RANSAC</a> ， <a href="#theil-sen-regression">Theil Sen</a> 和 <a href="#huber-regression">HuberRegressor</a></p>
<ul>
<li><a href="#huber-regression">HuberRegressor</a> 一般快于 <a href="#ransac-regression">RANSAC</a> 和 <a href="#theil-sen-regression">Theil Sen</a> ，除非样本数很大，即 <code>n_samples</code> &gt;&gt; <code>n_features</code> 。 这是因为 <a href="#ransac-regression">RANSAC</a> 和 <a href="#theil-sen-regression">Theil Sen</a> 都是基于数据的较小子集进行拟合。但使用默认参数时， <a href="#theil-sen-regression">Theil Sen</a> 和 <a href="#ransac-regression">RANSAC</a> 可能不如 <a href="#huber-regression">HuberRegressor</a> 鲁棒。</li>
<li><a href="#ransac-regression">RANSAC</a> 比 <a href="#theil-sen-regression">Theil Sen</a> 更快，在样本数量上的伸缩性（适应性）更好。</li>
<li><a href="#ransac-regression">RANSAC</a> 能更好地处理y方向的大值离群点（通常情况下）。</li>
<li><a href="#theil-sen-regression">Theil Sen</a> 能更好地处理x方向中等大小的离群点，但在高维情况下无法保证这一特点。</li>
</ul>
</blockquote>
<p>实在决定不了的话，请使用 <a href="#ransac-regression">RANSAC</a></p>
<h3 id="11152-ransac-随机抽样一致性算法random-sample-consensus">1.1.15.2. RANSAC： 随机抽样一致性算法（RANdom SAmple Consensus）</h3>
<p>随机抽样一致性算法（RANdom SAmple Consensus， RANSAC）利用全体数据中局内点（inliers）的一个随机子集拟合模型。</p>
<p>RANSAC 是一种非确定性算法，以一定概率输出一个可能的合理结果，依赖于迭代次数（参数 &lt;cite&gt;max_trials&lt;/cite&gt; ）。这种算法主要解决线性或非线性回归问题，在计算机视觉摄影测绘领域尤为流行。</p>
<p>算法从全体样本输入中分出一个局内点集合，全体样本可能由于测量错误或对数据的假设错误而含有噪点、离群点。最终的模型仅从这个局内点集合中得出。</p>
<p><a href="../auto_examples/linear_model/plot_ransac.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/658840c7508dc5a73ca6180323904862.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/658840c7508dc5a73ca6180323904862.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_ransac_0011.png"></a></a></p>
<h4 id="111521-算法细节">1.1.15.2.1. 算法细节</h4>
<p>每轮迭代执行以下步骤:</p>
<ol>
<li>从原始数据中抽样 <code>min_samples</code> 数量的随机样本，检查数据是否合法（见 <code>is_data_valid</code> ）。</li>
<li>用一个随机子集拟合模型（ <code>base_estimator.fit</code> ）。检查模型是否合法（见 <code>is_model_valid</code> ）。</li>
<li>计算预测模型的残差（residual），将全体数据分成局内点和离群点（ <code>base_estimator.predict(X) - y</code> ）</li>
</ol>
<blockquote>
<ul>
<li>绝对残差小于 <code>residual_threshold</code> 的全体数据认为是局内点。</li>
</ul>
</blockquote>
<ol>
<li>若局内点样本数最大，保存当前模型为最佳模型。以免当前模型离群点数量恰好相等（而出现未定义情况），规定仅当数值大于当前最值时认为是最佳模型。</li>
</ol>
<p>上述步骤或者迭代到最大次数（ <code>max_trials</code> ），或者某些终止条件满足时停下（见 <code>stop_n_inliers</code> 和 <code>stop_score</code> )。最终模型由之前确定的最佳模型的局内点样本（一致性集合，consensus set）预测。</p>
<p>函数 <code>is_data_valid</code> 和 <code>is_model_valid</code> 可以识别出随机样本子集中的退化组合（degenerate combinations）并予以丢弃（reject）。即便不需要考虑退化情况，也会使用 <code>is_data_valid</code> ，因为在拟合模型之前调用它能得到更高的计算性能。</p>
<p>示例：</p>
<ul>
<li><a href="../auto_examples/linear_model/plot_ransac.html#sphx-glr-auto-examples-linear-model-plot-ransac-py">Robust linear model estimation using RANSAC</a></li>
<li><a href="../auto_examples/linear_model/plot_robust_fit.html#sphx-glr-auto-examples-linear-model-plot-robust-fit-py">Robust linear estimator fitting</a></li>
</ul>
<p>参考文献：</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/RANSAC">https://en.wikipedia.org/wiki/RANSAC</a></li>
<li><a href="http://www.cs.columbia.edu/~belhumeur/courses/compPhoto/ransac.pdf">“Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography”</a> Martin A. Fischler and Robert C. Bolles - SRI International (1981)</li>
<li><a href="http://www.bmva.org/bmvc/2009/Papers/Paper355/Paper355.pdf">“Performance Evaluation of RANSAC Family”</a> Sunglok Choi, Taemin Kim and Wonpil Yu - BMVC (2009)</li>
</ul>
<h3 id="11153-theil-sen-预估器-广义中值估计器generalized-median-based-estimator">1.1.15.3. Theil-Sen 预估器: 广义中值估计器（generalized-median-based estimator）</h3>
<p><a href="generated/sklearn.linear_model.TheilSenRegressor.html#sklearn.linear_model.TheilSenRegressor" title="sklearn.linear_model.TheilSenRegressor"><code>TheilSenRegressor</code></a> 估计器：使用中位数在多个维度泛化，对多元异常值更具有鲁棒性，但问题是，随着维数的增加，估计器的准确性在迅速下降。准确性的丢失，导致在高维上的估计值比不上普通的最小二乘法。</p>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/linear_model/plot_theilsen.html#sphx-glr-auto-examples-linear-model-plot-theilsen-py">Theil-Sen Regression</a></li>
<li><a href="../auto_examples/linear_model/plot_robust_fit.html#sphx-glr-auto-examples-linear-model-plot-robust-fit-py">Robust linear estimator fitting</a></li>
</ul>
<p>参考文献:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Theil%E2%80%93Sen_estimator">https://en.wikipedia.org/wiki/Theil%E2%80%93Sen_estimator</a></li>
</ul>
<h4 id="111531-算法理论细节">1.1.15.3.1. 算法理论细节</h4>
<p><a href="generated/sklearn.linear_model.TheilSenRegressor.html#sklearn.linear_model.TheilSenRegressor" title="sklearn.linear_model.TheilSenRegressor"><code>TheilSenRegressor</code></a> 在渐近效率和无偏估计方面足以媲美 <a href="#ordinary-least-squares">Ordinary Least Squares (OLS)</a> （普通最小二乘法（OLS））。与 OLS 不同的是， Theil-Sen 是一种非参数方法，这意味着它没有对底层数据的分布假设。由于 Theil-Sen 是基于中值的估计，它更适合于损坏的数据即离群值。 在单变量的设置中，Theil-Sen 在简单的线性回归的情况下，其崩溃点大约 29.3% ，这意味着它可以容忍任意损坏的数据高达 29.3% 。</p>
<p><a href="../auto_examples/linear_model/plot_theilsen.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9d2e3befcfa08a4b6a7cfed8dadbd5c0.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9d2e3befcfa08a4b6a7cfed8dadbd5c0.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_theilsen_0011.png"></a></a></p>
<p>scikit-learn 中实现的 <a href="generated/sklearn.linear_model.TheilSenRegressor.html#sklearn.linear_model.TheilSenRegressor" title="sklearn.linear_model.TheilSenRegressor"><code>TheilSenRegressor</code></a> 是多元线性回归模型的推广 <a href="8">8</a>(#f1) ，利用了空间中值方法，它是多维中值的推广 <a href="9">9</a>(#f2) 。</p>
<p>关于时间复杂度和空间复杂度，Theil-Sen 的尺度根据</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/00af2cbeb1deda7098a17d0491060339.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/00af2cbeb1deda7098a17d0491060339.jpg" alt="\binom{n_{samples}}{n_{subsamples}}"></a></p>
<p>这使得它不适用于大量样本和特征的问题。因此，可以选择一个亚群的大小来限制时间和空间复杂度，只考虑所有可能组合的随机子集。</p>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/linear_model/plot_theilsen.html#sphx-glr-auto-examples-linear-model-plot-theilsen-py">Theil-Sen Regression</a></li>
</ul>
<p>参考文献:</p>
<p>| <a href="8">8</a>(#id46) | Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang: <a href="http://home.olemiss.edu/~xdang/papers/MTSE.pdf">Theil-Sen Estimators in a Multiple Linear Regression Model.</a> |</p>
<p>| <a href="9">9</a>(#id47) |</p>
<ol>
<li>Kärkkäinen and S. Äyrämö: <a href="http://users.jyu.fi/~samiayr/pdf/ayramo_eurogen05.pdf">On Computation of Spatial Median for Robust Data Mining.</a></li>
</ol>
<p>|</p>
<h3 id="11154-huber-回归">1.1.15.4. Huber 回归</h3>
<p><a href="generated/sklearn.linear_model.HuberRegressor.html#sklearn.linear_model.HuberRegressor" title="sklearn.linear_model.HuberRegressor"><code>HuberRegressor</code></a> 与 <a href="generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code>Ridge</code></a> 不同，因为它对于被分为异常值的样本应用了一个线性损失。如果这个样品的绝对误差小于某一阈值，样品就被分为内围值。 它不同于 <a href="generated/sklearn.linear_model.TheilSenRegressor.html#sklearn.linear_model.TheilSenRegressor" title="sklearn.linear_model.TheilSenRegressor"><code>TheilSenRegressor</code></a> 和 <a href="generated/sklearn.linear_model.RANSACRegressor.html#sklearn.linear_model.RANSACRegressor" title="sklearn.linear_model.RANSACRegressor"><code>RANSACRegressor</code></a> ，因为它没有忽略异常值的影响，并分配给它们较小的权重。</p>
<p><a href="../auto_examples/linear_model/plot_huber_vs_ridge.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d06a11a8d6ed2efac238ab0bdbd33326.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d06a11a8d6ed2efac238ab0bdbd33326.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_huber_vs_ridge_001.png"></a></a></p>
<p>这个 <a href="generated/sklearn.linear_model.HuberRegressor.html#sklearn.linear_model.HuberRegressor" title="sklearn.linear_model.HuberRegressor"><code>HuberRegressor</code></a> 最小化的损失函数是：</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/dfab82d3a21680e5b6d3898a02dc6e01.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/dfab82d3a21680e5b6d3898a02dc6e01.jpg" alt="\underset{w, \sigma}{min\,} {\sum_{i=1}n\left(\sigma + H_m\left(\frac{X_{i}w - y_{i}}{\sigma}\right)\sigma\right) + \alpha {||w||_2}2}"></a></p>
<p>其中</p>
<pre><code class="language-py">
![H_m(z) = \begin{cases}
       z^2, &amp; \text {if } |z| &amp;lt; \epsilon, \\
       2\epsilon|z| - \epsilon^2, &amp; \text{otherwise}
\end{cases}](img/37e4251726a37bc02df4ef4390572e9a.jpg)

</code></pre>
<p>建议设置参数 <code>epsilon</code> 为 1.35 以实现 95% 统计效率。</p>
<h3 id="11155-注意">1.1.15.5. 注意</h3>
<p><a href="generated/sklearn.linear_model.HuberRegressor.html#sklearn.linear_model.HuberRegressor" title="sklearn.linear_model.HuberRegressor"><code>HuberRegressor</code></a> 与将损失设置为 &lt;cite&gt;huber&lt;/cite&gt; 的 <a href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code>SGDRegressor</code></a> 并不相同，体现在以下方面的使用方式上。</p>
<ul>
<li><a href="generated/sklearn.linear_model.HuberRegressor.html#sklearn.linear_model.HuberRegressor" title="sklearn.linear_model.HuberRegressor"><code>HuberRegressor</code></a> 是标度不变性的. 一旦设置了 <code>epsilon</code> ， 通过不同的值向上或向下缩放 <code>X</code> 和 <code>y</code> ，就会跟以前一样对异常值产生同样的鲁棒性。相比 <a href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code>SGDRegressor</code></a> 其中 <code>epsilon</code> 在 <code>X</code> 和 <code>y</code> 被缩放的时候必须再次设置。</li>
<li><a href="generated/sklearn.linear_model.HuberRegressor.html#sklearn.linear_model.HuberRegressor" title="sklearn.linear_model.HuberRegressor"><code>HuberRegressor</code></a> 应该更有效地使用在小样本数据，同时 <a href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code>SGDRegressor</code></a> 需要一些训练数据的 passes 来产生一致的鲁棒性。</li>
</ul>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/linear_model/plot_huber_vs_ridge.html#sphx-glr-auto-examples-linear-model-plot-huber-vs-ridge-py">HuberRegressor vs Ridge on dataset with strong outliers</a></li>
</ul>
<p>参考文献:</p>
<ul>
<li>Peter J. Huber, Elvezio M. Ronchetti: Robust Statistics, Concomitant scale estimates, pg 172</li>
</ul>
<p>另外，这个估计是不同于 R 实现的 Robust Regression (<a href="http://www.ats.ucla.edu/stat/r/dae/rreg.htm">http://www.ats.ucla.edu/stat/r/dae/rreg.htm</a>) ，因为 R 实现加权最小二乘，权重考虑到每个样本并基于残差大于某一阈值的量。</p>
<h2 id="1116-多项式回归用基函数展开线性模型">1.1.16. 多项式回归：用基函数展开线性模型</h2>
<p>机器学习中一种常见的模式，是使用线性模型训练数据的非线性函数。这种方法保持了一般快速的线性方法的性能，同时允许它们适应更广泛的数据范围。</p>
<p>例如，可以通过构造系数的 <strong>polynomial features</strong> 来扩展一个简单的线性回归。在标准线性回归的情况下，你可能有一个类似于二维数据的模型:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/76814b51cd880ede8da9a2b5ad3d4143.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/76814b51cd880ede8da9a2b5ad3d4143.jpg" alt="\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2"></a></p>
<p>如果我们想把抛物面拟合成数据而不是平面，我们可以结合二阶多项式的特征，使模型看起来像这样:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5656d5270c0ee866d09e2b271ed04a67.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5656d5270c0ee866d09e2b271ed04a67.jpg" alt="\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1 x_2 + w_4 x_12 + w_5 x_22"></a></p>
<p>观察到这 <em>还是一个线性模型</em> （这有时候是令人惊讶的）: 看到这个，想象创造一个新的变量</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/642372b631f22b9db0dc4f30d9ab67e6.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/642372b631f22b9db0dc4f30d9ab67e6.jpg" alt="z = x_1, x_2, x_1 x_2, x_12, x_22"></a></p>
<p>有了这些重新标记的数据，我们可以将问题写成</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b2996ad4866e8a26c7ba42c0229385af.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b2996ad4866e8a26c7ba42c0229385af.jpg" alt="\hat{y}(w, x) = w_0 + w_1 z_1 + w_2 z_2 + w_3 z_3 + w_4 z_4 + w_5 z_5"></a></p>
<p>我们看到，所得的 <em>polynomial regression</em> 与我们上文所述线性模型是同一类（即关于 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8a58e8df6a985a3273e39bac7dd72b1f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8a58e8df6a985a3273e39bac7dd72b1f.jpg" alt="w"></a> 是线性的），因此可以用同样的方法解决。通过用这些基函数建立的高维空间中的线性拟合，该模型具有灵活性，可以适应更广泛的数据范围。</p>
<p>这里是一个例子，使用不同程度的多项式特征将这个想法应用于一维数据:</p>
<p><a href="../auto_examples/linear_model/plot_polynomial_interpolation.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/cda529a3abe8af421f1f218b1a390091.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/cda529a3abe8af421f1f218b1a390091.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_polynomial_interpolation_0011.png"></a></a></p>
<p>这个图是使用&nbsp;<a href="generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures" title="sklearn.preprocessing.PolynomialFeatures"><code>PolynomialFeatures</code></a>&nbsp;预创建。该预处理器将输入数据矩阵转换为给定度的新数据矩阵。使用方法如下:</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn.preprocessing import PolynomialFeatures
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; X = np.arange(6).reshape(3, 2)
&gt;&gt;&gt; X
array([[0, 1],
 [2, 3],
 [4, 5]])
&gt;&gt;&gt; poly = PolynomialFeatures(degree=2)
&gt;&gt;&gt; poly.fit_transform(X)
array([[  1.,   0.,   1.,   0.,   0.,   1.],
 [  1.,   2.,   3.,   4.,   6.,   9.],
 [  1.,   4.,   5.,  16.,  20.,  25.]])

</code></pre>
<p><code>X</code> 的特征已经从 !<a href="x_1,-x_2">x_1, x_2</a>(img/58d06eb9b8003c392af19e09ce5ab1a4.jpg) 转换到 !<a href="1,-x_1,-x_2,-x_1^2,-x_1-x_2,-x_2^2">1, x_1, x_2, x_1^2, x_1 x_2, x_2^2</a>(img/6b474f60cd7fcc77b4a950334fc6483f.jpg), 并且现在可以用在任何线性模型。</p>
<p>这种预处理可以通过 <a href="pipeline.html#pipeline">Pipeline</a> 工具进行简化。可以创建一个表示简单多项式回归的单个对象，使用方法如下所示:</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn.preprocessing import PolynomialFeatures
&gt;&gt;&gt; from sklearn.linear_model import LinearRegression
&gt;&gt;&gt; from sklearn.pipeline import Pipeline
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; model = Pipeline([('poly', PolynomialFeatures(degree=3)),
...                   ('linear', LinearRegression(fit_intercept=False))])
&gt;&gt;&gt; # fit to an order-3 polynomial data
&gt;&gt;&gt; x = np.arange(5)
&gt;&gt;&gt; y = 3 - 2 * x + x ** 2 - x ** 3
&gt;&gt;&gt; model = model.fit(x[:, np.newaxis], y)
&gt;&gt;&gt; model.named_steps['linear'].coef_
array([ 3., -2.,  1., -1.])

</code></pre>
<p>利用多项式特征训练的线性模型能够准确地恢复输入多项式系数。</p>
<p>在某些情况下，没有必要包含任何单个特征的更高的幂，只需要相乘最多 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/adf83056bc2bd05628e24c40cb728b3d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/adf83056bc2bd05628e24c40cb728b3d.jpg" alt="d"></a> 个不同的特征即可，所谓 <em>interaction features（交互特征）</em> 。这些可通过设定 <a href="generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures" title="sklearn.preprocessing.PolynomialFeatures"><code>PolynomialFeatures</code></a> 的 <code>interaction_only=True</code> 得到。</p>
<p>例如，当处理布尔属性，对于所有 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c87d9110f3d32ffa5fa08671e4af11fb.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c87d9110f3d32ffa5fa08671e4af11fb.jpg" alt="n"></a> <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4d1bc681619acee3db7da4d570bcb4cd.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4d1bc681619acee3db7da4d570bcb4cd.jpg" alt="x_i^n = x_i"></a> ，因此是无用的；但 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/53fd9843c9af9a7ea05df92bce997456.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/53fd9843c9af9a7ea05df92bce997456.jpg" alt="x_i x_j"></a> 代表两布尔结合。这样我们就可以用线性分类器解决异或问题:</p>
<pre><code class="language-py">&gt;&gt;&gt; from sklearn.linear_model import Perceptron
&gt;&gt;&gt; from sklearn.preprocessing import PolynomialFeatures
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
&gt;&gt;&gt; y = X[:, 0] ^ X[:, 1]
&gt;&gt;&gt; y
array([0, 1, 1, 0])
&gt;&gt;&gt; X = PolynomialFeatures(interaction_only=True).fit_transform(X).astype(int)
&gt;&gt;&gt; X
array([[1, 0, 0, 0],
 [1, 0, 1, 0],
 [1, 1, 0, 0],
 [1, 1, 1, 1]])
&gt;&gt;&gt; clf = Perceptron(fit_intercept=False, max_iter=10, tol=None,
...                  shuffle=False).fit(X, y)

</code></pre>
<p>分类器的 “predictions” 是完美的:</p>
<pre><code class="language-py">&gt;&gt;&gt; clf.predict(X)
array([0, 1, 1, 0])
&gt;&gt;&gt; clf.score(X, y)
1.0

</code></pre>
</div>
<hr class="uk-article-divider">
<div class="uk-block uk-block-muted uk-padding-top-remove uk-padding-bottom-remove uk-margin-large-top  book-recommend-wrap">
<div class="uk-margin-top uk-margin-bottom uk-margin-left uk-margin-right">
<div class="uk-margin uk-text-muted "><i class="uk-icon-outdent uk-icon-justify uk-margin-small-right"></i>书籍推荐</div>
<div class="books">
<ul class="uk-book-list">
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/130/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/130/index.html">进击的Python</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/68.html">HuberTRoy</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">23页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年7月8日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 169个">169</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/165/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/165/index.html">Python学习知识库</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/94.html">coco369</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">85页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月26日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 190个">190</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/169/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/169/index.html">PyTorch 1.0 中文文档 & 教程</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/18.html">ApacheCN</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">87页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月26日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 874个">874</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/185/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/code_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/185/index.html">技术面试必备基础知识、Leetcode 题解、后端面试、Java 面试、春招、秋招、操作系统、计算机网络、系统设计</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/104.html">CyC2018</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="code">code</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">11页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月26日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 63551个">63551</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/120/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/spark_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/120/index.html">Openstack用户指南（简体中文版）</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/62.html">tzivanmoe</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="spark">spark</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">47页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年7月1日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 0个">0</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/9/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/java_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/9/index.html">分布式 Java</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/6.html">waylau</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="java">java</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">27页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年5月3日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 176个">176</span>
</div>
</div>
</div>
</li>
<hr>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
<nav class="tm-navbar uk-navbar uk-navbar-attached reader-nav">
<div class="uk-float-left uk-margin-small-top">
<a href="javascript:;" title="目录菜单" class="show-menu  uk-icon-hover  uk-icon-align-justify uk-margin-right"></a>
<div data-uk-dropdown="{mode:'click',pos:'bottom-left'}" class="font-setting-wrap">
<a class="uk-icon-hover uk-icon-font uk-margin-right" aria-label="字体设置" href="javascript:;"></a>
<div class="uk-dropdown dropdown-menu">
<div class="dropdown-caret"><span class="caret-outer"></span><span class="caret-inner"></span></div>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-reduce">小字</button>
<button class="uk-button-link button size-2 font-enlarge">大字</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-1 ">宋体</button>
<button class="uk-button-link button size-2 font-2 ">黑体</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-3 color-theme-sun "><i class="uk-icon-sun-o"></i>白天</button>
<button class="uk-button-link button size-3 color-theme-eye "><i class="uk-icon-eye"></i>护眼</button>
<button class="uk-button-link button size-3 color-theme-moon "><i class="uk-icon-moon-o"></i>夜晚</button></div>
</div>
</div>
<a class="logo uk-margin-right" href="../../../" title="返回首页"><img class="" src="../../../static/components/images/icon_32.png" /></a>
</div>
<div class="uk-navbar-flip  uk-hidden-small">
<div id="share-box"></div>
</div>
</nav>
<div id="menu-id" class="uk-offcanvas reader-offcanvas">
<div class="uk-offcanvas-bar">
<ul class="book-menu-bar uk-nav uk-nav-offcanvas" data-uk-nav>
<li>
<a href="../../../book/170/index.html" data-book-page-rel-url="index.html" data-book-page-id="0" title="封面">封面</a>
</li>
<li>
<a class="pjax" href="../../../book/170/readme.html" data-book-page-rel-url="readme.html" data-book-page-id="0" title="简介">简介</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/79.html" title="安装 scikit-learn" data-book-page-rel-url="docs/79.html" data-book-page-id="11491">安装 scikit-learn</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/1.html" title="1. 监督学习" data-book-page-rel-url="docs/1.html" data-book-page-id="11492">1. 监督学习</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/2.html" title="1.1. 广义线性模型" data-book-page-rel-url="docs/2.html" data-book-page-id="11493">1.1. 广义线性模型</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/3.html" title="1.2. 线性和二次判别分析" data-book-page-rel-url="docs/3.html" data-book-page-id="11494">1.2. 线性和二次判别分析</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/4.html" title="1.3. 内核岭回归" data-book-page-rel-url="docs/4.html" data-book-page-id="11495">1.3. 内核岭回归</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/5.html" title="1.4. 支持向量机" data-book-page-rel-url="docs/5.html" data-book-page-id="11496">1.4. 支持向量机</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/6.html" title="1.5. 随机梯度下降" data-book-page-rel-url="docs/6.html" data-book-page-id="11497">1.5. 随机梯度下降</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/7.html" title="1.6. 最近邻" data-book-page-rel-url="docs/7.html" data-book-page-id="11498">1.6. 最近邻</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/8.html" title="1.7. 高斯过程" data-book-page-rel-url="docs/8.html" data-book-page-id="11499">1.7. 高斯过程</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/9.html" title="1.8. 交叉分解" data-book-page-rel-url="docs/9.html" data-book-page-id="11500">1.8. 交叉分解</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/10.html" title="1.9. 朴素贝叶斯" data-book-page-rel-url="docs/10.html" data-book-page-id="11501">1.9. 朴素贝叶斯</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/11.html" title="1.10. 决策树" data-book-page-rel-url="docs/11.html" data-book-page-id="11502">1.10. 决策树</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/12.html" title="1.11. 集成方法" data-book-page-rel-url="docs/12.html" data-book-page-id="11503">1.11. 集成方法</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/13.html" title="1.12. 多类和多标签算法" data-book-page-rel-url="docs/13.html" data-book-page-id="11504">1.12. 多类和多标签算法</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/14.html" title="1.13. 特征选择" data-book-page-rel-url="docs/14.html" data-book-page-id="11505">1.13. 特征选择</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/15.html" title="1.14. 半监督学习" data-book-page-rel-url="docs/15.html" data-book-page-id="11506">1.14. 半监督学习</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/16.html" title="1.15. 等式回归" data-book-page-rel-url="docs/16.html" data-book-page-id="11507">1.15. 等式回归</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/17.html" title="1.16. 概率校准" data-book-page-rel-url="docs/17.html" data-book-page-id="11508">1.16. 概率校准</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/18.html" title="1.17. 神经网络模型（有监督）" data-book-page-rel-url="docs/18.html" data-book-page-id="11509">1.17. 神经网络模型（有监督）</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/19.html" title="2. 无监督学习" data-book-page-rel-url="docs/19.html" data-book-page-id="11510">2. 无监督学习</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/20.html" title="2.1. 高斯混合模型" data-book-page-rel-url="docs/20.html" data-book-page-id="11511">2.1. 高斯混合模型</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/21.html" title="2.2. 流形学习" data-book-page-rel-url="docs/21.html" data-book-page-id="11512">2.2. 流形学习</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/22.html" title="2.3. 聚类" data-book-page-rel-url="docs/22.html" data-book-page-id="11513">2.3. 聚类</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/23.html" title="2.4. 双聚类" data-book-page-rel-url="docs/23.html" data-book-page-id="11514">2.4. 双聚类</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/24.html" title="2.5. 分解成分中的信号（矩阵分解问题）" data-book-page-rel-url="docs/24.html" data-book-page-id="11515">2.5. 分解成分中的信号（矩阵分解问题）</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/25.html" title="2.6. 协方差估计" data-book-page-rel-url="docs/25.html" data-book-page-id="11516">2.6. 协方差估计</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/26.html" title="2.7. 经验协方差" data-book-page-rel-url="docs/26.html" data-book-page-id="11517">2.7. 经验协方差</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/27.html" title="2.8. 收敛协方差" data-book-page-rel-url="docs/27.html" data-book-page-id="11518">2.8. 收敛协方差</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/28.html" title="2.9. 稀疏逆协方差" data-book-page-rel-url="docs/28.html" data-book-page-id="11519">2.9. 稀疏逆协方差</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/29.html" title="2.10. Robust 协方差估计" data-book-page-rel-url="docs/29.html" data-book-page-id="11520">2.10. Robust 协方差估计</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/30.html" title="2.11. 新奇和异常值检测" data-book-page-rel-url="docs/30.html" data-book-page-id="11521">2.11. 新奇和异常值检测</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/31.html" title="2.12. 密度估计" data-book-page-rel-url="docs/31.html" data-book-page-id="11522">2.12. 密度估计</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/32.html" title="2.13. 神经网络模型（无监督）" data-book-page-rel-url="docs/32.html" data-book-page-id="11523">2.13. 神经网络模型（无监督）</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/33.html" title="3. 模型选择和评估" data-book-page-rel-url="docs/33.html" data-book-page-id="11524">3. 模型选择和评估</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/34.html" title="3.1. 交叉验证：评估估算器的表现" data-book-page-rel-url="docs/34.html" data-book-page-id="11525">3.1. 交叉验证：评估估算器的表现</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/35.html" title="3.2. 调整估计器的超参数" data-book-page-rel-url="docs/35.html" data-book-page-id="11526">3.2. 调整估计器的超参数</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/53.html" title="3.3. 模型评估: 量化预测的质量" data-book-page-rel-url="docs/53.html" data-book-page-id="11527">3.3. 模型评估: 量化预测的质量</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/54.html" title="3.4. 模型持久化" data-book-page-rel-url="docs/54.html" data-book-page-id="11528">3.4. 模型持久化</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/55.html" title="3.5. 验证曲线: 绘制分数以评估模型" data-book-page-rel-url="docs/55.html" data-book-page-id="11529">3.5. 验证曲线: 绘制分数以评估模型</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/56.html" title="4. 数据集转换" data-book-page-rel-url="docs/56.html" data-book-page-id="11530">4. 数据集转换</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/57.html" title="4.1. Pipeline（管道）和 FeatureUnion（特征联合）: 合并的评估器" data-book-page-rel-url="docs/57.html" data-book-page-id="11531">4.1. Pipeline（管道）和 FeatureUnion（特征联合）: 合并的评估器</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/58.html" title="4.2. 特征提取" data-book-page-rel-url="docs/58.html" data-book-page-id="11532">4.2. 特征提取</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/59.html" title="4.3. 预处理数据" data-book-page-rel-url="docs/59.html" data-book-page-id="11533">4.3. 预处理数据</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/60.html" title="4.4. 无监督降维" data-book-page-rel-url="docs/60.html" data-book-page-id="11534">4.4. 无监督降维</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/61.html" title="4.5. 随机投影" data-book-page-rel-url="docs/61.html" data-book-page-id="11535">4.5. 随机投影</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/62.html" title="4.6. 内核近似" data-book-page-rel-url="docs/62.html" data-book-page-id="11536">4.6. 内核近似</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/63.html" title="4.7. 成对的矩阵, 类别和核函数" data-book-page-rel-url="docs/63.html" data-book-page-id="11537">4.7. 成对的矩阵, 类别和核函数</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/64.html" title="4.8. 预测目标 (`y`.html) 的转换" data-book-page-rel-url="docs/64.html" data-book-page-id="11538">4.8. 预测目标 (`y`.html) 的转换</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/65.html" title="5. 数据集加载工具" data-book-page-rel-url="docs/65.html" data-book-page-id="11539">5. 数据集加载工具</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/66.html" title="6. 大规模计算的策略: 更大量的数据" data-book-page-rel-url="docs/66.html" data-book-page-id="11540">6. 大规模计算的策略: 更大量的数据</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/67.html" title="7. 计算性能" data-book-page-rel-url="docs/67.html" data-book-page-id="11541">7. 计算性能</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/68.html" title="使用 scikit-learn 介绍机器学习" data-book-page-rel-url="docs/68.html" data-book-page-id="11542">使用 scikit-learn 介绍机器学习</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/69.html" title="关于科学数据处理的统计学习教程" data-book-page-rel-url="docs/69.html" data-book-page-id="11543">关于科学数据处理的统计学习教程</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/70.html" title="机器学习: scikit-learn 中的设置以及预估对象" data-book-page-rel-url="docs/70.html" data-book-page-id="11544">机器学习: scikit-learn 中的设置以及预估对象</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/71.html" title="监督学习：从高维观察预测输出变量" data-book-page-rel-url="docs/71.html" data-book-page-id="11545">监督学习：从高维观察预测输出变量</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/72.html" title="模型选择：选择估计量及其参数" data-book-page-rel-url="docs/72.html" data-book-page-id="11546">模型选择：选择估计量及其参数</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/73.html" title="无监督学习: 寻求数据表示" data-book-page-rel-url="docs/73.html" data-book-page-id="11547">无监督学习: 寻求数据表示</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/74.html" title="把它们放在一起" data-book-page-rel-url="docs/74.html" data-book-page-id="11548">把它们放在一起</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/75.html" title="寻求帮助" data-book-page-rel-url="docs/75.html" data-book-page-id="11549">寻求帮助</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/76.html" title="处理文本数据" data-book-page-rel-url="docs/76.html" data-book-page-id="11550">处理文本数据</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/77.html" title="选择正确的评估器(estimator.html)" data-book-page-rel-url="docs/77.html" data-book-page-id="11551">选择正确的评估器(estimator.html)</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/78.html" title="外部资源，视频和谈话" data-book-page-rel-url="docs/78.html" data-book-page-id="11552">外部资源，视频和谈话</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/80.html" title="常见问题" data-book-page-rel-url="docs/80.html" data-book-page-id="11553">常见问题</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/81.html" title="时光轴" data-book-page-rel-url="docs/81.html" data-book-page-id="11554">时光轴</a>
</li>
</ul>
</div>
</div>
<script src="https://cdn.staticfile.net/jquery/1.12.4/jquery.min.js"></script>
<script type="text/javascript" src="../../../static/components/uikit-2.27.5/js/uikit.reader.js"></script>
<script type="text/javascript" src="../../../static/components/social-share/social-share.min.js"></script>
<script>(function(){var bp =document.createElement('script');var curProtocol =window.location.protocol.split(':')[0];if (curProtocol ==='https') {bp.src ='https://zz.bdstatic.com/linksubmit/push.js';}
else {bp.src ='http://push.zhanzhang.baidu.com/push.js';}
var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(bp,s);})();</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
<script src="https://cdn.staticfile.net/highlight.js/9.12.0/highlight.min.js"></script>
<script src="https://cdn.staticfile.net/jquery.pjax/2.0.1/jquery.pjax.min.js"></script>
<script src="https://cdn.staticfile.net/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="https://cdn.staticfile.net/uikit/2.27.5/js/components/lightbox.min.js"></script>
<link rel="dns-prefetch" href="../../..//cdn.mathjax.org" />
<script type="text/x-mathjax-config">
 function initMathJax() {
    var mathId = $("book-content-section")[0];
    MathJax.Hub.Config({
        tex2jax: {skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code','a']},
        showProcessingMessages: false,
        messageStyle: "none"
    });
    MathJax.Hub.Queue(["Typeset",MathJax.Hub,mathId]);
 };
initMathJax();
</script>
<script src='https://cdn.staticfile.net/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML' async></script>
<style>
	.MathJax_Display{display:inline!important;}
</style>
<script type="text/javascript" src="../../../static/components/js/reader.js"></script>
<script type="text/javascript">var bookId =170;var bookPageId =11493;var bookPageRelUrl ='docs/2.html';</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
</body>
</html>