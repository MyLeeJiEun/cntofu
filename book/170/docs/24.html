
<!DOCTYPE html>
<html lang="zh-CN">
<head>
<title>2.5. 分解成分中的信号（矩阵分解问题）-scikit-learn (sklearn) 官方文档中文版</title>
<meta content='2.5. 分解成分中的信号（矩阵分解问题）,scikit-learn (sklearn) 官方文档中文版' name='keywords'>
<meta content='2.5. 分解成分中的信号（矩阵分解问题）,scikit-learn (sklearn) 官方文档中文版' name='description'>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta http-equiv="Content-Language" content="zh-CN" />
<meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1, maximum-scale=1, user-scalable=no"../../../>
<meta name="applicable-device" content="pc,mobile">
<link rel="shortcut icon" href="../../../favicon.ico" type="image/x-icon" />
<meta name="renderer" content="webkit">
<link rel="stylesheet" href="../../../static/components/uikit-2.27.5/css/uikit.custom.css">
<link rel="stylesheet" href="../../../static/components/social-share/social-share.min.css">
<link rel="stylesheet" href="../../../static/components/highlight/styles/custom.css">
<link rel="stylesheet" href="../../../static/components/css/base.css">
<link rel="stylesheet" href="../../../static/components/css/reader.css">
<link rel="stylesheet" href="../../../static/components/css/markdown.css">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5313208362165053" crossorigin="anonymous"></script>
</head>
<body>
<div class=" book-main-wrap uk-container uk-container-center uk-margin-top ">
<div class="uk-grid">
<div class="uk-width-1-1 reader-wrap ">
<div class=" bottom-nav uk-clearfix ">
<div class="uk-align-left ">
<a href="../../../book/170/docs/23.html">
<i class="nav-icon-left uk-icon-small  uk-icon-caret-left"></i>
<span class="">2.4. 双聚类</span>
</a>
</div>
<div class="uk-align-right ">
<a href="../../../book/170/docs/25.html">
<span class="">2.6. 协方差估计</span>
<i class="nav-icon-right uk-icon-small  uk-icon-caret-right"></i>
</a>
</div>
</div>
<div class="uk-text-center">
<h2 class="book-page-title uk-container-center">
<a href="../../../book/170/index.html">scikit-learn (sklearn) 官方文档中文版</a>
<a target="_blank" rel="nofollow" href="https://github.com/apachecn/scikit-learn-doc-zh" class="uk-icon-button uk-icon-github" title="github项目地址"></a>
</h2>
</div>
<script type="text/javascript" src="../../../static/components/js/app_intro.js"></script>
<ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-5313208362165053" data-ad-slot="1328047120"></ins>
<script>(adsbygoogle =window.adsbygoogle ||[]).push({});</script>
<hr class="uk-article-divider">
<div class="book-content-section  md-content-section  uk-margin-bottom">
<h1 id="25-分解成分中的信号矩阵分解问题">2.5. 分解成分中的信号（矩阵分解问题）</h1>
<p>校验者: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@武器大师一个挑俩</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@png</a> 翻译者: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@柠檬</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh">@片刻</a></p>
<h2 id="251-主成分分析pca">2.5.1. 主成分分析（PCA）</h2>
<h3 id="2511-准确的pca和概率解释exact-pca-and-probabilistic-interpretation">2.5.1.1. 准确的PCA和概率解释（Exact PCA and probabilistic interpretation）</h3>
<p>PCA 用于对一组连续正交分量中的多变量数据集进行方差最大方向的分解。 在 scikit-learn 中， <a href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code>PCA</code></a> 被实现为一个变换对象， 通过 <code>fit</code> 方法可以降维成 &lt;cite&gt;n&lt;/cite&gt; 个成分， 并且可以将新的数据投影(project, 亦可理解为分解)到这些成分中。</p>
<p>可选参数 <code>whiten=True</code> 使得可以将数据投影到奇异（singular）空间上，同时将每个成分缩放到单位方差。 如果下游模型对信号的各向同性作出强烈的假设，这通常是有用的，例如，使用RBF内核的 SVM 算法和 K-Means 聚类算法。</p>
<p>以下是iris数据集的一个示例，该数据集包含4个特征， 通过PCA降维后投影到方差最大的二维空间上：</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/Images/6479476ebd2e5d741f1a712f671fccb6.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/Images/6479476ebd2e5d741f1a712f671fccb6.jpg" alt="https://scikit-learn.org/stable/_images/sphx_glr_plot_pca_vs_lda_001.png../auto_examples/decomposition/plot_pca_vs_lda.htmlcenter:scale:75%"></a></p>
<p><a href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code>PCA</code></a> 对象还提供了 PCA 的概率解释， 其可以基于其解释的方差量给出数据的可能性。</p>
<p>可以通过在交叉验证（cross-validation）中使用 &lt;cite&gt;score&lt;/cite&gt; 方法来实现：</p>
<p><a href="../auto_examples/decomposition/plot_pca_vs_fa_model_selection.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1c25ce7b0d4e2c7da1f0e73d2565c431.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1c25ce7b0d4e2c7da1f0e73d2565c431.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_pca_vs_fa_model_selection_0011.png"></a></a></p>
<p>例子:</p>
<ul>
<li><a href="../auto_examples/decomposition/plot_pca_vs_lda.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-lda-py">Comparison of LDA and PCA 2D projection of Iris dataset</a></li>
<li><a href="../auto_examples/decomposition/plot_pca_vs_fa_model_selection.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-fa-model-selection-py">Model selection with Probabilistic PCA and Factor Analysis (FA)</a></li>
</ul>
<h3 id="2512-增量pca-incremental-pca">2.5.1.2. 增量PCA (Incremental PCA)</h3>
<p><a href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code>PCA</code></a> 对象非常有用, 但对大型数据集有一定的限制。 最大的限制是 <a href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code>PCA</code></a> 仅支持批处理，这意味着所有要处理的数据必须适合主内存。 <a href="generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA" title="sklearn.decomposition.IncrementalPCA"><code>IncrementalPCA</code></a> 对象使用不同的处理形式使之允许部分计算， 这一形式几乎和 <a href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code>PCA</code></a> 以小型批处理方式处理数据的方法完全匹配。 <a href="generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA" title="sklearn.decomposition.IncrementalPCA"><code>IncrementalPCA</code></a> 可以通过以下方式实现核外（out-of-core）主成分分析：</p>
<blockquote>
<ul>
<li>使用 <code>partial_fit</code> 方法从本地硬盘或网络数据库中以此获取数据块。</li>
<li>通过 <code>numpy.memmap</code> 在一个 memory mapped file 上使用 fit 方法。</li>
</ul>
<p><a href="generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA" title="sklearn.decomposition.IncrementalPCA"><code>IncrementalPCA</code></a> 仅存储成分和噪声方差的估计值，并按顺序递增地更新解释方差比（<a href="#id28">explained_variance_ratio_</a>）。</p>
</blockquote>
<p>这就是为什么内存使用取决于每个批次的样本数，而不是数据集中要处理的样本数。</p>
<p><a href="../auto_examples/decomposition/plot_incremental_pca.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e84b0861711b65f28923da7c268645e7.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e84b0861711b65f28923da7c268645e7.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_incremental_pca_0011.png"></a></a><a href="../auto_examples/decomposition/plot_incremental_pca.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/22c1b7663568bf6c404a07778507a93e.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/22c1b7663568bf6c404a07778507a93e.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_incremental_pca_0021.png"></a></a></p>
<p>Examples:</p>
<ul>
<li><a href="../auto_examples/decomposition/plot_incremental_pca.html#sphx-glr-auto-examples-decomposition-plot-incremental-pca-py">Incremental PCA</a></li>
</ul>
<h3 id="2513-pca-使用随机svd">2.5.1.3. PCA 使用随机SVD</h3>
<p>通过丢弃具有较低奇异值的奇异向量成分，将数据降维到低维空间并保留大部分方差是非常有意义的。</p>
<p>例如，如果我们使用64x64像素的灰度级图像进行人脸识别，数据的维数为4096， 在这样大的数据上训练含RBF内核的支持向量机是很慢的。 此外我们知道数据本质上的维度远低于4096，因为人脸的所有照片都看起来有点相似。 样本位于许多的很低维度（例如约200维）。PCA算法可以用于线性变换数据，同时降低维数并同时保留大部分方差。</p>
<p>在这种情况下，使用可选参数 <code>svd_solver='randomized'</code> 的 <a href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code>PCA</code></a> 是非常有用的。 因为我们将要丢弃大部分奇异值，所以对我们将保留并实际执行变换的奇异向量进行近似估计的有限的计算更有效。</p>
<p>例如：以下显示了来自 Olivetti 数据集的 16 个样本肖像（以 0.0 为中心）。 右侧是前 16 个奇异向量重画为肖像。因为我们只需要使用大小为 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/eba6c21adbb5d5905624446cc970a7d3.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/eba6c21adbb5d5905624446cc970a7d3.jpg" alt="n_{samples} = 400"></a> 和 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f34cef4cb73bfa4cbe2f9b4300a8940c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f34cef4cb73bfa4cbe2f9b4300a8940c.jpg" alt="n_{features} = 64 \times 64 = 4096"></a> 的数据集的前 16 个奇异向量, 使得计算时间小于 1 秒。</p>
<p><strong><a href="../auto_examples/decomposition/plot_faces_decomposition.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/Images/4b3d9c4467b467af3714ba45c54e5c2e.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/Images/4b3d9c4467b467af3714ba45c54e5c2e.jpg" alt="orig_img"></a></a> <a href="../auto_examples/decomposition/plot_faces_decomposition.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/Images/9a55689143b2e4d90adcdfe1f95b9ffd.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/Images/9a55689143b2e4d90adcdfe1f95b9ffd.jpg" alt="pca_img"></a></a></strong></p>
<p>注意：使用可选参数 <code>svd_solver='randomized'</code> ，在 <a href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code>PCA</code></a> 中我们还需要给出输入低维空间大小 <code>n_components</code> 。</p>
<p>如果我们注意到： <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a6dba4aa006d4689de18a4de5acaa949.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a6dba4aa006d4689de18a4de5acaa949.jpg" alt="n_{\max} = \max(n_{\mathrm{samples}}, n_{\mathrm{features}})"></a> 且 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c3c22734435fdd94f6819708bc55d8a6.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c3c22734435fdd94f6819708bc55d8a6.jpg" alt="n_{\min} = \min(n_{\mathrm{samples}}, n_{\mathrm{features}})"></a>, 对于PCA中实施的确切方式，随机 <a href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code>PCA</code></a> 的时间复杂度是：<a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3a876d35f8a2c82a19a71b0fd52f7153.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3a876d35f8a2c82a19a71b0fd52f7153.jpg" alt="O(n_{\max}^2 \cdot n_{\mathrm{components}})"></a> ， 而不是 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/38a411931f9f49e71b888f7998427122.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/38a411931f9f49e71b888f7998427122.jpg" alt="O(n_{\max}^2 \cdot n_{\min})"></a> 。</p>
<p>对于确切的方式，随机 <a href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code>PCA</code></a> 的内存占用量正比于 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e7a3ba1e52e7e8add5e2c14602a92e3a.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e7a3ba1e52e7e8add5e2c14602a92e3a.jpg" alt="2 \cdot n_{\max} \cdot n_{\mathrm{components}}"></a> ， 而不是 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d3c1bd7a2bac2e7cc22203e423d56e7e.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d3c1bd7a2bac2e7cc22203e423d56e7e.jpg" alt="n_{\max}\cdot n_{\min}"></a></p>
<p>注意：选择参数 <code>svd_solver='randomized'</code> 的 <a href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code>PCA</code></a>，在执行 <code>inverse_transform</code> 时， 并不是 <code>transform</code> 的确切的逆变换操作（即使 参数设置为默认的 <code>whiten=False</code>）</p>
<p>例子:</p>
<ul>
<li><a href="../auto_examples/applications/plot_face_recognition.html#sphx-glr-auto-examples-applications-plot-face-recognition-py">Faces recognition example using eigenfaces and SVMs</a></li>
<li><a href="../auto_examples/decomposition/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py">Faces dataset decompositions</a></li>
</ul>
<p>参考文献:</p>
<ul>
<li><a href="http://arxiv.org/abs/0909.4061">“Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions”</a> Halko, et al., 2009</li>
</ul>
<h3 id="2514-核-pca">2.5.1.4. 核 PCA</h3>
<p><a href="generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA" title="sklearn.decomposition.KernelPCA"><code>KernelPCA</code></a> 是 PCA 的扩展，通过使用核方法实现非线性降维（dimensionality reduction） (参阅 <a href="metrics.html#metrics">成对的矩阵, 类别和核函数</a>)。 它具有许多应用，包括去噪, 压缩和结构化预测（ structured prediction ） (kernel dependency estimation（内核依赖估计）)。</p>
<blockquote>
<p><a href="generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA" title="sklearn.decomposition.KernelPCA"><code>KernelPCA</code></a> 支持 <code>transform</code> 和 <code>inverse_transform</code> 。</p>
</blockquote>
<p><a href="../auto_examples/decomposition/plot_kernel_pca.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/64ecb0afa71752378a987a33e1e4e76f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/64ecb0afa71752378a987a33e1e4e76f.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_kernel_pca_0011.png"></a></a></p>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/decomposition/plot_kernel_pca.html#sphx-glr-auto-examples-decomposition-plot-kernel-pca-py">Kernel PCA</a></li>
</ul>
<h3 id="2515-稀疏主成分分析--sparsepca-和-minibatchsparsepca-">2.5.1.5. 稀疏主成分分析 ( SparsePCA 和 MiniBatchSparsePCA )</h3>
<p><a href="generated/sklearn.decomposition.SparsePCA.html#sklearn.decomposition.SparsePCA" title="sklearn.decomposition.SparsePCA"><code>SparsePCA</code></a> 是 PCA 的一个变体，目的是提取能最好地重建数据的稀疏组分集合。</p>
<p>小批量稀疏 PCA ( <a href="generated/sklearn.decomposition.MiniBatchSparsePCA.html#sklearn.decomposition.MiniBatchSparsePCA" title="sklearn.decomposition.MiniBatchSparsePCA"><code>MiniBatchSparsePCA</code></a> ) 是一个 <a href="generated/sklearn.decomposition.SparsePCA.html#sklearn.decomposition.SparsePCA" title="sklearn.decomposition.SparsePCA"><code>SparsePCA</code></a> 的变种，它速度更快但准确度有所降低。对于给定的迭代次数，通过迭代该组特征的小块来达到速度的增加。</p>
<p>Principal component analysis（主成分分析） (<a href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code>PCA</code></a>) 的缺点在于，通过该方法提取的成分具有唯一的密度表达式，即当表示为原始变量的线性组合时，它们具有非零系数，使之难以解释。在许多情况下，真正的基础组件可以更自然地想象为稀疏向量; 例如在面部识别中，每个组件可能自然地映射到面部的某个部分。</p>
<p>稀疏的主成分产生更简洁、可解释的表达式，明确强调了样本之间的差异性来自哪些原始特征。</p>
<p>以下示例说明了使用稀疏 PCA 提取 Olivetti 人脸数据集中的 16 个组分。可以看出正则化项产生了许多零。此外，数据的自然结构导致了非零系数垂直相邻 （vertically adjacent）。该模型不会在数学上强制执行: 每个组分都是一个向量 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0fac1c7ec23344da41ff45485bb3ac12.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0fac1c7ec23344da41ff45485bb3ac12.jpg" alt="h \in \mathbf{R}^{4096}"></a>,除非人性化地的可视化为 64x64 像素的图像，否则没有垂直相邻性的概念。 下面显示的组分看起来局部化（appear local)是数据的内在结构的影响，这种局部模式使重建误差最小化。有一种考虑到邻接性和不同结构类型的导致稀疏的规范（sparsity-inducing norms）,参见 <a href="Jen09">Jen09</a>(#jen09) 对这种方法进行了解。 有关如何使用稀疏 PCA 的更多详细信息，请参阅下面的示例部分。 更多关于 Sparse PCA 使用的内容，参见示例部分，如下：</p>
<p><strong><a href="../auto_examples/decomposition/plot_faces_decomposition.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/Images/9a55689143b2e4d90adcdfe1f95b9ffd.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/Images/9a55689143b2e4d90adcdfe1f95b9ffd.jpg" alt="pca_img"></a></a> <a href="../auto_examples/decomposition/plot_faces_decomposition.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/Images/aba080a369ca542a90a264a04dd518c5.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/Images/aba080a369ca542a90a264a04dd518c5.jpg" alt="spca_img"></a></a></strong></p>
<p>请注意，有多种不同的计算稀疏PCA 问题的公式。 这里使用的方法基于 <a href="Mrl09">Mrl09</a>(#mrl09) 。优化问题的解决是一个带有惩罚项（L1范数的） <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3bf32d926cdf24f440b6b831f0d9cc37.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3bf32d926cdf24f440b6b831f0d9cc37.jpg" alt="\ell_1"></a> 的一个 PCA 问题（dictionary learning（字典学习））:</p>
<pre><code class="language-py">
![(U^*, V^*) = \underset{U, V}{\operatorname{arg\,min\,}} &amp; \frac{1}{2}
             ||X-UV||_2^2+\alpha||V||_1 \\
             \text{subject to\,} &amp; ||U_k||_2 = 1 \text{ for all }
             0 \leq k &amp;lt; n_{components}](img/d6d6e6638cd01ead4811579660e36b44.jpg)

</code></pre>
<p>导致稀疏（sparsity-inducing）的 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3bf32d926cdf24f440b6b831f0d9cc37.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3bf32d926cdf24f440b6b831f0d9cc37.jpg" alt="\ell_1"></a> 规范也可以避免当训练样本很少时从噪声中学习成分。可以通过超参数 <code>alpha</code> 来调整惩罚程度（从而减少稀疏度）。值较小会导致温和的正则化因式分解，而较大的值将许多系数缩小到零。</p>
<p>Note</p>
<p>虽然本着在线算法的精神， <a href="generated/sklearn.decomposition.MiniBatchSparsePCA.html#sklearn.decomposition.MiniBatchSparsePCA" title="sklearn.decomposition.MiniBatchSparsePCA"><code>MiniBatchSparsePCA</code></a> 类不实现 <code>partial_fit</code> , 因为在线算法沿特征方向，而不是样本方向。</p>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/decomposition/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py">Faces dataset decompositions</a></li>
</ul>
<p>参考文献:</p>
<p>| <a href="Mrl09">Mrl09</a>(#id5) | <a href="http://www.di.ens.fr/sierra/pdfs/icml09.pdf">“Online Dictionary Learning for Sparse Coding”</a> J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009 |</p>
<p>| <a href="Jen09">Jen09</a>(#id4) | <a href="www.di.ens.fr/~fbach/sspca_AISTATS2010.pdf">“Structured Sparse Principal Component Analysis”</a> R. Jenatton, G. Obozinski, F. Bach, 2009 |</p>
<h2 id="252-截断奇异值分解和隐语义分析">2.5.2. 截断奇异值分解和隐语义分析</h2>
<p><a href="generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD" title="sklearn.decomposition.TruncatedSVD"><code>TruncatedSVD</code></a> 实现了一个奇异值分解（SVD）的变体，它只计算 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> 个最大的奇异值，其中 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> 是用户指定的参数。</p>
<p>当截断的 SVD被应用于 term-document矩阵（由 <code>CountVectorizer</code> 或 <code>TfidfVectorizer</code> 返回）时，这种转换被称为 <a href="http://nlp.stanford.edu/IR-book/pdf/18lsi.pdf">latent semantic analysis</a> (LSA), 因为它将这样的矩阵转换为低纬度的 “semantic（语义）” 空间。 特别地是 LSA 能够抵抗同义词和多义词的影响（两者大致意味着每个单词有多重含义），这导致 term-document 矩阵过度稀疏，并且在诸如余弦相似性的度量下表现出差的相似性。</p>
<p>Note</p>
<p>LSA 也被称为隐语义索引 LSI，尽管严格地说它是指在持久索引（persistent indexes）中用于信息检索的目的。</p>
<p>数学表示中， 训练样本 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43c1fea57579e54f80c0535bc582626f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43c1fea57579e54f80c0535bc582626f.jpg" alt="X"></a> 用截断的SVD产生一个低秩的（ low-rank）近似值 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43c1fea57579e54f80c0535bc582626f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43c1fea57579e54f80c0535bc582626f.jpg" alt="X"></a> :</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/21b6e31779ad3b1a382b13e65ec917c7.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/21b6e31779ad3b1a382b13e65ec917c7.jpg" alt="X \approx X_k = U_k \Sigma_k V_k^\top"></a></p>
<p>在这个操作之后，<a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/dedef2ddd0f96df639d4c85fffb9bbd5.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/dedef2ddd0f96df639d4c85fffb9bbd5.jpg" alt="U_k \Sigma_k^\top"></a> 是转换后的训练集，其中包括 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a> 个特征（在 API 中被称为 <code>n_components</code> ）。</p>
<p>还需要转换一个测试集 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43c1fea57579e54f80c0535bc582626f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43c1fea57579e54f80c0535bc582626f.jpg" alt="X"></a>, 我们乘以 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/cc41a8d314f9b97ecdf236aa0c21d984.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/cc41a8d314f9b97ecdf236aa0c21d984.jpg" alt="V_k"></a>:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6081a672a0d5d6cc7563c531599dde91.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6081a672a0d5d6cc7563c531599dde91.jpg" alt="X' = X V_k"></a></p>
<p>Note</p>
<p>自然语言处理(NLP) 和信息检索(IR) 文献中的 LSA 的大多数处理方式是交换矩阵 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43c1fea57579e54f80c0535bc582626f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43c1fea57579e54f80c0535bc582626f.jpg" alt="X"></a> 的坐标轴,使其具有 <code>n_features</code> × <code>n_samples</code> 的形状。 我们以 scikit-learn API 相匹配的不同方式呈现 LSA, 但是找到的奇异值是相同的。</p>
<p><a href="generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD" title="sklearn.decomposition.TruncatedSVD"><code>TruncatedSVD</code></a> 非常类似于 <a href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code>PCA</code></a>, 但不同之处在于它工作在样本矩阵 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43c1fea57579e54f80c0535bc582626f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43c1fea57579e54f80c0535bc582626f.jpg" alt="X"></a> 而不是它们的协方差矩阵。 当从特征值中减去 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43c1fea57579e54f80c0535bc582626f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43c1fea57579e54f80c0535bc582626f.jpg" alt="X"></a> 的每列（每个特征per-feature）的均值时，在得到的矩阵上应用 truncated SVD 相当于 PCA 。 实际上，这意味着 <a href="generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD" title="sklearn.decomposition.TruncatedSVD"><code>TruncatedSVD</code></a> 转换器（transformer）接受 <code>scipy.sparse</code> 矩阵，而不需要对它们进行密集（density），因为即使对于中型大小文档的集合，密集化 （densifying）也可能填满内存。</p>
<p>虽然 <a href="generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD" title="sklearn.decomposition.TruncatedSVD"><code>TruncatedSVD</code></a> 转换器（transformer）可以在任何（稀疏的）特征矩阵上工作，但还是建议在 LSA/document 处理设置中，在 tf–idf 矩阵上的原始频率计数使用它。 特别地，应该打开子线性缩放（sublinear scaling）和逆文档频率（inverse document frequency） (<code>sublinear_tf=True, use_idf=True</code>) 以使特征值更接近于高斯分布，补偿 LSA 对文本数据的错误假设。</p>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/text/document_clustering.html#sphx-glr-auto-examples-text-document-clustering-py">Clustering text documents using k-means</a></li>
</ul>
<p>参考文献:</p>
<ul>
<li>Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze (2008), <em>Introduction to Information Retrieval</em>, Cambridge University Press, chapter 18: <a href="http://nlp.stanford.edu/IR-book/pdf/18lsi.pdf">Matrix decompositions &amp; latent semantic indexing</a></li>
</ul>
<h2 id="253-词典学习">2.5.3. 词典学习</h2>
<h3 id="2531-带有预计算词典的稀疏编码">2.5.3.1. 带有预计算词典的稀疏编码</h3>
<p><a href="generated/sklearn.decomposition.SparseCoder.html#sklearn.decomposition.SparseCoder" title="sklearn.decomposition.SparseCoder"><code>SparseCoder</code></a> 对象是一个估计器 （estimator），可以用来将信号转换成一个固定的预计算的词典内原子（atoms）的稀疏线性组合（sparse linear combination），如离散小波基（ discrete wavelet basis ） 。 因此，该对象不实现 <code>fit</code> 方法。该转换相当于一个稀疏编码问题: 将数据的表示为尽可能少的词典原子的线性组合。 词典学习的所有变体实现以下变换方法，可以通过 <code>transform_method</code> 初始化参数进行控制:</p>
<ul>
<li>Orthogonal matching pursuit(追求正交匹配) (<a href="linear_model.html#omp">正交匹配追踪法（OMP）</a>)</li>
<li>Least-angle regression (最小角度回归)(<a href="linear_model.html#least-angle-regression">最小角回归</a>)</li>
<li>Lasso computed by least-angle regression(最小角度回归的Lasso 计算)</li>
<li>Lasso using coordinate descent ( 使用坐标下降的Lasso)(<a href="linear_model.html#lasso">Lasso</a>)</li>
<li>Thresholding(阈值)</li>
</ul>
<p>阈值方法速度非常快，但是不能产生精确的重建。 它们在分类任务的文献中已被证明是有用的。对于图像重建任务，追求正交匹配可以产生最精确、无偏的重建。</p>
<p>词典学习对象通过 <code>split_code</code> 参数提供稀疏编码结果中的正值和负值分离的可能性。当使用词典学习来提取将用于监督学习的特征时，这是有用的，因为它允许学习算法将不同的权重从正加载（loading）分配给相应的负加载的特定原子。</p>
<p>单个样本的分割编码具有长度 <code>2 * n_components</code> ，并使用以下规则构造: 首先，计算长度为 <code>n_components</code> 的常规编码。然后， <code>split_code</code> 的第一个 <code>n_components</code> 条目将用正常编码向量的正部分填充。分割编码的第二部分用编码向量的负部分填充，只有一个正号。因此， split_code 是非负的。</p>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/decomposition/plot_sparse_coding.html#sphx-glr-auto-examples-decomposition-plot-sparse-coding-py">Sparse coding with a precomputed dictionary</a></li>
</ul>
<h3 id="2532-通用词典学习">2.5.3.2. 通用词典学习</h3>
<p>词典学习( <a href="generated/sklearn.decomposition.DictionaryLearning.html#sklearn.decomposition.DictionaryLearning" title="sklearn.decomposition.DictionaryLearning"><code>DictionaryLearning</code></a> ) 是一个矩阵因式分解问题，相当于找到一个在拟合数据的稀疏编码中表现良好的（通常是过完备的（overcomplete））词典。</p>
<p>将数据表示为来自过完备词典的原子的稀疏组合被认为是哺乳动物初级视觉皮层的工作方式。 因此，应用于图像补丁的词典学习已被证明在诸如图像完成、修复和去噪，以及有监督的识别图像处理任务中表现良好的结果。</p>
<p>词典学习是通过交替更新稀疏编码来解决的优化问题，作为解决多个 Lasso 问题的一个解决方案，考虑到字典固定，然后更新字典以最好地适合稀疏编码。</p>
<pre><code class="language-py">
![(U^*, V^*) = \underset{U, V}{\operatorname{arg\,min\,}} &amp; \frac{1}{2}
             ||X-UV||_2^2+\alpha||U||_1 \\
             \text{subject to\,} &amp; ||V_k||_2 = 1 \text{ for all }
             0 \leq k &amp;lt; n_{\mathrm{atoms}}](img/9b4b00422c0cec29f80a03fe1d772100.jpg)

</code></pre>
<p><strong><a href="../auto_examples/decomposition/plot_faces_decomposition.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/Images/9a55689143b2e4d90adcdfe1f95b9ffd.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/Images/9a55689143b2e4d90adcdfe1f95b9ffd.jpg" alt="pca_img2"></a></a> <a href="../auto_examples/decomposition/plot_faces_decomposition.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/Images/86f7969b00fb3d0914f0bababac102a0.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/Images/86f7969b00fb3d0914f0bababac102a0.jpg" alt="dict_img2"></a></a></strong></p>
<p>在使用这样一个过程来拟合词典之后，变换只是一个稀疏的编码步骤，与所有的词典学习对象共享相同的实现。(参见 <a href="#sparsecoder">带有预计算词典的稀疏编码</a>)。</p>
<p>以下图像显示了字典学习是如何从浣熊脸部的部分图像中提取的4x4像素图像补丁中进行词典学习的。</p>
<p><a href="../auto_examples/decomposition/plot_image_denoising.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1cde7e5f92efe9056f9f53e23ea04102.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1cde7e5f92efe9056f9f53e23ea04102.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_image_denoising_0011.png"></a></a></p>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/decomposition/plot_image_denoising.html#sphx-glr-auto-examples-decomposition-plot-image-denoising-py">Image denoising using dictionary learning</a></li>
</ul>
<p>参考文献:</p>
<ul>
<li><a href="http://www.di.ens.fr/sierra/pdfs/icml09.pdf">“Online dictionary learning for sparse coding”</a> J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009</li>
</ul>
<h3 id="2533-小批量字典学习">2.5.3.3. 小批量字典学习</h3>
<p><a href="generated/sklearn.decomposition.MiniBatchDictionaryLearning.html#sklearn.decomposition.MiniBatchDictionaryLearning" title="sklearn.decomposition.MiniBatchDictionaryLearning"><code>MiniBatchDictionaryLearning</code></a> 实现了更快、更适合大型数据集的字典学习算法，其运行速度更快，但准确度有所降低。</p>
<p>默认情况下，<a href="generated/sklearn.decomposition.MiniBatchDictionaryLearning.html#sklearn.decomposition.MiniBatchDictionaryLearning" title="sklearn.decomposition.MiniBatchDictionaryLearning"><code>MiniBatchDictionaryLearning</code></a> 将数据分成小批量，并通过在指定次数的迭代中循环使用小批量，以在线方式进行优化。但是，目前它没有实现停止条件。</p>
<p>估计器还实现了 <code>partial_fit</code>, 它通过在一个小批处理中仅迭代一次来更新字典。 当在线学习的数据从一开始就不容易获得，或者数据超出内存时，可以使用这种迭代方法。</p>
<p><a href="../auto_examples/cluster/plot_dict_face_patches.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/Images/b335c88114c4fec7e72304006810c82c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/Images/b335c88114c4fec7e72304006810c82c.jpg" alt="https://scikit-learn.org/stable/_images/sphx_glr_plot_dict_face_patches_001.png"></a></a></p>
<p><strong>字典学习聚类</strong></p>
<p>注意，当使用字典学习来提取表示（例如，用于稀疏编码）时，聚类可以是学习字典的良好中间方法。 例如，<a href="generated/sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans" title="sklearn.cluster.MiniBatchKMeans"><code>MiniBatchKMeans</code></a> 估计器能高效计算并使用 <code>partial_fit</code> 方法实现在线学习。</p>
<p>示例: 在线学习面部部分的字典 <a href="../auto_examples/cluster/plot_dict_face_patches.html#sphx-glr-auto-examples-cluster-plot-dict-face-patches-py">Online learning of a dictionary of parts of faces</a></p>
<h2 id="254-因子分析">2.5.4. 因子分析</h2>
<p>在无监督的学习中，我们只有一个数据集 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ddb7802ca4af9cffa650eec942feb790.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ddb7802ca4af9cffa650eec942feb790.jpg" alt="X = \{x_1, x_2, \dots, x_n\}"></a>. 这个数据集如何在数学上描述？ <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43c1fea57579e54f80c0535bc582626f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43c1fea57579e54f80c0535bc582626f.jpg" alt="X"></a> 的一个非常简单的连续隐变量模型</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/75a6f2c15bfb418edcb993c4e27873d0.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/75a6f2c15bfb418edcb993c4e27873d0.jpg" alt="x_i = W h_i + \mu + \epsilon"></a></p>
<p>矢量 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3d52bf36b893b26195748e89c94273f0.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3d52bf36b893b26195748e89c94273f0.jpg" alt="h_i"></a> 被称为 “隐性的”，因为它是不可观察的。 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/58ef9e1b5d2ee139dcb588a3879ca1a6.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/58ef9e1b5d2ee139dcb588a3879ca1a6.jpg" alt="\epsilon"></a> 被认为是符合高斯分布的噪声项，平均值为 0，协方差为 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/36f54997ff4df647587d1bfd2ddb3ee2.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/36f54997ff4df647587d1bfd2ddb3ee2.jpg" alt="\Psi"></a> （即 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e5012484ffa6afb2c720d363b39a36b0.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e5012484ffa6afb2c720d363b39a36b0.jpg" alt="\epsilon \sim \mathcal{N}(0, \Psi)"></a>）， <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b6cea83c1722562f844aebd98fb3f59d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b6cea83c1722562f844aebd98fb3f59d.jpg" alt="\mu"></a> 是偏移向量。 这样一个模型被称为 “生成的”，因为它描述了如何从 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3d52bf36b893b26195748e89c94273f0.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3d52bf36b893b26195748e89c94273f0.jpg" alt="h_i"></a> 生成 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/cf52655ee609af9f3c27c06448a5bf67.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/cf52655ee609af9f3c27c06448a5bf67.jpg" alt="x_i"></a> 。 如果我们使用所有的 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/cf52655ee609af9f3c27c06448a5bf67.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/cf52655ee609af9f3c27c06448a5bf67.jpg" alt="x_i"></a> 作为列来形成一个矩阵 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/36aff9afacf42a6a0b903bb6cd3409dc.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/36aff9afacf42a6a0b903bb6cd3409dc.jpg" alt="\mathbf{X}"></a> ，并将所有的 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3d52bf36b893b26195748e89c94273f0.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3d52bf36b893b26195748e89c94273f0.jpg" alt="h_i"></a> 作为矩阵 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a243fb2cabe46c32a3a66214f514d623.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a243fb2cabe46c32a3a66214f514d623.jpg" alt="\mathbf{H}"></a> 的列， 那么我们可以写（适当定义的 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4e6d5ce51d78cff57187dc09b6710a7c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4e6d5ce51d78cff57187dc09b6710a7c.jpg" alt="\mathbf{M}"></a> 和 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/355df435a05593b653d7d988c06e5d3c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/355df435a05593b653d7d988c06e5d3c.jpg" alt="\mathbf{E}"></a> ）:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/337f0e255aa71dafb655629cb09a0c14.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/337f0e255aa71dafb655629cb09a0c14.jpg" alt="\mathbf{X} = W \mathbf{H} + \mathbf{M} + \mathbf{E}"></a></p>
<p>换句话说，我们 <em>分解</em> 矩阵 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/36aff9afacf42a6a0b903bb6cd3409dc.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/36aff9afacf42a6a0b903bb6cd3409dc.jpg" alt="\mathbf{X}"></a>. 如果给出 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3d52bf36b893b26195748e89c94273f0.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3d52bf36b893b26195748e89c94273f0.jpg" alt="h_i"></a>，上述方程自动地表示以下概率解释：</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d47bd99afb1d5dd3bff5b9809371c476.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d47bd99afb1d5dd3bff5b9809371c476.jpg" alt="p(x_i|h_i) = \mathcal{N}(Wh_i + \mu, \Psi)"></a></p>
<p>对于一个完整的概率模型，我们还需要隐变量 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c5f49595b56010ad04fce358940848e5.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c5f49595b56010ad04fce358940848e5.jpg" alt="h"></a> 的先验分布。 最直接的假设（基于高斯分布的良好性质）是 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/55f40b1e092983fff81024042966adec.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/55f40b1e092983fff81024042966adec.jpg" alt="h \sim \mathcal{N}(0, \mathbf{I})"></a>. 这产生一个高斯分布作为 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5c82dbae35dc43d2f556f9f284d9d184.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5c82dbae35dc43d2f556f9f284d9d184.jpg" alt="x"></a> 的边际分布:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b6a1f98637a242005be08bb10541a524.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b6a1f98637a242005be08bb10541a524.jpg" alt="p(x) = \mathcal{N}(\mu, WW^T + \Psi)"></a></p>
<p>现在，在没有任何进一步假设的前提下，隐变量 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c5f49595b56010ad04fce358940848e5.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c5f49595b56010ad04fce358940848e5.jpg" alt="h"></a> 是多余的 – <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5c82dbae35dc43d2f556f9f284d9d184.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/5c82dbae35dc43d2f556f9f284d9d184.jpg" alt="x"></a> 完全可以用均值和协方差来建模。 我们需要对这两个参数之一进行更具体的构造。 一个简单的附加假设是将误差协方差 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/36f54997ff4df647587d1bfd2ddb3ee2.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/36f54997ff4df647587d1bfd2ddb3ee2.jpg" alt="\Psi"></a> 构造成如下:</p>
<ul>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/cd8ca25fe0dc0cc43949bcaa5d2674c2.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/cd8ca25fe0dc0cc43949bcaa5d2674c2.jpg" alt="\Psi = \sigma^2 \mathbf{I}"></a>: 这个假设能推导出 <a href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code>PCA</code></a> 的概率模型。</li>
<li><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e64298b4d9439c3db54eeddbf3d92b4b.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e64298b4d9439c3db54eeddbf3d92b4b.jpg" alt="\Psi = \mathrm{diag}(\psi_1, \psi_2, \dots, \psi_n)"></a>: 这个模型称为 <a href="generated/sklearn.decomposition.FactorAnalysis.html#sklearn.decomposition.FactorAnalysis" title="sklearn.decomposition.FactorAnalysis"><code>FactorAnalysis</code></a>, 一个经典的统计模型。 矩阵W有时称为 “因子加载矩阵”。</li>
</ul>
<p>两个模型基都基于高斯分布是低阶协方差矩阵的假设。 因为这两个模型都是概率性的，所以它们可以集成到更复杂的模型中， 例如因子分析器的混合。如果隐变量基于非高斯分布，则得到完全不同的模型（例如， <a href="generated/sklearn.decomposition.FastICA.html#sklearn.decomposition.FastICA" title="sklearn.decomposition.FastICA"><code>FastICA</code></a> ）。</p>
<p>因子分析 <em>可以</em> 产生与 :class:[<code>](#id13)PCA</code>类似的成分（例如其加载矩阵的列）。 然而，这些成分没有通用的性质（例如它们是否是正交的）:</p>
<p><strong><a href="../auto_examples/decomposition/plot_faces_decomposition.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/Images/9a55689143b2e4d90adcdfe1f95b9ffd.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/Images/9a55689143b2e4d90adcdfe1f95b9ffd.jpg" alt="pca_img3"></a></a> <a href="../auto_examples/decomposition/plot_faces_decomposition.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/Images/1f8c02d4fdbdbcaa014972bca5620cf3.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/Images/1f8c02d4fdbdbcaa014972bca5620cf3.jpg" alt="fa_img3"></a></a></strong></p>
<p>因子分析( <a href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code>PCA</code></a> ) 的主要优点是可以独立地对输入空间的每个方向（异方差噪声）的方差建模:</p>
<p><a href="../auto_examples/decomposition/plot_faces_decomposition.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/Images/67f62308a1f409829599e546c843d53f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/Images/67f62308a1f409829599e546c843d53f.jpg" alt="https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_008.png"></a></a></p>
<p>在异方差噪声存在的情况下，这可以比概率 PCA 作出更好的模型选择:</p>
<p><a href="../auto_examples/decomposition/plot_pca_vs_fa_model_selection.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/30fc2a610fc7de5c19317e1fc584765f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/30fc2a610fc7de5c19317e1fc584765f.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_pca_vs_fa_model_selection_0021.png"></a></a></p>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/decomposition/plot_pca_vs_fa_model_selection.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-fa-model-selection-py">Model selection with Probabilistic PCA and Factor Analysis (FA)</a></li>
</ul>
<h2 id="255-独立成分分析ica">2.5.5. 独立成分分析（ICA）</h2>
<p>独立分量分析将多变量信号分解为独立性最强的加性子组件。 它通过 <a href="generated/sklearn.decomposition.FastICA.html#sklearn.decomposition.FastICA" title="sklearn.decomposition.FastICA"><code>Fast ICA</code></a> 算法在 scikit-learn 中实现。 ICA 通常不用于降低维度，而是用于分离叠加信号。 由于 ICA 模型不包括噪声项，因此要使模型正确，必须使用白化。 这可以在内部调节白化参数或手动使用 PCA 的一种变体。</p>
<p>ICA 通常用于分离混合信号（称为 <em>盲源分离</em> 的问题），如下例所示:</p>
<p><a href="../auto_examples/decomposition/plot_ica_blind_source_separation.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/61a79d63783315d8e68d8ecf1324105f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/61a79d63783315d8e68d8ecf1324105f.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_ica_blind_source_separation_0011.png"></a></a></p>
<p>ICA 也可以用于具有稀疏子成分的非线性分解:</p>
<p><strong><a href="../auto_examples/decomposition/plot_faces_decomposition.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/Images/9a55689143b2e4d90adcdfe1f95b9ffd.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/Images/9a55689143b2e4d90adcdfe1f95b9ffd.jpg" alt="pca_img4"></a></a> <a href="../auto_examples/decomposition/plot_faces_decomposition.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/Images/390e2bdab30b6e7421082f13e8cfd6b0.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/Images/390e2bdab30b6e7421082f13e8cfd6b0.jpg" alt="ica_img4"></a></a></strong></p>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/decomposition/plot_ica_blind_source_separation.html#sphx-glr-auto-examples-decomposition-plot-ica-blind-source-separation-py">Blind source separation using FastICA</a></li>
<li><a href="../auto_examples/decomposition/plot_ica_vs_pca.html#sphx-glr-auto-examples-decomposition-plot-ica-vs-pca-py">FastICA on 2D point clouds</a></li>
<li><a href="../auto_examples/decomposition/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py">Faces dataset decompositions</a></li>
</ul>
<h2 id="256-非负矩阵分解nmf-或-nnmf">2.5.6. 非负矩阵分解(NMF 或 NNMF)</h2>
<h3 id="2561-nmf-与-frobenius-范数">2.5.6.1. NMF 与 Frobenius 范数</h3>
<p><a href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code>NMF</code></a> <a href="1">1</a>(#id22) 是在数据和分量是非负情况下的另一种降维方法。 在数据矩阵不包含负值的情况下，可以插入 <a href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code>NMF</code></a> 而不是 <a href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code>PCA</code></a> 或其变体。 通过优化 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43c1fea57579e54f80c0535bc582626f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43c1fea57579e54f80c0535bc582626f.jpg" alt="X"></a> 与矩阵乘积 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d822c46462e0ffda4dd99f74a070b6b3.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d822c46462e0ffda4dd99f74a070b6b3.jpg" alt="WH"></a> 之间的距离 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/adf83056bc2bd05628e24c40cb728b3d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/adf83056bc2bd05628e24c40cb728b3d.jpg" alt="d"></a> ，可以将样本 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43c1fea57579e54f80c0535bc582626f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43c1fea57579e54f80c0535bc582626f.jpg" alt="X"></a> 分解为两个非负矩阵 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/369b6e6bd43ee84fe99e14c8d78cdc9f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/369b6e6bd43ee84fe99e14c8d78cdc9f.jpg" alt="W"></a> 和 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9ad9b5d15124615ced9c9721a8ef4d3b.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/9ad9b5d15124615ced9c9721a8ef4d3b.jpg" alt="H"></a>。 最广泛使用的距离函数是 Frobenius 平方范数，它是欧几里德范数到矩阵的推广:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ad946e6478bb10e60ac9663066f26ee8.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ad946e6478bb10e60ac9663066f26ee8.jpg" alt="d_{\mathrm{Fro}}(X, Y) = \frac{1}{2} ||X - Y||{\mathrm{Fro}}^2 = \frac{1}{2} \sum{i,j} (X_{ij} - {Y}_{ij})^2"></a></p>
<p>与 <a href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code>PCA</code></a> 不同，通过叠加分量而不减去，以加法方式获得向量的表示。这种加性模型对于表示图像和文本是有效的。</p>
<blockquote>
<p>[Hoyer, 2004] <a href="2">2</a>(#id23) 研究表明，当处于一定约束时，<a href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code>NMF</code></a> 可以产生数据集基于某子部分的表示，从而获得可解释的模型。</p>
</blockquote>
<p>以下示例展示了与 PCA 特征面相比， <a href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code>NMF</code></a> 从 Olivetti 面部数据集中的图像中发现的16个稀疏组件。</p>
<p>Unlike <a href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code>PCA</code></a>, the representation of a vector is obtained in an additive fashion, by superimposing the components, without subtracting. Such additive models are efficient for representing images and text.</p>
<p><strong><a href="../auto_examples/decomposition/plot_faces_decomposition.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/Images/9a55689143b2e4d90adcdfe1f95b9ffd.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/Images/9a55689143b2e4d90adcdfe1f95b9ffd.jpg" alt="pca_img5"></a></a> <a href="../auto_examples/decomposition/plot_faces_decomposition.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/Images/bc08d23c9d8a06975f20eb514faae9ef.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/Images/bc08d23c9d8a06975f20eb514faae9ef.jpg" alt="nmf_img5"></a></a></strong></p>
<p><code>init</code> 属性确定了应用的初始化方法，这对方法的性能有很大的影响。 <a href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code>NMF</code></a> 实现了非负双奇异值分解方法。NNDSVD <a href="4">4</a>(#id24) 基于两个 SVD 过程，一个近似数据矩阵， 使用单位秩矩阵的代数性质，得到的部分SVD因子的其他近似正部分。 基本的 NNDSVD 算法更适合稀疏分解。其变体 NNDSVDa（全部零值替换为所有元素的平均值）和 NNDSVDar（零值替换为比数据平均值除以100小的随机扰动）在稠密情况时推荐使用。</p>
<p>请注意，乘法更新 (‘mu’) 求解器无法更新初始化中存在的零，因此当与引入大量零的基本 NNDSVD 算法联合使用时， 会导致较差的结果; 在这种情况下，应优先使用 NNDSVDa 或 NNDSVDar。</p>
<p>也可以通过设置 <code>init="random"</code>，使用正确缩放的随机非负矩阵初始化 <a href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code>NMF</code></a> 。 整数种子或 <code>RandomState</code> 也可以传递给 <code>random_state</code> 以控制重现性。</p>
<p>在 <a href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code>NMF</code></a> 中，L1 和 L2 先验可以被添加到损失函数中以使模型正规化。 L2 先验使用 Frobenius 范数，而L1 先验使用 L1 范数。与 <code>ElasticNet</code> 一样， 我们通过 <code>l1_ratio</code> (<a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b91e4507d9fd7068b02f689d697f8714.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/b91e4507d9fd7068b02f689d697f8714.jpg" alt="\rho"></a>) 参数和正则化强度参数 <code>alpha</code> (<a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d8b3d5242d513369a44f8bf0c6112744.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/d8b3d5242d513369a44f8bf0c6112744.jpg" alt="\alpha"></a>) 来控制 L1 和 L2 的组合。那么先验项是:</p>
<pre><code class="language-py">
![\alpha \rho ||W||_1 + \alpha \rho ||H||_1
+ \frac{\alpha(1-\rho)}{2} ||W||_{\mathrm{Fro}} ^ 2
+ \frac{\alpha(1-\rho)}{2} ||H||_{\mathrm{Fro}} ^ 2](img/be8c80153a3cafbe4309f1fe3b62d96b.jpg)

</code></pre>
<p>正则化目标函数为:</p>
<pre><code class="language-py">
![d_{\mathrm{Fro}}(X, WH)
+ \alpha \rho ||W||_1 + \alpha \rho ||H||_1
+ \frac{\alpha(1-\rho)}{2} ||W||_{\mathrm{Fro}} ^ 2
+ \frac{\alpha(1-\rho)}{2} ||H||_{\mathrm{Fro}} ^ 2](img/2c1da71c882c95ba6660cdad0d976f6d.jpg)

</code></pre>
<p><a href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code>NMF</code></a> 正则化 W 和 H . 公共函数 <code>non_negative_factorization</code> 允许通过 <code>regularization</code> 属性进行更精细的控制，将 仅W ，仅H 或两者正规化。</p>
<h3 id="2562-具有-beta-divergence-的-nmf">2.5.6.2. 具有 beta-divergence 的 NMF</h3>
<p>如前所述，最广泛使用的距离函数是平方 Frobenius 范数，这是欧几里得范数到矩阵的推广:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3776f899ba5f1d7432c553c6c3aae381.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/3776f899ba5f1d7432c553c6c3aae381.jpg" alt="d_{\mathrm{Fro}}(X, Y) = \frac{1}{2} ||X - Y||{Fro}^2 = \frac{1}{2} \sum{i,j} (X_{ij} - {Y}_{ij})^2"></a></p>
<p>其他距离函数可用于 NMF，例如（广义） Kullback-Leibler(KL) 散度，也称为 I-divergence:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/33f1b6fb64999d2af571c675b7f17f34.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/33f1b6fb64999d2af571c675b7f17f34.jpg" alt="d_{KL}(X, Y) = \sum_{i,j} (X_{ij} \log(\frac{X_{ij}}{Y_{ij}}) - X_{ij} + Y_{ij})"></a></p>
<p>或者， Itakura-Saito(IS) divergence:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/345ab99b5a1246fb019e249dae570191.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/345ab99b5a1246fb019e249dae570191.jpg" alt="d_{IS}(X, Y) = \sum_{i,j} (\frac{X_{ij}}{Y_{ij}} - \log(\frac{X_{ij}}{Y_{ij}}) - 1)"></a></p>
<p>这三个距离函数是 beta-divergence 函数族的特殊情况，其参数分别为 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c63c614232be2789284b906792195a15.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/c63c614232be2789284b906792195a15.jpg" alt="\beta = 2, 1, 0"></a> <a href="6">6</a>(#id26) 。 beta-divergence 定义如下:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1da4a8f74cdb166cdc91e2f691cf3ac5.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/1da4a8f74cdb166cdc91e2f691cf3ac5.jpg" alt="d_{\beta}(X, Y) = \sum_{i,j} \frac{1}{\beta(\beta - 1)}(X_{ij}\beta + (\beta-1)Y_{ij}\beta - \beta X_{ij} Y_{ij}^{\beta - 1})"></a></p>
<p><a href="../auto_examples/decomposition/plot_beta_divergence.html"><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/10bfda066ccebace59b1d11135e10196.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/10bfda066ccebace59b1d11135e10196.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_beta_divergence_0011.png"></a></a></p>
<p>请注意，在 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a19f2294e2649252ad2b5766d295e75e.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/a19f2294e2649252ad2b5766d295e75e.jpg" alt="\beta \in (0; 1)"></a> 上定义无效，仅仅在 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8c4beae51da320d8fffd739a9e9e3852.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8c4beae51da320d8fffd739a9e9e3852.jpg" alt="d_{KL}"></a> 和 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/aee8fc1c253da16851991ef3ceff663b.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/aee8fc1c253da16851991ef3ceff663b.jpg" alt="d_{IS}"></a> 的上可以分别连续扩展。</p>
<p><a href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code>NMF</code></a> 使用 Coordinate Descent (‘cd’) <a href="5">5</a>(#id25) 和乘法更新 (‘mu’) <a href="6">6</a>(#id26) 来实现两个求解器。 ‘mu’ 求解器可以优化每个 beta-divergence，包括 Frobenius 范数 (<a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6875a3a68e07bfa51a631f014fcf8a4f.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6875a3a68e07bfa51a631f014fcf8a4f.jpg" alt="\beta=2"></a>) ， （广义） Kullback-Leibler divergence (<a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/04388b884d40fc8b56559b6c2364e7ce.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/04388b884d40fc8b56559b6c2364e7ce.jpg" alt="\beta=1"></a>) 和Itakura-Saito divergence（beta = 0） ）。 请注意，对于 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/67f9d3900eb064f6354d23271f16c2b0.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/67f9d3900eb064f6354d23271f16c2b0.jpg" alt="\beta \in (1; 2)"></a>，’mu’ 求解器明显快于 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/533e54759d696211ebe7819cc107d3bc.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/533e54759d696211ebe7819cc107d3bc.jpg" alt="\beta"></a> 的其他值。 还要注意，使用负数（或0，即 ‘itakura-saito’ ） <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/533e54759d696211ebe7819cc107d3bc.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/533e54759d696211ebe7819cc107d3bc.jpg" alt="\beta"></a>，输入矩阵不能包含零值。</p>
<p>‘cd’ 求解器只能优化 Frobenius 范数。由于 NMF 的潜在非凸性，即使优化相同的距离函数， 不同的求解器也可能会收敛到不同的最小值。</p>
<p>NMF最适用于 <code>fit_transform</code> 方法，该方法返回矩阵W.矩阵 H 被 <code>components_</code> 属性中存储到拟合模型中; 方法 <code>transform</code> 将基于这些存储的组件分解新的矩阵 X_new:</p>
<pre><code class="language-py">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])
&gt;&gt;&gt; from sklearn.decomposition import NMF
&gt;&gt;&gt; model = NMF(n_components=2, init='random', random_state=0)
&gt;&gt;&gt; W = model.fit_transform(X)
&gt;&gt;&gt; H = model.components_
&gt;&gt;&gt; X_new = np.array([[1, 0], [1, 6.1], [1, 0], [1, 4], [3.2, 1], [0, 4]])
&gt;&gt;&gt; W_new = model.transform(X_new)

</code></pre>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/decomposition/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py">Faces dataset decompositions</a></li>
<li><a href="../auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py">Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation</a></li>
<li><a href="../auto_examples/decomposition/plot_beta_divergence.html#sphx-glr-auto-examples-decomposition-plot-beta-divergence-py">Beta-divergence loss functions</a></li>
</ul>
<p>参考文献:</p>
<p>| <a href="1">1</a>(#id16) | <a href="http://www.columbia.edu/~jwp2128/Teaching/W4721/papers/nmf_nature.pdf">“Learning the parts of objects by non-negative matrix factorization”</a> D. Lee, S. Seung, 1999 |</p>
<p>| <a href="2">2</a>(#id17) | <a href="http://www.jmlr.org/papers/volume5/hoyer04a/hoyer04a.pdf">“Non-negative Matrix Factorization with Sparseness Constraints”</a> P. Hoyer, 2004 |</p>
<p>| <a href="4">4</a>(#id18) | <a href="http://scgroup.hpclab.ceid.upatras.gr/faculty/stratis/Papers/HPCLAB020107.pdf">“SVD based initialization: A head start for nonnegative matrix factorization”</a> C. Boutsidis, E. Gallopoulos, 2008 |</p>
<p>| <a href="5">5</a>(#id20) | <a href="http://www.bsp.brain.riken.jp/publications/2009/Cichocki-Phan-IEICE_col.pdf">“Fast local algorithms for large scale nonnegative matrix and tensor factorizations.”</a> A. Cichocki, P. Anh-Huy, 2009 |</p>
<p>| [6] | <em>(<a href="#id19">1</a>, <a href="#id21">2</a>)</em> <a href="http://http://arxiv.org/pdf/1010.1763v3.pdf">“Algorithms for nonnegative matrix factorization with the beta-divergence”</a> C. Fevotte, J. Idier, 2011 |</p>
<h2 id="257-隐-dirichlet-分配lda">2.5.7. 隐 Dirichlet 分配（LDA）</h2>
<p>隐 Dirichlet 分配是离散数据集（如文本语料库）的集合的生成概率模型。 它也是一个主题模型，用于从文档集合中发现抽象主题。</p>
<p>LDA 的图形模型是一个三层贝叶斯模型:</p>
<p><a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/30641b10b766d35775b6bbb4d21e74b7.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/30641b10b766d35775b6bbb4d21e74b7.jpg" alt="http://sklearn.apachecn.org/cn/0.19.0/_images/lda_model_graph.png"></a></p>
<p>当建模文本语料库时，该模型假设具有 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e03066df748abd9273db055cb79f0f01.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e03066df748abd9273db055cb79f0f01.jpg" alt="D"></a> 文档和 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e279b8169ddd6581c5606c868ba52fae.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/e279b8169ddd6581c5606c868ba52fae.jpg" alt="K"></a> 主题的语料库的以下生成过程:</p>
<blockquote>
<ol>
<li>对于每个主题 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/f93871977da52a6d11045d57c3e18728.jpg" alt="k"></a>，绘制 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0333d4e1607c1cab19f576a212267ec1.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0333d4e1607c1cab19f576a212267ec1.jpg" alt="\beta_k \sim \mathrm{Dirichlet}(\eta),\: k =1...K"></a></li>
<li>对于每个文档 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/adf83056bc2bd05628e24c40cb728b3d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/adf83056bc2bd05628e24c40cb728b3d.jpg" alt="d"></a>，绘制 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7f1cafe91ec3b94b27f8759724287242.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7f1cafe91ec3b94b27f8759724287242.jpg" alt="\theta_d \sim \mathrm{Dirichlet}(\alpha), \: d=1...D"></a></li>
<li>对于文档 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/adf83056bc2bd05628e24c40cb728b3d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/adf83056bc2bd05628e24c40cb728b3d.jpg" alt="d"></a> 中的每个单词 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/43e13b580daefe5ba754b790dfbd216c.jpg" alt="i"></a>:</li>
</ol>
<p>&gt; 1. 绘制主题索引 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4a733ee899c074bde7a4d5292c9fc83e.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/4a733ee899c074bde7a4d5292c9fc83e.jpg" alt="z_{di} \sim \mathrm{Multinomial}(\theta_d)"></a> &gt; 2. 绘制观察词 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/526e2da298d085b5fd557f49433d4143.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/526e2da298d085b5fd557f49433d4143.jpg" alt="w_{ij} \sim \mathrm{Multinomial}(beta_{z_{di}}.)"></a></p>
</blockquote>
<p>对于参数估计，后验分布为:</p>
<pre><code class="language-py">
![p(z, \theta, \beta |w, \alpha, \eta) =
  \frac{p(z, \theta, \beta|\alpha, \eta)}{p(w|\alpha, \eta)}](img/5d0c433dc4dc7ca883ac8173e6e2096f.jpg)

</code></pre>
<p>由于后验分布难以处理，变体贝叶斯方法使用更简单的分布 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8fae035cff5a2ccfbc80e38fab4907cd.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/8fae035cff5a2ccfbc80e38fab4907cd.jpg" alt="q(z,\theta,\beta | \lambda, \phi, \gamma)"></a> 近似， 并且优化了这些变体参数 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0f92bc682b050115d03c625ce770c77d.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/0f92bc682b050115d03c625ce770c77d.jpg" alt="\lambda"></a>, <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ff5e98366afa13070d3b410c55a80db1.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/ff5e98366afa13070d3b410c55a80db1.jpg" alt="\phi"></a>, <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6552bde3d3999c1a9728016416932af7.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/6552bde3d3999c1a9728016416932af7.jpg" alt="\gamma"></a> 最大化Evidence Lower Bound (ELBO):</p>
<pre><code class="language-py">
![\log\: P(w | \alpha, \eta) \geq L(w,\phi,\gamma,\lambda) \overset{\triangle}{=}
  E_{q}[\log\:p(w,z,\theta,\beta|\alpha,\eta)] - E_{q}[\log\:q(z, \theta, \beta)]](img/6d8b62cf31afb168e2b2acb89d6abccd.jpg)

</code></pre>
<p>最大化 ELBO 相当于最小化 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2c2dcc83fc38e46810a36e59b2614a5c.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/2c2dcc83fc38e46810a36e59b2614a5c.jpg" alt="q(z,\theta,\beta)"></a> 和后验 <a href="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7efe29500f4af973643a15b3ed29a926.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/scikit-learn-doc-zh/docs/img/7efe29500f4af973643a15b3ed29a926.jpg" alt="p(z, \theta, \beta |w, \alpha, \eta)"></a> 之间的 Kullback-Leibler(KL) 散度。</p>
<p><a href="generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation" title="sklearn.decomposition.LatentDirichletAllocation"><code>LatentDirichletAllocation</code></a> 实现在线变体贝叶斯算法，支持在线和批量更新方法。 批处理方法在每次完全传递数据后更新变分变量，在线方法从小批量数据点中更新变体变量。</p>
<p>Note</p>
<p>虽然在线方法保证收敛到局部最优点，最优点的质量和收敛速度可能取决于与小批量大小和学习率相关的属性。</p>
<p>当 <a href="generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation" title="sklearn.decomposition.LatentDirichletAllocation"><code>LatentDirichletAllocation</code></a> 应用于 “document-term” 矩阵时，矩阵将被分解为 “topic-term” 矩阵和 “document-topic” 矩阵。 虽然 “topic-term” 矩阵在模型中被存储为 <code>components_</code> ，但是可以通过 <code>transform</code> 方法计算 “document-topic” 矩阵。</p>
<p><a href="generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation" title="sklearn.decomposition.LatentDirichletAllocation"><code>LatentDirichletAllocation</code></a> 还实现了 <code>partial_fit</code> 方法。这可用于当数据被顺序提取时.</p>
<p>示例:</p>
<ul>
<li><a href="../auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py">Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation</a></li>
</ul>
<p>参考:</p>
<ul>
<li><a href="https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf">“Latent Dirichlet Allocation”</a> D. Blei, A. Ng, M. Jordan, 2003</li>
<li><a href="https://www.cs.princeton.edu/~blei/papers/HoffmanBleiBach2010b.pdf">“Online Learning for Latent Dirichlet Allocation”</a> M. Hoffman, D. Blei, F. Bach, 2010</li>
<li><a href="http://www.columbia.edu/~jwp2128/Papers/HoffmanBleiWangPaisley2013.pdf">“Stochastic Variational Inference”</a> M. Hoffman, D. Blei, C. Wang, J. Paisley, 2013</li>
</ul>
</div>
<hr class="uk-article-divider">
<div class="uk-block uk-block-muted uk-padding-top-remove uk-padding-bottom-remove uk-margin-large-top  book-recommend-wrap">
<div class="uk-margin-top uk-margin-bottom uk-margin-left uk-margin-right">
<div class="uk-margin uk-text-muted "><i class="uk-icon-outdent uk-icon-justify uk-margin-small-right"></i>书籍推荐</div>
<div class="books">
<ul class="uk-book-list">
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/161/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/161/index.html">关于python的面试题</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/92.html">jackfrued</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">271页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月26日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 33个">33</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/96/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/96/index.html">零基础学Python</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/59.html">qiwsir</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">80页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月29日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 1635个">1635</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/162/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/162/index.html">Python方向综合面试题</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/92.html">jackfrued</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">115页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月26日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 35个">35</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/134/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/github_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/134/index.html">GitHub 漫游指南</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/72.html">phodal</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="github">github</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">1页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年8月3日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 3472个">3472</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/143/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/intellij_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/143/index.html">IntelliJ IDEA 简体中文专题教程</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/76.html">judasn</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="intellij">intellij</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">43页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年3月3日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 10995个">10995</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/156/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/156/index.html">pyspider中文文档</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/88.html">aaronhua123</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">18页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月12日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 1个">1</span>
</div>
</div>
</div>
</li>
<hr>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
<nav class="tm-navbar uk-navbar uk-navbar-attached reader-nav">
<div class="uk-float-left uk-margin-small-top">
<a href="javascript:;" title="目录菜单" class="show-menu  uk-icon-hover  uk-icon-align-justify uk-margin-right"></a>
<div data-uk-dropdown="{mode:'click',pos:'bottom-left'}" class="font-setting-wrap">
<a class="uk-icon-hover uk-icon-font uk-margin-right" aria-label="字体设置" href="javascript:;"></a>
<div class="uk-dropdown dropdown-menu">
<div class="dropdown-caret"><span class="caret-outer"></span><span class="caret-inner"></span></div>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-reduce">小字</button>
<button class="uk-button-link button size-2 font-enlarge">大字</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-1 ">宋体</button>
<button class="uk-button-link button size-2 font-2 ">黑体</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-3 color-theme-sun "><i class="uk-icon-sun-o"></i>白天</button>
<button class="uk-button-link button size-3 color-theme-eye "><i class="uk-icon-eye"></i>护眼</button>
<button class="uk-button-link button size-3 color-theme-moon "><i class="uk-icon-moon-o"></i>夜晚</button></div>
</div>
</div>
<a class="logo uk-margin-right" href="../../../" title="返回首页"><img class="" src="../../../static/components/images/icon_32.png" /></a>
</div>
<div class="uk-navbar-flip  uk-hidden-small">
<div id="share-box"></div>
</div>
</nav>
<div id="menu-id" class="uk-offcanvas reader-offcanvas">
<div class="uk-offcanvas-bar">
<ul class="book-menu-bar uk-nav uk-nav-offcanvas" data-uk-nav>
<li>
<a href="../../../book/170/index.html" data-book-page-rel-url="index.html" data-book-page-id="0" title="封面">封面</a>
</li>
<li>
<a class="pjax" href="../../../book/170/readme.html" data-book-page-rel-url="readme.html" data-book-page-id="0" title="简介">简介</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/79.html" title="安装 scikit-learn" data-book-page-rel-url="docs/79.html" data-book-page-id="11491">安装 scikit-learn</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/1.html" title="1. 监督学习" data-book-page-rel-url="docs/1.html" data-book-page-id="11492">1. 监督学习</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/2.html" title="1.1. 广义线性模型" data-book-page-rel-url="docs/2.html" data-book-page-id="11493">1.1. 广义线性模型</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/3.html" title="1.2. 线性和二次判别分析" data-book-page-rel-url="docs/3.html" data-book-page-id="11494">1.2. 线性和二次判别分析</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/4.html" title="1.3. 内核岭回归" data-book-page-rel-url="docs/4.html" data-book-page-id="11495">1.3. 内核岭回归</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/5.html" title="1.4. 支持向量机" data-book-page-rel-url="docs/5.html" data-book-page-id="11496">1.4. 支持向量机</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/6.html" title="1.5. 随机梯度下降" data-book-page-rel-url="docs/6.html" data-book-page-id="11497">1.5. 随机梯度下降</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/7.html" title="1.6. 最近邻" data-book-page-rel-url="docs/7.html" data-book-page-id="11498">1.6. 最近邻</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/8.html" title="1.7. 高斯过程" data-book-page-rel-url="docs/8.html" data-book-page-id="11499">1.7. 高斯过程</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/9.html" title="1.8. 交叉分解" data-book-page-rel-url="docs/9.html" data-book-page-id="11500">1.8. 交叉分解</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/10.html" title="1.9. 朴素贝叶斯" data-book-page-rel-url="docs/10.html" data-book-page-id="11501">1.9. 朴素贝叶斯</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/11.html" title="1.10. 决策树" data-book-page-rel-url="docs/11.html" data-book-page-id="11502">1.10. 决策树</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/12.html" title="1.11. 集成方法" data-book-page-rel-url="docs/12.html" data-book-page-id="11503">1.11. 集成方法</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/13.html" title="1.12. 多类和多标签算法" data-book-page-rel-url="docs/13.html" data-book-page-id="11504">1.12. 多类和多标签算法</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/14.html" title="1.13. 特征选择" data-book-page-rel-url="docs/14.html" data-book-page-id="11505">1.13. 特征选择</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/15.html" title="1.14. 半监督学习" data-book-page-rel-url="docs/15.html" data-book-page-id="11506">1.14. 半监督学习</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/16.html" title="1.15. 等式回归" data-book-page-rel-url="docs/16.html" data-book-page-id="11507">1.15. 等式回归</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/17.html" title="1.16. 概率校准" data-book-page-rel-url="docs/17.html" data-book-page-id="11508">1.16. 概率校准</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/18.html" title="1.17. 神经网络模型（有监督）" data-book-page-rel-url="docs/18.html" data-book-page-id="11509">1.17. 神经网络模型（有监督）</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/19.html" title="2. 无监督学习" data-book-page-rel-url="docs/19.html" data-book-page-id="11510">2. 无监督学习</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/20.html" title="2.1. 高斯混合模型" data-book-page-rel-url="docs/20.html" data-book-page-id="11511">2.1. 高斯混合模型</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/21.html" title="2.2. 流形学习" data-book-page-rel-url="docs/21.html" data-book-page-id="11512">2.2. 流形学习</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/22.html" title="2.3. 聚类" data-book-page-rel-url="docs/22.html" data-book-page-id="11513">2.3. 聚类</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/23.html" title="2.4. 双聚类" data-book-page-rel-url="docs/23.html" data-book-page-id="11514">2.4. 双聚类</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/24.html" title="2.5. 分解成分中的信号（矩阵分解问题）" data-book-page-rel-url="docs/24.html" data-book-page-id="11515">2.5. 分解成分中的信号（矩阵分解问题）</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/25.html" title="2.6. 协方差估计" data-book-page-rel-url="docs/25.html" data-book-page-id="11516">2.6. 协方差估计</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/26.html" title="2.7. 经验协方差" data-book-page-rel-url="docs/26.html" data-book-page-id="11517">2.7. 经验协方差</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/27.html" title="2.8. 收敛协方差" data-book-page-rel-url="docs/27.html" data-book-page-id="11518">2.8. 收敛协方差</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/28.html" title="2.9. 稀疏逆协方差" data-book-page-rel-url="docs/28.html" data-book-page-id="11519">2.9. 稀疏逆协方差</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/29.html" title="2.10. Robust 协方差估计" data-book-page-rel-url="docs/29.html" data-book-page-id="11520">2.10. Robust 协方差估计</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/30.html" title="2.11. 新奇和异常值检测" data-book-page-rel-url="docs/30.html" data-book-page-id="11521">2.11. 新奇和异常值检测</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/31.html" title="2.12. 密度估计" data-book-page-rel-url="docs/31.html" data-book-page-id="11522">2.12. 密度估计</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/32.html" title="2.13. 神经网络模型（无监督）" data-book-page-rel-url="docs/32.html" data-book-page-id="11523">2.13. 神经网络模型（无监督）</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/33.html" title="3. 模型选择和评估" data-book-page-rel-url="docs/33.html" data-book-page-id="11524">3. 模型选择和评估</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/34.html" title="3.1. 交叉验证：评估估算器的表现" data-book-page-rel-url="docs/34.html" data-book-page-id="11525">3.1. 交叉验证：评估估算器的表现</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/35.html" title="3.2. 调整估计器的超参数" data-book-page-rel-url="docs/35.html" data-book-page-id="11526">3.2. 调整估计器的超参数</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/53.html" title="3.3. 模型评估: 量化预测的质量" data-book-page-rel-url="docs/53.html" data-book-page-id="11527">3.3. 模型评估: 量化预测的质量</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/54.html" title="3.4. 模型持久化" data-book-page-rel-url="docs/54.html" data-book-page-id="11528">3.4. 模型持久化</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/55.html" title="3.5. 验证曲线: 绘制分数以评估模型" data-book-page-rel-url="docs/55.html" data-book-page-id="11529">3.5. 验证曲线: 绘制分数以评估模型</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/56.html" title="4. 数据集转换" data-book-page-rel-url="docs/56.html" data-book-page-id="11530">4. 数据集转换</a>
<ul>
<li>
<a class="pjax" href="../../../book/170/docs/57.html" title="4.1. Pipeline（管道）和 FeatureUnion（特征联合）: 合并的评估器" data-book-page-rel-url="docs/57.html" data-book-page-id="11531">4.1. Pipeline（管道）和 FeatureUnion（特征联合）: 合并的评估器</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/58.html" title="4.2. 特征提取" data-book-page-rel-url="docs/58.html" data-book-page-id="11532">4.2. 特征提取</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/59.html" title="4.3. 预处理数据" data-book-page-rel-url="docs/59.html" data-book-page-id="11533">4.3. 预处理数据</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/60.html" title="4.4. 无监督降维" data-book-page-rel-url="docs/60.html" data-book-page-id="11534">4.4. 无监督降维</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/61.html" title="4.5. 随机投影" data-book-page-rel-url="docs/61.html" data-book-page-id="11535">4.5. 随机投影</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/62.html" title="4.6. 内核近似" data-book-page-rel-url="docs/62.html" data-book-page-id="11536">4.6. 内核近似</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/63.html" title="4.7. 成对的矩阵, 类别和核函数" data-book-page-rel-url="docs/63.html" data-book-page-id="11537">4.7. 成对的矩阵, 类别和核函数</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/64.html" title="4.8. 预测目标 (`y`.html) 的转换" data-book-page-rel-url="docs/64.html" data-book-page-id="11538">4.8. 预测目标 (`y`.html) 的转换</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/65.html" title="5. 数据集加载工具" data-book-page-rel-url="docs/65.html" data-book-page-id="11539">5. 数据集加载工具</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/66.html" title="6. 大规模计算的策略: 更大量的数据" data-book-page-rel-url="docs/66.html" data-book-page-id="11540">6. 大规模计算的策略: 更大量的数据</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/67.html" title="7. 计算性能" data-book-page-rel-url="docs/67.html" data-book-page-id="11541">7. 计算性能</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/68.html" title="使用 scikit-learn 介绍机器学习" data-book-page-rel-url="docs/68.html" data-book-page-id="11542">使用 scikit-learn 介绍机器学习</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/69.html" title="关于科学数据处理的统计学习教程" data-book-page-rel-url="docs/69.html" data-book-page-id="11543">关于科学数据处理的统计学习教程</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/70.html" title="机器学习: scikit-learn 中的设置以及预估对象" data-book-page-rel-url="docs/70.html" data-book-page-id="11544">机器学习: scikit-learn 中的设置以及预估对象</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/71.html" title="监督学习：从高维观察预测输出变量" data-book-page-rel-url="docs/71.html" data-book-page-id="11545">监督学习：从高维观察预测输出变量</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/72.html" title="模型选择：选择估计量及其参数" data-book-page-rel-url="docs/72.html" data-book-page-id="11546">模型选择：选择估计量及其参数</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/73.html" title="无监督学习: 寻求数据表示" data-book-page-rel-url="docs/73.html" data-book-page-id="11547">无监督学习: 寻求数据表示</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/74.html" title="把它们放在一起" data-book-page-rel-url="docs/74.html" data-book-page-id="11548">把它们放在一起</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/75.html" title="寻求帮助" data-book-page-rel-url="docs/75.html" data-book-page-id="11549">寻求帮助</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/76.html" title="处理文本数据" data-book-page-rel-url="docs/76.html" data-book-page-id="11550">处理文本数据</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/77.html" title="选择正确的评估器(estimator.html)" data-book-page-rel-url="docs/77.html" data-book-page-id="11551">选择正确的评估器(estimator.html)</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/78.html" title="外部资源，视频和谈话" data-book-page-rel-url="docs/78.html" data-book-page-id="11552">外部资源，视频和谈话</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/80.html" title="常见问题" data-book-page-rel-url="docs/80.html" data-book-page-id="11553">常见问题</a>
</li>
<li>
<a class="pjax" href="../../../book/170/docs/81.html" title="时光轴" data-book-page-rel-url="docs/81.html" data-book-page-id="11554">时光轴</a>
</li>
</ul>
</div>
</div>
<script src="https://cdn.staticfile.net/jquery/1.12.4/jquery.min.js"></script>
<script type="text/javascript" src="../../../static/components/uikit-2.27.5/js/uikit.reader.js"></script>
<script type="text/javascript" src="../../../static/components/social-share/social-share.min.js"></script>
<script>(function(){var bp =document.createElement('script');var curProtocol =window.location.protocol.split(':')[0];if (curProtocol ==='https') {bp.src ='https://zz.bdstatic.com/linksubmit/push.js';}
else {bp.src ='http://push.zhanzhang.baidu.com/push.js';}
var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(bp,s);})();</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
<script src="https://cdn.staticfile.net/highlight.js/9.12.0/highlight.min.js"></script>
<script src="https://cdn.staticfile.net/jquery.pjax/2.0.1/jquery.pjax.min.js"></script>
<script src="https://cdn.staticfile.net/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="https://cdn.staticfile.net/uikit/2.27.5/js/components/lightbox.min.js"></script>
<link rel="dns-prefetch" href="../../..//cdn.mathjax.org" />
<script type="text/x-mathjax-config">
 function initMathJax() {
    var mathId = $("book-content-section")[0];
    MathJax.Hub.Config({
        tex2jax: {skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code','a']},
        showProcessingMessages: false,
        messageStyle: "none"
    });
    MathJax.Hub.Queue(["Typeset",MathJax.Hub,mathId]);
 };
initMathJax();
</script>
<script src='https://cdn.staticfile.net/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML' async></script>
<style>
	.MathJax_Display{display:inline!important;}
</style>
<script type="text/javascript" src="../../../static/components/js/reader.js"></script>
<script type="text/javascript">var bookId =170;var bookPageId =11515;var bookPageRelUrl ='docs/24.html';</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
</body>
</html>