
<!DOCTYPE html>
<html lang="zh-CN">
<head>
<title>HBase and Spark-HBase中文参考指南 3.0</title>
<meta content='HBase and Spark,HBase中文参考指南 3.0' name='keywords'>
<meta content='HBase and Spark,HBase中文参考指南 3.0' name='description'>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta http-equiv="Content-Language" content="zh-CN" />
<meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1, maximum-scale=1, user-scalable=no"../../../>
<meta name="applicable-device" content="pc,mobile">
<link rel="shortcut icon" href="../../../favicon.ico" type="image/x-icon" />
<meta name="renderer" content="webkit">
<link rel="stylesheet" href="../../../static/components/uikit-2.27.5/css/uikit.custom.css">
<link rel="stylesheet" href="../../../static/components/social-share/social-share.min.css">
<link rel="stylesheet" href="../../../static/components/highlight/styles/custom.css">
<link rel="stylesheet" href="../../../static/components/css/base.css">
<link rel="stylesheet" href="../../../static/components/css/reader.css">
<link rel="stylesheet" href="../../../static/components/css/markdown.css">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5313208362165053" crossorigin="anonymous"></script>
</head>
<body>
<div class=" book-main-wrap uk-container uk-container-center uk-margin-top ">
<div class="uk-grid">
<div class="uk-width-1-1 reader-wrap ">
<div class=" bottom-nav uk-clearfix ">
<div class="uk-align-left ">
<a href="../../../book/173/docs/16.html">
<i class="nav-icon-left uk-icon-small  uk-icon-caret-left"></i>
<span class="">Thrift API ..</span>
</a>
</div>
<div class="uk-align-right ">
<a href="../../../book/173/docs/18.html">
<span class="">Apache HBas..</span>
<i class="nav-icon-right uk-icon-small  uk-icon-caret-right"></i>
</a>
</div>
</div>
<div class="uk-text-center">
<h2 class="book-page-title uk-container-center">
<a href="../../../book/173/index.html">HBase中文参考指南 3.0</a>
<a target="_blank" rel="nofollow" href="https://github.com/apachecn/hbase-doc-zh" class="uk-icon-button uk-icon-github" title="github项目地址"></a>
</h2>
</div>
<script type="text/javascript" src="../../../static/components/js/app_intro.js"></script>
<ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-5313208362165053" data-ad-slot="1328047120"></ins>
<script>(adsbygoogle =window.adsbygoogle ||[]).push({});</script>
<hr class="uk-article-divider">
<div class="book-content-section  md-content-section  uk-margin-bottom">
<h1 id="hbase-与-spark">HBase 与 Spark</h1>
<blockquote>
<p>贡献者：<a href="https://github.com/TsingJyujing">TsingJyujing</a></p>
</blockquote>
<p><a href="https://spark.apache.org/">Apache Spark</a> 是一个分布式的、用于在内存中处理数据的软件框架，在许多场景中用于代替 MapReduce。</p>
<p>Spark 本身已经超出了本文档的范围，请参考 Spark 的项目及子项目的网站来获取更多信息。本文档将会集中在 4 个主要的 HBase 和 Spark 交互的要点上，这四点分别是：</p>
<p>基础 Spark</p>
<p>这可以在 Spark DAG 中的任意一点使用 HBase Connection。</p>
<p>Spark Streaming</p>
<p>这可以在 Spark Streaming 应用中的任意一点使用 HBase Connection。</p>
<p>Spark 批量加载</p>
<p>这可以允许在批量插入 HBase 的时候直接写 HBase 的 HFiles。</p>
<p>SparkSQL/DataFrames</p>
<p>这将提供为 HBase 中定义的表提供写 SparkSQL 的能力。</p>
<p>下面的部分将会用几个例子来说明上面几点交互。</p>
<h2 id="104-基础-spark">104. 基础 Spark</h2>
<p>这一部分将会在最底层和最简单的等级讨论 HBase 与 Spark 的整合。其他交互的要点都是基于这些操作构建的，我们会在这里完整描述。</p>
<p>一切 HBase 和 Spark 整合的基础都是 HBaseContext，HBaseContext 接受 HBase 配置并且会将其推送到 Spark 执行器（executor）中。这允许我们在每个 Spark 执行器（executor）中有一个静态的 HBase 连接。</p>
<p>作为参考，Spark 执行器（executor）既可以和 Region Server 在同一个节点，也可以在不同的节点，他们不存在共存的依赖关系。</p>
<p>可以认为每个 Spark 执行器（executor）都是一个多线程的客户端程序，这允许运行在不同的执行器上的 Spark 任务访问共享的连接对象。</p>
<p>例 31. HBaseContext 使用例程</p>
<p>这个例子展现了如何使用 Scala 语言在 RDD 的<code>foreachPartition</code>方法中使用 HBaseContext。</p>
<pre><code>val sc = new SparkContext("local", "test")
val config = new HBaseConfiguration()

...

val hbaseContext = new HBaseContext(sc, config)

rdd.hbaseForeachPartition(hbaseContext, (it, conn) =&gt; {
 val bufferedMutator = conn.getBufferedMutator(TableName.valueOf("t1"))
 it.foreach((putRecord) =&gt; {
. val put = new Put(putRecord._1)
. putRecord._2.foreach((putValue) =&gt; put.addColumn(putValue._1, putValue._2, putValue._3))
. bufferedMutator.mutate(put)
 })
 bufferedMutator.flush()
 bufferedMutator.close()
})
</code></pre>
<p>这里是使用 Java 编写的同样的例子。</p>
<pre><code>JavaSparkContext jsc = new JavaSparkContext(sparkConf);

try {
  List&lt;byte[]&gt; list = new ArrayList&lt;&gt;();
  list.add(Bytes.toBytes("1"));
  ...
  list.add(Bytes.toBytes("5"));

  JavaRDD&lt;byte[]&gt; rdd = jsc.parallelize(list);
  Configuration conf = HBaseConfiguration.create();

  JavaHBaseContext hbaseContext = new JavaHBaseContext(jsc, conf);

  hbaseContext.foreachPartition(rdd,
      new VoidFunction&lt;Tuple2&lt;Iterator&lt;byte[]&gt;, Connection&gt;&gt;() {
   public void call(Tuple2&lt;Iterator&lt;byte[]&gt;, Connection&gt; t)
        throws Exception {
    Table table = t._2().getTable(TableName.valueOf(tableName));
    BufferedMutator mutator = t._2().getBufferedMutator(TableName.valueOf(tableName));
    while (t._1().hasNext()) {
      byte[] b = t._1().next();
      Result r = table.get(new Get(b));
      if (r.getExists()) {
       mutator.mutate(new Put(b));
      }
    }

    mutator.flush();
    mutator.close();
    table.close();
   }
  });
} finally {
  jsc.stop();
}
</code></pre>
<p>所有的函数式都同时在 Spark 和 HBase 中，并且都支持用 Scala 或者 Java 开发。除了 SparkSQL 以外，所有 Spark 支持的语言在这里也都支持。 目前在余下的文档中，我们将会重点关注 Scala 的例程。</p>
<p>上面的例程阐释了如何在 foreachPartition 操作中使用连接。除此之外，许多 Spark 的基础函数都是支持的：</p>
<p><code>bulkPut</code></p>
<p>并行的写入大量数据到 HBase</p>
<p><code>bulkDelete</code></p>
<p>并行的删除 HBase 中大量数据</p>
<p><code>bulkGet</code></p>
<p>并行的从 HBase 中获取大量的数据，并且创建一个新的 RDD</p>
<p><code>mapPartition</code></p>
<p>在 Spark 的 Map 函数中使用连接对象，并且允许使用完整的 HBase 访问</p>
<p><code>hBaseRDD</code></p>
<p>简单的创建一个用于分布式扫描数据的 RDD</p>
<p>想要参看所有机能的例程，参见 HBase-Spark 模块。</p>
<h2 id="105-spark-streaming">105. Spark Streaming</h2>
<p><a href="https://spark.apache.org/streaming/">Spark Streaming</a> 是一个基于 Spark 构建的微批流处理框架。 HBase 和 Spark Streaming 的良好配合使得 HBase 可以提供一下益处：</p>
<ul>
<li> <p>可以动态的获取参考或者描述性数据</p> </li>
<li> <p>基于 Spark Streaming 提供的恰好一次处理，可以存储计数或者聚合结果</p> </li>
</ul>
<p>HBase-Spark 模块整合的和 Spark Streaming 的相关的点与 Spark 整合的点非常相似， 以下的指令可以在 Spark Streaming DStream 中立刻使用：</p>
<p><code>bulkPut</code></p>
<p>并行的写入大量数据到 HBase</p>
<p><code>bulkDelete</code></p>
<p>并行的删除 HBase 中大量数据</p>
<p><code>bulkGet</code></p>
<p>并行的从 HBase 中获取大量的数据，并且创建一个新的 RDD</p>
<p><code>mapPartition</code></p>
<p>在 Spark 的 Map 函数中使用连接对象，并且允许使用完整的 HBase 访问</p>
<p><code>hBaseRDD</code></p>
<p>简单的创建一个用于分布式扫描数据的 RDD。</p>
<p>例 32. <code>bulkPut</code>在 DStreams 中使用的例程</p>
<p>以下是 bulkPut 在 DStreams 中的使用例程，感觉上与 RDD 批量插入非常接近。</p>
<pre><code>val sc = new SparkContext("local", "test")
val config = new HBaseConfiguration()

val hbaseContext = new HBaseContext(sc, config)
val ssc = new StreamingContext(sc, Milliseconds(200))

val rdd1 = ...
val rdd2 = ...

val queue = mutable.Queue[RDD[(Array[Byte], Array[(Array[Byte],
    Array[Byte], Array[Byte])])]]()

queue += rdd1
queue += rdd2

val dStream = ssc.queueStream(queue)

dStream.hbaseBulkPut(
  hbaseContext,
  TableName.valueOf(tableName),
  (putRecord) =&gt; {
   val put = new Put(putRecord._1)
   putRecord._2.foreach((putValue) =&gt; put.addColumn(putValue._1, putValue._2, putValue._3))
   put
  })
</code></pre>
<p>这里到<code>hbaseBulkPut</code>函数有三个输入，hbaseContext 携带了配置广播信息，来帮助我们连接到执行器中的 HBase Connections。 表名用于指明我们要往哪个表放数据。一个函数将 DStream 中的记录转换为 HBase Put 对象。</p>
<h2 id="106-批量加载">106. 批量加载</h2>
<p>使用 Spark 加载大量的数据到 HBase 有两个选项。 基本的大量数据加载功能适用于你的行有数百万列数据，以及在 Spark 批量加载之前的 Map 操作列没有合并和分组的情况。</p>
<p>Spark 中还有一个轻量批量加载选项，这个第二选项设计给每一行少于一万的情况。 第二个选项的优势在于更高的吞吐量，以及 Spark 的 shuffle 操作中更轻的负载。</p>
<p>两种实现都或多或少的类似 MapReduce 批量加载过程， 因为分区器基于 Region 划分对行键进行分区。并且行键被顺序的发送到 Reducer 所以 HFile 可以在 reduce 阶段被直接写出。</p>
<p>依照 Spark 的术语来说，批量加载将会基于<code>repartitionAndSortWithinPartitions</code>实现，并且之后是 Spark 的<code>foreachPartition</code>。</p>
<p>让我们首先看一下使用批量加载功能的例子</p>
<p>例 33. 批量加载例程</p>
<p>下面的例子展现了 Spark 中的批量加载。</p>
<pre><code>val sc = new SparkContext("local", "test")
val config = new HBaseConfiguration()

val hbaseContext = new HBaseContext(sc, config)

val stagingFolder = ...
val rdd = sc.parallelize(Array(
      (Bytes.toBytes("1"),
        (Bytes.toBytes(columnFamily1), Bytes.toBytes("a"), Bytes.toBytes("foo1"))),
      (Bytes.toBytes("3"),
        (Bytes.toBytes(columnFamily1), Bytes.toBytes("b"), Bytes.toBytes("foo2.b"))), ...

rdd.hbaseBulkLoad(TableName.valueOf(tableName),
  t =&gt; {
   val rowKey = t._1
   val family:Array[Byte] = t._2(0)._1
   val qualifier = t._2(0)._2
   val value = t._2(0)._3

   val keyFamilyQualifier= new KeyFamilyQualifier(rowKey, family, qualifier)

   Seq((keyFamilyQualifier, value)).iterator
  },
  stagingFolder.getPath)

val load = new LoadIncrementalHFiles(config)
load.doBulkLoad(new Path(stagingFolder.getPath),
  conn.getAdmin, table, conn.getRegionLocator(TableName.valueOf(tableName)))
</code></pre>
<p><code>hbaseBulkLoad</code> 函数需要三个必要参数：</p>
<ol>
<li> <p>我们需要从之加载数据的表名</p> </li>
<li> <p>一个函数用于将 RDD 中的某个记录转化为一个元组形式的键值对。 其中键值是一个 KeyFamilyQualifer 对象，值是 cell value。 KeyFamilyQualifer 将会保存行键，列族和列标识位。 针对行键的随机操作会根据这三个值来排序。</p> </li>
<li> <p>写出 HFile 的临时路径</p> </li>
</ol>
<p>接下来的 Spark 批量加载指令，使用 HBase 的 LoadIncrementalHFiles 对象来加载 HBase 中新创建的 HFiles。</p>
<p>使用 Spark 批量加载的附加参数</p>
<p>你可以在 hbaseBulkLoad 中用附加参数设置以下属性：</p>
<ul>
<li>HFile 的最大文件大小</li>
<li>从压缩中排除 HFile 的标志</li>
<li>列族设置，包含 compression（压缩）, bloomType（布隆（过滤器）类型）, blockSize（块大小）, and dataBlockEncoding（数据块编码）</li>
</ul>
<p>例 34. 使用附加参数</p>
<pre><code>val sc = new SparkContext("local", "test")
val config = new HBaseConfiguration()

val hbaseContext = new HBaseContext(sc, config)

val stagingFolder = ...
val rdd = sc.parallelize(Array(
      (Bytes.toBytes("1"),
        (Bytes.toBytes(columnFamily1), Bytes.toBytes("a"), Bytes.toBytes("foo1"))),
      (Bytes.toBytes("3"),
        (Bytes.toBytes(columnFamily1), Bytes.toBytes("b"), Bytes.toBytes("foo2.b"))), ...

val familyHBaseWriterOptions = new java.util.HashMap[Array[Byte], FamilyHFileWriteOptions]
val f1Options = new FamilyHFileWriteOptions("GZ", "ROW", 128, "PREFIX")

familyHBaseWriterOptions.put(Bytes.toBytes("columnFamily1"), f1Options)

rdd.hbaseBulkLoad(TableName.valueOf(tableName),
  t =&gt; {
   val rowKey = t._1
   val family:Array[Byte] = t._2(0)._1
   val qualifier = t._2(0)._2
   val value = t._2(0)._3

   val keyFamilyQualifier= new KeyFamilyQualifier(rowKey, family, qualifier)

   Seq((keyFamilyQualifier, value)).iterator
  },
  stagingFolder.getPath,
  familyHBaseWriterOptions,
  compactionExclude = false,
  HConstants.DEFAULT_MAX_FILE_SIZE)

val load = new LoadIncrementalHFiles(config)
load.doBulkLoad(new Path(stagingFolder.getPath),
  conn.getAdmin, table, conn.getRegionLocator(TableName.valueOf(tableName)))
</code></pre>
<p>现在让我们来看一下如何调用轻量化对象批量加载的实现：</p>
<p>例 35. 使用轻量批量加载</p>
<pre><code>val sc = new SparkContext("local", "test")
val config = new HBaseConfiguration()

val hbaseContext = new HBaseContext(sc, config)

val stagingFolder = ...
val rdd = sc.parallelize(Array(
      ("1",
        (Bytes.toBytes(columnFamily1), Bytes.toBytes("a"), Bytes.toBytes("foo1"))),
      ("3",
        (Bytes.toBytes(columnFamily1), Bytes.toBytes("b"), Bytes.toBytes("foo2.b"))), ...

rdd.hbaseBulkLoadThinRows(hbaseContext,
      TableName.valueOf(tableName),
      t =&gt; {
        val rowKey = t._1

        val familyQualifiersValues = new FamiliesQualifiersValues
        t._2.foreach(f =&gt; {
          val family:Array[Byte] = f._1
          val qualifier = f._2
          val value:Array[Byte] = f._3

          familyQualifiersValues +=(family, qualifier, value)
        })
        (new ByteArrayWrapper(Bytes.toBytes(rowKey)), familyQualifiersValues)
      },
      stagingFolder.getPath,
      new java.util.HashMap[Array[Byte], FamilyHFileWriteOptions],
      compactionExclude = false,
      20)

val load = new LoadIncrementalHFiles(config)
load.doBulkLoad(new Path(stagingFolder.getPath),
  conn.getAdmin, table, conn.getRegionLocator(TableName.valueOf(tableName)))
</code></pre>
<p>注意在使用轻量行批量加载的时候函数返回的元组中： 第一个元素是行键，第二个元素是 FamiliesQualifiersValues，这个对象中含有这一行里所有的数值，并且包含了所有的列族。</p>
<h2 id="107-sparksqldataframes">107. SparkSQL/DataFrames</h2>
<p>（HBase-Spark 中的）HBase-Spark 连接器 提供了： <a href="https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html">DataSource API</a> (<a href="https://issues.apache.org/jira/browse/SPARK-3247">SPARK-3247</a>) 在 Spark 1.2.0 的时候被引入，连接了简单的 HBase 的键值存储与复杂的关系型 SQL 查询，并且使得用户可以使用 Spark 在 HBase 上施展复杂的数据分析工作。 HBase Dataframe 是 Spark Dataframe 的一个标准，并且它允许和其他任何数据源——例如 Hive, Orc, Parquet, JSON 之类。 HBase-Spark Connector 使用的关键技术例如分区修剪，列修剪，推断后置以及数据本地化。</p>
<p>为了使用 HBase-Spark connector，用户需要定义 HBase 到 Spark 表中的映射目录。 准备数据并且填充 HBase 的表，然后将其加载到 HBase DataFrame 中去。 在此之后，用户可以使用 SQL 查询语句整合查询与数据获取。 接下来的例子说明了最基本的过程</p>
<h3 id="1071-定义目录">107.1. 定义目录</h3>
<pre><code>def catalog = s"""{
       |"table":{"namespace":"default", "name":"table1"},
       |"rowkey":"key",
       |"columns":{
         |"col0":{"cf":"rowkey", "col":"key", "type":"string"},
         |"col1":{"cf":"cf1", "col":"col1", "type":"boolean"},
         |"col2":{"cf":"cf2", "col":"col2", "type":"double"},
         |"col3":{"cf":"cf3", "col":"col3", "type":"float"},
         |"col4":{"cf":"cf4", "col":"col4", "type":"int"},
         |"col5":{"cf":"cf5", "col":"col5", "type":"bigint"},
         |"col6":{"cf":"cf6", "col":"col6", "type":"smallint"},
         |"col7":{"cf":"cf7", "col":"col7", "type":"string"},
         |"col8":{"cf":"cf8", "col":"col8", "type":"tinyint"}
       |}
     |}""".stripMargin
</code></pre>
<p>目录定义了从 HBase 到 Spark 表的一个映射。 这个目录中有两个关键部分。 第一个是行键的定义，另一个是将 Spark 表中的列映射到 HBase 的列族和列标识位。 上面的 schema 定义了一个 HBase 中的表，名为 Table1，行键作为键与一些列(col1 <code>-</code> col8)。 注意行键也需要被定义为一个列（col0），该列具有特定的列族（rowkey）。</p>
<h3 id="1072-保存-dataframe">107.2. 保存 DataFrame</h3>
<pre><code>case class HBaseRecord(
   col0: String,
   col1: Boolean,
   col2: Double,
   col3: Float,
   col4: Int,
   col5: Long,
   col6: Short,
   col7: String,
   col8: Byte)

object HBaseRecord
{
   def apply(i: Int, t: String): HBaseRecord = {
      val s = s"""row${"%03d".format(i)}"""
      HBaseRecord(s,
      i % 2 == 0,
      i.toDouble,
      i.toFloat,
      i,
      i.toLong,
      i.toShort,
      s"String$i: $t",
      i.toByte)
  }
}

val data = (0 to 255).map { i =&gt;  HBaseRecord(i, "extra")}

sc.parallelize(data).toDF.write.options(
 Map(HBaseTableCatalog.tableCatalog -&gt; catalog, HBaseTableCatalog.newTable -&gt; "5"))
 .format("org.apache.hadoop.hbase.spark ")
 .save()
</code></pre>
<p>用户准备的数据（<code>data</code>）是一个本地的 Scala 集合，含有 256 个 HBaseRecord 对象。 <code>sc.parallelize(data)</code> 函数分发了从 RDD 中来的数据。<code>toDF</code>返回了一个 DataFrame。 <code>write</code> 函数返回了一个 DataFrameWriter 来将 DataFrame 中的数据到外部存储（例如这里是 HBase）。 <code>save</code> 函数将会创建一个具有 5 个 Region 的 HBase 表来在内部保存 DataFrame。</p>
<h3 id="1073-加载-dataframe">107.3. 加载 DataFrame</h3>
<pre><code>def withCatalog(cat: String): DataFrame = {
  sqlContext
  .read
  .options(Map(HBaseTableCatalog.tableCatalog-&gt;cat))
  .format("org.apache.hadoop.hbase.spark")
  .load()
}
val df = withCatalog(catalog)
</code></pre>
<p>在 withCatalog 函数中，sqlContext 是一个 SQLContext 的变量，是一个用于与 Spark 中结构化（行与列）的数据一起工作的一个入口点。 <code>read</code> 返回一个 DataFrameReader，他可以用于从 DataFrame 中读取数据。<code>option</code>函数为输出到 DataFrameReader 的底层的数据源增加了输入选项。 以及，<code>format</code>函数表示了 DataFrameReader 的输入数据源的格式。 <code>load()</code> 函数将其加载为一个 DataFrame， 数据帧 <code>df</code>将由<code>withCatalog</code>函数返回，用于访问 HBase 表，例如 4.4 与 4.5.</p>
<h3 id="1074-language-integrated-query">107.4. Language Integrated Query</h3>
<pre><code>val s = df.filter(($"col0" &lt;= "row050" &amp;&amp; $"col0" &gt; "row040") ||
  $"col0" === "row005" ||
  $"col0" &lt;= "row005")
  .select("col0", "col1", "col4")
s.show
</code></pre>
<p>DataFrame 可以做很多操作，例如 join, sort, select, filter, orderBy 等等等等。<code>df.filter</code> 通过指定的 SQL 表达式提供过滤器，<code>select</code>选择一系列的列：<code>col0</code>, <code>col1</code> 和 <code>col4</code>。</p>
<h3 id="1075-sql-查询">107.5. SQL 查询</h3>
<pre><code>df.registerTempTable("table1")
sqlContext.sql("select count(col1) from table1").show
</code></pre>
<p><code>registerTempTable</code> 注册了一个名为 <code>df</code> 的 DataFrame 作为临时表，表名为<code>table1</code>，临时表的生命周期和 SQLContext 有关，用于创建<code>df</code>。 <code>sqlContext.sql</code>函数允许用户执行 SQL 查询。</p>
<h3 id="1076-others">107.6. Others</h3>
<p>例 36. 查询不同的时间戳</p>
<p>在 HBaseSparkConf 中，可以设置 4 个和时间戳有关的参数，它们分别表示为 TIMESTAMP, MIN_TIMESTAMP, MAX_TIMESTAMP 和 MAX_VERSIONS。用户可以使用不同的时间戳或者利用 MIN_TIMESTAMP 和 MAX_TIMESTAMP 查询时间范围内的记录。同时，下面的例子使用了具体的数值取代了 tsSpecified 和 oldMs。</p>
<p>下例展示了如何使用不同的时间戳加载 df DataFrame。tsSpecified 由用户定义，HBaseTableCatalog 定义了 HBase 和 Relation 关系的 schema。writeCatalog 定义了 schema 映射的目录。</p>
<pre><code>val df = sqlContext.read
      .options(Map(HBaseTableCatalog.tableCatalog -&gt; writeCatalog, HBaseSparkConf.TIMESTAMP -&gt; tsSpecified.toString))
      .format("org.apache.hadoop.hbase.spark")
      .load()
</code></pre>
<p>下例展示了如何使用不同的时间范围加载 df DataFrame。oldMs 由用户定义。</p>
<pre><code>val df = sqlContext.read
      .options(Map(HBaseTableCatalog.tableCatalog -&gt; writeCatalog, HBaseSparkConf.MIN_TIMESTAMP -&gt; "0",
        HBaseSparkConf.MAX_TIMESTAMP -&gt; oldMs.toString))
      .format("org.apache.hadoop.hbase.spark")
      .load()
</code></pre>
<p>在加载 DataFrame 之后，用户就可以查询数据。</p>
<pre><code>df.registerTempTable("table")
sqlContext.sql("select count(col1) from table").show
</code></pre>
<p>例 37. 原生 Avro 支持</p>
<p>Example 37. Native Avro support</p>
<p>HBase-Spark Connector 支持不同类型的数据格式例如 Avro, Jason 等等。下面的用例展示了 Spark 如何支持 Avro。用户可以将 Avro 记录直接持久化进 HBase。 在内部，Avro schema 自动的转换为原生的 Spark Catalyst 数据类型。 注意，HBase 表中无论是键或者值的部分都可以在 Avro 格式定义。</p>
<ol>
<li>为 schema 映射定义目录：</li>
</ol>
<pre><code>def catalog = s"""{
                     |"table":{"namespace":"default", "name":"Avrotable"},
                      |"rowkey":"key",
                      |"columns":{
                      |"col0":{"cf":"rowkey", "col":"key", "type":"string"},
                      |"col1":{"cf":"cf1", "col":"col1", "type":"binary"}
                      |}
                      |}""".stripMargin
</code></pre>
<p><code>catalog</code>是一个 HBase 表的 schema，命名为 <code>Avrotable</code>。行键作为键，并且有一个列 col1。行键也被定义为详细的一列（col0），并且指定列族（rowkey）。</p>
<ol start="2">
<li>准备数据：</li>
</ol>
<pre><code> object AvroHBaseRecord {
   val schemaString =
     s"""{"namespace": "example.avro",
         |   "type": "record",      "name": "User",
         |    "fields": [
         |        {"name": "name", "type": "string"},
         |        {"name": "favorite_number",  "type": ["int", "null"]},
         |        {"name": "favorite_color", "type": ["string", "null"]},
         |        {"name": "favorite_array", "type": {"type": "array", "items": "string"}},
         |        {"name": "favorite_map", "type": {"type": "map", "values": "int"}}
         |      ]    }""".stripMargin

   val avroSchema: Schema = {
     val p = new Schema.Parser
     p.parse(schemaString)
   }

   def apply(i: Int): AvroHBaseRecord = {
     val user = new GenericData.Record(avroSchema);
     user.put("name", s"name${"%03d".format(i)}")
     user.put("favorite_number", i)
     user.put("favorite_color", s"color${"%03d".format(i)}")
     val favoriteArray = new GenericData.Array[String](2, avroSchema.getField("favorite_array").schema())
     favoriteArray.add(s"number${i}")
     favoriteArray.add(s"number${i+1}")
     user.put("favorite_array", favoriteArray)
     import collection.JavaConverters._
     val favoriteMap = Map[String, Int](("key1" -&gt; i), ("key2" -&gt; (i+1))).asJava
     user.put("favorite_map", favoriteMap)
     val avroByte = AvroSedes.serialize(user, avroSchema)
     AvroHBaseRecord(s"name${"%03d".format(i)}", avroByte)
   }
 }

 val data = (0 to 255).map { i =&gt;
    AvroHBaseRecord(i)
 }
</code></pre>
<p>首先定义 <code>schemaString</code>，然后它被解析来获取<code>avroSchema</code>，<code>avroSchema</code>是用来生成 <code>AvroHBaseRecord</code>的。<code>data</code> 由用户准备，是一个有 256 个<code>AvroHBaseRecord</code>对象的原生 Scala 集合。</p>
<ol start="3">
<li>保存 DataFrame:</li>
</ol>
<pre><code> sc.parallelize(data).toDF.write.options(
     Map(HBaseTableCatalog.tableCatalog -&gt; catalog, HBaseTableCatalog.newTable -&gt; "5"))
     .format("org.apache.spark.sql.execution.datasources.hbase")
     .save()
</code></pre>
<p>对于由 schema <code>catalog</code>提供的已有的数据帧，上述语句将会创建一个具有 5 个分区的 HBase 表，并且将数据存进去。</p>
<ol start="4">
<li>加载 DataFrame</li>
</ol>
<pre><code>def avroCatalog = s"""{
            |"table":{"namespace":"default", "name":"avrotable"},
            |"rowkey":"key",
            |"columns":{
              |"col0":{"cf":"rowkey", "col":"key", "type":"string"},
              |"col1":{"cf":"cf1", "col":"col1", "avro":"avroSchema"}
            |}
          |}""".stripMargin

 def withCatalog(cat: String): DataFrame = {
     sqlContext
         .read
         .options(Map("avroSchema" -&gt; AvroHBaseRecord.schemaString, HBaseTableCatalog.tableCatalog -&gt; avroCatalog))
         .format("org.apache.spark.sql.execution.datasources.hbase")
         .load()
 }
 val df = withCatalog(catalog)
</code></pre>
<p>在 <code>withCatalog</code> 函数中，<code>read</code> 会返回一个可以将数据读取成 DataFrame 格式的 DataFrameReader。 <code>option</code> 函数追加输入选项来指定 DataFrameReader 使用的底层数据源。这里有两个选项，一个是设置<code>avroSchema</code>为<code>AvroHBaseRecord.schemaString</code>，另一个是设置<code>HBaseTableCatalog.tableCatalog</code> 为 <code>avroCatalog</code>。<code>load()</code> 函数加载所有的数据为 DataFrame。数据帧 <code>df</code> 由<code>withCatalog</code> 函数返回，可用于访问 HBase 表中的数据。</p>
<ol start="5">
<li>SQL 查询</li>
</ol>
<pre><code> df.registerTempTable("avrotable")
 val c = sqlContext.sql("select count(1) from avrotable").
</code></pre>
<p>在加载 df DataFrame 之后，用户可以查询数据。registerTempTable 将 df DataFrame 注册为一个临时表，表名为 avrotable。 <code>sqlContext.sql</code>函数允许用户执行 SQL 查询。</p>
</div>
<hr class="uk-article-divider">
<div class="uk-block uk-block-muted uk-padding-top-remove uk-padding-bottom-remove uk-margin-large-top  book-recommend-wrap">
<div class="uk-margin-top uk-margin-bottom uk-margin-left uk-margin-right">
<div class="uk-margin uk-text-muted "><i class="uk-icon-outdent uk-icon-justify uk-margin-small-right"></i>书籍推荐</div>
<div class="books">
<ul class="uk-book-list">
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/205/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/code_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/205/index.html">从0到1实战微服务架构</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/115.html">skyline75489</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="code">code</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="java">java</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="mysql">mysql</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="netty">netty</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="nginx">nginx</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">62页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2021年10月24日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 1个">1</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/152/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/code_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/152/index.html">简明Excel VBA</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/84.html">Youchien</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="code">code</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="visualstudio">visualstudio</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">53页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年3月24日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 10个">10</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/159/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/code_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/159/index.html">im-service 简介</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/91.html">yu000hong</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="code">code</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">37页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月26日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 0个">0</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/200/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/code_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/200/index.html">软件开发基础知识宝典</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/112.html">frank-lam</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="code">code</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">20页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2021年10月24日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 0个">0</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/159/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/code_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/159/index.html">im-service 简介</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/91.html">yu000hong</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="code">code</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">37页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月26日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 0个">0</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/176/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/javascript_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/176/index.html">30秒学会常用JavaScript代码</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/98.html">30 seconds</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="javascript">javascript</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">701页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月26日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 44975个">44975</span>
</div>
</div>
</div>
</li>
<hr>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
<nav class="tm-navbar uk-navbar uk-navbar-attached reader-nav">
<div class="uk-float-left uk-margin-small-top">
<a href="javascript:;" title="目录菜单" class="show-menu  uk-icon-hover  uk-icon-align-justify uk-margin-right"></a>
<div data-uk-dropdown="{mode:'click',pos:'bottom-left'}" class="font-setting-wrap">
<a class="uk-icon-hover uk-icon-font uk-margin-right" aria-label="字体设置" href="javascript:;"></a>
<div class="uk-dropdown dropdown-menu">
<div class="dropdown-caret"><span class="caret-outer"></span><span class="caret-inner"></span></div>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-reduce">小字</button>
<button class="uk-button-link button size-2 font-enlarge">大字</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-1 ">宋体</button>
<button class="uk-button-link button size-2 font-2 ">黑体</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-3 color-theme-sun "><i class="uk-icon-sun-o"></i>白天</button>
<button class="uk-button-link button size-3 color-theme-eye "><i class="uk-icon-eye"></i>护眼</button>
<button class="uk-button-link button size-3 color-theme-moon "><i class="uk-icon-moon-o"></i>夜晚</button></div>
</div>
</div>
<a class="logo uk-margin-right" href="../../../" title="返回首页"><img class="" src="../../../static/components/images/icon_32.png" /></a>
</div>
<div class="uk-navbar-flip  uk-hidden-small">
<div id="share-box"></div>
</div>
</nav>
<div id="menu-id" class="uk-offcanvas reader-offcanvas">
<div class="uk-offcanvas-bar">
<ul class="book-menu-bar uk-nav uk-nav-offcanvas" data-uk-nav>
<li>
<a href="../../../book/173/index.html" data-book-page-rel-url="index.html" data-book-page-id="0" title="封面">封面</a>
</li>
<li>
<a class="pjax" href="../../../book/173/readme.html" data-book-page-rel-url="readme.html" data-book-page-id="0" title="简介">简介</a>
</li>
<li>
<a class="pjax" href="../../../book/173/README.html" title="HBase™ 中文参考指南 3.0" data-book-page-rel-url="README.html" data-book-page-id="11767">HBase™ 中文参考指南 3.0</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/0.html" title="Preface" data-book-page-rel-url="docs/0.html" data-book-page-id="11768">Preface</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/1.html" title="Getting Started" data-book-page-rel-url="docs/1.html" data-book-page-id="11769">Getting Started</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/2.html" title="Apache HBase Configuration" data-book-page-rel-url="docs/2.html" data-book-page-id="11770">Apache HBase Configuration</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/3.html" title="Upgrading" data-book-page-rel-url="docs/3.html" data-book-page-id="11771">Upgrading</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/4.html" title="The Apache HBase Shell" data-book-page-rel-url="docs/4.html" data-book-page-id="11772">The Apache HBase Shell</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/5.html" title="Data Model" data-book-page-rel-url="docs/5.html" data-book-page-id="11773">Data Model</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/6.html" title="HBase and Schema Design" data-book-page-rel-url="docs/6.html" data-book-page-id="11774">HBase and Schema Design</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/7.html" title="RegionServer Sizing Rules of Thumb" data-book-page-rel-url="docs/7.html" data-book-page-id="11775">RegionServer Sizing Rules of Thumb</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/8.html" title="HBase and MapReduce" data-book-page-rel-url="docs/8.html" data-book-page-id="11776">HBase and MapReduce</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/9.html" title="Securing Apache HBase" data-book-page-rel-url="docs/9.html" data-book-page-id="11777">Securing Apache HBase</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/10.html" title="Architecture" data-book-page-rel-url="docs/10.html" data-book-page-id="11778">Architecture</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/11.html" title="In-memory Compaction" data-book-page-rel-url="docs/11.html" data-book-page-id="11779">In-memory Compaction</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/12.html" title="Backup and Restore" data-book-page-rel-url="docs/12.html" data-book-page-id="11780">Backup and Restore</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/13.html" title="Synchronous Replication" data-book-page-rel-url="docs/13.html" data-book-page-id="11781">Synchronous Replication</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/14.html" title="Apache HBase APIs" data-book-page-rel-url="docs/14.html" data-book-page-id="11782">Apache HBase APIs</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/15.html" title="Apache HBase External APIs" data-book-page-rel-url="docs/15.html" data-book-page-id="11783">Apache HBase External APIs</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/16.html" title="Thrift API and Filter Language" data-book-page-rel-url="docs/16.html" data-book-page-id="11784">Thrift API and Filter Language</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/17.html" title="HBase and Spark" data-book-page-rel-url="docs/17.html" data-book-page-id="11785">HBase and Spark</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/18.html" title="Apache HBase Coprocessors" data-book-page-rel-url="docs/18.html" data-book-page-id="11786">Apache HBase Coprocessors</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/19.html" title="Apache HBase Performance Tuning" data-book-page-rel-url="docs/19.html" data-book-page-id="11787">Apache HBase Performance Tuning</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/20.html" title="Troubleshooting and Debugging Apache HBase" data-book-page-rel-url="docs/20.html" data-book-page-id="11788">Troubleshooting and Debugging Apache HBase</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/21.html" title="Apache HBase Case Studies" data-book-page-rel-url="docs/21.html" data-book-page-id="11789">Apache HBase Case Studies</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/22.html" title="Apache HBase Operational Management" data-book-page-rel-url="docs/22.html" data-book-page-id="11790">Apache HBase Operational Management</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/23.html" title="Building and Developing Apache HBase" data-book-page-rel-url="docs/23.html" data-book-page-id="11791">Building and Developing Apache HBase</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/24.html" title="Unit Testing HBase Applications" data-book-page-rel-url="docs/24.html" data-book-page-id="11792">Unit Testing HBase Applications</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/25.html" title="Protobuf in HBase" data-book-page-rel-url="docs/25.html" data-book-page-id="11793">Protobuf in HBase</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/26.html" title="Procedure Framework (Pv2): HBASE-12439" data-book-page-rel-url="docs/26.html" data-book-page-id="11794">Procedure Framework (Pv2): HBASE-12439</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/27.html" title="AMv2 Description for Devs" data-book-page-rel-url="docs/27.html" data-book-page-id="11795">AMv2 Description for Devs</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/28.html" title="ZooKeeper" data-book-page-rel-url="docs/28.html" data-book-page-id="11796">ZooKeeper</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/29.html" title="Community" data-book-page-rel-url="docs/29.html" data-book-page-id="11797">Community</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/30.html" title="Appendix" data-book-page-rel-url="docs/30.html" data-book-page-id="11798">Appendix</a>
</li>
</ul>
</div>
</div>
<script src="https://cdn.staticfile.net/jquery/1.12.4/jquery.min.js"></script>
<script type="text/javascript" src="../../../static/components/uikit-2.27.5/js/uikit.reader.js"></script>
<script type="text/javascript" src="../../../static/components/social-share/social-share.min.js"></script>
<script>(function(){var bp =document.createElement('script');var curProtocol =window.location.protocol.split(':')[0];if (curProtocol ==='https') {bp.src ='https://zz.bdstatic.com/linksubmit/push.js';}
else {bp.src ='http://push.zhanzhang.baidu.com/push.js';}
var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(bp,s);})();</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
<script src="https://cdn.staticfile.net/highlight.js/9.12.0/highlight.min.js"></script>
<script src="https://cdn.staticfile.net/jquery.pjax/2.0.1/jquery.pjax.min.js"></script>
<script src="https://cdn.staticfile.net/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="https://cdn.staticfile.net/uikit/2.27.5/js/components/lightbox.min.js"></script>
<link rel="dns-prefetch" href="../../..//cdn.mathjax.org" />
<script type="text/x-mathjax-config">
 function initMathJax() {
    var mathId = $("book-content-section")[0];
    MathJax.Hub.Config({
        tex2jax: {skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code','a']},
        showProcessingMessages: false,
        messageStyle: "none"
    });
    MathJax.Hub.Queue(["Typeset",MathJax.Hub,mathId]);
 };
initMathJax();
</script>
<script src='https://cdn.staticfile.net/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML' async></script>
<style>
	.MathJax_Display{display:inline!important;}
</style>
<script type="text/javascript" src="../../../static/components/js/reader.js"></script>
<script type="text/javascript">var bookId =173;var bookPageId =11785;var bookPageRelUrl ='docs/17.html';</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
</body>
</html>