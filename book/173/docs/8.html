
<!DOCTYPE html>
<html lang="zh-CN">
<head>
<title>HBase and MapReduce-HBase中文参考指南 3.0</title>
<meta content='HBase and MapReduce,HBase中文参考指南 3.0' name='keywords'>
<meta content='HBase and MapReduce,HBase中文参考指南 3.0' name='description'>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta http-equiv="Content-Language" content="zh-CN" />
<meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1, maximum-scale=1, user-scalable=no"../../../>
<meta name="applicable-device" content="pc,mobile">
<link rel="shortcut icon" href="../../../favicon.ico" type="image/x-icon" />
<meta name="renderer" content="webkit">
<link rel="stylesheet" href="../../../static/components/uikit-2.27.5/css/uikit.custom.css">
<link rel="stylesheet" href="../../../static/components/social-share/social-share.min.css">
<link rel="stylesheet" href="../../../static/components/highlight/styles/custom.css">
<link rel="stylesheet" href="../../../static/components/css/base.css">
<link rel="stylesheet" href="../../../static/components/css/reader.css">
<link rel="stylesheet" href="../../../static/components/css/markdown.css">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5313208362165053" crossorigin="anonymous"></script>
</head>
<body>
<div class=" book-main-wrap uk-container uk-container-center uk-margin-top ">
<div class="uk-grid">
<div class="uk-width-1-1 reader-wrap ">
<div class=" bottom-nav uk-clearfix ">
<div class="uk-align-left ">
<a href="../../../book/173/docs/7.html">
<i class="nav-icon-left uk-icon-small  uk-icon-caret-left"></i>
<span class="">RegionServe..</span>
</a>
</div>
<div class="uk-align-right ">
<a href="../../../book/173/docs/9.html">
<span class="">Securing Ap..</span>
<i class="nav-icon-right uk-icon-small  uk-icon-caret-right"></i>
</a>
</div>
</div>
<div class="uk-text-center">
<h2 class="book-page-title uk-container-center">
<a href="../../../book/173/index.html">HBase中文参考指南 3.0</a>
<a target="_blank" rel="nofollow" href="https://github.com/apachecn/hbase-doc-zh" class="uk-icon-button uk-icon-github" title="github项目地址"></a>
</h2>
</div>
<script type="text/javascript" src="../../../static/components/js/app_intro.js"></script>
<ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-5313208362165053" data-ad-slot="1328047120"></ins>
<script>(adsbygoogle =window.adsbygoogle ||[]).push({});</script>
<hr class="uk-article-divider">
<div class="book-content-section  md-content-section  uk-margin-bottom">
<h1 id="hbase-and-mapreduce">HBase and MapReduce</h1>
<blockquote>
<p>贡献者：<a href="https://github.com/BridgetLai">BridgetLai</a></p>
</blockquote>
<p>Apache MapReduce 是 <a href="https://hadoop.apache.org/">Apache Hadoop</a> 提供的软件框架,用来进行大规模数据分析.MapReduce 已超出本文档范围,可通过如下文档学习<a href="https://hadoop.apache.org/docs/r2.6.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html">https://hadoop.apache.org/docs/r2.6.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html</a>. MapReduce version 2 (MR2)目前是<a href="https://hadoop.apache.org/docs/r2.3.0/hadoop-yarn/hadoop-yarn-site/">YARN</a>的一部分.</p>
<p>本章将讨论在 HBase 中使用 MapReduce 处理数据时需要进行的一些特定配置步骤;另外,还将讨论 HBase 与 MapReduce jobs 之间的交互以及存在的一些问题;最后将介绍 MapReduce 的<a href="http://www.cascading.org/">替代 API</a>:<a href="#cascading">Cascading</a>.</p>
<blockquote>
<p><code>mapred</code> and <code>mapreduce</code></p>
<p>与 MapReduce 一样,在 HBase 中也有 2 种 mapreduce API 包. <em>org.apache.hadoop.hbase.mapred</em> and <em>org.apache.hadoop.hbase.mapreduce</em>. 前者使用旧式风格的 API,后者采用新的模式.相比于前者,后者更加灵活,你可以在旧式 API 中找到等价的.选择 API 时,请使用 MapReduce 部署时选择的包.如果不知道如何选择或者想从头再来,那就使用 <em>org.apache.hadoop.hbase.mapreduce</em>.在接下来的文章中,将使用 <em>o.a.h.h.mapreduce</em> 如果你使用的是 <em>o.a.h.h.mapred</em> 就自行替换.</p>
</blockquote>
<h2 id="48-hbase-mapreduce-and-the-classpath">48. HBase, MapReduce, and the CLASSPATH</h2>
<p>默认情况下,部署在 MapReduce 集群中的 MapReduce jobs 没有权限访问<code>$HBASE_CONF_DIR</code>路径下的 HBase 配置 或者 HBase classes.</p>
<p>通过以下方式可以为 MapReduce jobs 配置权限.</p>
<blockquote>
<p>增加 <em>hbase-site.xml</em> 到 <em>$HADOOP_HOME/conf</em> <br>然后将 HBase jars 添加到 <em>$HADOOP_HOME/lib</em> 目录下 <br> 最后需要将这些变更拷贝到 Hadoop 集群中所有服务上.</p>
</blockquote>
<p>或者</p>
<blockquote>
<p>编辑 <em>$HADOOP_HOME/conf/hadoop-env.sh</em> 将 HBase 依赖添加到 <code>HADOOP_CLASSPATH</code>.</p>
</blockquote>
<p>以上配置均不推荐,因为它会让 Hadoop 安装 HBase 的依赖,并且需要重启 Hadoop 集群才能使用 HBase 中的数据.</p>
<p>推荐的方式是 HBase 使用<code>HADOOP_CLASSPATH</code> or <code>-libjars</code>添加其依赖的 jar 包.</p>
<p>从 HBase <code>0.90.x</code>,HBase 添加依赖 jar 包到任务自身配置中. 依赖项只需要在本地<code>CLASSPATH</code>可用,然后被打包部署到 MapReduce 集群的 fat job jar 中.一种取巧的方式是传递全量的 HBase classpath(即 hbase,独立的 jars 还有配置)到 mapreduce job 运行器中令 hbase 工具从全量的 classpath 挑选依赖最终配置到 MapReduce job 的配置中(可以查看源码实现<code>TableMapReduceUtil#addDependencyJars(org.apache.hadoop.mapreduce.Job)</code>).</p>
<p>下面的例子是在表<code>usertable</code>上运行的 HBase 的 MapReduce 任务: 表行数统计任务<a href="https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/RowCounter.html">RowCounter</a>.设置在 MapReduce 上下文运行需要的 hbase jars 以及配置文件如 hbase-site.xml 到 <code>HADOOP_CLASSPATH</code>. 一定要确保使用了与你的系统相对应的 HBase Jar.替换以下命令中的 VERSION 字段为本地 HBASE 版本. 反引号(`)使 shell 执行子命令，将<code>hbase classpath</code>的输出设置为<code>HADOOP_CLASSPATH</code>. 这个例子需要在 Bash-compatible 执行.</p>
<pre><code>$ HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase classpath` \
${HADOOP_HOME}/bin/hadoop jar ${HBASE_HOME}/lib/hbase-mapreduce-VERSION.jar \
org.apache.hadoop.hbase.mapreduce.RowCounter usertable
</code></pre>
<p>以上命令将启动一个运行在本地配置指定的 hbase 集群的 mapreduce 作业,用来统计表行数.这个集群也是 Hadoop 配置指定的.</p>
<p><code>hbase-mapreduce.jar</code> 核心是一个驱动,罗列了 HBASE 装载的一些基础的 MapReduce 任务.例如,假设你安装的是<code>2.0.0-SNAPSHOT</code>版本:</p>
<pre><code>$ HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase classpath` \
  ${HADOOP_HOME}/bin/hadoop jar ${HBASE_HOME}/lib/hbase-mapreduce-2.0.0-SNAPSHOT.jar
An example program must be given as the first argument.
Valid program names are:
  CellCounter: Count cells in HBase table.
  WALPlayer: Replay WAL files.
  completebulkload: Complete a bulk data load.
  copytable: Export a table from local cluster to peer cluster.
  export: Write table data to HDFS.
  exportsnapshot: Export the specific snapshot to a given FileSystem.
  import: Import data written by Export.
  importtsv: Import data in TSV format.
  rowcounter: Count rows in HBase table.
  verifyrep: Compare the data from tables in two different clusters. WARNING: It doesn't work for incrementColumnValues'd cells since the timestamp is changed after being appended to the log.
</code></pre>
<p>您可以使用上面列出的 MapReduce 任务的简写采用以下命令重新执行表行数统计任务(同样,假设安装的 HBASE 是<code>2.0.0-SNAPSHOT</code>版本):</p>
<pre><code>$ HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase classpath` \
  ${HADOOP_HOME}/bin/hadoop jar ${HBASE_HOME}/lib/hbase-mapreduce-2.0.0-SNAPSHOT.jar \
  rowcounter usertable
</code></pre>
<p>您可能发现了<code>hbase mapredcp</code>工具的输出; 它列出了在 hbase 运行基础 mapreduce 作业所需的最小 jar 文件集合(不包括配置,如果希望 MapReduce 作业能准确找到目标集群，则可能需要添加些配置)。 一旦你开始做任何实质性的事情，你还需要添加额外依赖,这些依赖需在运行<code>hbase mapredcp</code>时通过传递系统属性<code>-Dtmpjars</code>来指定。</p>
<p>对于那些没有打包依赖的 jobs 或者直接调用<code>TableMapReduceUtil#addDependencyJars</code>,则下面的命令格式就非常必要了:</p>
<pre><code>$ HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase mapredcp`:${HBASE_HOME}/conf hadoop jar MyApp.jar MyJobMainClass -libjars $(${HBASE_HOME}/bin/hbase mapredcp | tr ':' ',') ...
</code></pre>
<p>如果您是在 HBase 的构建地址而不是安装地址执行以上示例,您会遇到如下错误:</p>
<pre><code>java.lang.RuntimeException: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.mapreduce.RowCounter$RowCounterMapper
</code></pre>
<p>如果出现了以上问题,请参照以下命令修改,它从构建环境的 <em>target/</em> 目录下使用 HBASE jars</p>
<pre><code>$ HADOOP_CLASSPATH=${HBASE_BUILD_HOME}/hbase-mapreduce/target/hbase-mapreduce-VERSION-SNAPSHOT.jar:`${HBASE_BUILD_HOME}/bin/hbase classpath` ${HADOOP_HOME}/bin/hadoop jar ${HBASE_BUILD_HOME}/hbase-mapreduce/target/hbase-mapreduce-VERSION-SNAPSHOT.jar rowcounter usertable
</code></pre>
<blockquote>
<p>Notice to MapReduce users of HBase between 0.96.1 and 0.98.4 一些 HBase MapReduce 任务启动失败,会出现以下类似的异常:</p>
</blockquote>
<blockquote>
<pre><code>Exception in thread "main" java.lang.IllegalAccessError: class
    com.google.protobuf.ZeroCopyLiteralByteString cannot access its superclass
    com.google.protobuf.LiteralByteString
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClass(ClassLoader.java:792)
    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
    at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)
    at java.net.URLClassLoader.access$100(URLClassLoader.java:71)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    at
    org.apache.hadoop.hbase.protobuf.ProtobufUtil.toScan(ProtobufUtil.java:818)
    at
    org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.convertScanToString(TableMapReduceUtil.java:433)
    at
    org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob(TableMapReduceUtil.java:186)
    at
    org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob(TableMapReduceUtil.java:147)
    at
    org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob(TableMapReduceUtil.java:270)
    at
    org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob(TableMapReduceUtil.java:100)
...
</code></pre>
<p>这是<a href="https://issues.apache.org/jira/browse/HBASE-9867">HBASE-9867</a>无意间增加了一个类加载器依赖引入的优化.</p>
</blockquote>
<blockquote>
<p>这个影响使用<code>-libjars</code> 和 'fat jar '的任务,他们将运行时依赖放在在<code>lib</code>路径下.</p>
</blockquote>
<p>为了满足新类加载器需要,<code>hbase-protocol.jar</code>必须包含在 Hadoop 的 环境变量下.可通过<a href="#hbase.mapreduce.classpath">HBase, MapReduce, and the CLASSPATH</a>查阅解决 一些 classpath 错误的推荐解决方法. The following is included for historical purposes.</p>
<blockquote>
<p>在 Hadoop 的 lib 目录里通过系统连接或者直接拷贝方式引入<code>hbase-protocol.jar</code>,可以系统范围内解决 classpath 问题.</p>
<p>这也可以在每个作业启动的基础上实现，方法是在作业提交时将其(<code>hbase-protocol.jar</code>)包含在<code>HADOOP_CLASSPATH</code>环境变量中。启动时打包其依赖项，以下所有三个作业启动命令都满足此要求</p>
</blockquote>
<blockquote>
<pre><code>$ HADOOP_CLASSPATH=/path/to/hbase-protocol.jar:/path/to/hbase/conf hadoop jar MyJob.jar MyJobMainClass
$ HADOOP_CLASSPATH=$(hbase mapredcp):/path/to/hbase/conf hadoop jar MyJob.jar MyJobMainClass
$ HADOOP_CLASSPATH=$(hbase classpath) hadoop jar MyJob.jar MyJobMainClass
</code></pre>
<p>下面的命令对于那些不打包自己依赖的 Jar 文件很有必要:</p>
</blockquote>
<blockquote>
<pre><code>$ HADOOP_CLASSPATH=$(hbase mapredcp):/etc/hbase/conf hadoop jar MyApp.jar MyJobMainClass -libjars $(hbase mapredcp &amp;#124; tr ':' ',') ...
</code></pre>
<p>可以查阅 <a href="https://issues.apache.org/jira/browse/HBASE-10304">HBASE-10304</a>进行更深入的讨论.</p>
</blockquote>
<h2 id="49-mapreduce-scan-caching">49. MapReduce Scan Caching</h2>
<p>现在 TableMapReduceUtil 恢复在传入的 Scan 对象上设置扫描器缓存（在将结果返回到客户端之前缓存的行数）的选项。由于 HBase 0.95 中的错误（[HBASE-11558]），此功能丢失 （https://issues.apache.org/jira/browse/HBASE-11558）），修正了 HBase 0.98.5 和 0.96.3。 选择扫描程序缓存的优先顺序如下：</p>
<p>1.在扫描对象上设置的缓存设置。</p>
<p>2.通过配置选项<code>hbase.client.scanner.caching</code>指定的缓存设置，可以在 <em>hbase-site.xml</em> 中手动设置，也可以通过辅助方法<code>TableMapReduceUtil.setScannerCaching（）</code>设置。</p>
<p>3.默认值<code>HConstants.DEFAULT_HBASE_CLIENT_SCANNER_CACHING</code>，设置为“100”。</p>
<p>优化缓存设置是平衡客户端等待结果的时间与客户端需要接收的结果集数量。 如果缓存设置太大，客户端可能会等待很长时间，甚至可能超时。 如果设置太小，则需要将结果分多个部分返回。 如果您将扫描看作是铲子，那么更大的缓存设置相当于更大的铲子，而更小的缓存设置等价于为了填满桶而进行更多的铲动。</p>
<p>上面提到的优先级列表允许您设置一个合理的默认值，也可以根据需要重写。</p>
<p>有关[Scan](https://hbase.apache.org/apidocs/org/apache/hadoop op/hbase/client/scan.html)的更多细节，请参阅 API 文档。</p>
<h2 id="50捆绑的-hbase-mapreduce-作业">50.捆绑的 HBase MapReduce 作业</h2>
<p>HBase JAR 还可用作某些捆绑 MapReduce 作业的驱动程序。要了解捆绑的 MapReduce 作业，请运行以下命令。</p>
<pre><code>$ ${HADOOP_HOME}/bin/hadoop jar ${HBASE_HOME}/hbase-mapreduce-VERSION.jar
An example program must be given as the first argument.
Valid program names are:
  copytable: Export a table from local cluster to peer cluster
  completebulkload: Complete a bulk data load.
  export: Write table data to HDFS.
  import: Import data written by Export.
  importtsv: Import data in TSV format.
  rowcounter: Count rows in HBase table
</code></pre>
<p>每个有效的程序名称都捆绑了 MapReduce 作业。 要运行其中一个作业,请根据以下示例构建命令.</p>
<pre><code>$ ${HADOOP_HOME}/bin/hadoop jar ${HBASE_HOME}/hbase-mapreduce-VERSION.jar rowcounter myTable
</code></pre>
<h2 id="51-hbase-作为-mapreduce-作业数据源和数据接收器">51. HBase 作为 MapReduce 作业数据源和数据接收器</h2>
<p>HBase 可以被用作 MapReduce Job 的数据源 <a href="https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/TableInputFormat.html">TableInputFormat</a>, 和数据接收器<a href="https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/TableOutputFormat.html">TableOutputFormat</a> or <a href="https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/MultiTableOutputFormat.html">MultiTableOutputFormat</a>.编写对 HBase 读写的 MapReduce jbos 时,建议使用子类 <a href="https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/TableMapper.html">TableMapper</a> 或者 <a href="https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/TableReducer.html">TableReducer</a>. 有关基本用法请参阅不做任何处理的传递类 <a href="https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/IdentityTableMapper.html">IdentityTableMapper</a> 和 <a href="https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/IdentityTableReducer.html">IdentityTableReducer</a> . 对于更复杂的例子, 请参阅 <a href="https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/RowCounter.html">RowCounter</a> 或者查看 <code>org.apache.hadoop.hbase.mapreduce.TestTableMapReduce</code> 的单元测试.</p>
<p>如果运行使用 HBase 作为源或接收器的 MapReduce job，则需要在配置中指定源和接收的表和列名称。 当您从 HBase 读取时，<code>TableInputFormat</code>从 HBase 请求分区列表并生成一个映射，该映射可以是“map-per-region”或“mapreduce.job.maps”映射，以较小者为准。如果您的 job 只有两个 maps，请将<code>mapreduce.job.maps</code>提升到大于分区数的数字。如果您每个节点正在运行 TaskTracer / NodeManager 和 RegionServer，则 Maps 将在相邻的 TaskTracker / NodeManager 上运行。写入 HBase 时，避免 Reduce 步骤并从 Map 中写回 HBase 可能是有意义的。当您的作业不需要 MapReduce 对 map 发出的数据进行排序和整理时，此方法有效。 在插入时，对于 HBase'排序',除非您需要，否则没有必要对您的 MapReduce 集群进行双重排序（以及对数据进行移动）。如果您不需要 Reduce，您的 map 可能会在作业结束时输出为了报告而处理的记录数，或者将 Reduced 数设置为零并使用 TableOutputFormat。如果在您的情况下运行 Reduce 步骤是有意义的，您通常应该使用多个 reducer，以便负载分布在 HBase 集群中。</p>
<p>新的 HBase 分区程序[HRegionPartitioner]（https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/HRegionPartitioner.html）可以尽可能多的减少运行分区的数量。当您的表很大时，HRegionPartitioner 是合适的，并且您的上传在完成后不会大大改变现有区域的数量。否则请使用默认分区程序。</p>
<h2 id="52-批量导入时直接写-hfiles">52. 批量导入时直接写 HFiles</h2>
<p>如果要导入新表，则可以绕过 HBase API 并将内容直接写入文件系统，格式化为 HBase 数据文件（HFiles）。您的导入将运行得更快，可能会快一个数量级。有关此机制如何工作的更多信息，请参阅<a href="#arch.bulk.load">Bulk Loading</a>。</p>
<h2 id="53-行数统计的例子">53. 行数统计的例子</h2>
<p>包含 <a href="https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/RowCounter.html">RowCounter</a> 的 MapReduce 作业使用 <code>TableInputFormat</code> 并且 统计了指定表格的行数.请使用以下命令运行:</p>
<pre><code>$ ./bin/hadoop jar hbase-X.X.X.jar
</code></pre>
<p>这将调用 HBase MapReduce 驱动程序类。请从提供的 jobs 中选择<code>rowcounter</code>。这将打印 rowcounter 使用建议到标准输出。指定表名，要计数的列和输出目录。如果您有类路径错误，请参阅 <a href="#hbase.mapreduce.classpath">HBase, MapReduce, and the CLASSPATH</a>.</p>
<h2 id="54-map-任务--拆分">54. Map 任务 拆分</h2>
<h3 id="541--hbase-默认的-mapreduce-拆分器">54.1. HBase 默认的 MapReduce 拆分器</h3>
<p>当<a href="https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/TableInputFormat.html">TableInputFormat</a>在 MapReduce job 中被用做获取 HBase 表时,它的拆分器将为表的每个分区指定一个 map 任务.因此如果表中有 100 个分区,无论要扫描多少列,都会为该任务 拆分出 100 个 map 任务.</p>
<h3 id="542-自定义拆分器">54.2. 自定义拆分器</h3>
<p>如果对自定义拆分器感兴趣,请参阅<a href="https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.html">TableInputFormatBase</a>中的<code>getSplits</code>方法,它是 map 任务拆分逻辑所在.</p>
<h2 id="55-hbase-mapreduce-示例">55. HBase MapReduce 示例</h2>
<h3 id="551-hbase-mapreduce-读示例">55.1. HBase MapReduce 读示例</h3>
<p>以下是以只读方式使用 HBase 作为 MapReduce 源的示例。 具体来说，有一个 Mapper 实例但没有 Reducer，并且 Mapper 没有发出任何内容。 Job 将定义如下......</p>
<pre><code>Configuration config = HBaseConfiguration.create();
Job job = new Job(config, "ExampleRead");
job.setJarByClass(MyReadJob.class);     // class that contains mapper

Scan scan = new Scan();
scan.setCaching(500);        // 1 is the default in Scan, which will be bad for MapReduce jobs
scan.setCacheBlocks(false);  // don't set to true for MR jobs
// set other scan attrs
...

TableMapReduceUtil.initTableMapperJob(
  tableName,        // input HBase table name
  scan,             // Scan instance to control CF and attribute selection
  MyMapper.class,   // mapper
  null,             // mapper output key
  null,             // mapper output value
  job);
job.setOutputFormatClass(NullOutputFormat.class);   // because we aren't emitting anything from mapper

boolean b = job.waitForCompletion(true);
if (!b) {
  throw new IOException("error with job!");
}
</code></pre>
<p>…​并且 mapper 实例将继承 <a href="https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/TableMapper.html">TableMapper</a>…​</p>
<pre><code>public static class MyMapper extends TableMapper&lt;Text, Text&gt; {

  public void map(ImmutableBytesWritable row, Result value, Context context) throws InterruptedException, IOException {
    // process data for the row from the Result instance.
   }
}
</code></pre>
<h3 id="552-hbase-mapreduce-读写示例">55.2. HBase MapReduce 读/写示例</h3>
<p>以下 HBase 既作为源也作为 MapReduce 的接收器的示例。 此示例将简单地将数据从一个表复制到另一个表。</p>
<pre><code>Configuration config = HBaseConfiguration.create();
Job job = new Job(config,"ExampleReadWrite");
job.setJarByClass(MyReadWriteJob.class);    // class that contains mapper

Scan scan = new Scan();
scan.setCaching(500);        // 1 is the default in Scan, which will be bad for MapReduce jobs
scan.setCacheBlocks(false);  // don't set to true for MR jobs
// set other scan attrs

TableMapReduceUtil.initTableMapperJob(
  sourceTable,      // input table
  scan,             // Scan instance to control CF and attribute selection
  MyMapper.class,   // mapper class
  null,             // mapper output key
  null,             // mapper output value
  job);
TableMapReduceUtil.initTableReducerJob(
  targetTable,      // output table
  null,             // reducer class
  job);
job.setNumReduceTasks(0);

boolean b = job.waitForCompletion(true);
if (!b) {
    throw new IOException("error with job!");
}
</code></pre>
<p>很有必要解释一下<code>TableMapReduceUtil</code>的作用是什么,尤其是 reducer. <a href="https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/TableOutputFormat.html">TableOutputFormat</a> 被用作 outputFormat class ,一些参数已经进行了配置,例如<code>TableOutputFormat.OUTPUT_TABLE</code>,同时设置了 reducer 的 output key 为<code>TableOutputFormat.OUTPUT_TABLE</code> 并且 value 为<code>Writable</code>. 这些配置项可以由开发工程师在 job 和配置文件中进行设置,<code>TableMapReduceUtil</code>试图将这些工作进行简化.</p>
<p>下面是一个 mapper 的例子,它将创建一个"Put" ,匹配输入的"Result "并输出.而这些工作正是 CopyTable 工具的作用.</p>
<pre><code>public static class MyMapper extends TableMapper&lt;ImmutableBytesWritable, Put&gt;  {

  public void map(ImmutableBytesWritable row, Result value, Context context) throws IOException, InterruptedException {
    // this example is just copying the data from the source table...
      context.write(row, resultToPut(row,value));
    }

    private static Put resultToPut(ImmutableBytesWritable key, Result result) throws IOException {
      Put put = new Put(key.get());
      for (KeyValue kv : result.raw()) {
        put.add(kv);
      }
      return put;
    }
}
</code></pre>
<p>这实际上并不是一个 reducer 过程, 所以由<code>TableOutputFormat</code> 负责将'Put'发送到目标表. 这只是一个例子,开发人员可以选择不使用<code>TableOutputFormat</code>并自行链接到目标表.</p>
<h3 id="553-hbase-mapreduce-多表输出的读写示例">55.3. HBase MapReduce 多表输出的读写示例</h3>
<p>TODO: <code>MultiTableOutputFormat</code> 样例.</p>
<h3 id="554-hbase-mapreduce-汇总到-hbase-示例">55.4. HBase MapReduce 汇总到 HBase 示例</h3>
<p>以下示例使用 HBase 作为 MapReduce 源并接收汇总信息。此示例将计算表中某个 value 的不同实例的数量，并将这些汇总计数写入另一个表中。</p>
<pre><code>Configuration config = HBaseConfiguration.create();
Job job = new Job(config,"ExampleSummary");
job.setJarByClass(MySummaryJob.class);     // class that contains mapper and reducer

Scan scan = new Scan();
scan.setCaching(500);        // 1 is the default in Scan, which will be bad for MapReduce jobs
scan.setCacheBlocks(false);  // don't set to true for MR jobs
// set other scan attrs

TableMapReduceUtil.initTableMapperJob(
  sourceTable,        // input table
  scan,               // Scan instance to control CF and attribute selection
  MyMapper.class,     // mapper class
  Text.class,         // mapper output key
  IntWritable.class,  // mapper output value
  job);
TableMapReduceUtil.initTableReducerJob(
  targetTable,        // output table
  MyTableReducer.class,    // reducer class
  job);
job.setNumReduceTasks(1);   // at least one, adjust as required

boolean b = job.waitForCompletion(true);
if (!b) {
  throw new IOException("error with job!");
}
</code></pre>
<p>在示例中的 mapper 在一个 String 类型的 value 上进行汇总操作,并将 value作为 mapper 输出的 key,<code>IntWritable</code>表示实例计数器。</p>
<pre><code>public static class MyMapper extends TableMapper&lt;Text, IntWritable&gt;  {
  public static final byte[] CF = "cf".getBytes();
  public static final byte[] ATTR1 = "attr1".getBytes();

  private final IntWritable ONE = new IntWritable(1);
  private Text text = new Text();

  public void map(ImmutableBytesWritable row, Result value, Context context) throws IOException, InterruptedException {
    String val = new String(value.getValue(CF, ATTR1));
    text.set(val);     // we can only emit Writables...
    context.write(text, ONE);
  }
}
</code></pre>
<p>在 reducer 中，计算“ones”（就像执行此操作的任何其他 MR 示例一样），然后发出“Put”。</p>
<pre><code>public static class MyTableReducer extends TableReducer&lt;Text, IntWritable, ImmutableBytesWritable&gt;  {
  public static final byte[] CF = "cf".getBytes();
  public static final byte[] COUNT = "count".getBytes();

  public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
    int i = 0;
    for (IntWritable val : values) {
      i += val.get();
    }
    Put put = new Put(Bytes.toBytes(key.toString()));
    put.add(CF, COUNT, Bytes.toBytes(i));

    context.write(null, put);
  }
}
</code></pre>
<h3 id="555-hbase-mapreduce-文件汇总示例">55.5. HBase MapReduce 文件汇总示例</h3>
<p>这与上面的汇总示例很相似,不同之处在于该汇总使用 HBase 作为 MapReduce 的数据源而使用 HDFS 作为接收器.这样的不同体现在 job 启动和 reduce 过程,而 mapper 过程没有区别.</p>
<pre><code>Configuration config = HBaseConfiguration.create();
Job job = new Job(config,"ExampleSummaryToFile");
job.setJarByClass(MySummaryFileJob.class);     // class that contains mapper and reducer

Scan scan = new Scan();
scan.setCaching(500);        // 1 is the default in Scan, which will be bad for MapReduce jobs
scan.setCacheBlocks(false);  // don't set to true for MR jobs
// set other scan attrs

TableMapReduceUtil.initTableMapperJob(
  sourceTable,        // input table
  scan,               // Scan instance to control CF and attribute selection
  MyMapper.class,     // mapper class
  Text.class,         // mapper output key
  IntWritable.class,  // mapper output value
  job);
job.setReducerClass(MyReducer.class);    // reducer class
job.setNumReduceTasks(1);    // at least one, adjust as required
FileOutputFormat.setOutputPath(job, new Path("../../../tmp/mr/mySummaryFile"));  // adjust directories as required

boolean b = job.waitForCompletion(true);
if (!b) {
  throw new IOException("error with job!");
}
</code></pre>
<p>如上所述,本示例的中的 mappper 与上例无异,至于 Reducer,则采用一个'通用'的而不是继承自 TableMapper 并且发出 Puts.</p>
<pre><code>public static class MyReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;  {

  public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
    int i = 0;
    for (IntWritable val : values) {
      i += val.get();
    }
    context.write(key, new IntWritable(i));
  }
}
</code></pre>
<h3 id="556-不使用-reducer-hbase-mapreduce-汇总到-hbase">55.6 不使用 Reducer ,HBase MapReduce 汇总到 HBase</h3>
<p>如果使用 HBase 作为 Reducer,也可以在没有 Reducer 的情况下进行汇总. 汇总任务要求 HBase 目标表存在.表方法<code>incrementColumnValue</code>将被用作值的原子增长.从性能角度看,为每个 map-task 中那些会值增长的值保留一个 Map,并且在 mapper 执行<code>cleanup</code> 方法时每个 key 更新一次,这可能是有意义的.但是，您的里程可能会根据要处理的行数和惟一键的不同而有所不同。</p>
<p>最后,汇总结果在 HBase 中.</p>
<h3 id="557-hbase-mapreduce-汇总到-rdbms">55.7. HBase MapReduce 汇总到 RDBMS</h3>
<p>有时，为 RDBMS 生成摘要更合适。对于这些情况，可以通过自定义 reducer 直接生成 RDBMS 的摘要。 <code>setup</code>方法可以连接到 RDBMS（连接信息可以通过上下文中的自定义参数传递），清理方法可以关闭连接。</p>
<p>一个 job 的 reducer 数量对汇总实现至关重要,您将必须将其设计到 reducer 中.具体来说,不管被设计成一个 reducer 还是多个 reducer,这没有对错之分,完全依赖于您的用例.指定给 job 的 reducer 越多,与 RDMS 建立的实时链接越多,这可以在一定程度上提高吞吐量.</p>
<pre><code>public static class MyRdbmsReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;  {

  private Connection c = null;

  public void setup(Context context) {
    // create DB connection...
  }

  public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
    // do summarization
    // in this example the keys are Text, but this is just an example
  }

  public void cleanup(Context context) {
    // close db connection
  }

}
</code></pre>
<p>最终,汇总结果写入到 RDMS 表中.</p>
<h2 id="56-在-mapreduce-任务中访问其他-hbase-表">56. 在 MapReduce 任务中访问其他 HBase 表</h2>
<p>尽管目前框架允许一个 HBase 表作为 MapReduce 作业的输入,但其他 HBase 表只可以通过作为查找表(lookup tables)才能访问,例如在 MapReduce 作业中通过 mapper 的 setup 方法创建 Table 实例.</p>
<pre><code>public class MyMapper extends TableMapper&lt;Text, LongWritable&gt; {
  private Table myOtherTable;

  public void setup(Context context) {
    // In here create a Connection to the cluster and save it or use the Connection
    // from the existing table
    myOtherTable = connection.getTable("myOtherTable");
  }

  public void map(ImmutableBytesWritable row, Result value, Context context) throws IOException, InterruptedException {
    // process Result...
    // use 'myOtherTable' for lookups
  }
</code></pre>
<h2 id="57-推测执行speculative-execution">57. 推测执行(Speculative Execution)</h2>
<p>通常建议关闭以 HBASE 为数据源的 MapReduce 作业的推测执行(speculative execution).关闭推测执行可以通过设置单个任务的属性,也可以设置整个集群.特对是对于执行时间较长的任务,推测执行(speculative execution)为其创建一个重复任务,将进行双重数据写入,这可能不是你想要的. 查阅 <a href="#spec.ex">spec.ex</a> 获取更多信息.</p>
<h2 id="58-级联cascading">58. 级联(Cascading)</h2>
<p>[[Cascading]（http://www.cascading.org/）是 MapReduce 的替代 API，本质上使用 MapReduce，但允许您以简化的方式编写 MapReduce 代码。</p>
<p>下面的示例显示了一个 Cascading“Flow”，它将数据“汇入”到 HBase 集群中。 同样的<code>hBaseTap</code> API 也可用于“获取”数据。</p>
<pre><code>// read data from the default filesystem
// emits two fields: "offset" and "line"
Tap source = new Hfs( new TextLine(), inputFileLhs );

// store data in an HBase cluster
// accepts fields "num", "lower", and "upper"
// will automatically scope incoming fields to their proper familyname, "left" or "right"
Fields keyFields = new Fields( "num" );
String[] familyNames = {"left", "right"};
Fields[] valueFields = new Fields[] {new Fields( "lower" ), new Fields( "upper" ) };
Tap hBaseTap = new HBaseTap( "multitable", new HBaseScheme( keyFields, familyNames, valueFields ), SinkMode.REPLACE );

// a simple pipe assembly to parse the input into fields
// a real app would likely chain multiple Pipes together for more complex processing
Pipe parsePipe = new Each( "insert", new Fields( "line" ), new RegexSplitter( new Fields( "num", "lower", "upper" ), "  " ) );

// "plan" a cluster executable Flow
// this connects the source Tap and hBaseTap (the sink Tap) to the parsePipe
Flow parseFlow = new FlowConnector( properties ).connect( source, hBaseTap, parsePipe );

// start the flow, and block until complete
parseFlow.complete();

// open an iterator on the HBase table we stuffed data into
TupleEntryIterator iterator = parseFlow.openSink();

while(iterator.hasNext())
  {
  // print out each tuple from HBase
  System.out.println( "iterator.next() = " + iterator.next() );
  }

iterator.close();
</code></pre>
</div>
<hr class="uk-article-divider">
<div class="uk-block uk-block-muted uk-padding-top-remove uk-padding-bottom-remove uk-margin-large-top  book-recommend-wrap">
<div class="uk-margin-top uk-margin-bottom uk-margin-left uk-margin-right">
<div class="uk-margin uk-text-muted "><i class="uk-icon-outdent uk-icon-justify uk-margin-small-right"></i>书籍推荐</div>
<div class="books">
<ul class="uk-book-list">
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/138/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/code_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/138/index.html">开发经验总结</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/72.html">phodal</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="code">code</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">1页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年8月3日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 641个">641</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/64/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/code_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/64/index.html">免费的编程中文书籍索引</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/40.html">justjavac</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="code">code</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">56页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月5日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 33914个">33914</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/155/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/code_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/155/index.html">CHNote，Apple设备、Git、Shell等使用教程</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/87.html">wanggw911</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="code">code</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">21页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月12日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 0个">0</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/162/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/python_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/162/index.html">Python方向综合面试题</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/92.html">jackfrued</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">115页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月26日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 35个">35</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/132/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/vuejs_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/132/index.html">vue-router文档</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/71.html">srzyhead</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="vuejs">vuejs</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">30页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年8月3日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 0个">0</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/100/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/lua_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/100/index.html">Lua编程入门</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/61.html">andycai</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="lua">lua</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">14页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月29日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 192个">192</span>
</div>
</div>
</div>
</li>
<hr>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
<nav class="tm-navbar uk-navbar uk-navbar-attached reader-nav">
<div class="uk-float-left uk-margin-small-top">
<a href="javascript:;" title="目录菜单" class="show-menu  uk-icon-hover  uk-icon-align-justify uk-margin-right"></a>
<div data-uk-dropdown="{mode:'click',pos:'bottom-left'}" class="font-setting-wrap">
<a class="uk-icon-hover uk-icon-font uk-margin-right" aria-label="字体设置" href="javascript:;"></a>
<div class="uk-dropdown dropdown-menu">
<div class="dropdown-caret"><span class="caret-outer"></span><span class="caret-inner"></span></div>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-reduce">小字</button>
<button class="uk-button-link button size-2 font-enlarge">大字</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-1 ">宋体</button>
<button class="uk-button-link button size-2 font-2 ">黑体</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-3 color-theme-sun "><i class="uk-icon-sun-o"></i>白天</button>
<button class="uk-button-link button size-3 color-theme-eye "><i class="uk-icon-eye"></i>护眼</button>
<button class="uk-button-link button size-3 color-theme-moon "><i class="uk-icon-moon-o"></i>夜晚</button></div>
</div>
</div>
<a class="logo uk-margin-right" href="../../../" title="返回首页"><img class="" src="../../../static/components/images/icon_32.png" /></a>
</div>
<div class="uk-navbar-flip  uk-hidden-small">
<div id="share-box"></div>
</div>
</nav>
<div id="menu-id" class="uk-offcanvas reader-offcanvas">
<div class="uk-offcanvas-bar">
<ul class="book-menu-bar uk-nav uk-nav-offcanvas" data-uk-nav>
<li>
<a href="../../../book/173/index.html" data-book-page-rel-url="index.html" data-book-page-id="0" title="封面">封面</a>
</li>
<li>
<a class="pjax" href="../../../book/173/readme.html" data-book-page-rel-url="readme.html" data-book-page-id="0" title="简介">简介</a>
</li>
<li>
<a class="pjax" href="../../../book/173/README.html" title="HBase™ 中文参考指南 3.0" data-book-page-rel-url="README.html" data-book-page-id="11767">HBase™ 中文参考指南 3.0</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/0.html" title="Preface" data-book-page-rel-url="docs/0.html" data-book-page-id="11768">Preface</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/1.html" title="Getting Started" data-book-page-rel-url="docs/1.html" data-book-page-id="11769">Getting Started</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/2.html" title="Apache HBase Configuration" data-book-page-rel-url="docs/2.html" data-book-page-id="11770">Apache HBase Configuration</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/3.html" title="Upgrading" data-book-page-rel-url="docs/3.html" data-book-page-id="11771">Upgrading</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/4.html" title="The Apache HBase Shell" data-book-page-rel-url="docs/4.html" data-book-page-id="11772">The Apache HBase Shell</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/5.html" title="Data Model" data-book-page-rel-url="docs/5.html" data-book-page-id="11773">Data Model</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/6.html" title="HBase and Schema Design" data-book-page-rel-url="docs/6.html" data-book-page-id="11774">HBase and Schema Design</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/7.html" title="RegionServer Sizing Rules of Thumb" data-book-page-rel-url="docs/7.html" data-book-page-id="11775">RegionServer Sizing Rules of Thumb</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/8.html" title="HBase and MapReduce" data-book-page-rel-url="docs/8.html" data-book-page-id="11776">HBase and MapReduce</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/9.html" title="Securing Apache HBase" data-book-page-rel-url="docs/9.html" data-book-page-id="11777">Securing Apache HBase</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/10.html" title="Architecture" data-book-page-rel-url="docs/10.html" data-book-page-id="11778">Architecture</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/11.html" title="In-memory Compaction" data-book-page-rel-url="docs/11.html" data-book-page-id="11779">In-memory Compaction</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/12.html" title="Backup and Restore" data-book-page-rel-url="docs/12.html" data-book-page-id="11780">Backup and Restore</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/13.html" title="Synchronous Replication" data-book-page-rel-url="docs/13.html" data-book-page-id="11781">Synchronous Replication</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/14.html" title="Apache HBase APIs" data-book-page-rel-url="docs/14.html" data-book-page-id="11782">Apache HBase APIs</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/15.html" title="Apache HBase External APIs" data-book-page-rel-url="docs/15.html" data-book-page-id="11783">Apache HBase External APIs</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/16.html" title="Thrift API and Filter Language" data-book-page-rel-url="docs/16.html" data-book-page-id="11784">Thrift API and Filter Language</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/17.html" title="HBase and Spark" data-book-page-rel-url="docs/17.html" data-book-page-id="11785">HBase and Spark</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/18.html" title="Apache HBase Coprocessors" data-book-page-rel-url="docs/18.html" data-book-page-id="11786">Apache HBase Coprocessors</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/19.html" title="Apache HBase Performance Tuning" data-book-page-rel-url="docs/19.html" data-book-page-id="11787">Apache HBase Performance Tuning</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/20.html" title="Troubleshooting and Debugging Apache HBase" data-book-page-rel-url="docs/20.html" data-book-page-id="11788">Troubleshooting and Debugging Apache HBase</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/21.html" title="Apache HBase Case Studies" data-book-page-rel-url="docs/21.html" data-book-page-id="11789">Apache HBase Case Studies</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/22.html" title="Apache HBase Operational Management" data-book-page-rel-url="docs/22.html" data-book-page-id="11790">Apache HBase Operational Management</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/23.html" title="Building and Developing Apache HBase" data-book-page-rel-url="docs/23.html" data-book-page-id="11791">Building and Developing Apache HBase</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/24.html" title="Unit Testing HBase Applications" data-book-page-rel-url="docs/24.html" data-book-page-id="11792">Unit Testing HBase Applications</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/25.html" title="Protobuf in HBase" data-book-page-rel-url="docs/25.html" data-book-page-id="11793">Protobuf in HBase</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/26.html" title="Procedure Framework (Pv2): HBASE-12439" data-book-page-rel-url="docs/26.html" data-book-page-id="11794">Procedure Framework (Pv2): HBASE-12439</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/27.html" title="AMv2 Description for Devs" data-book-page-rel-url="docs/27.html" data-book-page-id="11795">AMv2 Description for Devs</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/28.html" title="ZooKeeper" data-book-page-rel-url="docs/28.html" data-book-page-id="11796">ZooKeeper</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/29.html" title="Community" data-book-page-rel-url="docs/29.html" data-book-page-id="11797">Community</a>
</li>
<li>
<a class="pjax" href="../../../book/173/docs/30.html" title="Appendix" data-book-page-rel-url="docs/30.html" data-book-page-id="11798">Appendix</a>
</li>
</ul>
</div>
</div>
<script src="https://cdn.staticfile.net/jquery/1.12.4/jquery.min.js"></script>
<script type="text/javascript" src="../../../static/components/uikit-2.27.5/js/uikit.reader.js"></script>
<script type="text/javascript" src="../../../static/components/social-share/social-share.min.js"></script>
<script>(function(){var bp =document.createElement('script');var curProtocol =window.location.protocol.split(':')[0];if (curProtocol ==='https') {bp.src ='https://zz.bdstatic.com/linksubmit/push.js';}
else {bp.src ='http://push.zhanzhang.baidu.com/push.js';}
var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(bp,s);})();</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
<script src="https://cdn.staticfile.net/highlight.js/9.12.0/highlight.min.js"></script>
<script src="https://cdn.staticfile.net/jquery.pjax/2.0.1/jquery.pjax.min.js"></script>
<script src="https://cdn.staticfile.net/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="https://cdn.staticfile.net/uikit/2.27.5/js/components/lightbox.min.js"></script>
<link rel="dns-prefetch" href="../../..//cdn.mathjax.org" />
<script type="text/x-mathjax-config">
 function initMathJax() {
    var mathId = $("book-content-section")[0];
    MathJax.Hub.Config({
        tex2jax: {skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code','a']},
        showProcessingMessages: false,
        messageStyle: "none"
    });
    MathJax.Hub.Queue(["Typeset",MathJax.Hub,mathId]);
 };
initMathJax();
</script>
<script src='https://cdn.staticfile.net/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML' async></script>
<style>
	.MathJax_Display{display:inline!important;}
</style>
<script type="text/javascript" src="../../../static/components/js/reader.js"></script>
<script type="text/javascript">var bookId =173;var bookPageId =11776;var bookPageRelUrl ='docs/8.html';</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
</body>
</html>