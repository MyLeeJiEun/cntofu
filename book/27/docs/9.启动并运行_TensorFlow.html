
<!DOCTYPE html>
<html lang="zh-CN">
<head>
<title>九、启动并运行 TensorFlow-Sklearn 与 TensorFlow 机器学习实用指南(Hands On Machine Learning with Scikit Learn and TensorFlow)</title>
<meta content='九、启动并运行 TensorFlow,Sklearn 与 TensorFlow 机器学习实用指南,Hands On Machine Learning with Scikit Learn and TensorFlow' name='keywords'>
<meta content='九、启动并运行 TensorFlow,Sklearn 与 TensorFlow 机器学习实用指南,Hands On Machine Learning with Scikit Learn and TensorFlow' name='description'>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta http-equiv="Content-Language" content="zh-CN" />
<meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1, maximum-scale=1, user-scalable=no"../../../>
<meta name="applicable-device" content="pc,mobile">
<link rel="shortcut icon" href="../../../favicon.ico" type="image/x-icon" />
<meta name="renderer" content="webkit">
<link rel="stylesheet" href="../../../static/components/uikit-2.27.5/css/uikit.custom.css">
<link rel="stylesheet" href="../../../static/components/social-share/social-share.min.css">
<link rel="stylesheet" href="../../../static/components/highlight/styles/custom.css">
<link rel="stylesheet" href="../../../static/components/css/base.css">
<link rel="stylesheet" href="../../../static/components/css/reader.css">
<link rel="stylesheet" href="../../../static/components/css/markdown.css">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5313208362165053" crossorigin="anonymous"></script>
</head>
<body>
<div class=" book-main-wrap uk-container uk-container-center uk-margin-top ">
<div class="uk-grid">
<div class="uk-width-1-1 reader-wrap ">
<div class=" bottom-nav uk-clearfix ">
<div class="uk-align-left ">
<a href="../../../book/27/docs/8.降维.html">
<i class="nav-icon-left uk-icon-small  uk-icon-caret-left"></i>
<span class="">八、降维</span>
</a>
</div>
<div class="uk-align-right ">
<a href="../../../book/27/docs/10.人工神经网络介绍.html">
<span class="">十、人工神经网络介绍</span>
<i class="nav-icon-right uk-icon-small  uk-icon-caret-right"></i>
</a>
</div>
</div>
<div class="uk-text-center">
<h2 class="book-page-title uk-container-center">
<a href="../../../book/27/index.html">Sklearn 与 TensorFlow 机器学习实用指南 (Hands On Machine Learning with Scikit Learn and TensorFlow)</a>
<a target="_blank" rel="nofollow" href="https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF" class="uk-icon-button uk-icon-github" title="github项目地址"></a>
</h2>
</div>
<script type="text/javascript" src="../../../static/components/js/app_intro.js"></script>
<ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-5313208362165053" data-ad-slot="1328047120"></ins>
<script>(adsbygoogle =window.adsbygoogle ||[]).push({});</script>
<hr class="uk-article-divider">
<div class="book-content-section  md-content-section  uk-margin-bottom">
<h1 id="九启动并运行-tensorflow">九、启动并运行 TensorFlow</h1>
<p>TensorFlow 是一款用于数值计算的强大的开源软件库，特别适用于大规模机器学习的微调。 它的基本原理很简单：首先在 Python 中定义要执行的计算图（例如图 9-1），然后 TensorFlow 使用该图并使用优化的 C++ 代码高效运行该图。</p>
<p><a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_9/9-1.png" data-uk-lightbox><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_9/9-1.png" alt=""></a></p>
<p>最重要的是，Tensorflow 可以将图分解为多个块并在多个 CPU 或 GPU 上并行运行（如图 9-2 所示）。 TensorFlow 还支持分布式计算，因此您可以在数百台服务器上分割计算，从而在合理的时间内在庞大的训练集上训练庞大的神经网络（请参阅第 12 章）。 TensorFlow 可以训练一个拥有数百万个参数的网络，训练集由数十亿个具有数百万个特征的实例组成。 这应该不会让您吃惊，因为 TensorFlow 是 由Google 大脑团队开发的，它支持谷歌的大量服务，例如 Google Cloud Speech，Google Photos 和 Google Search。</p>
<p><a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_9/9-2.png" data-uk-lightbox><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_9/9-2.png" alt=""></a></p>
<p>当 TensorFlow 于 2015 年 11 月开放源代码时，已有许多深度学习的流行开源库（表 9-1 列出了一些），公平地说，大部分 TensorFlow 的功能已经存在于一个库或另一个库中。 尽管如此，TensorFlow 的整洁设计，可扩展性，灵活性和出色的文档（更不用说谷歌的名字）迅速将其推向了榜首。 简而言之，TensorFlow 的设计灵活性，可扩展性和生产就绪性，现有框架可以说只有其中三种可用。 这里有一些 TensorFlow 的亮点：</p>
<ul>
<li> <p>它不仅在 Windows，Linux 和 MacOS 上运行，而且在移动设备上运行，包括 iOS 和 Android。</p> <p>它提供了一个非常简单的 Python API，名为 TF.Learn2（<code>tensorflow.con trib.learn</code>），与 Scikit-Learn 兼容。正如你将会看到的，你可以用几行代码来训练不同类型的神经网络。之前是一个名为 Scikit Flow（或 Skow）的独立项目。</p> </li>
<li> <p>它还提供了另一个简单的称为 TF-slim（<code>tensorflow.contrib.slim</code>）的 API 来简化构建，训练和求出神经网络。</p> </li>
<li> <p>其他几个高级 API 已经在 TensorFlow 之上独立构建，如 <strong>Keras</strong> 或 <strong>Pretty Tensor</strong>。</p> </li>
<li> <p>它的主要 Python API 提供了更多的灵活性（以更高复杂度为代价）来创建各种计算，包括任何你能想到的神经网络结构。</p> </li>
<li> <p>它包括许多 ML 操作的高效 C ++ 实现，特别是构建神经网络所需的 C++ 实现。还有一个 C++ API 来定义您自己的高性能操作。</p> </li>
<li> <p>它提供了几个高级优化节点来搜索最小化损失函数的参数。由于 TensorFlow 自动处理计算您定义的函数的梯度，因此这些非常易于使用。这称为自动分解（或<code>autodi</code>）。</p> </li>
<li> <p>它还附带一个名为 TensorBoard 的强大可视化工具，可让您浏览计算图表，查看学习曲线等。</p> </li>
<li> <p>Google 还推出了云服务来运行 TensorFlow 表。</p> </li>
<li> <p>最后，它拥有一支充满热情和乐于助人的开发团队，以及一个不断成长的社区，致力于改善它。它是 GitHub 上最受欢迎的开源项目之一，并且越来越多的优秀项目正在构建之上（例如，查看 <a href="https://www.tensorflow.org/">https://www.tensorflow.org/</a> 或 <a href="https://github.com/jtoy/awesome-tensorflow">https://github.com/jtoy/awesome-tensorflow</a>）。 要问技术问题，您应该使用 <a href="http://stackoverflow.com/">http://stackoverflow.com/</a> 并用<code>tensorflow</code>标记您的问题。您可以通过 GitHub 提交错误和功能请求。有关一般讨论，请加入 <strong>Google 小组</strong>。</p> </li>
</ul>
<p><a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_9/Tbale 9-1.png" data-uk-lightbox><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_9/Tbale 9-1.png" alt=""></a></p>
<p>在本章中，我们将介绍 TensorFlow 的基础知识，从安装到创建，运行，保存和可视化简单的计算图。 在构建第一个神经网络之前掌握这些基础知识很重要（我们将在下一章中介绍）。</p>
<h2 id="安装">安装</h2>
<p>让我们开始吧！假设您按照第 2 章中的安装说明安装了 Jupyter 和 Scikit-Learn，您可以简单地使用<code>pip</code>来安装 TensorFlow。 如果你使用<code>virtualenv</code>创建了一个独立的环境，你首先需要激活它：</p>
<pre>
$ cd $ML_PATH                            #Your ML working directory(e.g., $HOME/ml)
$ source env/bin/activate
</pre>
<p>下一步，安装 Tensorflow。</p>
<pre><code>$ pip3 install --upgrade tensorflow
</code></pre>
<p>对于 GPU 支持，你需要安装<code>tensorflow-gpu</code>而不是<code>tensorflow</code>。具体请参见 12 章内容。</p>
<p>为了测试您的安装，请输入一下命令。其输出应该是您安装的 Tensorflow 的版本号。</p>
<pre><code>$ python -c 'import tensorflow; print(tensorflow.__version__)'
1.0.0
</code></pre>
<h2 id="创造第一个图谱然后运行它">创造第一个图谱，然后运行它</h2>
<pre><code class="language-python">import&nbsp;tensorflow&nbsp;as&nbsp;tf&nbsp;&nbsp;
x&nbsp;=&nbsp;tf.Variable(3,&nbsp;name="x")&nbsp;&nbsp;
y&nbsp;=&nbsp;tf.Variable(4,&nbsp;name="y")&nbsp;&nbsp;
f&nbsp;=&nbsp;x*x*y&nbsp;+&nbsp;y&nbsp;+&nbsp;2&nbsp;&nbsp;
</code></pre>
<p>这就是它的一切！ 最重要的是要知道这个代码实际上并不执行任何计算，即使它看起来像(尤其是最后一行）。 它只是创建一个计算图谱。 事实上，变量都没有初始化.要求出此图，您需要打开一个 TensorFlow 会话并使用它初始化变量并求出<code>f</code>。TensorFlow 会话负责处理在诸如 CPU 和 GPU 之类的设备上的操作并运行它们，并且它保留所有变量值。以下代码创建一个会话，初始化变量，并求出<code>f</code>，然后关闭会话（释放资源）：</p>
<pre><code class="language-python">#&nbsp;way1&nbsp;&nbsp;
sess&nbsp;=&nbsp;tf.Session()&nbsp;&nbsp;
sess.run(x.initializer)&nbsp;&nbsp;
sess.run(y.initializer)&nbsp;&nbsp;
result&nbsp;=&nbsp;sess.run(f)&nbsp;&nbsp;
&nbsp;&nbsp;
print(result)&nbsp;&nbsp;
sess.close()&nbsp;&nbsp;
</code></pre>
<p>不得不每次重复sess.run() 有点麻烦，但幸运的是有一个更好的方法：</p>
<pre><code class="language-python"># way2&nbsp;&nbsp;
with&nbsp;tf.Session()&nbsp;as&nbsp;sess:&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;x.initializer.run()&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;y.initializer.run()&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;result&nbsp;=&nbsp;f.eval()&nbsp;&nbsp;
print(result)&nbsp;
</code></pre>
<p>在<code>with</code>块中，会话被设置为默认会话。 调用<code>x.initializer.run()</code>等效于调用<code>tf.get_default_session().run(x.initial)</code>，<code>f.eval()</code>等效于调用<code>tf.get_default_session().run(f)</code>。 这使得代码更容易阅读。 此外，会话在块的末尾自动关闭。</p>
<p>你可以使用<code>global_variables_initializer()</code> 函数，而不是手动初始化每个变量。 请注意，它实际上没有立即执行初始化，而是在图谱中创建一个当程序运行时所有变量都会初始化的节点：</p>
<pre><code class="language-python"># way3&nbsp;&nbsp;
#&nbsp;init&nbsp;=&nbsp;tf.global_variables_initializer()&nbsp;&nbsp;
#&nbsp;with&nbsp;tf.Session()&nbsp;as&nbsp;sess:&nbsp;&nbsp;
#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;init.run()&nbsp;&nbsp;
#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;result&nbsp;=&nbsp;f.eval()&nbsp;&nbsp;
6. #&nbsp;print(result)&nbsp;&nbsp;
</code></pre>
<p>在 Jupyter 内部或在 Python shell 中，您可能更喜欢创建一个<code>InteractiveSession</code>。 与常规会话的唯一区别是，当创建<code>InteractiveSession</code>时，它将自动将其自身设置为默认会话，因此您不需要使用模块（但是您需要在完成后手动关闭会话）：</p>
<pre><code class="language-python"># way4&nbsp;&nbsp;
init&nbsp;=&nbsp;tf.global_variables_initializer()&nbsp;&nbsp;
sess&nbsp;=&nbsp;tf.InteractiveSession()&nbsp;&nbsp;
init.run()&nbsp;&nbsp;
result&nbsp;=&nbsp;f.eval()&nbsp;&nbsp;
print(result)&nbsp;&nbsp;
sess.close()&nbsp;&nbsp;
</code></pre>
<p>TensorFlow 程序通常分为两部分：第一部分构建计算图谱（这称为构造阶段），第二部分运行它（这是执行阶段）。 建设阶段通常构建一个表示 ML 模型的计算图谱,然后对其进行训练,计算。 执行阶段通常运行循环，重复地求出训练步骤（例如，每个小批次），逐渐改进模型参数。&nbsp;</p>
<h2 id="管理图谱">管理图谱</h2>
<p>您创建的任何节点都会自动添加到默认图形中：</p>
<pre><code class="language-python">&gt;&gt;&gt;&nbsp;x1&nbsp;=&nbsp;tf.Variable(1)&nbsp;&nbsp;
&gt;&gt;&gt;&nbsp;x1.graph&nbsp;is&nbsp;tf.get_default_graph()&nbsp;&nbsp;
True&nbsp;&nbsp;
</code></pre>
<p>在大多数情况下，这是很好的，但有时您可能需要管理多个独立图形。 您可以通过创建一个新的图形并暂时将其设置为一个块中的默认图形，如下所示：</p>
<pre><code class="language-python">&gt;&gt;&gt;&nbsp;graph&nbsp;=&nbsp;tf.Graph()&nbsp;&nbsp;
&gt;&gt;&gt;&nbsp;with&nbsp;graph.as_default():&nbsp;&nbsp;
...&nbsp;x2&nbsp;=&nbsp;tf.Variable(2)&nbsp;&nbsp;
...&nbsp;&nbsp;
&gt;&gt;&gt;&nbsp;x2.graph&nbsp;is&nbsp;graph&nbsp;&nbsp;
True&nbsp;&nbsp;
&gt;&gt;&gt;&nbsp;x2.graph&nbsp;is&nbsp;tf.get_default_graph()&nbsp;&nbsp;
False&nbsp;&nbsp;
</code></pre>
<p>在 Jupyter（或 Python shell）中，通常在实验时多次运行相同的命令。 因此，您可能会收到包含许多重复节点的默认图形。 一个解决方案是重新启动 Jupyter 内核（或 Python shell），但是一个更方便的解决方案是通过运行<code>tf.reset_default_graph()</code>来重置默认图。</p>
<h2 id="节点值的生命周期">节点值的生命周期</h2>
<p>求出节点时，TensorFlow 会自动确定所依赖的节点集，并首先求出这些节点。 例如，考虑以下代码：</p>
<pre><code class="language-python">#&nbsp;w&nbsp;=&nbsp;tf.constant(3)&nbsp;&nbsp;
#&nbsp;x&nbsp;=&nbsp;w&nbsp;+&nbsp;2&nbsp;&nbsp;
#&nbsp;y&nbsp;=&nbsp;x&nbsp;+&nbsp;5&nbsp;&nbsp;
#&nbsp;z&nbsp;=&nbsp;x&nbsp;*&nbsp;3&nbsp;&nbsp;
&nbsp;&nbsp;
#&nbsp;with&nbsp;tf.Session()&nbsp;as&nbsp;sess:&nbsp;&nbsp;
#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(y.eval())&nbsp;&nbsp;
#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(z.eval())&nbsp;&nbsp;
</code></pre>
<p>首先，这个代码定义了一个非常简单的图。然后，它启动一个会话并运行图来求出<code>y</code>：TensorFlow 自动检测到<code>y</code>取决于<code>x</code>，它取决于<code>w</code>，所以它首先求出<code>w</code>，然后<code>x</code>，然后<code>y</code>，并返回<code>y</code>的值。最后，代码运行图来求出<code>z</code>。同样，TensorFlow 检测到它必须首先求出<code>w</code>和<code>x</code>。重要的是要注意，它不会复用以前的w和x的求出结果。简而言之，前面的代码求出<code>w</code>和<code>x</code>两次。 所有节点值都在图运行之间删除，除了变量值，由会话跨图形运行维护（队列和读者也保持一些状态）。变量在其初始化程序运行时启动其生命周期，并且在会话关闭时结束。 如果要有效地求出<code>y</code>和<code>z</code>，而不像之前的代码那样求出<code>w</code>和<code>x</code>两次，那么您必须要求 TensorFlow 在一个图形运行中求出<code>y</code>和<code>z</code>，如下面的代码所示：</p>
<pre><code class="language-python">#&nbsp;with&nbsp;tf.Session()&nbsp;as&nbsp;sess:&nbsp;&nbsp;
#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;y_val,&nbsp;z_val&nbsp;=&nbsp;sess.run([y,&nbsp;z])&nbsp;&nbsp;
#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(y_val)&nbsp;#&nbsp;10&nbsp;&nbsp;
#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(z_val)&nbsp;#&nbsp;15&nbsp;&nbsp;
</code></pre>
<p>在单进程 TensorFlow 中，多个会话不共享任何状态，即使它们复用同一个图（每个会话都有自己的每个变量的副本）。 在分布式 TensorFlow 中，变量状态存储在服务器上，而不是在会话中，因此多个会话可以共享相同的变量。</p>
<h2 id="linear-regression-with-tensorflow">Linear Regression with TensorFlow</h2>
<p>TensorFlow 操作（也简称为 ops）可以采用任意数量的输入并产生任意数量的输出。 例如，加法运算和乘法运算都需要两个输入并产生一个输出。 常量和变量不输入（它们被称为源操作）。 输入和输出是称为张量的多维数组（因此称为“tensor flow”）。 就像 NumPy 数组一样，张量具有类型和形状。 实际上，在 Python API 中，张量简单地由 NumPy<code>ndarray</code>表示。 它们通常包含浮点数，但您也可以使用它们来传送字符串（任意字节数组）。</p>
<p>迄今为止的示例，张量只包含单个标量值，但是当然可以对任何形状的数组执行计算。例如，以下代码操作二维数组来对加利福尼亚房屋数据集进行线性回归（在第 2 章中介绍）。它从获取数据集开始；之后它会向所有训练实例添加一个额外的偏置输入特征（<code>x0 = 1</code>）（它使用 NumPy 进行，因此立即运行）；之后它创建两个 TensorFlow 常量节点<code>X</code>和<code>y</code>来保存该数据和目标，并且它使用 TensorFlow 提供的一些矩阵运算来定义<code>theta</code>。这些矩阵函数<code>transpose()</code>，<code>matmul()</code>和<code>matrix_inverse()</code>是不言自明的，但是像往常一样，它们不会立即执行任何计算；相反，它们会在图形中创建在运行图形时执行它们的节点。您可以认识到<code>θ</code>的定义对应于方程 <a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_9/o-9-1.png" data-uk-lightbox><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_9/o-9-1.png" alt=""></a>。</p>
<p>最后，代码创建一个<code>session</code>并使用它来求出<code>theta</code>。</p>
<pre><code class="language-python">import&nbsp;numpy&nbsp;as&nbsp;np&nbsp;&nbsp;
from&nbsp;sklearn.datasets&nbsp;import&nbsp;fetch_california_housing&nbsp;&nbsp;
housing&nbsp;=&nbsp;fetch_california_housing()&nbsp;&nbsp;
m,&nbsp;n&nbsp;=&nbsp;housing.data.shape&nbsp;&nbsp;
#np.c_按colunm来组合array&nbsp;&nbsp;
housing_data_plus_bias&nbsp;=&nbsp;np.c_[np.ones((m,&nbsp;1)),&nbsp;housing.data]&nbsp;&nbsp;
&nbsp;&nbsp;
X&nbsp;=&nbsp;tf.constant(housing_data_plus_bias,&nbsp;dtype=tf.float32,&nbsp;name="X")&nbsp;&nbsp;
y&nbsp;=&nbsp;tf.constant(housing.target.reshape(-1,&nbsp;1),&nbsp;dtype=tf.float32,&nbsp;name="y")&nbsp;&nbsp;
XT =&nbsp;tf.transpose(X)&nbsp;&nbsp;
theta&nbsp;=&nbsp;tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT,&nbsp;X)),&nbsp;XT),&nbsp;y)&nbsp;&nbsp;
with&nbsp;tf.Session()&nbsp;as&nbsp;sess:&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;theta_value&nbsp;=&nbsp;theta.eval()&nbsp;&nbsp;
print(theta_value)&nbsp;&nbsp;
</code></pre>
<p>如果您有一个 GPU 的话，上述代码相较于直接使用 NumPy 计算正态方程式的主要优点是 TensorFlow 会自动运行在您的 GPU 上（如果您安装了支持 GPU 的 TensorFlow，则 TensorFlow 将自动运行在 GPU 上，请参阅第 12 章了解更多详细信息）。</p>
<p>其实这里就是用最小二乘法算<code>θ</code></p>
<p><a href="http://blog.csdn.net/akon_wang_hkbu/article/details/77503725">http://blog.csdn.net/akon_wang_hkbu/article/details/77503725</a></p>
<h2 id="实现梯度下降">实现梯度下降</h2>
<p>让我们尝试使用批量梯度下降（在第 4 章中介绍），而不是普通方程。 首先，我们将通过手动计算梯度来实现，然后我们将使用 TensorFlow 的自动扩展功能来使 TensorFlow 自动计算梯度，最后我们将使用几个 TensorFlow 的优化器。</p>
<p>当使用梯度下降时，请记住，首先要对输入特征向量进行归一化，否则训练可能要慢得多。 您可以使用 TensorFlow，NumPy，Scikit-Learn 的<code>StandardScaler</code>或您喜欢的任何其他解决方案。 以下代码假定此规范化已经完成。</p>
<h2 id="手动计算渐变">手动计算渐变</h2>
<p>以下代码应该是相当不言自明的，除了几个新元素：</p>
<ul>
<li><code>random_uniform()</code>函数在图形中创建一个节点，它将生成包含随机值的张量，给定其形状和值作用域，就像 NumPy 的<code>rand()</code>函数一样。</li>
<li><code>assign()</code>函数创建一个为变量分配新值的节点。 在这种情况下，它实现了批次梯度下降步骤 <a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-5c180c9b9130b5dacda464ca73ee8f1e.gif" data-uk-lightbox><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-5c180c9b9130b5dacda464ca73ee8f1e.gif" alt="\theta(next step)= \theta - \eta  \nabla_{\theta}MSE(\theta)"></a>。</li>
<li>主循环一次又一次（共<code>n_epochs</code>次）执行训练步骤，每 100 次迭代都打印出当前均方误差（MSE）。 你应该看到 MSE 在每次迭代中都会下降。</li>
</ul>
<pre><code class="language-python">housing&nbsp;=&nbsp;fetch_california_housing()&nbsp;&nbsp;
m,&nbsp;n&nbsp;=&nbsp;housing.data.shape&nbsp;&nbsp;
m,&nbsp;n&nbsp;=&nbsp;housing.data.shape&nbsp;&nbsp;
#np.c_按colunm来组合array&nbsp;&nbsp;
housing_data_plus_bias&nbsp;=&nbsp;np.c_[np.ones((m,&nbsp;1)),&nbsp;housing.data]&nbsp;&nbsp;
scaled_housing_data_plus_bias&nbsp;=&nbsp;scale(housing_data_plus_bias)&nbsp;&nbsp;
n_epochs&nbsp;=&nbsp;1000&nbsp;&nbsp;
learning_rate&nbsp;=&nbsp;0.01&nbsp;&nbsp;
X&nbsp;=&nbsp;tf.constant(scaled_housing_data_plus_bias,&nbsp;dtype=tf.float32,&nbsp;name="X")&nbsp;&nbsp;
y&nbsp;=&nbsp;tf.constant(housing.target.reshape(-1,&nbsp;1),&nbsp;dtype=tf.float32,&nbsp;name="y")&nbsp;&nbsp;
theta&nbsp;=&nbsp;tf.Variable(tf.random_uniform([n&nbsp;+&nbsp;1,&nbsp;1],&nbsp;-1.0,&nbsp;1.0),&nbsp;name="theta")&nbsp;&nbsp;
y_pred&nbsp;=&nbsp;tf.matmul(X,&nbsp;theta,&nbsp;name="predictions")&nbsp;&nbsp;
error&nbsp;=&nbsp;y_pred&nbsp;-&nbsp;y&nbsp;&nbsp;
mse&nbsp;=&nbsp;tf.reduce_mean(tf.square(error),&nbsp;name="mse")&nbsp;&nbsp;
gradients&nbsp;=&nbsp;2/m&nbsp;*&nbsp;tf.matmul(tf.transpose(X),&nbsp;error)&nbsp;&nbsp;
training_op&nbsp;=&nbsp;tf.assign(theta,&nbsp;theta&nbsp;-&nbsp;learning_rate&nbsp;*&nbsp;gradients)&nbsp;&nbsp;
init&nbsp;=&nbsp;tf.global_variables_initializer()&nbsp;&nbsp;
with&nbsp;tf.Session()&nbsp;as&nbsp;sess:&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;sess.run(init)&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;epoch&nbsp;in&nbsp;range(n_epochs):&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;epoch&nbsp;%&nbsp;100&nbsp;==&nbsp;0:&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print("Epoch",&nbsp;epoch,&nbsp;"MSE&nbsp;=",&nbsp;mse.eval())&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sess.run(training_op)&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;best_theta&nbsp;=&nbsp;theta.eval()&nbsp;&nbsp;
</code></pre>
<h2 id="using-autodiﬀ">Using autodiﬀ</h2>
<p>前面的代码工作正常，但它需要从代价函数（MSE）中利用数学公式推导梯度。 在线性回归的情况下，这是相当容易的，但是如果你必须用深层神经网络来做这个事情，你会感到头痛：这将是乏味和容易出错的。 您可以使用符号求导来为您自动找到偏导数的方程式，但结果代码不一定非常有效。</p>
<p>为了理解为什么，考虑函数<code>f(x) = exp(exp(exp(x)))</code>。如果你知道微积分，你可以计算出它的导数<code>f'(x) = exp(x) * exp(exp(x)) * exp(exp(exp(x)))</code>。如果您按照普通的计算方式分别去写<code>f(x)</code>和<code>f'(x)</code>，您的代码将不会如此有效。 一个更有效的解决方案是写一个首先计算<code>exp(x)</code>，然后<code>exp(exp(x))</code>，然后<code>exp(exp(exp(x)))</code>的函数，并返回所有三个。这直接给你（第三项）<code>f(x)</code>，如果你需要求导，你可以把这三个子式相乘，你就完成了。 通过传统的方法，您不得不将<code>exp</code>函数调用 9 次来计算<code>f(x)</code>和<code>f'(x)</code>。 使用这种方法，你只需要调用它三次。</p>
<p>当您的功能由某些任意代码定义时，它会变得更糟。 你能找到方程（或代码）来计算以下函数的偏导数吗？</p>
<p>提示：不要尝试。</p>
<pre><code class="language-python">def&nbsp;my_func(a,&nbsp;b):&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;z&nbsp;=&nbsp;0&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;i&nbsp;in&nbsp;range(100):&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;    z&nbsp;=&nbsp;a&nbsp;*&nbsp;np.cos(z&nbsp;+&nbsp;i)&nbsp;+&nbsp;z&nbsp;*&nbsp;np.sin(b&nbsp;-&nbsp;i)&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;z&nbsp;&nbsp;
</code></pre>
<p>幸运的是，TensorFlow 的自动计算梯度功能可以计算这个公式：它可以自动高效地为您计算梯度。 只需用以下面这行代码替换上一节中代码的<code>gradients = ...</code>行，代码将继续工作正常：</p>
<pre><code class="language-python">gradients&nbsp;=&nbsp;tf.gradients(mse,&nbsp;[theta])[0]&nbsp;&nbsp;
</code></pre>
<p><code>gradients()</code>函数使用一个<code>op</code>（在这种情况下是MSE）和一个变量列表（在这种情况下只是<code>theta</code>），它创建一个<code>ops</code>列表（每个变量一个）来计算<code>op</code>的梯度变量。 因此，梯度节点将计算 MSE 相对于<code>theta</code>的梯度向量。</p>
<p>自动计算梯度有四种主要方法。 它们总结在表 9-2 中。 TensorFlow 使用反向模式，这是完美的（高效和准确），当有很多输入和少量的输出，如通常在神经网络的情况。 它只需要通过 <a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-378b3eaa9c01f52bc8987807984a5a88.gif" data-uk-lightbox><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/tex-378b3eaa9c01f52bc8987807984a5a88.gif" alt="n_{outputs} + 1"></a> 次图遍历即可计算所有输出的偏导数。</p>
<p><a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF//images/chapter_9/20171030144740842.jpg" data-uk-lightbox><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF//images/chapter_9/20171030144740842.jpg" alt=""></a></p>
<h2 id="使用优化器">使用优化器</h2>
<p>所以 TensorFlow 为您计算梯度。 但它还有更好的方法：它还提供了一些可以直接使用的优化器，包括梯度下降优化器。您可以使用以下代码简单地替换以前的<code>gradients = ...</code>和<code>training_op = ...</code>行，并且一切都将正常工作：</p>
<pre><code class="language-python">optimizer&nbsp;=&nbsp;tf.train.GradientDescentOptimizer(learning_rate=learning_rate)&nbsp;&nbsp;
training_op&nbsp;=&nbsp;optimizer.minimize(mse)&nbsp;&nbsp;
</code></pre>
<p>如果要使用其他类型的优化器，则只需要更改一行。 例如，您可以通过定义优化器来使用动量优化器（通常会比渐变收敛的收敛速度快得多；参见第 11 章）</p>
<pre><code class="language-python">optimizer&nbsp;=&nbsp;tf.train.MomentumOptimizer(learning_rate=learning_rate,&nbsp;momentum=0.9)&nbsp;&nbsp;
</code></pre>
<h2 id="将数据提供给训练算法">将数据提供给训练算法</h2>
<p>我们尝试修改以前的代码来实现小批量梯度下降（Mini-batch Gradient Descent）。 为此，我们需要一种在每次迭代时用下一个小批量替换<code>X</code>和<code>Y</code>的方法。 最简单的方法是使用占位符（placeholder）节点。 这些节点是特别的，因为它们实际上并不执行任何计算，只是输出您在运行时输出的数据。 它们通常用于在训练期间将训练数据传递给 TensorFlow。 如果在运行时没有为占位符指定值，则会收到异常。</p>
<p>要创建占位符节点，您必须调用<code>placeholder()</code>函数并指定输出张量的数据类型。 或者，您还可以指定其形状，如果要强制执行。 如果指定维度为<code>None</code>，则表示“任何大小”。例如，以下代码创建一个占位符节点<code>A</code>，还有一个节点<code>B = A + 5</code>。当我们求出<code>B</code>时，我们将一个<code>feed_dict</code>传递给<code>eval()</code>方法并指定<code>A</code>的值。注意，<code>A</code>必须具有 2 级（即它必须是二维的），并且必须有三列（否则引发异常），但它可以有任意数量的行。</p>
<pre><code class="language-python">&gt;&gt;&gt;&nbsp;A&nbsp;=&nbsp;tf.placeholder(tf.float32,&nbsp;shape=(None,&nbsp;3))&nbsp;&nbsp;
&gt;&gt;&gt;&nbsp;B&nbsp;=&nbsp;A&nbsp;+&nbsp;5&nbsp;&nbsp;
&gt;&gt;&gt;&nbsp;with&nbsp;tf.Session()&nbsp;as&nbsp;sess:&nbsp;&nbsp;
...&nbsp;B_val_1&nbsp;=&nbsp;B.eval(feed_dict={A:&nbsp;[[1,&nbsp;2,&nbsp;3]]})&nbsp;&nbsp;
...&nbsp;B_val_2&nbsp;=&nbsp;B.eval(feed_dict={A:&nbsp;[[4,&nbsp;5,&nbsp;6],&nbsp;[7,&nbsp;8,&nbsp;9]]})&nbsp;&nbsp;
...&nbsp;&nbsp;
&gt;&gt;&gt;&nbsp;print(B_val_1)&nbsp;&nbsp;
[[&nbsp;6.&nbsp;7.&nbsp;8.]]&nbsp;&nbsp;
&gt;&gt;&gt;&nbsp;print(B_val_2)&nbsp;&nbsp;
[[&nbsp;9.&nbsp;10.&nbsp;11.]&nbsp;&nbsp;
[&nbsp;12.&nbsp;13.&nbsp;14.]]&nbsp;&nbsp;
</code></pre>
<p>您实际上可以提供任何操作的输出，而不仅仅是占位符。 在这种情况下，TensorFlow 不会尝试求出这些操作；它使用您提供的值。</p>
<p>要实现小批量渐变下降，我们只需稍微调整现有的代码。 首先更改<code>X</code>和<code>Y</code>的定义，使其定义为占位符节点：</p>
<pre><code class="language-python">X&nbsp;=&nbsp;tf.placeholder(tf.float32,&nbsp;shape=(None,&nbsp;n&nbsp;+&nbsp;1),&nbsp;name="X")&nbsp;&nbsp;
y&nbsp;=&nbsp;tf.placeholder(tf.float32,&nbsp;shape=(None,&nbsp;1),&nbsp;name="y")&nbsp;&nbsp;
</code></pre>
<p>然后定义批量大小并计算总批次数：</p>
<pre><code class="language-python">batch_size&nbsp;=&nbsp;100&nbsp;&nbsp;
n_batches&nbsp;=&nbsp;int(np.ceil(m&nbsp;/&nbsp;batch_size))&nbsp;&nbsp;
</code></pre>
<p>最后，在执行阶段，逐个获取小批量，然后在求出依赖于<code>X</code>和<code>y</code>的值的任何一个节点时，通过<code>feed_dict</code>提供<code>X</code>和<code>y</code>的值。</p>
<pre><code class="language-python">def fetch_batch(epoch, batch_index, batch_size):
    [...] # load the data from disk
    return X_batch, y_batch

with tf.Session() as sess:
    sess.run(init)

    for epoch in range(n_epochs):
        for batch_index in range(n_batches):
            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)
            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})
    best_theta = theta.eval()
</code></pre>
<p>在求出theta时，我们不需要传递X和y的值，因为它不依赖于它们。</p>
<h2 id="mini-batch-完整代码">MINI-BATCH 完整代码</h2>
<pre><code class="language-python">import&nbsp;numpy&nbsp;as&nbsp;np&nbsp;&nbsp;
from&nbsp;sklearn.datasets&nbsp;import&nbsp;fetch_california_housing&nbsp;&nbsp;
import&nbsp;tensorflow&nbsp;as&nbsp;tf&nbsp;&nbsp;
from&nbsp;sklearn.preprocessing&nbsp;import&nbsp;StandardScaler&nbsp;&nbsp;
&nbsp;&nbsp;
housing&nbsp;=&nbsp;fetch_california_housing()&nbsp;&nbsp;
m,&nbsp;n&nbsp;=&nbsp;housing.data.shape&nbsp;&nbsp;
print("数据集:{}行,{}列".format(m,n))&nbsp;&nbsp;
housing_data_plus_bias&nbsp;=&nbsp;np.c_[np.ones((m,&nbsp;1)),&nbsp;housing.data]&nbsp;&nbsp;
scaler&nbsp;=&nbsp;StandardScaler()&nbsp;&nbsp;
scaled_housing_data&nbsp;=&nbsp;scaler.fit_transform(housing.data)&nbsp;&nbsp;
scaled_housing_data_plus_bias&nbsp;=&nbsp;np.c_[np.ones((m,&nbsp;1)),&nbsp;scaled_housing_data]&nbsp;&nbsp;
&nbsp;&nbsp;
n_epochs&nbsp;=&nbsp;1000&nbsp;&nbsp;
learning_rate&nbsp;=&nbsp;0.01&nbsp;&nbsp;
&nbsp;&nbsp;
X&nbsp;=&nbsp;tf.placeholder(tf.float32,&nbsp;shape=(None,&nbsp;n&nbsp;+&nbsp;1),&nbsp;name="X")&nbsp;&nbsp;
y&nbsp;=&nbsp;tf.placeholder(tf.float32,&nbsp;shape=(None,&nbsp;1),&nbsp;name="y")&nbsp;&nbsp;
theta&nbsp;=&nbsp;tf.Variable(tf.random_uniform([n&nbsp;+&nbsp;1,&nbsp;1],&nbsp;-1.0,&nbsp;1.0,&nbsp;seed=42),&nbsp;name="theta")&nbsp;&nbsp;
y_pred&nbsp;=&nbsp;tf.matmul(X,&nbsp;theta,&nbsp;name="predictions")&nbsp;&nbsp;
error&nbsp;=&nbsp;y_pred&nbsp;-&nbsp;y&nbsp;&nbsp;
mse&nbsp;=&nbsp;tf.reduce_mean(tf.square(error),&nbsp;name="mse")&nbsp;&nbsp;
optimizer&nbsp;=&nbsp;tf.train.GradientDescentOptimizer(learning_rate=learning_rate)&nbsp;&nbsp;
training_op&nbsp;=&nbsp;optimizer.minimize(mse)&nbsp;&nbsp;
&nbsp;&nbsp;
init&nbsp;=&nbsp;tf.global_variables_initializer()&nbsp;&nbsp;
&nbsp;&nbsp;
n_epochs&nbsp;=&nbsp;10&nbsp;&nbsp;
batch_size&nbsp;=&nbsp;100&nbsp;&nbsp;
n_batches&nbsp;=&nbsp;int(np.ceil(m&nbsp;/&nbsp;batch_size))&nbsp;# ceil() 方法返回 x 的值上限&nbsp;-&nbsp;不小于 x 的最小整数。&nbsp;&nbsp;
&nbsp;&nbsp;
def&nbsp;fetch_batch(epoch,&nbsp;batch_index,&nbsp;batch_size):&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;know&nbsp;=&nbsp;np.random.seed(epoch&nbsp;*&nbsp;n_batches&nbsp;+&nbsp;batch_index)&nbsp;&nbsp;#&nbsp;not&nbsp;shown&nbsp;in&nbsp;the&nbsp;book&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;print("我是know:",know)&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;indices&nbsp;=&nbsp;np.random.randint(m,&nbsp;size=batch_size)&nbsp;&nbsp;#&nbsp;not&nbsp;shown&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;X_batch&nbsp;=&nbsp;scaled_housing_data_plus_bias[indices]&nbsp;#&nbsp;not&nbsp;shown&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;y_batch&nbsp;=&nbsp;housing.target.reshape(-1,&nbsp;1)[indices]&nbsp;#&nbsp;not&nbsp;shown&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;X_batch,&nbsp;y_batch&nbsp;&nbsp;
&nbsp;&nbsp;
with&nbsp;tf.Session()&nbsp;as&nbsp;sess:&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;sess.run(init)&nbsp;&nbsp;
&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;epoch&nbsp;in&nbsp;range(n_epochs):&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;batch_index&nbsp;in&nbsp;range(n_batches):&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;X_batch,&nbsp;y_batch&nbsp;=&nbsp;fetch_batch(epoch,&nbsp;batch_index,&nbsp;batch_size)&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sess.run(training_op,&nbsp;feed_dict={X:&nbsp;X_batch,&nbsp;y:&nbsp;y_batch})&nbsp;&nbsp;
&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;best_theta&nbsp;=&nbsp;theta.eval()&nbsp;&nbsp;
&nbsp;&nbsp;
print(best_theta)&nbsp;&nbsp;
</code></pre>
<h2 id="保存和恢复模型">保存和恢复模型</h2>
<p>一旦你训练了你的模型，你应该把它的参数保存到磁盘，所以你可以随时随地回到它，在另一个程序中使用它，与其他模型比较，等等。 此外，您可能希望在训练期间定期保存检查点，以便如果您的计算机在训练过程中崩溃，您可以从上次检查点继续进行，而不是从头开始。</p>
<p>TensorFlow 可以轻松保存和恢复模型。 只需在构造阶段结束（创建所有变量节点之后）创建一个保存节点; 那么在执行阶段，只要你想保存模型，只要调用它的<code>save()</code>方法:</p>
<pre><code class="language-python">[...]
theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name="theta")
[...]
init = tf.global_variables_initializer()
saver = tf.train.Saver()
with tf.Session() as sess:
    sess.run(init)

    for epoch in range(n_epochs):
        if epoch % 100 == 0: # checkpoint every 100 epochs
            save_path = saver.save(sess, "../../../tmp/my_model.ckpt")

        sess.run(training_op)
    best_theta = theta.eval()
    save_path = saver.save(sess, "../../../tmp/my_model_final.ckpt")
</code></pre>
<p>恢复模型同样容易：在构建阶段结束时创建一个保存器，就像之前一样，但是在执行阶段的开始，而不是使用<code>init</code>节点初始化变量，你可以调用<code>restore()</code>方法 的保存器对象：</p>
<pre><code class="language-python">with tf.Session() as sess:
    saver.restore(sess, "../../../tmp/my_model_final.ckpt")
    [...]
</code></pre>
<p>默认情况下，保存器将以自己的名称保存并还原所有变量，但如果需要更多控制，则可以指定要保存或还原的变量以及要使用的名称。 例如，以下保存器将仅保存或恢复<code>theta</code>变量，它的键名称是<code>weights</code>：</p>
<pre><code>saver = tf.train.Saver({"weights": theta})
</code></pre>
<p>完整代码</p>
<pre><code class="language-python"> numpy as np
from sklearn.datasets import fetch_california_housing
import tensorflow as tf
from sklearn.preprocessing import StandardScaler

housing = fetch_california_housing()
m, n = housing.data.shape
print("数据集:{}行,{}列".format(m,n))
housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]
scaler = StandardScaler()
scaled_housing_data = scaler.fit_transform(housing.data)
scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]

n_epochs = 1000  # not shown in the book
learning_rate = 0.01  # not shown

X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name="X")  # not shown
y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name="y")  # not shown
theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name="theta")
y_pred = tf.matmul(X, theta, name="predictions")  # not shown
error = y_pred - y  # not shown
mse = tf.reduce_mean(tf.square(error), name="mse")  # not shown
optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)  # not shown
training_op = optimizer.minimize(mse)  # not shown

init = tf.global_variables_initializer()
saver = tf.train.Saver()

with tf.Session() as sess:
    sess.run(init)

    for epoch in range(n_epochs):
        if epoch % 100 == 0:
            print("Epoch", epoch, "MSE =", mse.eval())  # not shown
            save_path = saver.save(sess, "../../../tmp/my_model.ckpt")
        sess.run(training_op)

    best_theta = theta.eval()
    save_path = saver.save(sess, "../../../tmp/my_model_final.ckpt") #找到tmp文件夹就找到文件了
</code></pre>
<p>使用 TensorBoard 展现图形和训练曲线</p>
<p>所以现在我们有一个使用小批量梯度下降训练线性回归模型的计算图谱，我们正在定期保存检查点。 听起来很复杂，不是吗？ 然而，我们仍然依靠<code>print()</code>函数可视化训练过程中的进度。 有一个更好的方法：进入 TensorBoard。如果您提供一些训练统计信息，它将在您的网络浏览器中显示这些统计信息的良好交互式可视化（例如学习曲线）。 您还可以提供图形的定义，它将为您提供一个很好的界面来浏览它。 这对于识别图中的错误，找到瓶颈等是非常有用的。</p>
<p>第一步是调整程序，以便将图形定义和一些训练统计信息（例如，<code>training_error</code>（MSE））写入 TensorBoard 将读取的日志目录。 您每次运行程序时都需要使用不同的日志目录，否则 TensorBoard 将会合并来自不同运行的统计信息，这将会混乱可视化。 最简单的解决方案是在日志目录名称中包含时间戳。 在程序开头添加以下代码：</p>
<pre><code class="language-python">from datetime import datetime
now = datetime.utcnow().strftime("%Y%m%d%H%M%S")
root_logdir = "tf_logs"
logdir = "{}/run-{}/".format(root_logdir, now)
</code></pre>
<p>接下来，在构建阶段结束时添加以下代码：</p>
<pre><code class="language-python">mse_summary = tf.summary.scalar('MSE', mse)
file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())
</code></pre>
<p>第一行创建一个节点，这个节点将求出 MSE 值并将其写入 TensorBoard 兼容的二进制日志字符串（称为摘要）中。 第二行创建一个<code>FileWriter</code>，您将用它来将摘要写入日志目录中的日志文件中。 第一个参数指示日志目录的路径（在本例中为<code>tf_logs/run-20160906091959/</code>，相对于当前目录）。 第二个（可选）参数是您想要可视化的图形。 创建时，文件写入器创建日志目录（如果需要），并将其定义在二进制日志文件（称为事件文件）中。</p>
<p>接下来，您需要更新执行阶段，以便在训练期间定期求出<code>mse_summary</code>节点（例如，每 10 个小批量）。 这将输出一个摘要，然后可以使用<code>file_writer</code>写入事件文件。 以下是更新的代码：</p>
<pre><code class="language-python">[...]
for batch_index in range(n_batches):
    X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)
    if batch_index % 10 == 0:
        summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})
        step = epoch * n_batches + batch_index
        file_writer.add_summary(summary_str, step)
    sess.run(training_op, feed_dict={X: X_batch, y: y_batch})
[...]
</code></pre>
<p>避免在每一个训练阶段记录训练数据，因为这会大大减慢训练速度（以上代码每 10 个小批量记录一次）.</p>
<p>最后，要在程序结束时关闭<code>FileWriter</code>：</p>
<pre><code class="language-pytho">file_writer.close()
</code></pre>
<p>完整代码</p>
<pre><code class="language-python">import numpy as np
from sklearn.datasets import fetch_california_housing
import tensorflow as tf
from sklearn.preprocessing import StandardScaler

housing = fetch_california_housing()
m, n = housing.data.shape
print("数据集:{}行,{}列".format(m,n))
housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]
scaler = StandardScaler()
scaled_housing_data = scaler.fit_transform(housing.data)
scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]

from datetime import datetime

now = datetime.utcnow().strftime("%Y%m%d%H%M%S")
root_logdir = r"D://tf_logs"
logdir = "{}/run-{}/".format(root_logdir, now)
n_epochs = 1000
learning_rate = 0.01

X = tf.placeholder(tf.float32, shape=(None, n + 1), name="X")
y = tf.placeholder(tf.float32, shape=(None, 1), name="y")
theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name="theta")
y_pred = tf.matmul(X, theta, name="predictions")
error = y_pred - y
mse = tf.reduce_mean(tf.square(error), name="mse")
optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)
training_op = optimizer.minimize(mse)

init = tf.global_variables_initializer()
mse_summary = tf.summary.scalar('MSE', mse)
file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())

n_epochs = 10
batch_size = 100
n_batches = int(np.ceil(m / batch_size))

def fetch_batch(epoch, batch_index, batch_size):
    np.random.seed(epoch * n_batches + batch_index)  # not shown in the book
    indices = np.random.randint(m, size=batch_size)  # not shown
    X_batch = scaled_housing_data_plus_bias[indices] # not shown
    y_batch = housing.target.reshape(-1, 1)[indices] # not shown
    return X_batch, y_batch

with tf.Session() as sess:                                                        # not shown in the book
    sess.run(init)                                                                # not shown

    for epoch in range(n_epochs):                                                 # not shown
        for batch_index in range(n_batches):
            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)
            if batch_index % 10 == 0:
                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})
                step = epoch * n_batches + batch_index
                file_writer.add_summary(summary_str, step)
            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})

    best_theta = theta.eval()
file_writer.close()
print(best_theta)
</code></pre>
<p><a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_9/o-9-2.png" data-uk-lightbox><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_9/o-9-2.png" alt=""></a></p>
<h2 id="名称作用域">名称作用域</h2>
<p>当处理更复杂的模型（如神经网络）时，该图可以很容易地与数千个节点混淆。 为了避免这种情况，您可以创建名称作用域来对相关节点进行分组。 例如，我们修改以前的代码来定义名为<code>loss</code>的名称作用域内的错误和<code>mse</code>操作：</p>
<pre><code class="language-python">with tf.name_scope("loss") as scope:
    error = y_pred - y
    mse = tf.reduce_mean(tf.square(error), name="mse")
</code></pre>
<p>在作用域内定义的每个<code>op</code>的名称现在以<code>loss/</code>为前缀：</p>
<pre><code class="language-python">&gt;&gt;&gt; print(error.op.name)
loss/sub
&gt;&gt;&gt; print(mse.op.name)
loss/mse
</code></pre>
<p>在 TensorBoard 中，<code>mse</code>和<code>error</code>节点现在出现在<code>loss</code>命名空间中，默认情况下会出现崩溃（图 9-5）。</p>
<p><a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_9/9-5.png" data-uk-lightbox><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_9/9-5.png" alt=""></a></p>
<p>完整代码</p>
<pre><code class="language-python">import numpy as np
from sklearn.datasets import fetch_california_housing
import tensorflow as tf
from sklearn.preprocessing import StandardScaler

housing = fetch_california_housing()
m, n = housing.data.shape
print("数据集:{}行,{}列".format(m,n))
housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]
scaler = StandardScaler()
scaled_housing_data = scaler.fit_transform(housing.data)
scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]

from datetime import datetime

now = datetime.utcnow().strftime("%Y%m%d%H%M%S")
root_logdir = r"D://tf_logs"
logdir = "{}/run-{}/".format(root_logdir, now)

n_epochs = 1000
learning_rate = 0.01

X = tf.placeholder(tf.float32, shape=(None, n + 1), name="X")
y = tf.placeholder(tf.float32, shape=(None, 1), name="y")
theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name="theta")
y_pred = tf.matmul(X, theta, name="predictions")

def fetch_batch(epoch, batch_index, batch_size):
    np.random.seed(epoch * n_batches + batch_index)  # not shown in the book
    indices = np.random.randint(m, size=batch_size)  # not shown
    X_batch = scaled_housing_data_plus_bias[indices] # not shown
    y_batch = housing.target.reshape(-1, 1)[indices] # not shown
    return X_batch, y_batch

with tf.name_scope("loss") as scope:
    error = y_pred - y
    mse = tf.reduce_mean(tf.square(error), name="mse")

optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)
training_op = optimizer.minimize(mse)

init = tf.global_variables_initializer()

mse_summary = tf.summary.scalar('MSE', mse)
file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())

n_epochs = 10
batch_size = 100
n_batches = int(np.ceil(m / batch_size))

with tf.Session() as sess:
    sess.run(init)

    for epoch in range(n_epochs):
        for batch_index in range(n_batches):
            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)
            if batch_index % 10 == 0:
                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})
                step = epoch * n_batches + batch_index
                file_writer.add_summary(summary_str, step)
            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})

    best_theta = theta.eval()

file_writer.flush()
file_writer.close()
print("Best theta:")
print(best_theta)
</code></pre>
<p><a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_9/o-9-3.png" data-uk-lightbox><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_9/o-9-3.png" alt=""></a></p>
<h2 id="模块性">模块性</h2>
<p>假设您要创建一个图，它的作用是将两个整流线性单元（ReLU）的输出值相加。 ReLU 计算一个输入值的对应线性函数输出值，如果为正，则输出该结值，否则为 0，如等式 9-1 所示。</p>
<p><a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_9/e-9-1.png" data-uk-lightbox><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_9/e-9-1.png" alt=""></a></p>
<p>下面的代码做这个工作，但是它是相当重复的：</p>
<pre><code class="language-python">n_features = 3
X = tf.placeholder(tf.float32, shape=(None, n_features), name="X")
w1 = tf.Variable(tf.random_normal((n_features, 1)), name="weights1")
w2 = tf.Variable(tf.random_normal((n_features, 1)), name="weights2")
b1 = tf.Variable(0.0, name="bias1")
b2 = tf.Variable(0.0, name="bias2")
z1 = tf.add(tf.matmul(X, w1), b1, name="z1")
z2 = tf.add(tf.matmul(X, w2), b2, name="z2")
relu1 = tf.maximum(z1, 0., name="relu1")
relu2 = tf.maximum(z1, 0., name="relu2")
output = tf.add(relu1, relu2, name="output")
</code></pre>
<p>这样的重复代码很难维护，容易出错（实际上，这个代码包含了一个剪贴错误，你发现了吗？） 如果你想添加更多的 ReLU，会变得更糟。 幸运的是，TensorFlow 可以让您保持 DRY（不要重复自己）：只需创建一个功能来构建 ReLU。 以下代码创建五个 ReLU 并输出其总和（注意，<code>add_n()</code>创建一个计算张量列表之和的操作）：</p>
<pre><code class="language-python">def relu(X):
    w_shape = (int(X.get_shape()[1]), 1)
    w = tf.Variable(tf.random_normal(w_shape), name="weights")
    b = tf.Variable(0.0, name="bias")
    z = tf.add(tf.matmul(X, w), b, name="z")
    return tf.maximum(z, 0., name="relu")

n_features = 3
X = tf.placeholder(tf.float32, shape=(None, n_features), name="X")
relus = [relu(X) for i in range(5)]
output = tf.add_n(relus, name="output")
</code></pre>
<p>请注意，创建节点时，TensorFlow 将检查其名称是否已存在，如果它已经存在，则会附加一个下划线，后跟一个索引，以使该名称是唯一的。 因此，第一个 ReLU 包含名为<code>weights</code>，<code>bias</code>，<code>z</code>和<code>relu</code>的节点（加上其他默认名称的更多节点，如<code>MatMul</code>）; 第二个 ReLU 包含名为<code>weights_1</code>，<code>bias_1</code>等节点的节点; 第三个 ReLU 包含名为 <code>weights_2</code>，<code>bias_2</code>的节点，依此类推。 TensorBoard 识别这样的系列并将它们折叠在一起以减少混乱（如图 9-6 所示）</p>
<p><a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_9/9-6.png" data-uk-lightbox><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_9/9-6.png" alt=""></a></p>
<p>使用名称作用域，您可以使图形更清晰。 简单地将<code>relu()</code>函数的所有内容移动到名称作用域内。 图 9-7 显示了结果图。 请注意，TensorFlow 还通过附加<code>_1</code>，<code>_2</code>等来提供名称作用域的唯一名称。</p>
<pre><code class="language-python">def relu(X):
    with tf.name_scope("relu"):
        w_shape = (int(X.get_shape()[1]), 1)                          # not shown in the book
        w = tf.Variable(tf.random_normal(w_shape), name="weights")    # not shown
        b = tf.Variable(0.0, name="bias")                             # not shown
        z = tf.add(tf.matmul(X, w), b, name="z")                      # not shown
        return tf.maximum(z, 0., name="max")                          # not shown
</code></pre>
<p><a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_9/9-7.png" data-uk-lightbox><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_9/9-7.png" alt=""></a></p>
<h2 id="共享变量">共享变量</h2>
<p>如果要在图形的各个组件之间共享一个变量，一个简单的选项是首先创建它，然后将其作为参数传递给需要它的函数。 例如，假设要使用所有 ReLU 的共享阈值变量来控制 ReLU 阈值（当前硬编码为 0）。 您可以先创建该变量，然后将其传递给<code>relu()</code>函数：</p>
<pre><code class="language-python">reset_graph()

def relu(X, threshold):
    with tf.name_scope("relu"):
        w_shape = (int(X.get_shape()[1]), 1)                        # not shown in the book
        w = tf.Variable(tf.random_normal(w_shape), name="weights")  # not shown
        b = tf.Variable(0.0, name="bias")                           # not shown
        z = tf.add(tf.matmul(X, w), b, name="z")                    # not shown
        return tf.maximum(z, threshold, name="max")

threshold = tf.Variable(0.0, name="threshold")
X = tf.placeholder(tf.float32, shape=(None, n_features), name="X")
relus = [relu(X, threshold) for i in range(5)]
output = tf.add_n(relus, name="output")
</code></pre>
<p>这很好:现在您可以使用阈值变量来控制所有 ReLU 的阈值。但是，如果有许多共享参数，比如这一项，那么必须一直将它们作为参数传递，这将是非常痛苦的。许多人创建了一个包含模型中所有变量的 Python 字典，并将其传递给每个函数。另一些则为每个模块创建一个类（例如：一个使用类变量来处理共享参数的 ReLU 类）。另一种选择是在第一次调用时将共享变量设置为<code>relu()</code>函数的属性，如下所示:</p>
<pre><code class="language-python">def relu(X):
    with tf.name_scope("relu"):
        if not hasattr(relu, "threshold"):
            relu.threshold = tf.Variable(0.0, name="threshold")
        w_shape = int(X.get_shape()[1]), 1                          # not shown in the book
        w = tf.Variable(tf.random_normal(w_shape), name="weights")  # not shown
        b = tf.Variable(0.0, name="bias")                           # not shown
        z = tf.add(tf.matmul(X, w), b, name="z")                    # not shown
        return tf.maximum(z, relu.threshold, name="max")
</code></pre>
<p>TensorFlow 提供了另一个选项，这将提供比以前的解决方案稍微更清洁和更模块化的代码。首先要明白一点，这个解决方案很刁钻难懂，但是由于它在 TensorFlow 中使用了很多，所以值得我们去深入细节。 这个想法是使用<code>get_variable()</code>函数来创建共享变量，如果它还不存在，或者如果已经存在，则复用它。 所需的行为（创建或复用）由当前<code>variable_scope()</code>的属性控制。 例如，以下代码将创建一个名为<code>relu/threshold</code>的变量（作为标量，因为<code>shape = ()</code>，并使用 0.0 作为初始值）：</p>
<pre><code class="language-python">with tf.variable_scope("relu"):
    threshold = tf.get_variable("threshold", shape=(),
                                initializer=tf.constant_initializer(0.0))
</code></pre>
<p>请注意，如果变量已经通过较早的<code>get_variable()</code>调用创建，则此代码将引发异常。 这种行为可以防止错误地复用变量。如果要复用变量，则需要通过将变量<code>scope</code>的复用属性设置为<code>True</code>来明确说明（在这种情况下，您不必指定形状或初始值）：</p>
<pre><code class="language-python">with tf.variable_scope("relu", reuse=True):
    threshold = tf.get_variable("threshold")
</code></pre>
<p>该代码将获取现有的<code>relu/threshold</code>变量，如果不存在会引发异常（如果没有使用<code>get_variable()</code>创建）。 或者，您可以通过调用<code>scope</code>的<code>reuse_variables()</code>方法将复用属性设置为<code>true</code>：</p>
<pre><code class="language-python">with tf.variable_scope("relu") as scope:
    scope.reuse_variables()
    threshold = tf.get_variable("threshold")
</code></pre>
<p>一旦重新使用设置为<code>True</code>，它将不能在块内设置为<code>False</code>。 而且，如果在其中定义其他变量作用域，它们将自动继承<code>reuse = True</code>。 最后，只有通过<code>get_variable()</code>创建的变量才可以这样复用.</p>
<p>现在，您拥有所有需要的部分，使<code>relu()</code>函数访问阈值变量，而不必将其作为参数传递：</p>
<pre><code class="language-python">def relu(X):
    with tf.variable_scope("relu", reuse=True):
        threshold = tf.get_variable("threshold")
        w_shape = int(X.get_shape()[1]), 1                          # not shown
        w = tf.Variable(tf.random_normal(w_shape), name="weights")  # not shown
        b = tf.Variable(0.0, name="bias")                           # not shown
        z = tf.add(tf.matmul(X, w), b, name="z")                    # not shown
        return tf.maximum(z, threshold, name="max")

X = tf.placeholder(tf.float32, shape=(None, n_features), name="X")
with tf.variable_scope("relu"):
    threshold = tf.get_variable("threshold", shape=(),
                                initializer=tf.constant_initializer(0.0))
relus = [relu(X) for relu_index in range(5)]
output = tf.add_n(relus, name="output")
</code></pre>
<p>该代码首先定义<code>relu()</code>函数，然后创建<code>relu/threshold</code>变量（作为标量，稍后将被初始化为 0.0），并通过调用<code>relu()</code>函数构建五个ReLU。<code>relu()</code>函数复用<code>relu/threshold</code>变量，并创建其他 ReLU 节点。</p>
<p>使用<code>get_variable()</code>创建的变量始终以其<code>variable_scope</code>的名称作为前缀命名（例如，<code>relu/threshold</code>），但对于所有其他节点（包括使用<code>tf.Variable()</code>创建的变量），变量作用域的行为就像一个新名称的作用域。 特别是，如果已经创建了具有相同名称的名称作用域，则添加后缀以使该名称是唯一的。 例如，在前面的代码中创建的所有节点（阈值变量除外）的名称前缀为<code>relu_1/</code>到<code>relu_5/</code>，如图 9-8 所示。</p>
<p><a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_9/9-8.png" data-uk-lightbox><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_9/9-8.png" alt=""></a></p>
<p>不幸的是，必须在<code>relu()</code>函数之外定义阈值变量，其中 ReLU 代码的其余部分都驻留在其中。 要解决此问题，以下代码在第一次调用时在<code>relu()</code>函数中创建阈值变量，然后在后续调用中重新使用。 现在，<code>relu()</code>函数不必担心名称作用域或变量共享：它只是调用<code>get_variable()</code>，它将创建或复用阈值变量（它不需要知道是哪种情况）。 其余的代码调用<code>relu()</code>五次，确保在第一次调用时设置<code>reuse = False</code>，而对于其他调用来说，<code>reuse = True</code>。</p>
<pre><code class="language-python">def relu(X):
    threshold = tf.get_variable("threshold", shape=(),
                                initializer=tf.constant_initializer(0.0))
    w_shape = (int(X.get_shape()[1]), 1)                        # not shown in the book
    w = tf.Variable(tf.random_normal(w_shape), name="weights")  # not shown
    b = tf.Variable(0.0, name="bias")                           # not shown
    z = tf.add(tf.matmul(X, w), b, name="z")                    # not shown
    return tf.maximum(z, threshold, name="max")

X = tf.placeholder(tf.float32, shape=(None, n_features), name="X")
relus = []
for relu_index in range(5):
    with tf.variable_scope("relu", reuse=(relu_index &gt;= 1)) as scope:
        relus.append(relu(X))
output = tf.add_n(relus, name="output")
</code></pre>
<p>生成的图形与之前略有不同，因为共享变量存在于第一个 ReLU 中（见图 9-9）。</p>
<p><a href="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_9/9-9.png" data-uk-lightbox><img src="https://img.cntofu.com/book/hands_on_Ml_with_Sklearn_and_TF/images/chapter_9/9-9.png" alt=""></a></p>
<p>TensorFlow 的这个介绍到此结束。 我们将在以下章节中讨论更多高级课题，特别是与深层神经网络，卷积神经网络和递归神经网络相关的许多操作，以及如何使用多线程，队列，多个 GPU 以及如何将 TensorFlow 扩展到多台服务器。</p>
</div>
<hr class="uk-article-divider">
<div class="uk-block uk-block-muted uk-padding-top-remove uk-padding-bottom-remove uk-margin-large-top  book-recommend-wrap">
<div class="uk-margin-top uk-margin-bottom uk-margin-left uk-margin-right">
<div class="uk-margin uk-text-muted "><i class="uk-icon-outdent uk-icon-justify uk-margin-small-right"></i>书籍推荐</div>
<div class="books">
<ul class="uk-book-list">
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/48/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/tensorflow_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/48/index.html">机器学习基础笔记</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/25.html">zhjunqin</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="tensorflow">tensorflow</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">43页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月2日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 0个">0</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/59/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/tensorflow_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/59/index.html">TensorFlow 官方文档中文版</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/35.html">jikexueyuanwiki</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="tensorflow">tensorflow</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">33页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月5日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 8767个">8767</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/42/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/docker_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/42/index.html">Docker-從入門到實踐</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/22.html">jasonblog</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="docker">docker</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">82页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年5月30日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 1个">1</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/52/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/logstash_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/52/index.html">Logstash最佳实践</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/29.html">chenryn</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="logstash">logstash</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">54页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月3日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 693个">693</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/63/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/photoshop_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/63/index.html">Web前端写给Web设计师的注意事项</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/39.html">onface</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="photoshop">photoshop</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="html5">html5</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">33页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月5日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 116个">116</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../book/126/index.html">
<img class="uk-book-cover" src="../../../static/icons/48/html5_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../book/126/index.html">前端晚自修</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../user/67.html">if2er</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="html5">html5</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">22页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年7月8日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 1个">1</span>
</div>
</div>
</div>
</li>
<hr>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
<nav class="tm-navbar uk-navbar uk-navbar-attached reader-nav">
<div class="uk-float-left uk-margin-small-top">
<a href="javascript:;" title="目录菜单" class="show-menu  uk-icon-hover  uk-icon-align-justify uk-margin-right"></a>
<div data-uk-dropdown="{mode:'click',pos:'bottom-left'}" class="font-setting-wrap">
<a class="uk-icon-hover uk-icon-font uk-margin-right" aria-label="字体设置" href="javascript:;"></a>
<div class="uk-dropdown dropdown-menu">
<div class="dropdown-caret"><span class="caret-outer"></span><span class="caret-inner"></span></div>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-reduce">小字</button>
<button class="uk-button-link button size-2 font-enlarge">大字</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-1 ">宋体</button>
<button class="uk-button-link button size-2 font-2 ">黑体</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-3 color-theme-sun "><i class="uk-icon-sun-o"></i>白天</button>
<button class="uk-button-link button size-3 color-theme-eye "><i class="uk-icon-eye"></i>护眼</button>
<button class="uk-button-link button size-3 color-theme-moon "><i class="uk-icon-moon-o"></i>夜晚</button></div>
</div>
</div>
<a class="logo uk-margin-right" href="../../../" title="返回首页"><img class="" src="../../../static/components/images/icon_32.png" /></a>
</div>
<div class="uk-navbar-flip  uk-hidden-small">
<div id="share-box"></div>
</div>
</nav>
<div id="menu-id" class="uk-offcanvas reader-offcanvas">
<div class="uk-offcanvas-bar">
<ul class="book-menu-bar uk-nav uk-nav-offcanvas" data-uk-nav>
<li>
<a href="../../../book/27/index.html" data-book-page-rel-url="index.html" data-book-page-id="0" title="封面">封面</a>
</li>
<li>
<a class="pjax" href="../../../book/27/readme.html" data-book-page-rel-url="readme.html" data-book-page-id="0" title="简介">简介</a>
</li>
<li>
<a class="pjax" href="../../../book/27/docs/0.前言.html" title="零、前言" data-book-page-rel-url="docs/0.前言.html" data-book-page-id="9223">零、前言</a>
</li>
<li>
<a class="pjax" href="../../../book/27/docs/1.机器学习概览.html" title="一、机器学习概览" data-book-page-rel-url="docs/1.机器学习概览.html" data-book-page-id="9224">一、机器学习概览</a>
</li>
<li>
<a class="pjax" href="../../../book/27/docs/2.一个完整的机器学习项目.html" title="二、一个完整的机器学习项目" data-book-page-rel-url="docs/2.一个完整的机器学习项目.html" data-book-page-id="9225">二、一个完整的机器学习项目</a>
</li>
<li>
<a class="pjax" href="../../../book/27/docs/3.分类.html" title="三、分类" data-book-page-rel-url="docs/3.分类.html" data-book-page-id="9226">三、分类</a>
</li>
<li>
<a class="pjax" href="../../../book/27/docs/4.训练模型.html" title="四、训练模型" data-book-page-rel-url="docs/4.训练模型.html" data-book-page-id="9227">四、训练模型</a>
</li>
<li>
<a class="pjax" href="../../../book/27/docs/5.支持向量机.html" title="五、支持向量机" data-book-page-rel-url="docs/5.支持向量机.html" data-book-page-id="9228">五、支持向量机</a>
</li>
<li>
<a class="pjax" href="../../../book/27/docs/6.决策树.html" title="六、决策树" data-book-page-rel-url="docs/6.决策树.html" data-book-page-id="9229">六、决策树</a>
</li>
<li>
<a class="pjax" href="../../../book/27/docs/7.集成学习和随机森林.html" title="七、集成学习和随机森林" data-book-page-rel-url="docs/7.集成学习和随机森林.html" data-book-page-id="9230">七、集成学习和随机森林</a>
</li>
<li>
<a class="pjax" href="../../../book/27/docs/8.降维.html" title="八、降维" data-book-page-rel-url="docs/8.降维.html" data-book-page-id="9231">八、降维</a>
</li>
<li>
<a class="pjax" href="../../../book/27/docs/9.启动并运行_TensorFlow.html" title="九、启动并运行 TensorFlow" data-book-page-rel-url="docs/9.启动并运行_TensorFlow.html" data-book-page-id="9232">九、启动并运行 TensorFlow</a>
</li>
<li>
<a class="pjax" href="../../../book/27/docs/10.人工神经网络介绍.html" title="十、人工神经网络介绍" data-book-page-rel-url="docs/10.人工神经网络介绍.html" data-book-page-id="9233">十、人工神经网络介绍</a>
</li>
<li>
<a class="pjax" href="../../../book/27/docs/11.训练深层神经网络.html" title="十一、训练深层神经网络" data-book-page-rel-url="docs/11.训练深层神经网络.html" data-book-page-id="9234">十一、训练深层神经网络</a>
</li>
<li>
<a class="pjax" href="../../../book/27/docs/12.设备和服务器上的分布式_TensorFlow.html" title="十二、设备和服务器上的分布式 TensorFlow" data-book-page-rel-url="docs/12.设备和服务器上的分布式_TensorFlow.html" data-book-page-id="9235">十二、设备和服务器上的分布式 TensorFlow</a>
</li>
<li>
<a class="pjax" href="../../../book/27/docs/13.卷积神经网络.html" title="十三、卷积神经网络" data-book-page-rel-url="docs/13.卷积神经网络.html" data-book-page-id="9236">十三、卷积神经网络</a>
</li>
<li>
<a class="pjax" href="../../../book/27/docs/14.循环神经网络.html" title="十四、循环神经网络" data-book-page-rel-url="docs/14.循环神经网络.html" data-book-page-id="9237">十四、循环神经网络</a>
</li>
<li>
<a class="pjax" href="../../../book/27/docs/15.自编码器.html" title="十五、自编码器" data-book-page-rel-url="docs/15.自编码器.html" data-book-page-id="9238">十五、自编码器</a>
</li>
<li>
<a class="pjax" href="../../../book/27/docs/16.强化学习.html" title="十六、强化学习" data-book-page-rel-url="docs/16.强化学习.html" data-book-page-id="9239">十六、强化学习</a>
</li>
<li>
<a class="pjax" href="../../../book/27/docs/C.SVM_对偶问题.html" title="附录 C、SVM 对偶问题" data-book-page-rel-url="docs/C.SVM_对偶问题.html" data-book-page-id="9240">附录 C、SVM 对偶问题</a>
</li>
<li>
<a class="pjax" href="../../../book/27/docs/D.自动微分.html" title="附录 D、自动微分" data-book-page-rel-url="docs/D.自动微分.html" data-book-page-id="9241">附录 D、自动微分</a>
</li>
</ul>
</div>
</div>
<script src="https://cdn.staticfile.net/jquery/1.12.4/jquery.min.js"></script>
<script type="text/javascript" src="../../../static/components/uikit-2.27.5/js/uikit.reader.js"></script>
<script type="text/javascript" src="../../../static/components/social-share/social-share.min.js"></script>
<script>(function(){var bp =document.createElement('script');var curProtocol =window.location.protocol.split(':')[0];if (curProtocol ==='https') {bp.src ='https://zz.bdstatic.com/linksubmit/push.js';}
else {bp.src ='http://push.zhanzhang.baidu.com/push.js';}
var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(bp,s);})();</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
<script src="https://cdn.staticfile.net/highlight.js/9.12.0/highlight.min.js"></script>
<script src="https://cdn.staticfile.net/jquery.pjax/2.0.1/jquery.pjax.min.js"></script>
<script src="https://cdn.staticfile.net/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="https://cdn.staticfile.net/uikit/2.27.5/js/components/lightbox.min.js"></script>
<link rel="dns-prefetch" href="../../..//cdn.mathjax.org" />
<script type="text/x-mathjax-config">
 function initMathJax() {
    var mathId = $("book-content-section")[0];
    MathJax.Hub.Config({
        tex2jax: {skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code','a']},
        showProcessingMessages: false,
        messageStyle: "none"
    });
    MathJax.Hub.Queue(["Typeset",MathJax.Hub,mathId]);
 };
initMathJax();
</script>
<script src='https://cdn.staticfile.net/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML' async></script>
<style>
	.MathJax_Display{display:inline!important;}
</style>
<script type="text/javascript" src="../../../static/components/js/reader.js"></script>
<script type="text/javascript">var bookId =27;var bookPageId =9232;var bookPageRelUrl ='docs/9.启动并运行_TensorFlow.html';</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
</body>
</html>