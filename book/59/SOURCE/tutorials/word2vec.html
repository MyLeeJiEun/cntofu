
<!DOCTYPE html>
<html lang="zh-CN">
<head>
<title>字词的向量表示-TensorFlow 官方文档中文版</title>
<meta content='字词的向量表示,TensorFlow 官方文档中文版' name='keywords'>
<meta content='字词的向量表示,TensorFlow 官方文档中文版' name='description'>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta http-equiv="Content-Language" content="zh-CN" />
<meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1, maximum-scale=1, user-scalable=no"../../../../>
<meta name="applicable-device" content="pc,mobile">
<link rel="shortcut icon" href="../../../../favicon.ico" type="image/x-icon" />
<meta name="renderer" content="webkit">
<link rel="stylesheet" href="../../../../static/components/uikit-2.27.5/css/uikit.custom.css">
<link rel="stylesheet" href="../../../../static/components/social-share/social-share.min.css">
<link rel="stylesheet" href="../../../../static/components/highlight/styles/custom.css">
<link rel="stylesheet" href="../../../../static/components/css/base.css">
<link rel="stylesheet" href="../../../../static/components/css/reader.css">
<link rel="stylesheet" href="../../../../static/components/css/markdown.css">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5313208362165053" crossorigin="anonymous"></script>
</head>
<body>
<div class=" book-main-wrap uk-container uk-container-center uk-margin-top ">
<div class="uk-grid">
<div class="uk-width-1-1 reader-wrap ">
<div class=" bottom-nav uk-clearfix ">
<div class="uk-align-left ">
<a href="../../../../book/59/SOURCE/tutorials/deep_cnn.html">
<i class="nav-icon-left uk-icon-small  uk-icon-caret-left"></i>
<span class="">卷积神经网络</span>
</a>
</div>
<div class="uk-align-right ">
<a href="../../../../book/59/SOURCE/tutorials/recurrent.html">
<span class="">递归神经网络</span>
<i class="nav-icon-right uk-icon-small  uk-icon-caret-right"></i>
</a>
</div>
</div>
<div class="uk-text-center">
<h2 class="book-page-title uk-container-center">
<a href="../../../../book/59/index.html">TensorFlow 官方文档中文版</a>
<a target="_blank" rel="nofollow" href="https://github.com/jikexueyuanwiki/tensorflow-zh" class="uk-icon-button uk-icon-github" title="github项目地址"></a>
</h2>
</div>
<script type="text/javascript" src="../../../../static/components/js/app_intro.js"></script>
<ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-5313208362165053" data-ad-slot="1328047120"></ins>
<script>(adsbygoogle =window.adsbygoogle ||[]).push({});</script>
<hr class="uk-article-divider">
<div class="book-content-section  md-content-section  uk-margin-bottom">
<h1 id="vector-representations-of-words">Vector Representations of Words <a class="md-anchor" id="AUTOGENERATED-vector-representations-of-words"></a></h1>
<p>在本教程我们来看一下<a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Mikolov et al</a>中提到的word2vec模型。该模型是用于学习文字的向量表示，称之为“word embedding”。</p>
<h2 id="亮点">亮点 <a class="md-anchor" id="AUTOGENERATED-highlights"></a></h2>
<p>本教程意在展现出在TensorfLow中构建word2vec模型有趣、本质的部分。</p>
<ul>
<li>我们从我们为何需要使用向量表示文字开始。</li>
<li>我们通过直观地例子观察模型背后的本质，以及它是如何训练的（通过一些数学方法评估）。</li>
<li>同时我们也展示了TensorFlow对该模型的简单实现。</li>
<li>最后，我们着眼于让给这个简单版本的模型表现更好。</li>
</ul>
<p>我们会在教程的推进中循序渐进地解释代码，但是如果你更希望直入主题，可以在 <a href="https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/g3doc/tutorials/word2vec/word2vec_basic.py">tensorflow/g3doc/tutorials/word2vec/word2vec_basic.py</a>查看到一个最简单的实现。这个基本的例子提供的代码可以完成下载一些数据，简单训练后展示结果。一旦你觉得已经完全掌握了这个简单版本，你可以查看 <a href="https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/embedding/word2vec.py">tensorflow/models/embedding/word2vec.py</a>，这里提供了一些更复杂的实现，同时也展示了TensorFlow的一些更进阶的特性，比如如何更高效地使用线程将数据送入文本模型，再比如如何在训练中设置检查点等等。</p>
<p>但是首先，让我们来看一下为何需要学习word embeddings。如果你对word embeddings相关内容已经是个专家了，那么请安心跳过本节内容，直接深入细节干一些脏活吧。</p>
<h2 id="动机-为什么需要学习-word-embeddings">动机: 为什么需要学习 Word Embeddings? <a class="md-anchor" id="AUTOGENERATED-motivation--why-learn-word-embeddings-"></a></h2>
<p>通常图像或音频系统处理的是由图片中所有单个原始像素点强度值或者音频中功率谱密度的强度值，把它们编码成丰富、高纬度的向量数据集。对于物体或语音识别这一类的任务，我们所需的全部信息已经都存储在原始数据中（显然人类本身就是依赖原始数据进行日常的物体或语音识别的）。然后，自然语言处理系统通常将词汇作为离散的单一符号，例如 "cat" 一词或可表示为 <code>Id537</code> ，而 "dog" 一词或可表示为 <code>Id143</code>。这些符号编码毫无规律，无法提供不同词汇之间可能存在的关联信息。换句话说，在处理关于 "dogs" 一词的信息时，模型将无法利用已知的关于 "cats" 的信息（例如，它们都是动物，有四条腿，可作为宠物等等）。可见，将词汇表达为上述的独立离散符号将进一步导致数据稀疏，使我们在训练统计模型时不得不寻求更多的数据。而词汇的向量表示将克服上述的难题。</p>
<div style="width:100%; margin:auto; margin-bottom:10px; margin-top:20px;">
<a href="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/audio-image-text.png" data-uk-lightbox><img style="width:100%" src="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/audio-image-text.png" alt></a>
</div>
<p><a href="https://en.wikipedia.org/wiki/Vector_space_model">向量空间模型</a> (VSMs)将词汇表达（嵌套）于一个连续的向量空间中，语义近似的词汇被映射为相邻的数据点。向量空间模型在自然语言处理领域中有着漫长且丰富的历史，不过几乎所有利用这一模型的方法都依赖于 <a href="https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_Hypothesis">分布式假设</a>，其核心思想为出现于上下文情景中的词汇都有相类似的语义。采用这一假设的研究方法大致分为以下两类：<em>基于计数的方法</em> (e.g. <a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis">潜在语义分析</a>)， 和 <em>预测方法</em> (e.g. <a href="http://www.scholarpedia.org/article/Neural_net_language_models">神经概率化语言模型</a>).</p>
<p>其中它们的区别在如下论文中又详细阐述 <a href="http://clic.cimec.unitn.it/marco/publications/acl2014/baroni-etal-countpredict-acl2014.pdf">Baroni et al.</a>，不过简而言之：基于计数的方法计算某词汇与其邻近词汇在一个大型语料库中共同出现的频率及其他统计量，然后将这些统计量映射到一个小型且稠密的向量中。预测方法则试图直接从某词汇的邻近词汇对其进行预测，在此过程中利用已经学习到的小型且稠密的<em>嵌套向量</em>。</p>
<p>Word2vec是一种可以进行高效率词嵌套学习的预测模型。其两种变体分别为：连续词袋模型（CBOW）及Skip-Gram模型。从算法角度看，这两种方法非常相似，其区别为CBOW根据源词上下文词汇（'the cat sits on the'）来预测目标词汇（例如，‘mat’），而Skip-Gram模型做法相反，它通过目标词汇来预测源词汇。Skip-Gram模型采取CBOW的逆过程的动机在于：CBOW算法对于很多分布式信息进行了平滑处理（例如将一整段上下文信息视为一个单一观察量）。很多情况下，对于小型的数据集，这一处理是有帮助的。相形之下，Skip-Gram模型将每个“上下文-目标词汇”的组合视为一个新观察量，这种做法在大型数据集中会更为有效。本教程余下部分将着重讲解Skip-Gram模型。</p>
<h2 id="处理噪声对比训练">处理噪声对比训练 <a class="md-anchor" id="AUTOGENERATED-scaling-up-with-noise-contrastive-training"></a></h2>
<p>神经概率化语言模型通常使用<a href="https://en.wikipedia.org/wiki/Maximum_likelihood">极大似然法</a> (ML) 进行训练，其中通过 <a href="https://en.wikipedia.org/wiki/Softmax_function"><em>softmax</em> function</a> 来最大化当提供前一个单词 <strong>h</strong> (代表 "history")，后一个单词的概率 <a href="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/vr1.png" data-uk-lightbox><img src="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/vr1.png" alt=""></a> (代表 "target")，</p>
<p><a href="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/vr2.png" data-uk-lightbox><img src="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/vr2.png" alt=""></a></p>
<p>当 <strong>score(w_t,h)</strong> 计算了文字 <strong>w_t</strong> 和 上下文 <strong>h</strong> 的相容性（通常使用向量积）。我们使用对数似然函数来训练训练集的最大值，比如通过：</p>
<p><a href="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/word2vec2.png" data-uk-lightbox><img src="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/word2vec2.png" alt=""></a></p>
<p>这里提出了一个解决语言概率模型的合适的通用方法。然而这个方法实际执行起来开销非常大，因为我们需要去计算并正则化当前上下文环境 <strong>h</strong> 中所有其他 <strong>V</strong> 单词 <strong>w'</strong> 的概率得分，<em>在每一步训练迭代中</em>。</p>
<div style="width:60%; margin:auto; margin-bottom:10px; margin-top:20px;">
<a href="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/softmax-nplm.png" data-uk-lightbox><img style="width:100%" src="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/softmax-nplm.png" alt></a>
</div>
<p>从另一个角度来说，当使用word2vec模型时，我们并不需要对概率模型中的所有特征进行学习。而CBOW模型和Skip-Gram模型为了避免这种情况发生，使用一个二分类器（逻辑回归）在同一个上下文环境里从 <strong>k</strong> 虚构的 (噪声) 单词 <a href="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/rw5.png" data-uk-lightbox><img src="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/rw5.png" alt=""></a> 区分出真正的目标单词 <a href="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/rw4.png" data-uk-lightbox><img src="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/rw4.png" alt=""></a>。我们下面详细阐述一下CBOW模型，对于Skip-Gram模型只要简单地做相反的操作即可。</p>
<div style="width:60%; margin:auto; margin-bottom:10px; margin-top:20px;">
<a href="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/nce-nplm.png" data-uk-lightbox><img style="width:100%" src="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/nce-nplm.png" alt></a>
</div>
<p>从数学角度来说，我们的目标是对每个样本最大化：</p>
<p><a href="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/rw6.png" data-uk-lightbox><img src="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/rw6.png" alt=""></a></p>
<p>其中 <a href="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/rw7.png" data-uk-lightbox><img src="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/rw7.png" alt=""></a> 代表的是数据集在当前上下文 <strong>h</strong> ，根据所学习的嵌套向量 <a href="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/theta.png" data-uk-lightbox><img src="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/theta.png" alt=""></a> ，目标单词 <strong>w</strong> 使用二分类逻辑回归计算得出的概率。在实践中，我们通过在噪声分布中绘制比对文字来获得近似的期望值（通过计算<a href="https://en.wikipedia.org/wiki/Monte_Carlo_integration">蒙特卡洛平均值</a>）。</p>
<p>当真实地目标单词被分配到较高的概率，同时噪声单词的概率很低时，目标函数也就达到最大值了。从技术层面来说，这种方法叫做 <a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">负抽样</a>，而且使用这个损失函数在数学层面上也有很好的解释：这个更新过程也近似于softmax函数的更新。这在计算上将会有很大的优势，因为当计算这个损失函数时，只是有我们挑选出来的 <strong>k</strong> 个 <em>噪声单词</em>，而没有使用整个语料库 <strong>V</strong>。这使得训练变得非常快。我们实际上使用了与<a href="http://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf">noise-contrastive estimation (NCE)</a>介绍的非常相似的方法，这在TensorFlow中已经封装了一个很便捷的函数<code>tf.nn.nce_loss()</code>。</p>
<p>让我们在实践中来直观地体会它是如何运作的！</p>
<h2 id="skip-gram-模型">Skip-gram 模型<a class="md-anchor" id="AUTOGENERATED-the-skip-gram-model"></a></h2>
<p>下面来看一下这个数据集</p>
<p><code>the quick brown fox jumped over the lazy dog</code></p>
<p>我们首先对一些单词以及它们的上下文环境建立一个数据集。我们可以以任何合理的方式定义‘上下文’，而通常上这个方式是根据文字的句法语境的（使用语法原理的方式处理当前目标单词可以看一下这篇文献 <a href="https://levyomer.files.wordpress.com/2014/04/dependency-based-word-embeddings-acl-2014.pdf">Levy et al.</a>，比如说把目标单词左边的内容当做一个‘上下文’，或者以目标单词右边的内容，等等。现在我们把目标单词的左右单词视作一个上下文， 使用大小为1的窗口，这样就得到这样一个由<code>(上下文, 目标单词)</code> 组成的数据集：</p>
<p><code>([the, brown], quick), ([quick, fox], brown), ([brown, jumped], fox), ...</code></p>
<p>前文提到Skip-Gram模型是把目标单词和上下文颠倒过来，所以在这个问题中，举个例子，就是用'quick'来预测 'the' 和 'brown' ，用 'brown' 预测 'quick' 和 'brown' 。因此这个数据集就变成由<code>(输入, 输出)</code>组成的：</p>
<p><code>(quick, the), (quick, brown), (brown, quick), (brown, fox), ...</code></p>
<p>目标函数通常是对整个数据集建立的，但是本问题中要对每一个样本（或者是一个<code>batch_size</code> 很小的样本集，通常设置为<code>16 &lt;= batch_size &lt;= 512</code>）在同一时间执行特别的操作，称之为<a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">随机梯度下降</a> (SGD)。我们来看一下训练过程中每一步的执行。</p>
<p>假设用 <strong>t</strong> 表示上面这个例子中<code>quick</code> 来预测 <code>the</code> 的训练的单个循环。用 <code>num_noise</code> 定义从噪声分布中挑选出来的噪声（相反的）单词的个数，通常使用一元分布，<strong>P(w)</strong>。为了简单起见，我们就定<code>num_noise=1</code>，用 <code>sheep</code> 选作噪声词。接下来就可以计算每一对观察值和噪声值的损失函数了，每一个执行步骤就可表示为：</p>
<p><a href="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/vr4.png" data-uk-lightbox><img src="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/vr4.png" alt=""></a></p>
<p>整个计算过程的目标是通过更新嵌套参数 <a href="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/theta.png" data-uk-lightbox><img src="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/theta.png" alt=""></a> 来逼近目标函数（这个这个例子中就是使目标函数最大化）。为此我们要计算损失函数中嵌套参数 <a href="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/theta.png" data-uk-lightbox><img src="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/theta.png" alt=""></a> 的梯度，比如， <a href="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/vr5.png" data-uk-lightbox><img src="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/vr5.png" alt=""></a> (幸好TensorFlow封装了工具函数可以简单调用!)。对于整个数据集，当梯度下降的过程中不断地更新参数，对应产生的效果就是不断地移动每个单词的嵌套向量，直到可以把真实单词和噪声单词很好得区分开。</p>
<p>我们可以把学习向量映射到2维中以便我们观察，其中用到的技术可以参考 <a href="http://lvdmaaten.github.io/tsne/">t-SNE 降纬技术</a>。当我们用可视化的方式来观察这些向量，就可以很明显的获取单词之间语义信息的关系，这实际上是非常有用的。当我们第一次发现这样的诱导向量空间中，展示了一些特定的语义关系，这是非常有趣的，比如文字中 <em>male-female</em>，<em>gender</em> 甚至还有 <em>country-capital</em> 的关系, 如下方的图所示 (也可以参考 <a href="http://www.aclweb.org/anthology/N13-1090">Mikolov et al., 2013</a>论文中的例子)。</p>
<div style="width:100%; margin:auto; margin-bottom:10px; margin-top:20px;">
<a href="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/linear-relationships.png" data-uk-lightbox><img style="width:100%" src="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/linear-relationships.png" alt></a>
</div>
<p>这也解释了为什么这些向量在传统的NLP问题中可作为特性使用，比如用在对一个演讲章节打个标签，或者对一个专有名词的识别 (看看如下这个例子 <a href="http://arxiv.org/pdf/1103.0398v1.pdf">Collobert et al.</a>或者 <a href="http://www.aclweb.org/anthology/P10-1040">Turian et al.</a>)。</p>
<p>不过现在让我们用它们来画漂亮的图表吧！</p>
<h2 id="建立图形">建立图形 <a class="md-anchor" id="AUTOGENERATED-building-the-graph"></a></h2>
<p>这里谈得都是嵌套，那么先来定义一个嵌套参数矩阵。我们用唯一的随机值来初始化这个大矩阵。</p>
<pre><code class="language-python">embeddings = tf.Variable(
    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
</code></pre>
<p>对噪声-比对的损失计算就使用一个逻辑回归模型。对此，我们需要对语料库中的每个单词定义一个权重值和偏差值。(也可称之为<code>输出权重</code> 与之对应的 <code>输入嵌套值</code>)。定义如下。</p>
<pre><code class="language-python">nce_weights = tf.Variable(
  tf.truncated_normal([vocabulary_size, embedding_size],
                      stddev=1.0 / math.sqrt(embedding_size)))
nce_biases = tf.Variable(tf.zeros([vocabulary_size]))
</code></pre>
<p>我们有了这些参数之后，就可以定义Skip-Gram模型了。简单起见，假设我们已经把语料库中的文字整型化了，这样每个整型代表一个单词(细节请查看 <a href="https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/g3doc/tutorials/word2vec/word2vec_basic.py">tensorflow/g3doc/tutorials/word2vec/word2vec_basic.py</a>)。Skip-Gram模型有两个输入。一个是一组用整型表示的上下文单词，另一个是目标单词。给这些输入建立占位符节点，之后就可以填入数据了。</p>
<pre><code class="language-python"># 建立输入占位符
train_inputs = tf.placeholder(tf.int32, shape=[batch_size])
train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])
</code></pre>
<p>然后我们需要对批数据中的单词建立嵌套向量，TensorFlow提供了方便的工具函数。</p>
<pre><code class="language-python">embed = tf.nn.embedding_lookup(embeddings, train_inputs)
</code></pre>
<p>好了，现在我们有了每个单词的嵌套向量，接下来就是使用噪声-比对的训练方式来预测目标单词。</p>
<pre><code class="language-python"># 计算 NCE 损失函数, 每次使用负标签的样本.
loss = tf.reduce_mean(
  tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels,
                 num_sampled, vocabulary_size))
</code></pre>
<p>我们对损失函数建立了图形节点，然后我们需要计算相应梯度和更新参数的节点，比如说在这里我们会使用随机梯度下降法，TensorFlow也已经封装好了该过程。</p>
<pre><code class="language-python"># 使用 SGD 控制器.
optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)
</code></pre>
<h2 id="训练模型">训练模型 <a class="md-anchor" id="AUTOGENERATED-training-the-model"></a></h2>
<p>训练的过程很简单，只要在循环中使用<code>feed_dict</code>不断给占位符填充数据，同时调用 <a href="tensorflow-zh/SOURCE/api_docs/python/client.html#Session.run"><code>session.run</code></a>即可。</p>
<pre><code class="language-python">for inputs, labels in generate_batch(...):
  feed_dict = {training_inputs: inputs, training_labels: labels}
  _, cur_loss = session.run([optimizer, loss], feed_dict=feed_dict)
</code></pre>
<p>完整地例子可参考 <a href="./word2vec_basic.py">tensorflow/g3doc/tutorials/word2vec/word2vec_basic.py</a>.</p>
<h2 id="嵌套学习结果可视化">嵌套学习结果可视化 <a class="md-anchor" id="AUTOGENERATED-visualizing-the-learned-embeddings"></a></h2>
<p>使用t-SNE来看一下嵌套学习完成的结果。</p>
<div style="width:100%; margin:auto; margin-bottom:10px; margin-top:20px;">
<a href="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/tsne.png" data-uk-lightbox><img style="width:100%" src="https://img.cntofu.com/book/tensorflow-zh/SOURCE/images/tsne.png" alt></a>
</div>
<p>Et voila! 与预期的一样，相似的单词被聚类在一起。对word2vec模型更复杂的实现需要用到TensorFlow一些更高级的特性，具体是实现可以参考 <a href="https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/embedding/word2vec.py">tensorflow/models/embedding/word2vec.py</a>。</p>
<h2 id="嵌套学习的评估-类比推理">嵌套学习的评估: 类比推理 <a class="md-anchor" id="AUTOGENERATED-evaluating-embeddings--analogical-reasoning"></a></h2>
<p>词嵌套在NLP的预测问题中是非常有用且使用广泛地。如果要检测一个模型是否是可以成熟地区分词性或者区分专有名词的模型，最简单的办法就是直接检验它的预测词性、语义关系的能力，比如让它解决形如<code>king is to queen as father is to ?</code>这样的问题。这种方法叫做<em>类比推理</em> ，可参考<a href="http://msr-waypoint.com/en-us/um/people/gzweig/Pubs/NAACL2013Regularities.pdf">Mikolov and colleagues</a>，数据集下载地址为: https://word2vec.googlecode.com/svn/trunk/questions-words.txt。</p>
<p>To see how we do this evaluation如何执行这样的评估，可以看<code>build_eval_graph()</code>和 <code>eval()</code>这两个函数在下面源码中的使用 <a href="https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/embedding/word2vec.py">tensorflow/models/embedding/word2vec.py</a>.</p>
<p>超参数的选择对该问题解决的准确性有巨大的影响。想要模型具有很好的表现，需要有一个巨大的训练数据集，同时仔细调整参数的选择并且使用例如二次抽样的一些技巧。不过这些问题已经超出了本教程的范围。</p>
<h2 id="优化实现">优化实现 <a class="md-anchor" id="AUTOGENERATED-optimizing-the-implementation"></a></h2>
<p>以上简单的例子展示了TensorFlow的灵活性。比如说，我们可以很轻松得用现成的<code>tf.nn.sampled_softmax_loss()</code>来代替<code>tf.nn.nce_loss()</code>构成目标函数。如果你对损失函数想做新的尝试，你可以用TensorFlow手动编写新的目标函数的表达式，然后用控制器执行计算。这种灵活性的价值体现在，当我们探索一个机器学习模型时，我们可以很快地遍历这些尝试，从中选出最优。</p>
<p>一旦你有了一个满意的模型结构，或许它就可以使实现运行地更高效（在短时间内覆盖更多的数据）。比如说，在本教程中使用的简单代码，实际运行速度都不错，因为我们使用Python来读取和填装数据，而这些在TensorFlow后台只需执行非常少的工作。如果你发现你的模型在输入数据时存在严重的瓶颈，你可以根据自己的实际问题自行实现一个数据阅读器，参考 <a href="tensorflow-zh/SOURCE/how_tos/new_data_formats/index.html">新的数据格式</a>。对于Skip-Gram 模型，我们已经完成了如下这个例子 <a href="https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/embedding/word2vec.py">tensorflow/models/embedding/word2vec.py</a>。</p>
<p>如果I/O问题对你的模型已经不再是个问题，并且想进一步地优化性能，或许你可以自行编写TensorFlow操作单元，详见 <a href="tensorflow-zh/SOURCE/how_tos/adding_an_op/index.html">添加一个新的操作</a>。相应的，我们也提供了Skip-Gram模型的例子 <a href="https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/embedding/word2vec_optimized.py">tensorflow/models/embedding/word2vec_optimized.py</a>。请自行调节以上几个过程的标准，使模型在每个运行阶段有更好地性能。</p>
<h2 id="总结">总结 <a class="md-anchor" id="AUTOGENERATED-conclusion"></a></h2>
<p>在本教程中我们介绍了word2vec模型,它在解决词嵌套问题中具有良好的性能。我们解释了使用词嵌套模型的实用性，并且讨论了如何使用TensorFlow实现该模型的高效训练。总的来说，我们希望这个例子能够让向你展示TensorFlow可以提供实验初期的灵活性，以及在后期优化模型时对模型内部的可操控性。</p>
<p>原文地址：<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/tutorials/word2vec/index.html">Vector Representation of Words</a> 翻译：<a href="https://github.com/btpeter">btpeter</a> 校对：waiwaizheng</p>
</div>
<hr class="uk-article-divider">
<div class="uk-block uk-block-muted uk-padding-top-remove uk-padding-bottom-remove uk-margin-large-top  book-recommend-wrap">
<div class="uk-margin-top uk-margin-bottom uk-margin-left uk-margin-right">
<div class="uk-margin uk-text-muted "><i class="uk-icon-outdent uk-icon-justify uk-margin-small-right"></i>书籍推荐</div>
<div class="books">
<ul class="uk-book-list">
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/48/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/tensorflow_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/48/index.html">机器学习基础笔记</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/25.html">zhjunqin</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="tensorflow">tensorflow</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="python">python</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">43页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年6月2日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 0个">0</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/27/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/tensorflow_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/27/index.html">Sklearn 与 TensorFlow 机器学习实用指南</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/18.html">ApacheCN</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="tensorflow">tensorflow</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">20页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年5月3日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 916个">916</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/105/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/code_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/105/index.html">程序员的自我修养</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/64.html">leohxj</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="code">code</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">136页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2018年7月1日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 130个">130</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/189/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/code_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/189/index.html">每日 30 秒 , 一段代码 ,一个场景</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/106.html">pushmetop</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="code">code</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">46页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2019年5月26日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 265个">265</span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/198/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/storm_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/198/index.html">大数据入门指南</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/113.html">heibaiying</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="storm">storm</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">98页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2021年10月24日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 个"></span>
</div>
</div>
</div>
</li>
<hr>
<li>
<div class="uk-book-item">
<div class="uk-book-header uk-clearfix">
<a href="../../../../book/195/index.html">
<img class="uk-book-cover" src="../../../../static/icons/48/linux_48.png" height="48px" alt="">
</a>
<h4 class="uk-book-title uk-margin-small-bottom"><a href="../../../../book/195/index.html">Linux命令大全搜索工具</a></h4>
<div class="uk-book-meta  uk-text-middle uk-float-left">
<a class="uk-margin-small-right  uk-text-middle user-name " href="../../../../user/111.html">jaywcjlove</a>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-badge uk-badge-notification  book-subject" title="linux">linux</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">30页</span>
<span class="uk-margin-small-right  uk-text-middle">•</span>
<span class="uk-margin-small-right  uk-text-middle">2021年10月24日</span>
</div>
<div class="uk-book-tip uk-float-right  uk-text-middle">
<span class="uk-badge uk-badge-notification" title="github star 个"></span>
</div>
</div>
</div>
</li>
<hr>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
<nav class="tm-navbar uk-navbar uk-navbar-attached reader-nav">
<div class="uk-float-left uk-margin-small-top">
<a href="javascript:;" title="目录菜单" class="show-menu  uk-icon-hover  uk-icon-align-justify uk-margin-right"></a>
<div data-uk-dropdown="{mode:'click',pos:'bottom-left'}" class="font-setting-wrap">
<a class="uk-icon-hover uk-icon-font uk-margin-right" aria-label="字体设置" href="javascript:;"></a>
<div class="uk-dropdown dropdown-menu">
<div class="dropdown-caret"><span class="caret-outer"></span><span class="caret-inner"></span></div>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-reduce">小字</button>
<button class="uk-button-link button size-2 font-enlarge">大字</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-2 font-1 ">宋体</button>
<button class="uk-button-link button size-2 font-2 ">黑体</button>
</div>
<hr>
<div class="buttons uk-clearfix">
<button class="uk-button-link button size-3 color-theme-sun "><i class="uk-icon-sun-o"></i>白天</button>
<button class="uk-button-link button size-3 color-theme-eye "><i class="uk-icon-eye"></i>护眼</button>
<button class="uk-button-link button size-3 color-theme-moon "><i class="uk-icon-moon-o"></i>夜晚</button></div>
</div>
</div>
<a class="logo uk-margin-right" href="../../../../" title="返回首页"><img class="" src="../../../../static/components/images/icon_32.png" /></a>
</div>
<div class="uk-navbar-flip  uk-hidden-small">
<div id="share-box"></div>
</div>
</nav>
<div id="menu-id" class="uk-offcanvas reader-offcanvas">
<div class="uk-offcanvas-bar">
<ul class="book-menu-bar uk-nav uk-nav-offcanvas" data-uk-nav>
<li>
<a href="../../../../book/59/index.html" data-book-page-rel-url="index.html" data-book-page-id="0" title="封面">封面</a>
</li>
<li>
<a class="pjax" href="../../../../book/59/readme.html" data-book-page-rel-url="readme.html" data-book-page-id="0" title="简介">简介</a>
</li>
<li>
<a class="pjax" href="javascript:;" class="uk-link-muted uk-text-muted" title="起步" disabled data-book-page-rel-url="" data-book-page-id="5196">起步</a>
<ul>
<li>
<a class="pjax" href="../../../../book/59/SOURCE/get_started/introduction.html" title="介绍" data-book-page-rel-url="SOURCE/get_started/introduction.html" data-book-page-id="5197">介绍</a>
</li>
<li>
<a class="pjax" href="../../../../book/59/SOURCE/get_started/os_setup.html" title="下载及安装" data-book-page-rel-url="SOURCE/get_started/os_setup.html" data-book-page-id="5198">下载及安装</a>
</li>
<li>
<a class="pjax" href="../../../../book/59/SOURCE/get_started/basic_usage.html" title="基本用法" data-book-page-rel-url="SOURCE/get_started/basic_usage.html" data-book-page-id="5199">基本用法</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="javascript:;" class="uk-link-muted uk-text-muted" title="教程" disabled data-book-page-rel-url="" data-book-page-id="5200">教程</a>
<ul>
<li>
<a class="pjax" href="../../../../book/59/SOURCE/tutorials/overview.html" title="总览" data-book-page-rel-url="SOURCE/tutorials/overview.html" data-book-page-id="5201">总览</a>
</li>
<li>
<a class="pjax" href="../../../../book/59/SOURCE/tutorials/mnist_beginners.html" title="MNIST 机器学习入门" data-book-page-rel-url="SOURCE/tutorials/mnist_beginners.html" data-book-page-id="5202">MNIST 机器学习入门</a>
</li>
<li>
<a class="pjax" href="../../../../book/59/SOURCE/tutorials/mnist_pros.html" title="深入 MNIST" data-book-page-rel-url="SOURCE/tutorials/mnist_pros.html" data-book-page-id="5203">深入 MNIST</a>
</li>
<li>
<a class="pjax" href="../../../../book/59/SOURCE/tutorials/mnist_tf.html" title="TensorFlow 运作方式入门" data-book-page-rel-url="SOURCE/tutorials/mnist_tf.html" data-book-page-id="5204">TensorFlow 运作方式入门</a>
</li>
<li>
<a class="pjax" href="../../../../book/59/SOURCE/tutorials/deep_cnn.html" title="卷积神经网络" data-book-page-rel-url="SOURCE/tutorials/deep_cnn.html" data-book-page-id="5205">卷积神经网络</a>
</li>
<li>
<a class="pjax" href="../../../../book/59/SOURCE/tutorials/word2vec.html" title="字词的向量表示" data-book-page-rel-url="SOURCE/tutorials/word2vec.html" data-book-page-id="5206">字词的向量表示</a>
</li>
<li>
<a class="pjax" href="../../../../book/59/SOURCE/tutorials/recurrent.html" title="递归神经网络" data-book-page-rel-url="SOURCE/tutorials/recurrent.html" data-book-page-id="5207">递归神经网络</a>
</li>
<li>
<a class="pjax" href="../../../../book/59/SOURCE/tutorials/mandelbrot.html" title="曼德布洛特(Mandelbrot)集合" data-book-page-rel-url="SOURCE/tutorials/mandelbrot.html" data-book-page-id="5208">曼德布洛特(Mandelbrot)集合</a>
</li>
<li>
<a class="pjax" href="../../../../book/59/SOURCE/tutorials/pdes.html" title="偏微分方程" data-book-page-rel-url="SOURCE/tutorials/pdes.html" data-book-page-id="5209">偏微分方程</a>
</li>
<li>
<a class="pjax" href="../../../../book/59/SOURCE/tutorials/mnist_download.html" title="MNIST数据下载" data-book-page-rel-url="SOURCE/tutorials/mnist_download.html" data-book-page-id="5210">MNIST数据下载</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="javascript:;" class="uk-link-muted uk-text-muted" title="运作方式" disabled data-book-page-rel-url="" data-book-page-id="5211">运作方式</a>
<ul>
<li>
<a class="pjax" href="../../../../book/59/SOURCE/how_tos/overview.html" title="总览" data-book-page-rel-url="SOURCE/how_tos/overview.html" data-book-page-id="5212">总览</a>
</li>
<li>
<a class="pjax" href="../../../../book/59/SOURCE/how_tos/variables.html" title="变量:创建、初始化、保存和加载" data-book-page-rel-url="SOURCE/how_tos/variables.html" data-book-page-id="5213">变量:创建、初始化、保存和加载</a>
</li>
<li>
<a class="pjax" href="../../../../book/59/SOURCE/how_tos/summaries_and_tensorboard.html" title="TensorBoard:可视化学习" data-book-page-rel-url="SOURCE/how_tos/summaries_and_tensorboard.html" data-book-page-id="5214">TensorBoard:可视化学习</a>
</li>
<li>
<a class="pjax" href="../../../../book/59/SOURCE/how_tos/graph_viz.html" title="TensorBoard:图表可视化" data-book-page-rel-url="SOURCE/how_tos/graph_viz.html" data-book-page-id="5215">TensorBoard:图表可视化</a>
</li>
<li>
<a class="pjax" href="../../../../book/59/SOURCE/how_tos/reading_data.html" title="读取数据" data-book-page-rel-url="SOURCE/how_tos/reading_data.html" data-book-page-id="5216">读取数据</a>
</li>
<li>
<a class="pjax" href="../../../../book/59/SOURCE/how_tos/threading_and_queues.html" title="线程和队列" data-book-page-rel-url="SOURCE/how_tos/threading_and_queues.html" data-book-page-id="5217">线程和队列</a>
</li>
<li>
<a class="pjax" href="../../../../book/59/SOURCE/how_tos/adding_an_op.html" title="添加新的Op" data-book-page-rel-url="SOURCE/how_tos/adding_an_op.html" data-book-page-id="5218">添加新的Op</a>
</li>
<li>
<a class="pjax" href="../../../../book/59/SOURCE/how_tos/new_data_formats.html" title="自定义数据读取" data-book-page-rel-url="SOURCE/how_tos/new_data_formats.html" data-book-page-id="5219">自定义数据读取</a>
</li>
<li>
<a class="pjax" href="../../../../book/59/SOURCE/how_tos/using_gpu.html" title="使用gpu" data-book-page-rel-url="SOURCE/how_tos/using_gpu.html" data-book-page-id="5220">使用gpu</a>
</li>
<li>
<a class="pjax" href="../../../../book/59/SOURCE/how_tos/variable_scope.html" title="共享变量" data-book-page-rel-url="SOURCE/how_tos/variable_scope.html" data-book-page-id="5221">共享变量</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="javascript:;" class="uk-link-muted uk-text-muted" title="资源" disabled data-book-page-rel-url="" data-book-page-id="5222">资源</a>
<ul>
<li>
<a class="pjax" href="../../../../book/59/SOURCE/resources/overview.html" title="总览" data-book-page-rel-url="SOURCE/resources/overview.html" data-book-page-id="5223">总览</a>
</li>
<li>
<a class="pjax" href="../../../../book/59/SOURCE/resources/bib.html" title="BibTex 引用" data-book-page-rel-url="SOURCE/resources/bib.html" data-book-page-id="5224">BibTex 引用</a>
</li>
<li>
<a class="pjax" href="../../../../book/59/SOURCE/resources/uses.html" title="示例使用" data-book-page-rel-url="SOURCE/resources/uses.html" data-book-page-id="5225">示例使用</a>
</li>
<li>
<a class="pjax" href="../../../../book/59/SOURCE/resources/faq.html" title="FAQ" data-book-page-rel-url="SOURCE/resources/faq.html" data-book-page-id="5226">FAQ</a>
</li>
<li>
<a class="pjax" href="../../../../book/59/SOURCE/resources/glossary.html" title="术语表" data-book-page-rel-url="SOURCE/resources/glossary.html" data-book-page-id="5227">术语表</a>
</li>
<li>
<a class="pjax" href="../../../../book/59/SOURCE/resources/dims_types.html" title="Tensor排名、形状和类型" data-book-page-rel-url="SOURCE/resources/dims_types.html" data-book-page-id="5228">Tensor排名、形状和类型</a>
</li>
</ul>
</li>
<li>
<a class="pjax" href="javascript:;" class="uk-link-muted uk-text-muted" title="其他" disabled data-book-page-rel-url="" data-book-page-id="5229">其他</a>
<ul>
<li>
<a class="pjax" href="../../../../book/59/SOURCE/faq.html" title="常见问题汇总" data-book-page-rel-url="SOURCE/faq.html" data-book-page-id="5230">常见问题汇总</a>
</li>
<li>
<a class="pjax" href="../../../../book/59/SOURCE/resource.html" title="相关资源" data-book-page-rel-url="SOURCE/resource.html" data-book-page-id="5231">相关资源</a>
</li>
<li>
<a class="pjax" href="../../../../book/59/SOURCE/personal.html" title="个人学习心得" data-book-page-rel-url="SOURCE/personal.html" data-book-page-id="5232">个人学习心得</a>
</li>
</ul>
</li>
</ul>
</div>
</div>
<script src="https://cdn.staticfile.net/jquery/1.12.4/jquery.min.js"></script>
<script type="text/javascript" src="../../../../static/components/uikit-2.27.5/js/uikit.reader.js"></script>
<script type="text/javascript" src="../../../../static/components/social-share/social-share.min.js"></script>
<script>(function(){var bp =document.createElement('script');var curProtocol =window.location.protocol.split(':')[0];if (curProtocol ==='https') {bp.src ='https://zz.bdstatic.com/linksubmit/push.js';}
else {bp.src ='http://push.zhanzhang.baidu.com/push.js';}
var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(bp,s);})();</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
<script src="https://cdn.staticfile.net/highlight.js/9.12.0/highlight.min.js"></script>
<script src="https://cdn.staticfile.net/jquery.pjax/2.0.1/jquery.pjax.min.js"></script>
<script src="https://cdn.staticfile.net/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="https://cdn.staticfile.net/uikit/2.27.5/js/components/lightbox.min.js"></script>
<link rel="dns-prefetch" href="../../../..//cdn.mathjax.org" />
<script type="text/x-mathjax-config">
 function initMathJax() {
    var mathId = $("book-content-section")[0];
    MathJax.Hub.Config({
        tex2jax: {skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code','a']},
        showProcessingMessages: false,
        messageStyle: "none"
    });
    MathJax.Hub.Queue(["Typeset",MathJax.Hub,mathId]);
 };
initMathJax();
</script>
<script src='https://cdn.staticfile.net/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML' async></script>
<style>
	.MathJax_Display{display:inline!important;}
</style>
<script type="text/javascript" src="../../../../static/components/js/reader.js"></script>
<script type="text/javascript">var bookId =59;var bookPageId =5206;var bookPageRelUrl ='SOURCE/tutorials/word2vec.html';</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-38429407-1"></script>
<script>window.dataLayer =window.dataLayer ||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-38429407-1');</script>
<script>var _hmt =_hmt ||[];(function() {var hm =document.createElement("script");hm.src ="https://hm.baidu.com/hm.js?f28e71bd2b5dee3439448dca9f534107";var s =document.getElementsByTagName("script")[0];s.parentNode.insertBefore(hm,s);})();</script>
</body>
</html>